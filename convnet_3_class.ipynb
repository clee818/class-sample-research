{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60f06154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  \n",
    "\n",
    "\n",
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops \n",
    "\n",
    "NUM_CLASSES = 10\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "          #   \"mean_40\", \"variance_40\",\n",
    "          #   \"mean_50\", \"variance_50\",\n",
    "             \"cap\", \"normalization\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de6e5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 3 classes\n",
    "\n",
    "NUM_CLASSES_REDUCED = 3\n",
    "nums = (0, 3, 1)\n",
    "ratio = (20, 2, 1)\n",
    "\n",
    "norm=True\n",
    "\n",
    "if norm:\n",
    "    transform=torchvision.transforms.Compose([torchvision.transforms.Normalize(mean=[134.1855, 122.7346, 118.3749], std=[70.5125, 64.4848, 66.5604])])\n",
    "else:\n",
    "    transform=None\n",
    "\n",
    "    \n",
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "\n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums, transform=transform)\n",
    "targets = ratio_train_CIFAR10.labels \n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED, transform=transform)\n",
    "\n",
    "triplet_train_CIFAR10 = class_sampling.ForTripletLoss(reduced_train_CIFAR10, smote=False, transform=transform, num_classes=3)\n",
    "triplet_ratio_train_CIFAR10 = class_sampling.ForTripletLoss(ratio_train_CIFAR10, smote=False, transform=transform, num_classes=3)\n",
    "triplet_smote_train_CIFAR10 = class_sampling.ForTripletLoss(smote_train_CIFAR10, smote=True, transform=transform, num_classes=3)\n",
    "\n",
    "weight = 1. / class_count\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= max(class_count)\n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss = DataLoader(triplet_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_ratio = DataLoader(triplet_ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_smote = DataLoader(triplet_smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f63185",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3 class normal\n",
    "\n",
    "learning_rates = [1e-4, 1e-3, 5e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 3, nums, (1, 1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2bc162",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a575ff83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011061461766560872, AUC: 0.42147850000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011229241689046225, AUC: 0.43375308333333323\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011965232292811077, AUC: 0.50397825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013694471518198648, AUC: 0.5547101666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010940184195836384, AUC: 0.5872949166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011234662532806396, AUC: 0.6271675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012737938165664673, AUC: 0.629696\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014157256285349529, AUC: 0.6302791666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011149876117706298, AUC: 0.5022431666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00111220649878184, AUC: 0.5008525833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012121287981669109, AUC: 0.56545225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014367966651916504, AUC: 0.5884317499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011054763793945312, AUC: 0.4172478333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001166960875193278, AUC: 0.43572633333333327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013708451986312866, AUC: 0.49362174999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015549939473470053, AUC: 0.53454675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095820943514506, AUC: 0.5638450833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105216383934021, AUC: 0.4935483333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011233636140823365, AUC: 0.47746724999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001183780312538147, AUC: 0.48623008333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011089888016382853, AUC: 0.41577333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011525053977966308, AUC: 0.3984995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012871325810750326, AUC: 0.4894044166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014728634357452392, AUC: 0.5435405833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001104168971379598, AUC: 0.4722545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011033732096354166, AUC: 0.43424208333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011190467675526938, AUC: 0.4819325833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012458680073420207, AUC: 0.5395418333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010908493995666505, AUC: 0.6574895000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006234884262086, AUC: 0.5329389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011211116313934327, AUC: 0.4269961666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011847285032272339, AUC: 0.4419906666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011011676788330077, AUC: 0.4581423333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011169705390930175, AUC: 0.46246341666666657\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001220570166905721, AUC: 0.5314848333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001437263528505961, AUC: 0.569773\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010969032446543376, AUC: 0.5885786666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001122740348180135, AUC: 0.5203583333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001271380583445231, AUC: 0.5326188333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015056567986806233, AUC: 0.5638988333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001112145741780599, AUC: 0.4242493333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016349367300669353, AUC: 0.6327603333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001613512953122457, AUC: 0.6550275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015832540591557821, AUC: 0.6671811666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010980756680170694, AUC: 0.53390225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015820627212524414, AUC: 0.6219455833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016296453873316447, AUC: 0.6423263333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001630396842956543, AUC: 0.6520825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010918147166570028, AUC: 0.6079949166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001676248073577881, AUC: 0.6263639166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001664703925450643, AUC: 0.6445807499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016197384595870972, AUC: 0.6560960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011019526720046997, AUC: 0.4964216666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001632081667582194, AUC: 0.6210678333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016278040409088135, AUC: 0.6432103333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016112867593765258, AUC: 0.6562335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101717988650004, AUC: 0.522838\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016656769116719563, AUC: 0.6182343333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016407173077265423, AUC: 0.6350966666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016459356943766277, AUC: 0.6458038333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011065028508504232, AUC: 0.4244266666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016550063689549765, AUC: 0.6306688333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00163234810034434, AUC: 0.6503546666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001607961416244507, AUC: 0.6612121666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010968776941299439, AUC: 0.5330711666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001665834903717041, AUC: 0.6346676666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001642743945121765, AUC: 0.650899\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016440763076146444, AUC: 0.6629368333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001083850622177124, AUC: 0.6434860833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001639939785003662, AUC: 0.6284074999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016484309434890746, AUC: 0.6419738333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016242753267288208, AUC: 0.6514551666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011106374661127726, AUC: 0.44136375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016721423864364624, AUC: 0.6323215833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016416507562001547, AUC: 0.6519043333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016472928126653036, AUC: 0.6620974999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010967384576797486, AUC: 0.5794741666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016378400723139445, AUC: 0.6267774166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016413303216298422, AUC: 0.647477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00162498939037323, AUC: 0.6606173333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102028727531433, AUC: 0.4988660833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016483097076416015, AUC: 0.585541\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016358747482299804, AUC: 0.6249103333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016404126087824504, AUC: 0.6380630833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001092117190361023, AUC: 0.62371\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016925978660583497, AUC: 0.6120930833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017032538652420044, AUC: 0.6269243333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016912979284922282, AUC: 0.635705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011123327414194744, AUC: 0.4300435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015657088359196981, AUC: 0.6156084999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016640859444936116, AUC: 0.634675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016723535458246868, AUC: 0.64300325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011009188493092854, AUC: 0.4769921666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015778950452804566, AUC: 0.6111888333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016318429708480835, AUC: 0.6275443333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016303228537241617, AUC: 0.6377224166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101452668507894, AUC: 0.4844045833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016591010888417563, AUC: 0.6115911666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016814649899800618, AUC: 0.6275201666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016700348854064941, AUC: 0.6371092500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098446528116862, AUC: 0.5275884166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001588223934173584, AUC: 0.59632275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016332670052846272, AUC: 0.621799\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001641542355219523, AUC: 0.6322205833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010887952248255413, AUC: 0.6064093333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001608059326807658, AUC: 0.5843077499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016374020179112752, AUC: 0.6193405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016355714797973632, AUC: 0.6330794166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010946396589279176, AUC: 0.6147824166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016433739662170411, AUC: 0.6064198333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016634252071380615, AUC: 0.6323009166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016547354459762573, AUC: 0.6444563333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011099478006362914, AUC: 0.40076975000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015669296582539877, AUC: 0.6127045000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016375202337900797, AUC: 0.6260739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016353535652160645, AUC: 0.6349885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006582578023275, AUC: 0.4923398333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001590704361597697, AUC: 0.5984766666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016473388671875, AUC: 0.6221355833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016332290569941203, AUC: 0.6346148333333334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class ratio\n",
    "\n",
    "learning_rates =  [1e-4, 1e-3, 5e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f04a8ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "530a865c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010947994788487753, AUC: 0.56657275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010222713152567545, AUC: 0.7476595000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009208483298619589, AUC: 0.8058216666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008615500529607137, AUC: 0.8321021666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011039157311121624, AUC: 0.42707975000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010247631072998046, AUC: 0.7248186666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009506714145342509, AUC: 0.785685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008906665245691936, AUC: 0.8202210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011069478988647461, AUC: 0.5771989166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009920546213785808, AUC: 0.7478355833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009075014392534892, AUC: 0.79576525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008648690780003865, AUC: 0.81608775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011047943433125814, AUC: 0.462027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010138102769851685, AUC: 0.7138214166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009392054279645284, AUC: 0.7847578333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008817243178685507, AUC: 0.8207239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011121695041656494, AUC: 0.39168125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010056382616360983, AUC: 0.7192958333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009398807287216187, AUC: 0.7777051666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000885651429494222, AUC: 0.8104823333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011061255534489949, AUC: 0.42042158333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010406555732091268, AUC: 0.7452725833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009465389053026835, AUC: 0.7968890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000880358099937439, AUC: 0.8262693333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011038944323857626, AUC: 0.4452737500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010125950972239178, AUC: 0.7557140833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009242969950040181, AUC: 0.8052934999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000878119170665741, AUC: 0.8260964166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011050917307535808, AUC: 0.4117874166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010151830911636352, AUC: 0.738823\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009267864425977072, AUC: 0.8011893333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008750240405400594, AUC: 0.8262401666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011056460539499919, AUC: 0.39601175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010393957297007244, AUC: 0.7504175833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009418634970982869, AUC: 0.7943295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008780870238939921, AUC: 0.8204088333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011081876754760742, AUC: 0.40340491666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010009846488634746, AUC: 0.7357255833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009232005079587301, AUC: 0.7877563333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008780222535133362, AUC: 0.8120811666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011100558837254843, AUC: 0.39855599999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010667788982391358, AUC: 0.7185389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010016055504480997, AUC: 0.7590685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009518531163533529, AUC: 0.777882\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010885504086812338, AUC: 0.6070483333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010242029825846355, AUC: 0.7062701666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000976915677388509, AUC: 0.7528984166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009313291311264038, AUC: 0.7919255833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001092877745628357, AUC: 0.6160568333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010489562352498372, AUC: 0.6855595833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010130883852640788, AUC: 0.7069123333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009817729791005453, AUC: 0.74509525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011099079847335816, AUC: 0.413997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010376245180765788, AUC: 0.71958475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000983574668566386, AUC: 0.7463424166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009490183194478353, AUC: 0.7721009166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011125486294428508, AUC: 0.45571575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010650033950805665, AUC: 0.7327706666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00101482359568278, AUC: 0.7484781666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009585350751876831, AUC: 0.7763306666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011012206077575684, AUC: 0.47361116666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010617451270421346, AUC: 0.6918648333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010373634099960327, AUC: 0.7027510833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010142723719278972, AUC: 0.7215282499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010974370638529459, AUC: 0.52514125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001076143225034078, AUC: 0.73293825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010096780061721801, AUC: 0.7623972499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009621233940124512, AUC: 0.7834414166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011049287716547649, AUC: 0.4411844166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010752166509628296, AUC: 0.6701711666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001026446262995402, AUC: 0.7226565833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009884505271911621, AUC: 0.7660008333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010949628750483195, AUC: 0.59598075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001008028229077657, AUC: 0.7609671666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009489291111628214, AUC: 0.7870726666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009125749071439108, AUC: 0.8034591666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011078249216079712, AUC: 0.43746775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010407994190851848, AUC: 0.707855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009920170704523722, AUC: 0.7574261666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009429348707199097, AUC: 0.7884486666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010971549352010092, AUC: 0.56204725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010826850732167561, AUC: 0.6368035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010697279771169027, AUC: 0.6624476666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001059957464536031, AUC: 0.6747935833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011044430335362752, AUC: 0.43772425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010941485166549682, AUC: 0.6249318333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010847903490066528, AUC: 0.6859108333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010740249156951905, AUC: 0.69677575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001104617198308309, AUC: 0.4446491666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096126914024353, AUC: 0.5780809166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010885404348373413, AUC: 0.6857635833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001079582651456197, AUC: 0.7216269166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010891990264256795, AUC: 0.6089283333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010772257248560587, AUC: 0.67233025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010654816230138142, AUC: 0.6848738333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001052990158398946, AUC: 0.6891791666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011038104295730592, AUC: 0.5503728333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010863869190216064, AUC: 0.6676211666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010750894149144491, AUC: 0.6968663333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010636978149414062, AUC: 0.7166974166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011110479831695557, AUC: 0.468621\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010904828707377116, AUC: 0.6672943333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001076267679532369, AUC: 0.6982275000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010661786794662475, AUC: 0.7070133333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998457670211791, AUC: 0.47347049999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010954383611679077, AUC: 0.5987515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010917614698410035, AUC: 0.6641506666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087450623512268, AUC: 0.6950899166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101628581682841, AUC: 0.49939550000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001079486886660258, AUC: 0.6933345000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010615885257720947, AUC: 0.7218119166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010457186301549275, AUC: 0.7284505833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991034110387167, AUC: 0.5214043333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086639960606893, AUC: 0.64193325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107461412747701, AUC: 0.69077775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001061879595120748, AUC: 0.7094055833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011040982802708944, AUC: 0.46686524999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010942014455795287, AUC: 0.5827779999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010864121516545614, AUC: 0.640715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010784890254338583, AUC: 0.67792975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class oversampled \n",
    "\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70f213ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9125ace9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011024348735809326, AUC: 0.5284704166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010927425622940063, AUC: 0.6219938333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010809800624847412, AUC: 0.68663525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001063308556874593, AUC: 0.7213240833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103477398554484, AUC: 0.47049291666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010985154310862223, AUC: 0.5479706666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001090846339861552, AUC: 0.6222283333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010767723321914672, AUC: 0.6630774166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010997201601664224, AUC: 0.5258056666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010976532300313313, AUC: 0.55747225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010960997740427653, AUC: 0.5830667500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010943124294281006, AUC: 0.6087030833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101622740427653, AUC: 0.4751273333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010785504976908366, AUC: 0.6937432499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010639700889587401, AUC: 0.7155593333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001052898605664571, AUC: 0.7198476666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097723364830017, AUC: 0.5508884166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010935763915379843, AUC: 0.6353795833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010884424050649007, AUC: 0.6701460833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010820165475209553, AUC: 0.6820927499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010963228940963745, AUC: 0.557875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010903691053390503, AUC: 0.6260149166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001081290046374003, AUC: 0.6747011666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001068249543507894, AUC: 0.7011964166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991369485855102, AUC: 0.5505920833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010971985658009848, AUC: 0.5774106666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010947448412577312, AUC: 0.59412625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010902157227198283, AUC: 0.6157350833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010947285095850626, AUC: 0.6144802500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010829692284266153, AUC: 0.6917420833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010688069264094035, AUC: 0.7172219999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010522404114405315, AUC: 0.7176845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001108680248260498, AUC: 0.38728516666666674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010958919525146484, AUC: 0.5863843333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010875272353490194, AUC: 0.67000475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010787942012151083, AUC: 0.6960026666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010966986020406087, AUC: 0.549798\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010888640880584717, AUC: 0.6890821666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010785835186640422, AUC: 0.724383\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010638198057810466, AUC: 0.7271279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011087204217910766, AUC: 0.48456974999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010752893686294556, AUC: 0.6932048333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010165888865788777, AUC: 0.6969088333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009849780201911925, AUC: 0.7072439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011042091846466065, AUC: 0.45013841666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010488210121790567, AUC: 0.7318949166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009941916267077128, AUC: 0.7304418333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009710790912310283, AUC: 0.7345144166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011010266145070394, AUC: 0.5153213333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010211097796758016, AUC: 0.6835919166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009897510409355163, AUC: 0.6947178333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009776385426521301, AUC: 0.7059343333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989794731140136, AUC: 0.5567823333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010775295893351237, AUC: 0.7440445833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010081398089726765, AUC: 0.7292985000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000983731210231781, AUC: 0.7352614166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011079745690027872, AUC: 0.48192383333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001058285117149353, AUC: 0.7237051666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010025838216145833, AUC: 0.7063416666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009882234334945678, AUC: 0.7182980833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006521781285604, AUC: 0.48980158333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010586648384730021, AUC: 0.7053438333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010232718388239542, AUC: 0.7067695833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010008274912834166, AUC: 0.7140745000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010928149620691935, AUC: 0.5847998333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001028776208559672, AUC: 0.6967596666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000996351142724355, AUC: 0.7092576666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009837405880292257, AUC: 0.7205255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008036931355793, AUC: 0.5024975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001032541553179423, AUC: 0.6813719166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009931134780248006, AUC: 0.6917071666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000981962263584137, AUC: 0.7024751666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011013498703638712, AUC: 0.46723600000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010749706029891967, AUC: 0.6941590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010153197447458904, AUC: 0.701143\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000994517187277476, AUC: 0.7146690000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011039557456970215, AUC: 0.4515388333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010539083480834962, AUC: 0.6934508333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010060965617497762, AUC: 0.6991585833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009790690739949544, AUC: 0.7064375000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011098228693008424, AUC: 0.37510166666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010203198591868083, AUC: 0.686401\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009799052476882934, AUC: 0.718816\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009511307279268901, AUC: 0.7414601666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102452834447225, AUC: 0.5388784999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010077983538309733, AUC: 0.6833378333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009887859622637431, AUC: 0.70393025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000983680486679077, AUC: 0.7197625833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005537509918212, AUC: 0.49526458333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000996394197146098, AUC: 0.7019704999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009735412200291951, AUC: 0.7182025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009916103680928548, AUC: 0.721701\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010970897277196249, AUC: 0.5715446666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010057050585746765, AUC: 0.6874408333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009744316140810649, AUC: 0.7104406666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000989678998788198, AUC: 0.7213505000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011048128604888917, AUC: 0.5077158333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010115872224171955, AUC: 0.688205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009738564888636271, AUC: 0.7122881666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000962768812974294, AUC: 0.7263366666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010996450185775757, AUC: 0.509943\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000987881580988566, AUC: 0.7104620833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009575816790262858, AUC: 0.7372740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009480073650677998, AUC: 0.7490015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010981672604878743, AUC: 0.5680080833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000997404416402181, AUC: 0.6858220833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009802550673484803, AUC: 0.7090484166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000997372547785441, AUC: 0.7176103333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100647767384847, AUC: 0.532055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009965072075525919, AUC: 0.7142352500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009570863644282024, AUC: 0.7438875833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009527987837791443, AUC: 0.7472013333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021209160486857, AUC: 0.44864966666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010217579205830892, AUC: 0.7018975833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000969485600789388, AUC: 0.7140478333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009706219633420309, AUC: 0.7250671666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105571707089742, AUC: 0.48061208333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009702065189679463, AUC: 0.72151625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009459812641143799, AUC: 0.7445121666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009371939698855082, AUC: 0.75094875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class SMOTE\n",
    "\n",
    "learning_rates = [1e-4, 5e-4, 1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58a2cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e02deec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011089017788569132, AUC: 0.5046929166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001574310541152954, AUC: 0.6560488333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015633835792541503, AUC: 0.685893\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015301477511723836, AUC: 0.6995961666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011047988732655843, AUC: 0.4485218333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016434243122736612, AUC: 0.6248416666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001616351842880249, AUC: 0.6463213333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001616426666577657, AUC: 0.6586984166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010929834445317587, AUC: 0.59649675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016810382604599, AUC: 0.6184855833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016556649605433146, AUC: 0.6387421666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016343777974446615, AUC: 0.6486046666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011026423374811808, AUC: 0.52387\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001606564203898112, AUC: 0.6057905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016171462933222453, AUC: 0.6415721666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016189109086990356, AUC: 0.65396775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011122514406840007, AUC: 0.3726277500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016631600062052409, AUC: 0.6234056666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016452818314234416, AUC: 0.6505803333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016331039667129516, AUC: 0.6658244999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011047800381978354, AUC: 0.45253441666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017252515554428101, AUC: 0.62393025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001671244740486145, AUC: 0.6435193333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016329911947250365, AUC: 0.6568725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010956584612528482, AUC: 0.553465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016736312707265218, AUC: 0.6277284166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016294746001561482, AUC: 0.6485523333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016160229444503785, AUC: 0.6629605833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011063141028086344, AUC: 0.40977375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001663601040840149, AUC: 0.6373181666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016519038279851279, AUC: 0.6550365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016179747581481934, AUC: 0.6650178333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011024260520935058, AUC: 0.4730988333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016626677910486858, AUC: 0.6328811666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016418880621592205, AUC: 0.6522860833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016218027273813883, AUC: 0.6622691666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001111098845799764, AUC: 0.44498575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016343064705530803, AUC: 0.6368716666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001585454781850179, AUC: 0.6548883333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015675542751948039, AUC: 0.6684758333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011081144412358602, AUC: 0.5278162500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014031659364700318, AUC: 0.6016026666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014851824045181274, AUC: 0.6498640833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014631366729736328, AUC: 0.6750686666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010953156153361004, AUC: 0.5810783333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016927225589752196, AUC: 0.54164475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016647184292475383, AUC: 0.6094385833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016545368830362956, AUC: 0.62754275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011134256919225058, AUC: 0.42293208333333326\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013185396591822306, AUC: 0.6181414166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012967453797658283, AUC: 0.6766141666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012941898902257283, AUC: 0.7101576666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011052189668019612, AUC: 0.43883625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016699913342793783, AUC: 0.6158988333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016732853253682454, AUC: 0.63102875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016835877100626628, AUC: 0.6408001666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010959289868672688, AUC: 0.5566835833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001593072811762492, AUC: 0.6210516666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016107395887374877, AUC: 0.6397563333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016206005016962688, AUC: 0.6500867499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011059112946192424, AUC: 0.4146191666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015870983203252157, AUC: 0.5990670000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00165247909228007, AUC: 0.6238128333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016588765780131021, AUC: 0.6371486666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011019312540690104, AUC: 0.4711055833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016236224571863811, AUC: 0.6113851666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016571965217590332, AUC: 0.628604\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001642165184020996, AUC: 0.6401276666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011094997723897298, AUC: 0.3978396666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015507295529047648, AUC: 0.6236778333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001605536739031474, AUC: 0.6485556666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015303659041722615, AUC: 0.6644595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110604727268219, AUC: 0.4590765833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016531257629394532, AUC: 0.5976413333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016614023049672446, AUC: 0.6246900833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001651646137237549, AUC: 0.6348584166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099194288253784, AUC: 0.5447735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001669742743174235, AUC: 0.5874149166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016743556261062623, AUC: 0.6249489166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016599034070968628, AUC: 0.6395366666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010938975016276042, AUC: 0.6127414166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011232004165649414, AUC: 0.6006709166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012168110211690268, AUC: 0.5995258333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013111001253128052, AUC: 0.6036118333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100923498471578, AUC: 0.54470875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011482404073079426, AUC: 0.554666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013431535561879476, AUC: 0.573788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015245301723480224, AUC: 0.5900523333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011062897046407063, AUC: 0.4377605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011270270347595215, AUC: 0.4939096666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001234443505605062, AUC: 0.5837625833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001415958285331726, AUC: 0.6051726666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011047545671463012, AUC: 0.4684829166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011407869656880696, AUC: 0.50160075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001227465828259786, AUC: 0.57394925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012941938638687134, AUC: 0.6044099166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011000234683354695, AUC: 0.5161180833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011010071436564127, AUC: 0.5528181666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103195587793986, AUC: 0.5689275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011127348343531291, AUC: 0.5786971666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011060269276301066, AUC: 0.4770485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011433895031611125, AUC: 0.5478271666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012836116949717204, AUC: 0.5788291666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001457457661628723, AUC: 0.5943081666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011039744615554809, AUC: 0.43221266666666663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011356417735417684, AUC: 0.4955133333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012377427419026693, AUC: 0.5726235000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013402673403422039, AUC: 0.5893193333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102218468983968, AUC: 0.5234848333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010985681613286337, AUC: 0.5599709166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012239468495051065, AUC: 0.5756610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001476577639579773, AUC: 0.5844038333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010945330858230591, AUC: 0.58882125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011001479625701903, AUC: 0.5731177500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011851567427317302, AUC: 0.5674901666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013512725432713827, AUC: 0.5734806666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011049220164616902, AUC: 0.46788266666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010982290108998616, AUC: 0.5196033333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101766029993693, AUC: 0.5391981666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011330749193827312, AUC: 0.5802556666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011053978204727173, AUC: 0.45536249999999995\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001016865611076355, AUC: 0.7214776666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009618866840998332, AUC: 0.7313865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009408379594484965, AUC: 0.7478806666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011057717800140381, AUC: 0.44706758333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010132566690444946, AUC: 0.6974945833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009697758158047995, AUC: 0.7195119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009472548166910808, AUC: 0.7383986666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001104608138402303, AUC: 0.47800099999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001021358609199524, AUC: 0.67475775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009923197627067567, AUC: 0.6997320833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009891886909802754, AUC: 0.71357275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110628612836202, AUC: 0.40635058333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010014968315760295, AUC: 0.6926616666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009727685252825419, AUC: 0.7189756666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009684410691261292, AUC: 0.7226713333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011004873911539714, AUC: 0.529878\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001003419021765391, AUC: 0.7033860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009627599517504375, AUC: 0.7249108333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009513429999351502, AUC: 0.7320258333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010979227622350057, AUC: 0.5279772500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009896150430043539, AUC: 0.6962398333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009658880829811096, AUC: 0.7157766666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009756133357683818, AUC: 0.7260941666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010968628327051799, AUC: 0.5126463333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010019842783610027, AUC: 0.686765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009914306998252869, AUC: 0.7045265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009927921891212464, AUC: 0.7167458333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011037655671437582, AUC: 0.44445766666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009904644290606181, AUC: 0.6999566666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009527820150057475, AUC: 0.7268228333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009406721591949463, AUC: 0.7387316666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00109584112962087, AUC: 0.5939502499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009992383122444154, AUC: 0.69303525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009722860852877299, AUC: 0.7254153333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009482051332791646, AUC: 0.7421646666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101455012957255, AUC: 0.4830859166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000987704614798228, AUC: 0.702121\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009601519107818603, AUC: 0.728068\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009557054241498311, AUC: 0.737197\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011042465368906657, AUC: 0.4364065833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001069465438524882, AUC: 0.7020306666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010366774400075278, AUC: 0.7088811666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010060601234436035, AUC: 0.7149721666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100319504737854, AUC: 0.5054018333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010786911646525066, AUC: 0.68569075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010284910996754965, AUC: 0.6930999166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009930736819903055, AUC: 0.701086\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010914923747380575, AUC: 0.5913616666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010228408177693684, AUC: 0.6813896666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009927250544230143, AUC: 0.696422\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009787726799647012, AUC: 0.7121878333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011089414358139039, AUC: 0.45110325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010560332536697387, AUC: 0.6885900833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000999017039934794, AUC: 0.694157\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009726752241452535, AUC: 0.7081513333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994391441345214, AUC: 0.51281275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010520110130310058, AUC: 0.72955075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010081866979598998, AUC: 0.7267109166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009765644470850626, AUC: 0.7352096666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094225804011027, AUC: 0.61503775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010449320475260416, AUC: 0.7174961666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010023299058278402, AUC: 0.7132791666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009747423728307088, AUC: 0.7213768333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103117823600769, AUC: 0.4974705833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010811359484990438, AUC: 0.7080445000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010430444876352947, AUC: 0.7013697499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010000009536743164, AUC: 0.6912837499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010977011124293009, AUC: 0.55122075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010280601183573405, AUC: 0.6942199166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000991398016611735, AUC: 0.7027800000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009785742362340292, AUC: 0.7128466666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010993788639704386, AUC: 0.58005625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010094603101412456, AUC: 0.6961333333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009864559968312582, AUC: 0.7074115000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009749111533164978, AUC: 0.7147823333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011054786046346028, AUC: 0.46682266666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010513281424840292, AUC: 0.7032081666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000999683658281962, AUC: 0.7011811666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009771108031272888, AUC: 0.7091393333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010999737580617269, AUC: 0.5061970833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001089211901028951, AUC: 0.6220160833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001081154227256775, AUC: 0.664754\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001072630246480306, AUC: 0.6880802500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011033376852671306, AUC: 0.48093841666666676\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010952330827713012, AUC: 0.5723735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010867668390274047, AUC: 0.6426308333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010753151178359986, AUC: 0.6820054999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011041287183761596, AUC: 0.4477312500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010942618449529013, AUC: 0.5806199166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010865585406621297, AUC: 0.6536506666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010777090390523274, AUC: 0.6944938333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010952356259028117, AUC: 0.6017760833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010868229866027833, AUC: 0.6724667499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001080253521601359, AUC: 0.6943111666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010734790563583375, AUC: 0.6993040833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094273805618286, AUC: 0.5722785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088521122932434, AUC: 0.6332031666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010811470746994018, AUC: 0.6712993333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010705636739730836, AUC: 0.6929240833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011001490354537963, AUC: 0.4884920833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010905079046885174, AUC: 0.5940458333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010796295007069905, AUC: 0.6620765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010674027204513549, AUC: 0.6840158333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010955171585083007, AUC: 0.5708624166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001073305050532023, AUC: 0.6646449166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010516763130823772, AUC: 0.7024268333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010318750540415447, AUC: 0.7078696666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097732424736023, AUC: 0.5524836666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010821943283081054, AUC: 0.6245480833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010676754713058471, AUC: 0.6611784166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010530317227045694, AUC: 0.6818993333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010969289938608806, AUC: 0.55848975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010877684752146402, AUC: 0.66111825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010747206608454386, AUC: 0.6829044166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010610635677973429, AUC: 0.6832696666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001107967972755432, AUC: 0.41803799999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010892477035522462, AUC: 0.5633341666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010737431446711221, AUC: 0.6566150833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001055533488591512, AUC: 0.6877071666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011062400738398234, AUC: 0.49481916666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001007875919342041, AUC: 0.7062289999999999\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0009719972014427185, AUC: 0.7290505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009754973649978637, AUC: 0.7297843333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011115421454111734, AUC: 0.39468024999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010144439140955606, AUC: 0.7207508333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009580605427424113, AUC: 0.7364392500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009509671926498413, AUC: 0.7410335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096882939338684, AUC: 0.554111\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001015723983446757, AUC: 0.6777006666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009884311159451802, AUC: 0.6946431666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009627226789792379, AUC: 0.7193\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010926958322525025, AUC: 0.58256075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009930734435717266, AUC: 0.6946941666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009740681250890096, AUC: 0.7106511666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009724552830060323, AUC: 0.7214235000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011114898522694907, AUC: 0.4406507499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009966768821080526, AUC: 0.69904775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009706426064173381, AUC: 0.72052225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009601373076438904, AUC: 0.7337446666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011014117002487182, AUC: 0.542372\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000995580514272054, AUC: 0.7043071666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009689123431841532, AUC: 0.71760675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009723257621129354, AUC: 0.7254146666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010956465005874634, AUC: 0.5674234166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010071017940839133, AUC: 0.6921143333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009764358599980672, AUC: 0.713507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009814298152923584, AUC: 0.7249859166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011038429339726766, AUC: 0.44201324999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009905632535616557, AUC: 0.7133875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009582896828651429, AUC: 0.73615575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009481472174326579, AUC: 0.7363306666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011015613079071046, AUC: 0.5225498333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010398069222768147, AUC: 0.68700525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000979607621828715, AUC: 0.7029099166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000960983653863271, AUC: 0.7273831666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011056549151738486, AUC: 0.44795333333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001007700483004252, AUC: 0.7031053333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009705914855003357, AUC: 0.7250055833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009549684524536133, AUC: 0.7310926666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010944173336029054, AUC: 0.5829344166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010103292862574259, AUC: 0.696236\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009889210263888041, AUC: 0.7010220833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009750781059265136, AUC: 0.7142266666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096513271331787, AUC: 0.5933495833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001041736642519633, AUC: 0.6943860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010149417320887248, AUC: 0.7034220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000988546848297119, AUC: 0.7078725833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101948062578837, AUC: 0.45581783333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010483996073404947, AUC: 0.6969367499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000989051620165507, AUC: 0.7111415833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009722118775049845, AUC: 0.7178496666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011065871715545655, AUC: 0.4554355833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010772390365600586, AUC: 0.7092221666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010097807248433432, AUC: 0.6975265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000974940558274587, AUC: 0.7081922500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097979187965393, AUC: 0.5268825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010600234667460123, AUC: 0.6881793333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010216348965962727, AUC: 0.70135775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000985104739665985, AUC: 0.713292\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011221830050150553, AUC: 0.36586750000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010450720389684042, AUC: 0.6763453333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010047987302144368, AUC: 0.6846905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009930389126141866, AUC: 0.6970153333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992647806803385, AUC: 0.5768114166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010567493041356406, AUC: 0.7001322499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009984789292017619, AUC: 0.6992731666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009776504238446554, AUC: 0.71246025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011035903294881184, AUC: 0.45453241666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00101862899462382, AUC: 0.6934304166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000985908051331838, AUC: 0.7051000833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000973049263159434, AUC: 0.7139278333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011004151900609333, AUC: 0.5257338333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010016639431317648, AUC: 0.701168\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009746123552322387, AUC: 0.7123812500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009589720765749613, AUC: 0.7244985833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010959818363189698, AUC: 0.5883956666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010540573994318645, AUC: 0.7062365833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010067509214083353, AUC: 0.70203925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009920012950897217, AUC: 0.7116534166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011112993955612182, AUC: 0.4100371666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00109033203125, AUC: 0.6643200833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010711795886357625, AUC: 0.7113744166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010487956206003824, AUC: 0.7098438333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001109967311223348, AUC: 0.40438850000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010990811983744304, AUC: 0.5213463333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001093440294265747, AUC: 0.6123046666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086102286974589, AUC: 0.6633731666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105281154314677, AUC: 0.4216844166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001091419021288554, AUC: 0.5673343333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010763777097066243, AUC: 0.64821425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001061094601949056, AUC: 0.6737093333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011037206649780274, AUC: 0.4933363333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094670295715332, AUC: 0.5949188333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010842831134796143, AUC: 0.6826373333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010675246715545655, AUC: 0.6961435833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011063148180643718, AUC: 0.4154453333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010985499620437622, AUC: 0.5436104999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010924484729766845, AUC: 0.6139313333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001085168997446696, AUC: 0.6603929166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011109771728515625, AUC: 0.4684686666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010846800804138184, AUC: 0.6366701666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010723954836527507, AUC: 0.6786285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010614782174428305, AUC: 0.6901265833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088683565457662, AUC: 0.6420290833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001074983557065328, AUC: 0.68561675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010610239505767821, AUC: 0.69217475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001045675794283549, AUC: 0.6926423333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010947150389353435, AUC: 0.5720689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010808433294296265, AUC: 0.6402516666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010677833557128907, AUC: 0.6563325833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010543439388275145, AUC: 0.6640208333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010957956314086915, AUC: 0.5552435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087375521659851, AUC: 0.6424845833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001079699993133545, AUC: 0.6717888333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010716220140457153, AUC: 0.6848460833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011011456648508707, AUC: 0.43912525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094576915105184, AUC: 0.5971466666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010896365245183308, AUC: 0.6437066666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010840715169906617, AUC: 0.6639816666666666\n",
      "\n",
      "[['capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.001, 0.4780067, 0.004094256687825275, 0.6287301916666667, 0.00016022713457090236, 0.6517391583333333, 0.00015682803844506925, 0.6642287416666667, 0.00017116082644645875, 1, True], ['capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0005, 0.48147609999999996, 0.0039050487956886148, 0.6017525583333334, 0.0005257325636709028, 0.6357313416666667, 0.00031962470603673484, 0.6519786916666666, 0.0005607649001014601, 1, True], ['capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0001, 0.5069261583333333, 0.0033950826039811804, 0.5399697999999999, 0.001161114412662779, 0.5733755166666666, 0.00020502742395111048, 0.5903711583333333, 0.00011676978082562476, 1, True], ['capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.001, 0.4878777083333333, 0.0026612054461434007, 0.6967895416666667, 0.0001309296507892357, 0.7195126416666667, 9.617045941118006e-05, 0.7315482583333334, 0.00011608990968673663, 5, True], ['capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0005, 0.5207694083333333, 0.0034035017796242386, 0.7006353999999998, 0.00019841010778444426, 0.7045292583333332, 9.240266324090348e-05, 0.7121035916666665, 0.00012151076560340311, 5, True], ['capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0001, 0.519728725, 0.0032871366634931246, 0.6188370666666667, 0.001432589794777498, 0.6691847249999998, 0.000316774852748682, 0.6901569416666666, 6.539724408756968e-05, 5, True], ['capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.001, 0.498913375, 0.003729501164135071, 0.6998342, 0.00014339087455583307, 0.7186491333333332, 0.00016905762752250078, 0.7290493083333333, 4.091007596395778e-05, 10, True], ['capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0005, 0.5125760750000001, 0.005310625192697845, 0.6962272833333334, 7.804901665861128e-05, 0.7017954166666668, 5.309899138333398e-05, 0.7120988583333333, 4.570860191395845e-05, 10, True], ['capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0001, 0.48218272500000003, 0.0060505549865847936, 0.6093699916666667, 0.0025826351710992396, 0.6611093333333333, 0.000947567891441666, 0.6799080333333333, 0.00026494284587666743, 10, True]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "610b55d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4206a567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010936386982599894, AUC: 0.6369764166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011055919726689657, AUC: 0.6861483333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011112023194630942, AUC: 0.7031156666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010712438821792603, AUC: 0.7245791666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098901589711507, AUC: 0.5035550833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011472790241241456, AUC: 0.6577565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011363714933395387, AUC: 0.679912\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011391181945800781, AUC: 0.7001859166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011044926643371583, AUC: 0.47181349999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011109636227289836, AUC: 0.6991806666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984382629394532, AUC: 0.7210651666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010675038894017537, AUC: 0.7430861666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011056922674179076, AUC: 0.492054\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011312808195749918, AUC: 0.68086425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011157657305399576, AUC: 0.7060698333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010835485061009725, AUC: 0.7349460000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011074279149373373, AUC: 0.38495166666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011314080556233724, AUC: 0.6690140833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001135978062947591, AUC: 0.6911385833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001119261066118876, AUC: 0.703653\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011060329675674438, AUC: 0.3908685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011127819220225017, AUC: 0.6894318333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021840969721475, AUC: 0.7166343333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010954421361287436, AUC: 0.7346012499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010950309038162231, AUC: 0.5928199166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011488225062688193, AUC: 0.6674150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011356139183044433, AUC: 0.6918772500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011393789052963257, AUC: 0.7113791666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106267770131429, AUC: 0.48593766666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011324230035146077, AUC: 0.6771251666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011099210580190022, AUC: 0.7061552500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010815819501876832, AUC: 0.7316371666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011023016373316447, AUC: 0.46502591666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001131794253985087, AUC: 0.6688313333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011282833814620973, AUC: 0.7010929166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973244905471801, AUC: 0.72247125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010954011678695678, AUC: 0.5755549166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001136072834332784, AUC: 0.6911878333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011315944194793702, AUC: 0.7171906666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011209161281585693, AUC: 0.7292589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010932981173197428, AUC: 0.6214080833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011522661050160725, AUC: 0.6347050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011614456574122111, AUC: 0.6641650833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011421260833740234, AUC: 0.6816788333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011072122653325398, AUC: 0.3656194166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011354360977808634, AUC: 0.6379548333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011327895323435466, AUC: 0.6779329166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011276049613952637, AUC: 0.69614675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011046294371287028, AUC: 0.4924332500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011428091128667195, AUC: 0.655156\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011283722321192423, AUC: 0.6746711666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011265020370483398, AUC: 0.6853726666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005074580510458, AUC: 0.5063869166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001130368192990621, AUC: 0.6755663333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001125267744064331, AUC: 0.6978476666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011072938044865927, AUC: 0.7131230833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010985739628473917, AUC: 0.5350445833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011462002992630004, AUC: 0.6554986666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011355008681615195, AUC: 0.67519925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011270433266957602, AUC: 0.6876950833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010997620820999145, AUC: 0.509181\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011455697218577068, AUC: 0.6482555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011345961888631186, AUC: 0.6760966666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001128129482269287, AUC: 0.6939676666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011017454067866008, AUC: 0.5194621666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001152315378189087, AUC: 0.6385309166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001142965316772461, AUC: 0.6742100833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011308258771896363, AUC: 0.693085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011046844720840453, AUC: 0.42448474999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001140614668528239, AUC: 0.6558189166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001137476404507955, AUC: 0.6729920833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011299472252527872, AUC: 0.6829098333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011092524925867717, AUC: 0.3972340833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001129891037940979, AUC: 0.63770525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011226304769515992, AUC: 0.66155175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011162403424580891, AUC: 0.67539675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110301411151886, AUC: 0.4778616666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001128994862238566, AUC: 0.6776205000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011276738246281941, AUC: 0.6996245833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001113592783610026, AUC: 0.7149228333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011034666697184244, AUC: 0.47951366666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011206547021865845, AUC: 0.40536083333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011383857329686482, AUC: 0.4531185833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001148959239323934, AUC: 0.54882775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010875709056854248, AUC: 0.6542470833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100107232729594, AUC: 0.5966303333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011217983563741047, AUC: 0.6018559166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011358858744303386, AUC: 0.6297838333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011020046472549438, AUC: 0.48920241666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011168365081151326, AUC: 0.4427461666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011393246253331502, AUC: 0.46660491666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011624261935551962, AUC: 0.5201106666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002709865570069, AUC: 0.5131611666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003367106119791, AUC: 0.57842325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011157701015472412, AUC: 0.6149095833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011261167923609415, AUC: 0.6366363333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001092295010884603, AUC: 0.6113636666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011070590019226073, AUC: 0.6270746666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011273154417673747, AUC: 0.6388415833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011401593685150147, AUC: 0.646816\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010970658461252848, AUC: 0.5773196666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010981876055399576, AUC: 0.5297934166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011113189061482747, AUC: 0.5774004166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011241145133972167, AUC: 0.6121340833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010936765670776368, AUC: 0.6166936666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106093486150106, AUC: 0.5191608333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001128653327624003, AUC: 0.5438262500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011410189867019653, AUC: 0.5878803333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010971920490264893, AUC: 0.5301541666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001104970932006836, AUC: 0.584768\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011350012222925822, AUC: 0.6164410000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001148671507835388, AUC: 0.6315444166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010944877465565999, AUC: 0.5830225833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010913997888565064, AUC: 0.6205889166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011085407336552937, AUC: 0.6368674166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011245766480763753, AUC: 0.6512603333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011042916377385456, AUC: 0.487798\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011212885379791259, AUC: 0.54871375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011443039973576864, AUC: 0.5888240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011531668504079183, AUC: 0.6149534999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011092381477355957, AUC: 0.45920958333333334\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010006168087323506, AUC: 0.7007906666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000991052786509196, AUC: 0.7465673333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000992796262105306, AUC: 0.7577124999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011017233928044636, AUC: 0.49381433333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001044129490852356, AUC: 0.676765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010413487354914348, AUC: 0.7089582499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001047049840291341, AUC: 0.7347656666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011043426195780435, AUC: 0.41822224999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010062861442565918, AUC: 0.6886078333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009938278992970785, AUC: 0.7191567499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009720776081085205, AUC: 0.7530308333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010985084772109986, AUC: 0.5633248333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010073503057161967, AUC: 0.7039656666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010310287872950237, AUC: 0.7285898333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001004262109597524, AUC: 0.7496721666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011018152236938476, AUC: 0.50457275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009927019079526266, AUC: 0.7070563333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982668916384379, AUC: 0.7305366666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009616727828979492, AUC: 0.7676113333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011027238766352335, AUC: 0.4866646666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010337328513463338, AUC: 0.7035854166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010102491974830628, AUC: 0.7425748333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001035577972730001, AUC: 0.7545378333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994813839594524, AUC: 0.49779675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010150831540425619, AUC: 0.6862318333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001000135103861491, AUC: 0.715658\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009993505676587423, AUC: 0.7457275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010955736637115478, AUC: 0.622265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010285507043202718, AUC: 0.6880580833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010043800671895346, AUC: 0.7148335833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000999167521794637, AUC: 0.7349296666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010979270935058594, AUC: 0.5242953333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010235259334246318, AUC: 0.6918743333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010050415595372517, AUC: 0.7224486666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010168134768803914, AUC: 0.7424721666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010967379411061604, AUC: 0.5427161666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001007876733938853, AUC: 0.6852680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010363654295603435, AUC: 0.7244481666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00100600532690684, AUC: 0.744694\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105154593785604, AUC: 0.45354283333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001027836283047994, AUC: 0.6929316666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000996140758196513, AUC: 0.7010236666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000995235045750936, AUC: 0.7189250833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998360713322958, AUC: 0.5391403333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010074087381362915, AUC: 0.7111426666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009880446394284566, AUC: 0.7199663333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009914915561676026, AUC: 0.7373383333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106828252474467, AUC: 0.46873899999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010116079648335775, AUC: 0.67631275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010257517298062643, AUC: 0.6864549166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010341983636220297, AUC: 0.7040088333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011054137150446574, AUC: 0.40368424999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010338973999023436, AUC: 0.6863109999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010176490942637125, AUC: 0.697609\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010237230062484742, AUC: 0.7164019999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097049593925476, AUC: 0.5371325833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010313517649968465, AUC: 0.6832599166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010425858100255331, AUC: 0.7024453333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010318625768025716, AUC: 0.7245368333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010963772137959798, AUC: 0.55996225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001011492649714152, AUC: 0.68178625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010291471481323241, AUC: 0.702783\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010222679773966472, AUC: 0.7163913333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011011217832565309, AUC: 0.48999325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010466321309407552, AUC: 0.7248991666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001006702462832133, AUC: 0.7400889166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000963780164718628, AUC: 0.7405101666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011033064126968384, AUC: 0.47936341666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010413281122843424, AUC: 0.738893\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009834797183672587, AUC: 0.7181180833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009972732265790304, AUC: 0.7351230833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00109390123685201, AUC: 0.5775082500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001015678842862447, AUC: 0.7044716666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010090984304745993, AUC: 0.7215069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009981198906898498, AUC: 0.7383165833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011107863982518513, AUC: 0.41735291666666674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010445671478907267, AUC: 0.7163928333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010011342167854308, AUC: 0.7104713333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009919388095537822, AUC: 0.7229055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011047691504160564, AUC: 0.40639158333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097446362177531, AUC: 0.5454716666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010908048947652182, AUC: 0.6531803333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010828895966211954, AUC: 0.6887084166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011059724489847819, AUC: 0.4088800833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097112496693929, AUC: 0.532828\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010883542696634929, AUC: 0.6286982499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010740296443303426, AUC: 0.6705788333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095553159713745, AUC: 0.5641919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010846720139185589, AUC: 0.66604625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010711976687113444, AUC: 0.6851346666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010539424816767375, AUC: 0.6843407499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001114752451578776, AUC: 0.38610716666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010955020984013874, AUC: 0.5883575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010841799974441529, AUC: 0.6500084166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010667299429575603, AUC: 0.6730686666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010980667273203532, AUC: 0.5247461666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010769569079081217, AUC: 0.64408475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010503416856129965, AUC: 0.6689962500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010282588402430216, AUC: 0.6683296666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011014615297317504, AUC: 0.47038699999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010968159039815267, AUC: 0.5437768333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010938104391098023, AUC: 0.59565175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010902483860651651, AUC: 0.6369679166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010974303483963013, AUC: 0.5301920833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882081985473632, AUC: 0.6694408333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010740822950998942, AUC: 0.7236684166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010521570046742757, AUC: 0.71370975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011033823490142823, AUC: 0.47099433333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010957374175389607, AUC: 0.58172775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010869891246159872, AUC: 0.6640915000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010770019292831421, AUC: 0.69820875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010967666308085123, AUC: 0.5806090833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088910182317098, AUC: 0.6954725833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010774726470311482, AUC: 0.7117268333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010595961411794027, AUC: 0.7059735833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010954041878382364, AUC: 0.5769973333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087959329287211, AUC: 0.638062\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001072364608446757, AUC: 0.6718185833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010493715604146321, AUC: 0.6793269166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098700761795044, AUC: 0.52868725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010119130214055379, AUC: 0.6965364166666667\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010055485367774964, AUC: 0.7125833333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009834045171737671, AUC: 0.7223613333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001110142191251119, AUC: 0.3632611666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009840183854103088, AUC: 0.7084878333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009811877806981404, AUC: 0.7262844999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009945996999740601, AUC: 0.7435213333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106520414352417, AUC: 0.4813669166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010102336009343465, AUC: 0.7366194999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000978839119275411, AUC: 0.7450051666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009691664377848308, AUC: 0.7418221666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011050666173299153, AUC: 0.3887366666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009948948224385579, AUC: 0.689836\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009786287546157838, AUC: 0.7108009166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009680883487065633, AUC: 0.7248308333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001108383854230245, AUC: 0.41894258333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009952727158864339, AUC: 0.6965880833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009914476076761881, AUC: 0.714662\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010137707591056824, AUC: 0.7243174166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010977652072906494, AUC: 0.5399925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009892647663752238, AUC: 0.706248\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009841970602671305, AUC: 0.7334023333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010053621729214986, AUC: 0.7315656666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010999977588653565, AUC: 0.49252233333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009990983406702677, AUC: 0.6937531666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009850684801737467, AUC: 0.7202125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009689814845720927, AUC: 0.730881\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986047983169555, AUC: 0.5552579166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009869631727536519, AUC: 0.71117375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009585743347803752, AUC: 0.7300963333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000970589280128479, AUC: 0.7334785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010972554286321004, AUC: 0.5470600833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000987870951493581, AUC: 0.6986551666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010100048383076985, AUC: 0.7191795833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010489758253097534, AUC: 0.727576\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011009087959925334, AUC: 0.48203625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009873311916987102, AUC: 0.7168086666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982661485671997, AUC: 0.7361708333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00097922154267629, AUC: 0.7394754999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010964082479476929, AUC: 0.5665667499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001042488137880961, AUC: 0.6994584166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009897015690803529, AUC: 0.7070808333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00096351824204127, AUC: 0.7196211666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011132952372233072, AUC: 0.42684541666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010254615942637125, AUC: 0.6839608333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009931799173355104, AUC: 0.6987935833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009892159303029379, AUC: 0.71172275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010990638335545858, AUC: 0.49474975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010152608553568522, AUC: 0.6854737499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009962836305300396, AUC: 0.7003201666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009952582319577535, AUC: 0.7096873333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103213906288147, AUC: 0.51880925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010519259373346964, AUC: 0.6759266666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010027546087900798, AUC: 0.6923385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009896816809972127, AUC: 0.70894675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097952365875244, AUC: 0.5523101666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010467573801676432, AUC: 0.6787899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009921009341875711, AUC: 0.68980725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009827738602956135, AUC: 0.7070259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010981725056966145, AUC: 0.531704\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010385859409968057, AUC: 0.7070135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009809529980023702, AUC: 0.7086610833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009850863615671794, AUC: 0.7158401666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011029236714045207, AUC: 0.48347483333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010457384983698526, AUC: 0.7239354166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009835648934046427, AUC: 0.7215205833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009685914516448975, AUC: 0.7290349166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011107045412063598, AUC: 0.49426758333333326\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010573599338531494, AUC: 0.7075325000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009980215628941855, AUC: 0.7005918333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009726643959681193, AUC: 0.7113318333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010956507921218873, AUC: 0.5617001666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010352311134338378, AUC: 0.6898036666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009888253410657247, AUC: 0.6946365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009876829584439595, AUC: 0.7019982499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099257469177246, AUC: 0.4979205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010472716887791951, AUC: 0.71022425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000990996539592743, AUC: 0.7051513333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000981047809123993, AUC: 0.7111804999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096542994181315, AUC: 0.60178325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010739115476608276, AUC: 0.6744163333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010573295354843139, AUC: 0.683503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010408416986465454, AUC: 0.6868566666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010951813459396363, AUC: 0.5929781666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010771705309549967, AUC: 0.6540566666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010652968486150106, AUC: 0.6742737500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010560468435287477, AUC: 0.6759391666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010982043743133546, AUC: 0.5589021666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010840997695922851, AUC: 0.65081775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010714203914006552, AUC: 0.6659092499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010602133671442668, AUC: 0.6711944166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100808620452881, AUC: 0.52504925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010920580228169759, AUC: 0.64601775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010839176177978515, AUC: 0.68050975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010735880136489868, AUC: 0.6891422500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011055591106414796, AUC: 0.4458986666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991091728210448, AUC: 0.5261494999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010947023232777914, AUC: 0.58680875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010898993810017903, AUC: 0.6323678333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011045989592870077, AUC: 0.4890019166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010879849990208944, AUC: 0.6480795833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010748086770375569, AUC: 0.6871329166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010610545078913371, AUC: 0.6892906666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010978775024414063, AUC: 0.5408835833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010890957911809285, AUC: 0.6356809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010771211783091228, AUC: 0.6709663333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001059997320175171, AUC: 0.6820809166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002465883890788, AUC: 0.49527583333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010812344948450724, AUC: 0.6503397499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010621159076690675, AUC: 0.6725855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010408162673314412, AUC: 0.6773811666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011029127438863119, AUC: 0.43478208333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00108952530225118, AUC: 0.6601785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010757880608240763, AUC: 0.70339625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010599418878555298, AUC: 0.7086988333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103028655052185, AUC: 0.47109775000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010920553207397462, AUC: 0.5880115833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010828181902567545, AUC: 0.6436754166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010714584986368815, AUC: 0.6692600833333334\n",
      "\n",
      "[['distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.001, 0.4999557583333333, 0.00605235968466729, 0.6786955, 0.00015104064609027754, 0.7034251666666665, 0.00015323069874166625, 0.7235798083333334, 0.0001823769879542361, 1, True], ['distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0005, 0.48491159166666664, 0.004906419355246458, 0.6516811916666667, 0.00021437119237923582, 0.677429125, 0.00013822385485868051, 0.6924298500000001, 0.00015193170765527813, 1, True], ['distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0001, 0.5542476083333333, 0.0035022147446986764, 0.5453260166666667, 0.004868080909577499, 0.5738689666666668, 0.003969514571110001, 0.6079947249999998, 0.001692318543337568, 1, True], ['distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.001, 0.5112881666666667, 0.002869278821847221, 0.6932203166666667, 9.042344138444407e-05, 0.7253772083333334, 0.00013050674809479237, 0.7485153666666666, 9.397617392666677e-05, 5, True], ['distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0005, 0.4926419083333332, 0.00317411657063257, 0.7016400916666667, 0.00039290216996034667, 0.7100467583333334, 0.00021008257917145824, 0.7254457750000001, 0.0001300790683736812, 5, True], ['distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0001, 0.4919496833333333, 0.004917809603330275, 0.6105268166666666, 0.003169907709310834, 0.6652975000000001, 0.0012584637193875001, 0.681921325, 0.0004325508267075693, 5, True], ['distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.001, 0.47978636666666674, 0.0041977683730975, 0.7054706583333333, 0.00017279475213534642, 0.7248397499999999, 0.0001142418398277778, 0.731982975, 5.1161775872291294e-05, 10, True], ['distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0005, 0.5128348416666666, 0.0016375420446617353, 0.6962119, 0.00022417849916222293, 0.7018901666666666, 7.723630886666653e-05, 0.7126389666666666, 4.995759994611118e-05, 10, True], ['distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0001, 0.5155652666666667, 0.0030509338308524984, 0.6333748416666666, 0.0017359131216506246, 0.6668760916666667, 0.0009260429718103471, 0.6782212000000001, 0.0003498279119016668, 10, True]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class euclidean distance capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['distance'] = 'euclidean'\n",
    "\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"distance_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e5ab41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cef71afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011055415074030558, AUC: 0.38005533333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010076440175374349, AUC: 0.7001938333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009683122436205546, AUC: 0.712987\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009642910559972128, AUC: 0.7338803333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010966946681340536, AUC: 0.5500838333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009827250440915425, AUC: 0.7104226666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009693577686945597, AUC: 0.7252480833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009774929285049439, AUC: 0.7279855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001093033711115519, AUC: 0.57653375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009999072551727295, AUC: 0.6984451666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009814204374949138, AUC: 0.7221749166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009723171393076579, AUC: 0.7307384166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010961555242538451, AUC: 0.5658584999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000990861475467682, AUC: 0.7180896666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009716771443684896, AUC: 0.7399297499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009970495700836182, AUC: 0.73124725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010981086492538451, AUC: 0.5267281666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009798616766929626, AUC: 0.7128989166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009645132223765055, AUC: 0.7240156666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009601595600446065, AUC: 0.7360005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011092440287272136, AUC: 0.40758608333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009840831160545349, AUC: 0.7093241666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009574979543685913, AUC: 0.7255145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009760175943374634, AUC: 0.731451\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011102139552434285, AUC: 0.44438858333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009907662669817606, AUC: 0.7027665000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000969053308169047, AUC: 0.7285764166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009626888036727905, AUC: 0.7367231666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984530051549276, AUC: 0.5153800000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009985286394755045, AUC: 0.6997309999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009831726948420208, AUC: 0.7150606666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009959041674931844, AUC: 0.7242605000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103417197863261, AUC: 0.4929263333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009831470648447673, AUC: 0.7012436666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009686529835065205, AUC: 0.7237313333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010174641013145447, AUC: 0.729566\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095681111017863, AUC: 0.54128075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009948317011197409, AUC: 0.69929825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009675939083099365, AUC: 0.7307468333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009551441073417663, AUC: 0.7393921666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011011814673741657, AUC: 0.4802135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001034870982170105, AUC: 0.7019035000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000979999303817749, AUC: 0.7151145833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009661948283513387, AUC: 0.73391475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003093719482422, AUC: 0.5242878333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001030031681060791, AUC: 0.7039885833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000979445735613505, AUC: 0.7092829166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009667896231015524, AUC: 0.7208541666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011025415658950806, AUC: 0.48878383333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010506976842880249, AUC: 0.6865288333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000993418534596761, AUC: 0.6905524999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009733984271685282, AUC: 0.7138154166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106962243715922, AUC: 0.43093183333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001062252680460612, AUC: 0.710839\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000995100438594818, AUC: 0.7019169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000980635166168213, AUC: 0.7106736666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973428885142009, AUC: 0.55773675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010598077376683553, AUC: 0.7384158333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009901023507118225, AUC: 0.7178029166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000971978743871053, AUC: 0.7203365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096545378367106, AUC: 0.5752206666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010363455216089884, AUC: 0.6651193333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010160196622212728, AUC: 0.68161875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010052545468012492, AUC: 0.6966804166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001111013372739156, AUC: 0.42273000000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010705944697062175, AUC: 0.688929\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010122761329015096, AUC: 0.6893984999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009838730891545615, AUC: 0.7015495833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010995482206344605, AUC: 0.4963464166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001038753628730774, AUC: 0.6776165000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009921019872029622, AUC: 0.6850688333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009809404810269674, AUC: 0.7006335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010995942354202271, AUC: 0.49645075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010425503253936768, AUC: 0.7062591666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009819940129915873, AUC: 0.7112534999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000968742847442627, AUC: 0.7281688333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010933478275934854, AUC: 0.5495095833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010354516506195068, AUC: 0.6657668333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010048553148905436, AUC: 0.6765690000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010011937618255615, AUC: 0.6892845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011026333967844644, AUC: 0.4738980833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010892231067021689, AUC: 0.65193775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001079315145810445, AUC: 0.6918280833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010704590479532878, AUC: 0.7027359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010948436657587687, AUC: 0.5402254999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010827039480209352, AUC: 0.6013937500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010662997166315715, AUC: 0.6427556666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010446224610010783, AUC: 0.6608144166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001110588828722636, AUC: 0.38182774999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095192829767863, AUC: 0.6029726666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010877013206481933, AUC: 0.6636700833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001079970121383667, AUC: 0.6785919166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100618044535319, AUC: 0.4672956666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010880887905756632, AUC: 0.6640145000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001077536662419637, AUC: 0.69998825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001067815899848938, AUC: 0.7086403333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011033072074254353, AUC: 0.4844753333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010667807658513388, AUC: 0.7058769166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010360169013341268, AUC: 0.6971150000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010125041802724203, AUC: 0.6924649166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011019148429234822, AUC: 0.5016382500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010953882137934366, AUC: 0.5978929166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010871536334355672, AUC: 0.6895581666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010733681519826254, AUC: 0.71495725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010995999972025553, AUC: 0.5093553333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010869801044464111, AUC: 0.6142754166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010763723055521647, AUC: 0.6769667500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010658553838729858, AUC: 0.6970258333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011019210815429688, AUC: 0.4953782500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010939366022745768, AUC: 0.6122040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010824947754542033, AUC: 0.6787265833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010669530232747396, AUC: 0.7035905833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011056739886601767, AUC: 0.46151766666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096468210220337, AUC: 0.5666750833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010889730056126913, AUC: 0.6285142499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001080587387084961, AUC: 0.6531237500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011031617720921834, AUC: 0.44867758333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010795571406682332, AUC: 0.6391474166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010546860297520955, AUC: 0.6731246666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010300560394922893, AUC: 0.6785831666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011037120421727498, AUC: 0.53326725\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0009933680097262065, AUC: 0.6939047500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009753385782241821, AUC: 0.7173980000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00098874165614446, AUC: 0.7309538333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011000401576360067, AUC: 0.5157429166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009977991382280986, AUC: 0.6880851666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009763535857200623, AUC: 0.7121859166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009658528765042623, AUC: 0.7250770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096102754275004, AUC: 0.5486070833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010320355892181397, AUC: 0.6862285833333331\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009796069463094076, AUC: 0.7096381666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009827236930529276, AUC: 0.7238424166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010975841681162516, AUC: 0.5274588333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009785786271095276, AUC: 0.7142916666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009575179815292359, AUC: 0.7239957499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009451408584912618, AUC: 0.7343130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099854588508606, AUC: 0.5256431666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009998059272766113, AUC: 0.6942456666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009735611279805502, AUC: 0.7138934166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009520652492841085, AUC: 0.7300987500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010996334552764893, AUC: 0.4966715833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009897953867912292, AUC: 0.6965201666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009654563069343567, AUC: 0.7174335833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009763970573743185, AUC: 0.7232609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011019275188446046, AUC: 0.5468907500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009941213726997375, AUC: 0.7097021666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000955148716767629, AUC: 0.7307740833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009456361929575602, AUC: 0.7446893333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011217705806096396, AUC: 0.40554025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010207380056381225, AUC: 0.7181020833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000965886652469635, AUC: 0.7228948333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009633832971254984, AUC: 0.7289340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011068859497706095, AUC: 0.48376866666666657\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010130354166030884, AUC: 0.7280469166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009606264630953471, AUC: 0.7396796666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009721100131670633, AUC: 0.7423266666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994282563527426, AUC: 0.5265781666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010164970954259236, AUC: 0.7252064166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009852842688560487, AUC: 0.740841\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000981162985165914, AUC: 0.7480154166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010906168222427368, AUC: 0.60025425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010391772985458374, AUC: 0.6929910833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009847773710886638, AUC: 0.7013340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009633631110191345, AUC: 0.7171275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00109976327419281, AUC: 0.4985484999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010181133349736532, AUC: 0.6782583333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009933713674545289, AUC: 0.6896056666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009819384415944417, AUC: 0.702252\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105753223101298, AUC: 0.47448775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010215953588485719, AUC: 0.6882348333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009898062745730082, AUC: 0.7060144166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009756027658780415, AUC: 0.7163151666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096645434697469, AUC: 0.54444375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010320676962534588, AUC: 0.7006186666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009862554868062337, AUC: 0.708327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009735315243403116, AUC: 0.7179876666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010931159257888793, AUC: 0.5953687499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001037131110827128, AUC: 0.6853193333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009968475103378296, AUC: 0.6916953333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009839722315470377, AUC: 0.7016330833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011147438287734985, AUC: 0.38789941666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010620124340057374, AUC: 0.7007642500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010041818022727966, AUC: 0.7133313333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009648323655128478, AUC: 0.7237187500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011019854148228963, AUC: 0.5198239166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010239654779434205, AUC: 0.6795355833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000991766889890035, AUC: 0.6967560833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000978262722492218, AUC: 0.7117275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010936743418375652, AUC: 0.648752\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010079890886942546, AUC: 0.7111616666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009756610989570618, AUC: 0.7287232499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009638175368309021, AUC: 0.7424058333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100032091140747, AUC: 0.5310124166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010189031759897868, AUC: 0.6924528333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009845101435979207, AUC: 0.7055155000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000966492493947347, AUC: 0.7248566666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010922473669052123, AUC: 0.5850732499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010060842434565227, AUC: 0.7108060833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009770966172218322, AUC: 0.7233375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009721662004788717, AUC: 0.729034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010941413243611654, AUC: 0.5924703333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010869431098302206, AUC: 0.65606775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010774533351262411, AUC: 0.6841829166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010621951818466186, AUC: 0.6909705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011066710154215495, AUC: 0.4434330833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992168188095092, AUC: 0.516272\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010892236630121867, AUC: 0.6190806666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010735640128453573, AUC: 0.6695529166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094882567723592, AUC: 0.6322990833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010878568092981974, AUC: 0.6979017500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010770898660024007, AUC: 0.71652175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001060237209002177, AUC: 0.7122615833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973535378774007, AUC: 0.539057\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010847857395807903, AUC: 0.6416796666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010704317092895509, AUC: 0.6674018333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010539050102233887, AUC: 0.6758102500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011039867401123047, AUC: 0.4594861666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087087869644165, AUC: 0.6462298333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010707984765370687, AUC: 0.6818249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010487864017486572, AUC: 0.6874583333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010996895631154378, AUC: 0.48684625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087522506713867, AUC: 0.6300895833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010751468737920126, AUC: 0.6599954166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010583542982737224, AUC: 0.6676905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011000777880350748, AUC: 0.47585408333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010949413379033406, AUC: 0.5905589166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00108851957321167, AUC: 0.6451659166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010781171719233195, AUC: 0.672103\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099092443784078, AUC: 0.5761975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086751898129781, AUC: 0.6598915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001076120416323344, AUC: 0.6904326666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010635225772857666, AUC: 0.7021608333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010997341473897298, AUC: 0.49828491666666674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010896214246749877, AUC: 0.64417325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010816086133321127, AUC: 0.6847320833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010732522408167522, AUC: 0.6968124166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010970072746276855, AUC: 0.5384927500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010834428071975707, AUC: 0.6580592500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001067517876625061, AUC: 0.6844808333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010477991898854573, AUC: 0.6854675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011010510126749674, AUC: 0.46647108333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010206903616587321, AUC: 0.7147884166666666\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0009509822328885397, AUC: 0.7341509999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009344067573547364, AUC: 0.7491564999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00109254789352417, AUC: 0.6488634166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009836358030637104, AUC: 0.7124772500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009623388648033142, AUC: 0.7287971666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009584565758705139, AUC: 0.7339683333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011012711922327677, AUC: 0.5230758333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009854709307352701, AUC: 0.70752\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009705777764320374, AUC: 0.7231626666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001023399511973063, AUC: 0.7213120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010993149677912394, AUC: 0.5271454166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009839840531349182, AUC: 0.7254368333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009517814119656881, AUC: 0.7478110833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009533281127611797, AUC: 0.7558744166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011024358669916789, AUC: 0.41486324999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010296360651652018, AUC: 0.7001301666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009760027527809143, AUC: 0.7191375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009722209572792053, AUC: 0.7295590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011014281908671062, AUC: 0.46696525000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010054442485173544, AUC: 0.68444775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009805226325988769, AUC: 0.7073184166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009960000117619832, AUC: 0.7225263333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011048919359842935, AUC: 0.445614\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009881594777107238, AUC: 0.6997901666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009711306889851888, AUC: 0.7200696666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009601845741271973, AUC: 0.733502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010946261485417683, AUC: 0.6038255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009981984694798786, AUC: 0.6981883333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000976282815138499, AUC: 0.7145818333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000994658907254537, AUC: 0.7163765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011041518847147624, AUC: 0.44184675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009948964913686117, AUC: 0.6939673333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009675133029619853, AUC: 0.72104625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009688562353452046, AUC: 0.7325957500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010924862623214722, AUC: 0.5826776666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000986083984375, AUC: 0.7153691666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009509746034940084, AUC: 0.7446368333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009432052373886109, AUC: 0.7480472499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010978821913401285, AUC: 0.54150825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010428974628448487, AUC: 0.67863225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009963451623916627, AUC: 0.6889840833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009775745073954264, AUC: 0.7123613333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011051112413406371, AUC: 0.49391749999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010330010652542115, AUC: 0.7031429166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000978282372156779, AUC: 0.7170234999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000961552103360494, AUC: 0.7280587500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010982201099395753, AUC: 0.5589429166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010108989477157593, AUC: 0.7109275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009734891851743062, AUC: 0.7260583333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009595861434936523, AUC: 0.7365838333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011007001399993897, AUC: 0.4910070833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010597948630650838, AUC: 0.6898526666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009995215535163878, AUC: 0.6926531666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009838724136352539, AUC: 0.7058039166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010958906809488932, AUC: 0.5996755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001007156769434611, AUC: 0.7069319166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009733301003774007, AUC: 0.720903\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009604413906733195, AUC: 0.7242335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011000384092330932, AUC: 0.4935508333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010275734265645345, AUC: 0.6844690833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009974042971928915, AUC: 0.70163275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009795195857683818, AUC: 0.71328275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998860994974773, AUC: 0.48756400000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001065655827522278, AUC: 0.7013265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009920486410458882, AUC: 0.7039720833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009665324489275614, AUC: 0.7193080833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011106939315795898, AUC: 0.3832073333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001045412023862203, AUC: 0.70135725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010069151322046916, AUC: 0.7050711666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009775866270065308, AUC: 0.7094805833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011019858519236247, AUC: 0.47805875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001047797123591105, AUC: 0.70882625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009840997854868572, AUC: 0.7062410833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009671140313148498, AUC: 0.7158386666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008138259251913, AUC: 0.54236075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010209712982177735, AUC: 0.6754565833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009965850114822387, AUC: 0.6934123333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009826659957567852, AUC: 0.7108183333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001108626405398051, AUC: 0.5267659166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010904630025227865, AUC: 0.5910959166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010778307914733887, AUC: 0.63535875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001061173717180888, AUC: 0.6534398333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010895458857218424, AUC: 0.6583811666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010779867966969807, AUC: 0.7096395833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010675806999206543, AUC: 0.7201870833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001058023730913798, AUC: 0.722735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010935858488082886, AUC: 0.5940255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010798561970392863, AUC: 0.6969\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010604241291681926, AUC: 0.7165489166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010365758736928304, AUC: 0.7110679166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011018239657084147, AUC: 0.46545216666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010912928581237793, AUC: 0.6283551666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001083767056465149, AUC: 0.6761189166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010756539503733317, AUC: 0.693423\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010940395991007486, AUC: 0.59962975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010747767289479573, AUC: 0.70282175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010597528616587322, AUC: 0.7129019166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001046986738840739, AUC: 0.7115255833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011017162402470906, AUC: 0.5245753333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010705790122350056, AUC: 0.677763\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010559093952178956, AUC: 0.6856062500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010417054494222006, AUC: 0.6865145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010959489345550538, AUC: 0.5915353333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010845593611399332, AUC: 0.6748413333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010724939902623494, AUC: 0.6853001666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010571380853652953, AUC: 0.6859393333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010916908582051595, AUC: 0.641298\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001079548160235087, AUC: 0.70056575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010654787222544352, AUC: 0.6992199166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010493157307306926, AUC: 0.6952677500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991293589274087, AUC: 0.51069175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088830828666687, AUC: 0.6473267500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010795984268188476, AUC: 0.6873141666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001069460074106852, AUC: 0.7010223333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006331443786622, AUC: 0.5754439166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010906176169713337, AUC: 0.63117725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010812335809071859, AUC: 0.6580953333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010672510862350464, AUC: 0.6892820833333332\n",
      "\n",
      "[['cosine_distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.001, 0.5000821333333334, 0.004142784090698887, 0.7052413833333333, 4.266892844333332e-05, 0.7247985166666666, 5.222966240249992e-05, 0.7321244833333334, 1.8073717664999854e-05, 1, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0005, 0.5022211166666667, 0.002326776346211388, 0.6945366583333333, 0.0004547119478395133, 0.6978578499999999, 0.0002027464007136109, 0.7115911333333333, 0.00018820562213222274, 1, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0001, 0.4764289416666666, 0.001624516948752847, 0.6256390416666667, 0.0014442717518170136, 0.67422475, 0.0004952861743666667, 0.6890528166666666, 0.0003835847128483329, 1, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.001, 0.5110168666666667, 0.0016017364199391675, 0.7054333583333332, 0.00021668371781118138, 0.7228734416666666, 0.00010985715876395832, 0.7331511416666668, 7.212052816256922e-05, 5, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0005, 0.5385664, 0.005018658350914997, 0.6940142666666667, 0.00012355492078722335, 0.7064640083333333, 0.0001454799490242353, 0.7187058166666667, 0.00013509498575111132, 5, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0001, 0.5242421166666669, 0.003459038254473889, 0.63409235, 0.00218938934234278, 0.6733819083333333, 0.000655146097378402, 0.6860287833333334, 0.00019975614796694557, 5, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.001, 0.5121348166666666, 0.005537586445459444, 0.7052115416666667, 0.000132418020036459, 0.7260712416666666, 0.00014914960465479092, 0.7342918083333332, 0.00015298448371534623, 10, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0005, 0.5069792916666667, 0.003087579620415626, 0.6960922916666668, 0.000150928001280903, 0.70559515, 0.00013945495653861064, 0.717576975, 8.169350319173656e-05, 10, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0001, 0.5687798833333334, 0.003320860640053055, 0.66604865, 0.0014215438615344431, 0.6876651416666666, 0.0006415959596625706, 0.6950217333333334, 0.0003253366885038886, 10, True]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class cosine distance capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['distance'] = 'cosine'\n",
    "\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"cosine_distance_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a533399",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cb796e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001090798298517863, AUC: 0.6275455833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010112351179122924, AUC: 0.6849049166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009967645009358725, AUC: 0.7066909999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009790867567062377, AUC: 0.7257449166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010908244848251343, AUC: 0.6068476666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001025878111521403, AUC: 0.69062775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001001077930132548, AUC: 0.7137581666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010044774611790974, AUC: 0.7283343333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010957770744959513, AUC: 0.5945498333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010208751757939656, AUC: 0.6770616666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001009067436059316, AUC: 0.7041759166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010336569150288899, AUC: 0.7188218333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010945513248443604, AUC: 0.5797924166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010250928401947022, AUC: 0.6944658333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001034887393315633, AUC: 0.7108225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010345330635706583, AUC: 0.7198331666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011043479442596434, AUC: 0.4265506666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010184155702590942, AUC: 0.6887305833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010134309927622477, AUC: 0.7083205833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010007899800936382, AUC: 0.7265195000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010980952978134155, AUC: 0.528323\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010138742923736573, AUC: 0.6959814166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009954316020011902, AUC: 0.7281779999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009729594786961873, AUC: 0.7465138333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011004029512405396, AUC: 0.5260038333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001018747886021932, AUC: 0.694031\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010159102280934652, AUC: 0.7093643333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010139923493067424, AUC: 0.7256191666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097307801246643, AUC: 0.64414475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010083740750948587, AUC: 0.704289\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000994466225306193, AUC: 0.7289811666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009724103212356567, AUC: 0.7400271666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001104670484860738, AUC: 0.38416066666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010231262842814127, AUC: 0.6829076666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010066951314608257, AUC: 0.70381525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001009363055229187, AUC: 0.7224164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011049298842748007, AUC: 0.4508880833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010222194592158, AUC: 0.7251249166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009841704964637756, AUC: 0.7321736666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009599187572797139, AUC: 0.7493233333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011040268341700236, AUC: 0.44758991666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010334347486495971, AUC: 0.6742983333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001025054613749186, AUC: 0.6873469166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010274400313695271, AUC: 0.6959359166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010971453189849855, AUC: 0.5255206666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010497441291809082, AUC: 0.6750853333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010241682529449462, AUC: 0.6999456666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010137119690577189, AUC: 0.7151114166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992250442504884, AUC: 0.5019013333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010330652793248494, AUC: 0.6845991666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010021549661954243, AUC: 0.7074976666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000984927237033844, AUC: 0.7210646666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011033175786336264, AUC: 0.4414886666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010276315609614054, AUC: 0.6826331666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001002385139465332, AUC: 0.7008181666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009941230416297912, AUC: 0.7114830833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011016172965367635, AUC: 0.46847583333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010267895460128783, AUC: 0.6904719166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010117382208506267, AUC: 0.70606425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010032941699028015, AUC: 0.7173723333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010976636012395224, AUC: 0.5274855833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001044935425122579, AUC: 0.6627908333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010312343835830688, AUC: 0.6779958333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010361535549163819, AUC: 0.6875653333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011024382909138998, AUC: 0.45752024999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010790215333302815, AUC: 0.6662384166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010259364048639933, AUC: 0.6772323333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010250589450200398, AUC: 0.6886048333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010993709166844687, AUC: 0.5044979166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010315725406010946, AUC: 0.6935040833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010167344411214192, AUC: 0.70592475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010117725729942321, AUC: 0.7119378333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994343360265096, AUC: 0.5376054166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010670217672983805, AUC: 0.6858831666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001028091828028361, AUC: 0.6873602500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010133273005485536, AUC: 0.6922785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010948459704717, AUC: 0.5656058333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010398535331090292, AUC: 0.6782773333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010180822610855102, AUC: 0.6980190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001007249097029368, AUC: 0.7145390833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006187200546264, AUC: 0.4808746666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010961087942123412, AUC: 0.5573257500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010935521920522053, AUC: 0.5838022500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087339440981547, AUC: 0.6159591666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100088596343994, AUC: 0.49039516666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010968905687332154, AUC: 0.5447114166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010949957768122355, AUC: 0.5811298333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010922112862269083, AUC: 0.6152561666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110038693745931, AUC: 0.5129754166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001093944231669108, AUC: 0.6117519166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001090210199356079, AUC: 0.6437488333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010832778215408324, AUC: 0.6602855833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994913975397745, AUC: 0.4912829166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010872755448023477, AUC: 0.6608446666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001077945590019226, AUC: 0.6877664166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010654464960098266, AUC: 0.6904363333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011052674849828085, AUC: 0.4773559166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010962444146474203, AUC: 0.5593838333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010929728349049887, AUC: 0.6158244166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001089497168858846, AUC: 0.6485754166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010928326845169067, AUC: 0.5817805833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010829600095748901, AUC: 0.6612614166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010757408936818442, AUC: 0.6685356666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010671913623809814, AUC: 0.6796116666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010956864356994628, AUC: 0.5627876666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010892361402511596, AUC: 0.6266145000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010810300509134928, AUC: 0.6451809166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010694435437520345, AUC: 0.6499782500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105512301127116, AUC: 0.3950855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00109817639986674, AUC: 0.5211429166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010924388964970906, AUC: 0.5959426666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001084126035372416, AUC: 0.6374721666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010958169301350912, AUC: 0.56052975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001082641879717509, AUC: 0.6530635833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010700990358988444, AUC: 0.6790653333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010590966542561849, AUC: 0.6887896666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010956270297368368, AUC: 0.5683148333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010892151991526286, AUC: 0.6085137500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010825005769729615, AUC: 0.6259185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010736865599950155, AUC: 0.6430104166666667\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011042052110036215, AUC: 0.47445875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010264458656311034, AUC: 0.6871260833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00100686909755071, AUC: 0.7113415000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009901551008224486, AUC: 0.727234\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011103670994440714, AUC: 0.3206058333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010133507649103801, AUC: 0.681817\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010078470309575398, AUC: 0.7050957499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010084237257639568, AUC: 0.7230279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011009645064671834, AUC: 0.48435758333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010037092765172323, AUC: 0.6946527499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000972671627998352, AUC: 0.7302672500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009687488079071045, AUC: 0.742565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011146032412846882, AUC: 0.39922541666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009936522046724956, AUC: 0.6980645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009849374294281007, AUC: 0.72684025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001001283903916677, AUC: 0.73475225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001109102725982666, AUC: 0.42704241666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009963614344596862, AUC: 0.70922875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009770647883415222, AUC: 0.7349008333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009489417672157288, AUC: 0.7539038333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010911259253819784, AUC: 0.6070571666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009896055857340495, AUC: 0.7192701666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009635964830716451, AUC: 0.7423000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009485488732655843, AUC: 0.754625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011051610708236695, AUC: 0.3954675833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010085143248240152, AUC: 0.70419375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009684146444002787, AUC: 0.740495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009217474857966105, AUC: 0.7666303333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010941660801569622, AUC: 0.5809459166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010020330945650737, AUC: 0.70547775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000995762864748637, AUC: 0.7232718333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009742741783459981, AUC: 0.7391618333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998607873916625, AUC: 0.5626500833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009986496369043987, AUC: 0.6947659999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000991542359193166, AUC: 0.7094563333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009862677057584127, AUC: 0.7267454999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011041675408681234, AUC: 0.40248975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010128312508265178, AUC: 0.6876679166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010084819396336874, AUC: 0.7091998333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010203626155853272, AUC: 0.7254941666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011023016373316447, AUC: 0.4143161666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010404238303502401, AUC: 0.6866729166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000997809906800588, AUC: 0.6952690833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009956242442131042, AUC: 0.7065826666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010945557753245035, AUC: 0.5684308333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010556970834732055, AUC: 0.6989744999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001001050313313802, AUC: 0.7023615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000981044034163157, AUC: 0.7186099166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010977689027786256, AUC: 0.5251201666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001031454841295878, AUC: 0.66299575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001024035374323527, AUC: 0.6764504166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010174167156219481, AUC: 0.6915666666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011043524344762166, AUC: 0.4670040833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010164989829063416, AUC: 0.6867853333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010016092459360759, AUC: 0.7146282500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009948590199152629, AUC: 0.7332957499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010999863147735595, AUC: 0.5103293333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001030396620432536, AUC: 0.67454325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001017722447713216, AUC: 0.6918776666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010111367106437684, AUC: 0.70615775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001107398509979248, AUC: 0.4634863333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001059143304824829, AUC: 0.6930666666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010217333634694418, AUC: 0.6970818333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000992731511592865, AUC: 0.7005079999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010917320648829141, AUC: 0.59844575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010236164728800456, AUC: 0.6845798333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001002890189488729, AUC: 0.6938713333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009970810810724894, AUC: 0.7054649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003119150797525, AUC: 0.47524625000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010366677840550741, AUC: 0.6673779999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010179561773935955, AUC: 0.6923238333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010092426538467407, AUC: 0.7079360833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011047364075978598, AUC: 0.3913238333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010538249413172404, AUC: 0.658947\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001024723251660665, AUC: 0.6755411666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010230513413747153, AUC: 0.689637\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001111425240834554, AUC: 0.38147916666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010610402822494507, AUC: 0.6845195000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010045130848884583, AUC: 0.6862330833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010007025202115377, AUC: 0.69793\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011027448574701946, AUC: 0.44836666666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010953339338302611, AUC: 0.5950059166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010892232259114584, AUC: 0.6470231666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010803961356480917, AUC: 0.6690821666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008984247843424, AUC: 0.47227733333333327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010914988517761231, AUC: 0.6210217499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001081612229347229, AUC: 0.6823481666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010682508150736492, AUC: 0.6966875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011011332670847575, AUC: 0.4786086666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010896191596984864, AUC: 0.64963275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010735963980356852, AUC: 0.6984812499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010495477914810182, AUC: 0.7028786666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010954999128977457, AUC: 0.5711128333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010889843702316284, AUC: 0.6572031666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010821066697438559, AUC: 0.6988549166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010737097263336182, AUC: 0.7180029166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011046305497487386, AUC: 0.4307064166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001080897609392802, AUC: 0.6622176666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010591391722361246, AUC: 0.6719175833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001039194663365682, AUC: 0.6733120833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998594760894776, AUC: 0.50886775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001089020848274231, AUC: 0.62468975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010793463786443074, AUC: 0.671714\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010667157173156739, AUC: 0.6854265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011007350285847982, AUC: 0.4879179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010820403099060058, AUC: 0.6235665833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010635017951329548, AUC: 0.6502256666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010470181703567504, AUC: 0.6582119166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011105584700902304, AUC: 0.38778191666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010977534850438436, AUC: 0.5251300833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088876763979594, AUC: 0.6480906666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010787332852681478, AUC: 0.6867980833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008881330490112, AUC: 0.45920391666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010937438011169433, AUC: 0.6211850833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010865768988927206, AUC: 0.68618525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010783092578252156, AUC: 0.7001897499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011027877330780029, AUC: 0.44225625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class triplet loss capped smote\n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "loss_caps = [1, 5, 10]\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = loss_cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                _, _ = train.train_triplet_capped_loss(epoch, train_loader_tripletloss_smote, network, optimizer, verbose=False, cap_calc=loss_fns.TripletLoss,loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"triplet_loss_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dbc0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80bb7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
