{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ad7e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  \n",
    "\n",
    "\n",
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops \n",
    "\n",
    "NUM_CLASSES = 10\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "          #   \"mean_40\", \"variance_40\",\n",
    "          #   \"mean_50\", \"variance_50\",\n",
    "             \"cap\", \"normalization\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c61a5021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 3 classes\n",
    "\n",
    "NUM_CLASSES_REDUCED = 3\n",
    "nums = (0, 3, 1)\n",
    "ratio = (200, 20, 1)\n",
    "\n",
    "norm=True\n",
    "\n",
    "if norm:\n",
    "    transform=torchvision.transforms.Compose([torchvision.transforms.Normalize(mean=[134.1855, 122.7346, 118.3749], std=[70.5125, 64.4848, 66.5604])])\n",
    "else:\n",
    "    transform=None\n",
    "\n",
    "    \n",
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "\n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums, transform=transform)\n",
    "targets = ratio_train_CIFAR10.labels \n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED, transform=transform)\n",
    "\n",
    "triplet_train_CIFAR10 = class_sampling.ForTripletLoss(reduced_train_CIFAR10, smote=False, transform=transform, num_classes=3)\n",
    "triplet_ratio_train_CIFAR10 = class_sampling.ForTripletLoss(ratio_train_CIFAR10, smote=False, transform=transform, num_classes=3)\n",
    "triplet_smote_train_CIFAR10 = class_sampling.ForTripletLoss(smote_train_CIFAR10, smote=True, transform=transform, num_classes=3)\n",
    "\n",
    "weight = 1. / class_count\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= max(class_count)\n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss = DataLoader(triplet_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_ratio = DataLoader(triplet_ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_smote = DataLoader(triplet_smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "170fce52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.006297228495279948, AUC: 0.40847066666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011013633012771607, AUC: 0.4996666666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100907325744629, AUC: 0.4998333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005379756291707, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034327227274576822, AUC: 0.5633294166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011028808355331421, AUC: 0.49883833333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102209726969401, AUC: 0.49883683333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101684848467509, AUC: 0.49950083333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001514667789141337, AUC: 0.4563873333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086326042811076, AUC: 0.5949351666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010733679135640462, AUC: 0.6530543333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010519079367319743, AUC: 0.6970986666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006923106988271078, AUC: 0.46537175000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098378618558248, AUC: 0.5143645000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097153385480245, AUC: 0.5229321666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096191167831421, AUC: 0.5352345833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003263075828552246, AUC: 0.509595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011015702883402506, AUC: 0.499519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005100806554158, AUC: 0.5045793333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010908829768498738, AUC: 0.5711784166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015284932454427084, AUC: 0.5907836666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010975186030069987, AUC: 0.5163181666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010881919860839843, AUC: 0.5874068333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010716219743092854, AUC: 0.64150325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017461446523666382, AUC: 0.4886918333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010996406475702921, AUC: 0.5004998333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994924704233805, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010993741353352866, AUC: 0.5000001666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004542219956715902, AUC: 0.42329666666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098918875058492, AUC: 0.49983383333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988046725591025, AUC: 0.49983433333333327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010987661679585775, AUC: 0.49983416666666675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014803661108016967, AUC: 0.5712355833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010887737274169922, AUC: 0.6021689166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010774553616841635, AUC: 0.6517348333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010646572907765707, AUC: 0.6852165833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005940155188242594, AUC: 0.4656658333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010577219724655152, AUC: 0.706546\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010352855126063028, AUC: 0.7490925833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010128463109334309, AUC: 0.7745410833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0042724491755167645, AUC: 0.520358\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010328739881515503, AUC: 0.7049486666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008789941867192587, AUC: 0.8205735000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000857577403386434, AUC: 0.8357318333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003657262722651164, AUC: 0.4698994166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010993303855260214, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010987322727839153, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098631739616394, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038829514980316163, AUC: 0.5011393333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009745399951934814, AUC: 0.7940036666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009059278170267741, AUC: 0.8341739166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008827355901400248, AUC: 0.8495634999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011502512296040853, AUC: 0.53530925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009794333775838217, AUC: 0.7919741666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009077478051185608, AUC: 0.83422375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008600973089536031, AUC: 0.8533787500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006377369244893392, AUC: 0.4455759166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010394285917282105, AUC: 0.6970361666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009406381845474244, AUC: 0.7967021666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008825643062591553, AUC: 0.8239488333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004981070995330811, AUC: 0.5043585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992831389109294, AUC: 0.5003335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010185734033584595, AUC: 0.7027511666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009866801897684733, AUC: 0.757758\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003450544754664103, AUC: 0.44614400000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010993530750274658, AUC: 0.5006658333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009930496414502462, AUC: 0.7367675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009753357569376627, AUC: 0.7439675833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002645992914835612, AUC: 0.6008601666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010644568999608358, AUC: 0.61661625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010501598119735719, AUC: 0.6375385000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010174054304758708, AUC: 0.6935996666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009119009971618653, AUC: 0.5799555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010225125551223755, AUC: 0.7421935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009311155279477437, AUC: 0.8189576666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008407000303268432, AUC: 0.85428\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022672084172566734, AUC: 0.5680725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010635302464167278, AUC: 0.6248856666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010320152441660564, AUC: 0.6754006666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000954870343208313, AUC: 0.7505833333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002106921037038167, AUC: 0.4656176666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986962715784708, AUC: 0.5120286666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010671189228693644, AUC: 0.6518783333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010254997809727986, AUC: 0.7228560833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004185671806335449, AUC: 0.4558903333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011010967095692952, AUC: 0.4991668333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010996074676513671, AUC: 0.4996666666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010990128914515178, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007150139490763346, AUC: 0.44471675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001016627311706543, AUC: 0.7278205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009709982673327128, AUC: 0.7898910833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009075331489245096, AUC: 0.831265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003530624310175578, AUC: 0.5114538333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002238194147745, AUC: 0.5003333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010993646383285522, AUC: 0.5041471666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010993322531382243, AUC: 0.5039978333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009243070284525553, AUC: 0.4814516666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010626388788223266, AUC: 0.68091475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001037128488222758, AUC: 0.7071172499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009790645043055216, AUC: 0.7775655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019522505998611451, AUC: 0.4920811666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010937068859736125, AUC: 0.5489676666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010515597661336262, AUC: 0.6744083333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010054901440938314, AUC: 0.7112215000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0063404574394226075, AUC: 0.41579733333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010987649361292521, AUC: 0.5004998333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010987801551818849, AUC: 0.5003333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010990970929463705, AUC: 0.5006661666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0064954557418823245, AUC: 0.4352373333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011009731690088907, AUC: 0.49966616666666663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099481463432312, AUC: 0.5044921666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001056458830833435, AUC: 0.6902779166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018277053833007813, AUC: 0.59760625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010308250188827515, AUC: 0.7428351666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009827314217885335, AUC: 0.7793081666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000900732696056366, AUC: 0.8463228333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004975475470225016, AUC: 0.403921\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010692356427510579, AUC: 0.6490821666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001010363022486369, AUC: 0.7264350833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009852001667022706, AUC: 0.7313525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class normal\n",
    "\n",
    "learning_rates = [1e-4, 1e-3, 5e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 3, nums, (1, 1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b97f0654",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47bf6798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0034308363596598306, AUC: 0.5116964166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027943440278371174, AUC: 0.5888170833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002969173272450765, AUC: 0.6337191666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026788889567057293, AUC: 0.6404518333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010200992584228515, AUC: 0.5203337499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014580308596293132, AUC: 0.5384295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013363298972447714, AUC: 0.628893\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014429691632588705, AUC: 0.66305925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00281195060412089, AUC: 0.5770663333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00220535675684611, AUC: 0.6147014166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023105909824371338, AUC: 0.6349646666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024558690388997396, AUC: 0.6477726666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036946707566579183, AUC: 0.5016439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020329121748606364, AUC: 0.5190675833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021662095387776693, AUC: 0.5582425833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021678760051727294, AUC: 0.5794676666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034959588050842284, AUC: 0.4637625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001862504760424296, AUC: 0.5398455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002232117811838786, AUC: 0.5742095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002289778470993042, AUC: 0.5907150833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007764035542805989, AUC: 0.5431356666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020873581171035765, AUC: 0.6002964999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002313773473103841, AUC: 0.6162718333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002377427419026693, AUC: 0.622216\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003627910216649373, AUC: 0.5251078333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033077236811319987, AUC: 0.565592\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002917507330576579, AUC: 0.5872778333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025567999680836994, AUC: 0.5874988333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00490424664815267, AUC: 0.5304025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035042344729105633, AUC: 0.5979710833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003249402681986491, AUC: 0.6243990833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003062591314315796, AUC: 0.6421066666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005183787186940511, AUC: 0.42397725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001914297103881836, AUC: 0.6559733333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011413304805755616, AUC: 0.47809216666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002238653341929118, AUC: 0.6753936666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037974352836608887, AUC: 0.4632503333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002381080786387126, AUC: 0.5164833333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027282633781433104, AUC: 0.5698009166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027364304065704346, AUC: 0.5900610833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006711633682250977, AUC: 0.4382114166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002523393710454305, AUC: 0.6479104166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027551825046539307, AUC: 0.6731781666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002452608346939087, AUC: 0.69686075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0066550809542338055, AUC: 0.4167079166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002071466048558553, AUC: 0.6301158333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019424758354822794, AUC: 0.64915375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014425463279088338, AUC: 0.6691744166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037723589738210043, AUC: 0.5746255833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027357101440429687, AUC: 0.6333795833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002082867383956909, AUC: 0.6407938333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027454819679260255, AUC: 0.651604\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009529210408528646, AUC: 0.40651675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022218425273895265, AUC: 0.6634549166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011856011946996053, AUC: 0.6630182499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015599876244862874, AUC: 0.7215625833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007337440013885498, AUC: 0.5611281666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002013138254483541, AUC: 0.64862875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023135058879852296, AUC: 0.6619849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022291186650594077, AUC: 0.6735233333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007358764012654622, AUC: 0.38435250000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023185502688090006, AUC: 0.6354615833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002006119648615519, AUC: 0.6422615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002321414073308309, AUC: 0.6534398333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005222848256429036, AUC: 0.38029925000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002190793514251709, AUC: 0.6421753333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001842764377593994, AUC: 0.6522376666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001895538608233134, AUC: 0.6578150833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003462463061014811, AUC: 0.47331166666666674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026403398513793947, AUC: 0.6471071666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002970578908920288, AUC: 0.6577601666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025200313727060954, AUC: 0.6645823333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005602498690287272, AUC: 0.4549924166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001683531125386556, AUC: 0.6298073333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020471984148025513, AUC: 0.6563679166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018263955116271972, AUC: 0.6656934166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0049307347933451335, AUC: 0.37020274999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022954068183898926, AUC: 0.6313905833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022845780849456787, AUC: 0.6572198333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001922422448794047, AUC: 0.6736795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002194918473561605, AUC: 0.4976156666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025892218748728435, AUC: 0.5612221666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002077325661977132, AUC: 0.6004983333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001872358520825704, AUC: 0.6296705833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002398321787516276, AUC: 0.6324755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025626872380574545, AUC: 0.6382580833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002319868882497152, AUC: 0.6683896666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002151050567626953, AUC: 0.6829635833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008817579905192057, AUC: 0.42869450000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024319469928741457, AUC: 0.6134573333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00222760804494222, AUC: 0.6352825833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002354779561360677, AUC: 0.6500789166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009816247622172038, AUC: 0.45375525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026585988998413085, AUC: 0.60148975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021614089806874593, AUC: 0.6291701666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001969052831331889, AUC: 0.6423525833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005994597276051839, AUC: 0.6279246666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002619588851928711, AUC: 0.6193335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002834444761276245, AUC: 0.6413831666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029879539012908937, AUC: 0.6441461666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013802298863728841, AUC: 0.6060741666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028906663258870444, AUC: 0.6128289166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022290199597676595, AUC: 0.6439094166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002456218401590983, AUC: 0.6545953333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004157656192779541, AUC: 0.450981\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00235213033358256, AUC: 0.6502761666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020782477855682373, AUC: 0.6680915000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002125028928120931, AUC: 0.6857611666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0126146240234375, AUC: 0.44705791666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027431639035542807, AUC: 0.5774883333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024121209780375162, AUC: 0.6503111666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021468427975972495, AUC: 0.6578741666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012639744758605957, AUC: 0.48564525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031431158383687335, AUC: 0.6296049166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029019534587860106, AUC: 0.6479423333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027327394485473634, AUC: 0.6520215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005275733153025309, AUC: 0.5535933333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022074038982391356, AUC: 0.5999976666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002114061117172241, AUC: 0.6252779999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001908456047375997, AUC: 0.6395935833333334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class ratio\n",
    "\n",
    "learning_rates =  [1e-4, 1e-3, 5e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "052e7d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8774b790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.007841265678405762, AUC: 0.4284085833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010583848158518473, AUC: 0.687993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002482561747233073, AUC: 0.7706633333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003099634488423665, AUC: 0.7672075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013887118021647136, AUC: 0.42574649999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010073497692743938, AUC: 0.7167925833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014987698396046957, AUC: 0.7083903333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026214383443196616, AUC: 0.7372626666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008022495905558268, AUC: 0.6410836666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992498795191446, AUC: 0.5000011666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001090564489364624, AUC: 0.6250315833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001974099357922872, AUC: 0.7543869166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011487732569376628, AUC: 0.5060690833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00105541463692983, AUC: 0.6406730833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00183162256081899, AUC: 0.6706220000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021299949487050373, AUC: 0.7201981666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008428269386291504, AUC: 0.575314\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001089770515759786, AUC: 0.6577935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014849545955657958, AUC: 0.64461925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002427160978317261, AUC: 0.7000744999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006212652524312337, AUC: 0.37725191666666663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010479257901509602, AUC: 0.6870074166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015368237495422363, AUC: 0.7440253333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026358659267425536, AUC: 0.7465800833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011559378941853842, AUC: 0.6054430833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014945395787556966, AUC: 0.6980699166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002315357287724813, AUC: 0.711\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024366071224212645, AUC: 0.715182\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003336264928181966, AUC: 0.4939436666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010768072605133057, AUC: 0.6772988333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020180659294128417, AUC: 0.6884395833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003240294377009074, AUC: 0.6891169166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004101686795552572, AUC: 0.5072671666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010187785625457764, AUC: 0.7370886666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013917672634124756, AUC: 0.7832421666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002109833240509033, AUC: 0.7816209999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007042920748392741, AUC: 0.5375656666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010516610145568847, AUC: 0.6977634166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014505608876546224, AUC: 0.6571933333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020196786721547446, AUC: 0.66146625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006075712045033773, AUC: 0.4808753333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010145072142283122, AUC: 0.7201125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010939369201660156, AUC: 0.7821011666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013048724333445231, AUC: 0.7614496666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018570766051610311, AUC: 0.5096893333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010054115653038026, AUC: 0.7432445833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012481475273768106, AUC: 0.7760183333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016504865090052286, AUC: 0.7696996666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003723547140757243, AUC: 0.5100569166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00098207821448644, AUC: 0.7562646666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010260080893834433, AUC: 0.7301066666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001062218427658081, AUC: 0.7549208333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00436029322942098, AUC: 0.405963\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087259570757548, AUC: 0.58788225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010419190724690755, AUC: 0.6607856666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00118052875995636, AUC: 0.65674375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005057913780212402, AUC: 0.5047628333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010434755086898805, AUC: 0.6920128333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012485949595769247, AUC: 0.6769989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014870481093724568, AUC: 0.6960405833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008403849919637045, AUC: 0.49292108333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010508048534393312, AUC: 0.6701651666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011004396279652912, AUC: 0.7695978333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019320346117019653, AUC: 0.7795589166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002745992104212443, AUC: 0.5174126666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010460313161214192, AUC: 0.6823106666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010764415661493937, AUC: 0.6689515833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014176646073659261, AUC: 0.6776689166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009171319643656414, AUC: 0.5868734166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001056402325630188, AUC: 0.6445124999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001020784060160319, AUC: 0.6938760833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001175164779027303, AUC: 0.7157953333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034493844509124755, AUC: 0.4152708333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001049614389737447, AUC: 0.66208225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014743905067443849, AUC: 0.7554795000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001921565334002177, AUC: 0.7586935833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00219153618812561, AUC: 0.5093845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010995538632074993, AUC: 0.5044576666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010580138365427654, AUC: 0.6615224999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011588475306828817, AUC: 0.6985505000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036074090003967284, AUC: 0.47927575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001080019990603129, AUC: 0.6297763333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010680156946182252, AUC: 0.6449248333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001064011295636495, AUC: 0.6477215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008833540280659994, AUC: 0.5050685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010872312784194946, AUC: 0.5848355000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010638678471247356, AUC: 0.6489484999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010350225369135538, AUC: 0.6969669166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006801804224650065, AUC: 0.5327739166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009975239038467407, AUC: 0.7298506666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009574913183848064, AUC: 0.7683028333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009563260078430175, AUC: 0.7738135833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015160040458043417, AUC: 0.5544296666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009993977348009746, AUC: 0.7214424999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009566624959309896, AUC: 0.7548361666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009393988649050394, AUC: 0.771043\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005451770464579265, AUC: 0.56154775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010499190092086792, AUC: 0.6513535833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010159402290980022, AUC: 0.7034191666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010205288728078207, AUC: 0.7356769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005784485816955566, AUC: 0.42929875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001038422425587972, AUC: 0.6579865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010407910744349162, AUC: 0.6938915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001001028796037038, AUC: 0.7203486666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015877811908721923, AUC: 0.49918016666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001075486938158671, AUC: 0.6168399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010620100100835164, AUC: 0.6518606666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010522518952687581, AUC: 0.6636858333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032916433811187745, AUC: 0.6132468333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010892879168192546, AUC: 0.5650898333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010588326851526896, AUC: 0.64867125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010364394585291544, AUC: 0.6661625833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002506932179133097, AUC: 0.4573435833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003915866216025, AUC: 0.520628\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010911309321721394, AUC: 0.5647880833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010802791515986125, AUC: 0.5945723333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028264092604319254, AUC: 0.48608616666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010993447303771972, AUC: 0.5144148333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001089823842048645, AUC: 0.5997963333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010646794239679972, AUC: 0.66343575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class oversampled \n",
    "\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "923429dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8aa19a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0031121970812479655, AUC: 0.5074040833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001012291153271993, AUC: 0.6870238333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010018948117891948, AUC: 0.7024875833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009718345602353414, AUC: 0.7123998333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036430826981862386, AUC: 0.44375975000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010793885389963785, AUC: 0.6463530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001073374072710673, AUC: 0.6677694166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010547064542770386, AUC: 0.6988854999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003707357088724772, AUC: 0.5160290833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010790506998697917, AUC: 0.6308525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001070495843887329, AUC: 0.6627665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010717769861221314, AUC: 0.6658920833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004282213370005289, AUC: 0.5589395833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010035297473271689, AUC: 0.7068214999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010145838459332784, AUC: 0.7175729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009785688916842143, AUC: 0.7204093333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002940722624460856, AUC: 0.5217644166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991450945536296, AUC: 0.517135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010890431801478069, AUC: 0.5983115833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010793478091557821, AUC: 0.638576\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021444389820098877, AUC: 0.5433676666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010612613757451375, AUC: 0.6392782499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010493971506754557, AUC: 0.6625989999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001041617751121521, AUC: 0.6793768333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01861020024617513, AUC: 0.6044510000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011010262171427408, AUC: 0.5006336666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011004323561986287, AUC: 0.49659633333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002333164215087, AUC: 0.49843066666666686\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010883930206298829, AUC: 0.5094754166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008155345916747, AUC: 0.5011598333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010686599413553873, AUC: 0.6687179166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010907162427902222, AUC: 0.6102465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003932813326517741, AUC: 0.44797416666666673\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001018357237180074, AUC: 0.7030193333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010398705403010051, AUC: 0.7008623333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009870229164759318, AUC: 0.7156185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020563451449076334, AUC: 0.48492683333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011004976828893026, AUC: 0.4998333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100160280863444, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998852252960204, AUC: 0.4998333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0055213239987691246, AUC: 0.4626996666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001065146048863729, AUC: 0.6689868333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010641868114471436, AUC: 0.6852602500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010875749190648396, AUC: 0.6850888333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007911431630452473, AUC: 0.49496333333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001066536784172058, AUC: 0.6684894166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010475507179896037, AUC: 0.6952090000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088719685872396, AUC: 0.6527231666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005510725498199463, AUC: 0.41299083333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010579142570495605, AUC: 0.6935935833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010423096021016439, AUC: 0.7140761666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009523568749427796, AUC: 0.7397081666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018450146516164143, AUC: 0.3834863333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010914470752080281, AUC: 0.594652\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010686133702596029, AUC: 0.6883301666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010915136337280274, AUC: 0.6791869166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020358848571777346, AUC: 0.5571706666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010836608409881593, AUC: 0.6116576666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010801368951797485, AUC: 0.6420436666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001090671141942342, AUC: 0.652571\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0167697385152181, AUC: 0.44725183333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001078627347946167, AUC: 0.63348875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010430614153544109, AUC: 0.70341475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001084995945294698, AUC: 0.6836400833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002794758558273315, AUC: 0.5301900833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010808715422948201, AUC: 0.6222375000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001076610008875529, AUC: 0.6281528333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010411550998687745, AUC: 0.6728322499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014227446238199869, AUC: 0.41658375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010990206797917685, AUC: 0.49933350000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010886128743489583, AUC: 0.5829809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882899363835652, AUC: 0.6212261666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007477245648701986, AUC: 0.36442949999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011254422664642334, AUC: 0.6914245833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009551324248313904, AUC: 0.7387839166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009332788785298665, AUC: 0.7435279166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022321158250172934, AUC: 0.6244033333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010183146993319194, AUC: 0.71225225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00113379967212677, AUC: 0.7253745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011316871643066407, AUC: 0.72890625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006870575269063314, AUC: 0.43228099999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010614458719889323, AUC: 0.67280175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011478854815165202, AUC: 0.6046513333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011314289967219034, AUC: 0.6970867500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003651854912439982, AUC: 0.42268424999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010610081354777018, AUC: 0.6747791666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011318807601928711, AUC: 0.5804538333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011759847005208333, AUC: 0.6310371666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021680224736531574, AUC: 0.46810383333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001021060864130656, AUC: 0.70736175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010559933185577393, AUC: 0.7096395833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010819374720255535, AUC: 0.7290716666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027989369710286458, AUC: 0.472061\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010499072074890137, AUC: 0.7038229166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011101412773132325, AUC: 0.6852051666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001129215399424235, AUC: 0.70255675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006906053066253662, AUC: 0.5699600833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010914477904637656, AUC: 0.6044792500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010931334892908733, AUC: 0.6758046666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011157235701878866, AUC: 0.6927771666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007799578507741292, AUC: 0.5429644166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001020263433456421, AUC: 0.73303175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010723970731099446, AUC: 0.7232234166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010961862007776897, AUC: 0.7487546666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012885144869486491, AUC: 0.5730320833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010503378311793009, AUC: 0.6948004999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010487880706787109, AUC: 0.7175010833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145840565363566, AUC: 0.6989608333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0070607115427653, AUC: 0.5586720833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010382163921991985, AUC: 0.7328125000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012454922199249267, AUC: 0.7349078333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00105166757106781, AUC: 0.7721023333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038500086466471354, AUC: 0.45271791666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010979156891504924, AUC: 0.5611479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011147896448771158, AUC: 0.6256155833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012141537268956502, AUC: 0.5294466666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003323823928833008, AUC: 0.448623\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010344678560892742, AUC: 0.6952583333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010767376025517781, AUC: 0.6873416666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010827142794926962, AUC: 0.7059709166666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class SMOTE\n",
    "\n",
    "learning_rates = [1e-4, 5e-4, 1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e34ddade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0dc17d94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004110835552215576, AUC: 0.5681081666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002226279099782308, AUC: 0.6757903333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002117084503173828, AUC: 0.6495369999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017974026203155518, AUC: 0.6927331666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031340863704681396, AUC: 0.512281\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00269014310836792, AUC: 0.6720663333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002087400476137797, AUC: 0.6719768333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021087907155354817, AUC: 0.7046515000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004356918017069499, AUC: 0.4547190833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002342305024464925, AUC: 0.6622166666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020787399609883627, AUC: 0.6480883333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017181493838628133, AUC: 0.6838915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00953905709584554, AUC: 0.4645290833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023876633644104002, AUC: 0.6692125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022252449989318846, AUC: 0.6947236666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002465261459350586, AUC: 0.7038990833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004715966065724691, AUC: 0.53898175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002578310966491699, AUC: 0.6952795833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002414023001988729, AUC: 0.6950270833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023896484375, AUC: 0.7011184999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005681670824686686, AUC: 0.5025104166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002848108450571696, AUC: 0.66790725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028154688676198325, AUC: 0.6950079166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002280729611714681, AUC: 0.7113286666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005386822541554769, AUC: 0.554916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002445377508799235, AUC: 0.6759821666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031411839326222738, AUC: 0.664832\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028916955788930256, AUC: 0.6752206666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008012901465098064, AUC: 0.6529544166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022342080275217693, AUC: 0.6722935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00203266183535258, AUC: 0.7023285833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001824159860610962, AUC: 0.7097643333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015477877060572306, AUC: 0.4651630833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018213262955347697, AUC: 0.6713621666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016354876359303792, AUC: 0.7221593333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001671494960784912, AUC: 0.7331618333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003225452184677124, AUC: 0.3600230833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001974823236465454, AUC: 0.6654656666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026550543308258056, AUC: 0.6927519166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020108642578125, AUC: 0.7079781666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004451344966888428, AUC: 0.44722766666666663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016763635476430257, AUC: 0.6787905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017509211699167887, AUC: 0.6987114166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001784122069676717, AUC: 0.6953976666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008177823384602864, AUC: 0.45335908333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025450301965077716, AUC: 0.6908453333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025085124174753825, AUC: 0.680059\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002380187114079793, AUC: 0.6962663333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006031959533691406, AUC: 0.49072275000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002728323459625244, AUC: 0.6526719166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024287723700205485, AUC: 0.6866840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021046314239501953, AUC: 0.6973718333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006942504405975341, AUC: 0.5354949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002082485755284627, AUC: 0.6825009999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019012845754623412, AUC: 0.69254125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018945773442586263, AUC: 0.6997164166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008070012092590333, AUC: 0.44192824999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002267015775044759, AUC: 0.6624026666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021089860598246255, AUC: 0.6906495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018992329835891724, AUC: 0.7070256666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008444100697835286, AUC: 0.5446301666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016644502878189087, AUC: 0.6536446666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001873444120089213, AUC: 0.6740753333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017901968558629354, AUC: 0.6742266666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002932355801264445, AUC: 0.47042066666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026738557815551758, AUC: 0.6724836666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002958200772603353, AUC: 0.6865753333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002587732235590617, AUC: 0.70271325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006756103674570719, AUC: 0.5545322500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002858376423517863, AUC: 0.6702665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023847698370615643, AUC: 0.6833048333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029360285600026448, AUC: 0.6840219166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015860133171081543, AUC: 0.484807\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002326175530751546, AUC: 0.6529910833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022744293212890623, AUC: 0.6434226666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001984721263249715, AUC: 0.6638745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008300702095031738, AUC: 0.63582275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023900620142618817, AUC: 0.7063321666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024423062801361083, AUC: 0.6978840833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022148747444152832, AUC: 0.6984039166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021076629161834717, AUC: 0.46942966666666663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002588709115982056, AUC: 0.5847558333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002618545691172282, AUC: 0.61975575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024639347394307455, AUC: 0.6443248333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008586212793986003, AUC: 0.6012085833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022898859182993573, AUC: 0.6337905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023615378538767496, AUC: 0.6785194999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002682613213857015, AUC: 0.6881738333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007514012495676676, AUC: 0.4918760833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001941166639328003, AUC: 0.6422711666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020417344570159914, AUC: 0.6509871666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020610854625701903, AUC: 0.6540328333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005721577326456706, AUC: 0.5987819166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017042430639266969, AUC: 0.49018508333333327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001678192138671875, AUC: 0.5739445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017786046663920085, AUC: 0.6360444166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008135049343109131, AUC: 0.526547\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003168341318766276, AUC: 0.6073153333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002901183764139811, AUC: 0.6418500833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003155737241109212, AUC: 0.65070575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033777721722920736, AUC: 0.4582785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002833363691965739, AUC: 0.5419290000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026580209732055666, AUC: 0.6187830833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024322959582010906, AUC: 0.6351916666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008413151741027832, AUC: 0.4508455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028990957736968995, AUC: 0.6594768333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002738965034484863, AUC: 0.6780881666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002713566700617472, AUC: 0.6863079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004344799836476644, AUC: 0.45817258333333327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019054909149805704, AUC: 0.5730030833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018798662821451822, AUC: 0.6242458333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001843912720680237, AUC: 0.6381053333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024405748049418133, AUC: 0.5472125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002098490079243978, AUC: 0.5469956666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023191200097401935, AUC: 0.6022994166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025459251403808594, AUC: 0.6521945000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032469909191131592, AUC: 0.4322166666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016132819652557374, AUC: 0.5932534166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017870930433273315, AUC: 0.6372181666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017878391742706299, AUC: 0.6442014166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011866551717122396, AUC: 0.4549335000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010821378628412882, AUC: 0.6835279999999999\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011222988367080688, AUC: 0.6803429166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011310815811157227, AUC: 0.7075522500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00996462885538737, AUC: 0.6086563333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101412057876587, AUC: 0.5254231666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097700317700704, AUC: 0.6515381666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011688193480173747, AUC: 0.6121513333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003925378719965617, AUC: 0.569997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989242792129516, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098648428916931, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986142953236897, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003779754082361857, AUC: 0.5326775833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010660351912180582, AUC: 0.6873114166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010976146856943767, AUC: 0.6920904999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010330626169840495, AUC: 0.7241146666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009398465474446614, AUC: 0.43843733333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102439006169637, AUC: 0.5054965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010733975172042846, AUC: 0.7015405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011106356779734294, AUC: 0.7170985833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002359842300415039, AUC: 0.5027305000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010937001307805378, AUC: 0.5825655833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010913424491882323, AUC: 0.6529636666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001141235629717509, AUC: 0.6459775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015958029747009276, AUC: 0.3979914166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009705840150515239, AUC: 0.7281080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010134087403615316, AUC: 0.7335568333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010064732631047566, AUC: 0.7206869166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023503469626108804, AUC: 0.5005238333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010819711287816365, AUC: 0.6356518333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010876756111780803, AUC: 0.6724600833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001157651662826538, AUC: 0.6520244166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003197563886642456, AUC: 0.5768045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011059982379277547, AUC: 0.5965480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011300813357035318, AUC: 0.64130925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011540943384170533, AUC: 0.6772092500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0046701453526814775, AUC: 0.5249968333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991252263387043, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988645950953165, AUC: 0.5001666666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010996812582015991, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014450246493021648, AUC: 0.4819103333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010432709852854411, AUC: 0.691707\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010412681102752687, AUC: 0.6930941666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010423659880956014, AUC: 0.6962926666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00721587340037028, AUC: 0.6160760000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012216061751047771, AUC: 0.6775493333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012686595916748047, AUC: 0.7039925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001020506501197815, AUC: 0.7323126666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005776538530985515, AUC: 0.54327175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000976769725481669, AUC: 0.7242841666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009643754363059997, AUC: 0.7342539166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011340717474619547, AUC: 0.7297864166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022555378278096517, AUC: 0.5438744166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099877913792928, AUC: 0.4996665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010777326424916585, AUC: 0.6445773333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001084529439608256, AUC: 0.6449491666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013745811462402344, AUC: 0.50310475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010717119773228964, AUC: 0.6747450833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010605779886245727, AUC: 0.6937990833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010948075453440348, AUC: 0.6715975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004417345682779948, AUC: 0.5128273333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010846527020136516, AUC: 0.6346206666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00104364542166392, AUC: 0.71045725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010079725980758666, AUC: 0.7164744166666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008162604649861654, AUC: 0.4878123333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010932400226593018, AUC: 0.5387068333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010654444694519042, AUC: 0.6632506666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010355210701624552, AUC: 0.6877885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009510751088460287, AUC: 0.53538375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010657296180725097, AUC: 0.6798443333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010518227418263753, AUC: 0.7041725833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001026487946510315, AUC: 0.7089951666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010410000483194987, AUC: 0.5250319166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010867557525634766, AUC: 0.6258710000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010469815731048585, AUC: 0.6963064166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010262608528137207, AUC: 0.6971351666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011590937614440917, AUC: 0.42424449999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010860188007354736, AUC: 0.6123161666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010654761393864948, AUC: 0.6949376666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001041830062866211, AUC: 0.7071917499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035850100517272948, AUC: 0.533943\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010770697991053263, AUC: 0.6390025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010795340935389201, AUC: 0.6505453333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010689385732014973, AUC: 0.6686328333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0181281369527181, AUC: 0.4535283333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094977100690206, AUC: 0.5549733333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010818959871927898, AUC: 0.6205498333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010813297430674236, AUC: 0.633561\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006329846223195394, AUC: 0.5566661666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011013060808181762, AUC: 0.5021631666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010736199617385864, AUC: 0.6456429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010620916684468587, AUC: 0.6752683333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007995837688446045, AUC: 0.5438926666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010946450233459473, AUC: 0.5940099166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087837020556132, AUC: 0.6305156666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001081147313117981, AUC: 0.6560855833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007241947968800863, AUC: 0.53001725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010538411537806193, AUC: 0.6683751666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010517477989196778, AUC: 0.6808704999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010514487822850546, AUC: 0.6828803333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002510809262593587, AUC: 0.3797595833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010945979356765747, AUC: 0.5621258333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010869006315867107, AUC: 0.619006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010804337263107299, AUC: 0.6451030833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006681114673614502, AUC: 0.46715550000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003953615824381, AUC: 0.5003323333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002916892369587, AUC: 0.49999949999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011004149119059245, AUC: 0.5001645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004039362907409668, AUC: 0.42622049999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010981416304906208, AUC: 0.5173\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010856227080027262, AUC: 0.6020275833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010825082461039224, AUC: 0.6296715833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022988692919413247, AUC: 0.4674024166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001083946665128072, AUC: 0.6107300833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010760523875554403, AUC: 0.6520838333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010677473147710165, AUC: 0.6724049166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007605107943216959, AUC: 0.39409725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010709348519643148, AUC: 0.6639195833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010664156675338746, AUC: 0.673903\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010531865358352661, AUC: 0.6964594166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001966240922609965, AUC: 0.4818415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010895986159642538, AUC: 0.6230591666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010313677787780761, AUC: 0.7154505833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001115052103996277, AUC: 0.6958165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034882008234659832, AUC: 0.4660751666666667\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010924318631490072, AUC: 0.63493825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011097737153371175, AUC: 0.6636036666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010841543674468994, AUC: 0.6970113333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032196237246195475, AUC: 0.410906\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010428651968638102, AUC: 0.7286165000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011829702854156494, AUC: 0.7313481666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013428943554560343, AUC: 0.7375265833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00943775240580241, AUC: 0.43502641666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001110884428024292, AUC: 0.56527575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010660007397333782, AUC: 0.6873901666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001128097176551819, AUC: 0.6843335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010154059727986654, AUC: 0.4283825833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010519113540649414, AUC: 0.7015693333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011187071800231934, AUC: 0.6874246666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011560682853062948, AUC: 0.7038700833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005124370416005453, AUC: 0.5105754166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010990891456604003, AUC: 0.5028336666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001066487232844035, AUC: 0.6966136666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011312641302744547, AUC: 0.6597786666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011269169489542643, AUC: 0.48480283333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098641316095988, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098674972852071, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005578835805256, AUC: 0.5000003333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004199995835622152, AUC: 0.5055989166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010409730275472006, AUC: 0.69931825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101622740427653, AUC: 0.6779686666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010658724705378214, AUC: 0.7132795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00303213628133138, AUC: 0.5969111666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010276695887247722, AUC: 0.706591\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010811389684677124, AUC: 0.6917836666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011428776184717814, AUC: 0.6848545000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006660881360371907, AUC: 0.5546274999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010963438351949057, AUC: 0.6367171666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010807211399078368, AUC: 0.7111602499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011252032915751138, AUC: 0.7068466666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004923964500427246, AUC: 0.48362649999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010822487274805704, AUC: 0.6331105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010508964061737062, AUC: 0.67971075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011308683156967163, AUC: 0.5677698333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00509737777709961, AUC: 0.562183\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010610513289769491, AUC: 0.6759085000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010778178373972574, AUC: 0.6686056666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010460286140441895, AUC: 0.7153345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035063997904459633, AUC: 0.6487780833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010982213020324708, AUC: 0.5056623333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010619305769602458, AUC: 0.6640923333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010799269278844198, AUC: 0.7015836666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010668930371602376, AUC: 0.6661399166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010795690615971883, AUC: 0.6461702500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010567970673243205, AUC: 0.6933805833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011179367701212564, AUC: 0.600816\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007365250905354817, AUC: 0.47234491666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994366804758708, AUC: 0.4996666666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001091023604075114, AUC: 0.5942101666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100967764854431, AUC: 0.6012303333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005005316893259684, AUC: 0.5369098333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010676839351654052, AUC: 0.6810929166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000995707631111145, AUC: 0.7142161666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009931413133939108, AUC: 0.71830075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032070152759552002, AUC: 0.4746698333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010728015104929606, AUC: 0.6756124166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010842804511388142, AUC: 0.6371221666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011153852542241415, AUC: 0.5512168333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007860821723937988, AUC: 0.49259175000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100125789642334, AUC: 0.5127556666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010637391408284504, AUC: 0.6731763333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010913126468658446, AUC: 0.6484733333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003217757781346639, AUC: 0.47592799999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010527263085047403, AUC: 0.7024800833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010535397132237752, AUC: 0.6976338333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010493046045303345, AUC: 0.6977521666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004165691057840983, AUC: 0.41828125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010813964207967122, AUC: 0.6340149166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010838215351104737, AUC: 0.656277\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010902642409006754, AUC: 0.6627761666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01225475279490153, AUC: 0.6201785833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010556480884552002, AUC: 0.6616871666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010185317993164063, AUC: 0.6872900833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010395210186640422, AUC: 0.69690375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004590764204661052, AUC: 0.5515414166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984695355097452, AUC: 0.50893275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010928800503412883, AUC: 0.5826568333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010850236415863037, AUC: 0.6259383333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006153896649678548, AUC: 0.365332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011017595529556274, AUC: 0.5044885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998871326446534, AUC: 0.5113346666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010912100474039714, AUC: 0.5555954166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002291819095611572, AUC: 0.4909115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010711674292882283, AUC: 0.67162225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010613499085108439, AUC: 0.6919601666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00106001611550649, AUC: 0.6928323333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037088717619578044, AUC: 0.5423472500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010462452967961629, AUC: 0.6696683333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000980541507403056, AUC: 0.7058038333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009771384596824646, AUC: 0.71690275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004688516457875569, AUC: 0.5176455833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010718711217244466, AUC: 0.6638748333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010648799339930217, AUC: 0.6831713333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010746919711430868, AUC: 0.6744318333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017307077248891194, AUC: 0.4873056666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001090986172358195, AUC: 0.6059585833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010742340882619222, AUC: 0.6666094166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010603753725687663, AUC: 0.6899831666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027217761675516765, AUC: 0.50183875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010896032651265461, AUC: 0.5826525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010806103150049846, AUC: 0.6473889166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010678505500157675, AUC: 0.6810316666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002057839234670003, AUC: 0.42292383333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010877179304758708, AUC: 0.6239992499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010861615737279256, AUC: 0.6405379166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010830237865447998, AUC: 0.6519719166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023366736729939778, AUC: 0.503982\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010790102084477743, AUC: 0.6453914166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010775025288263956, AUC: 0.6549014166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001069311777750651, AUC: 0.6811794999999999\n",
      "\n",
      "[['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.5074186083333333, 0.005629158279609793, 0.6727576166666667, 7.29852703933336e-05, 0.6836432666666665, 0.0005214477901511106, 0.7023747416666666, 0.00022950415921729194, 1, False], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.5058945583333333, 0.0033850328327889576, 0.67229295, 0.00028565476934611057, 0.6833907416666668, 0.00022975203193534597, 0.6919018166666666, 0.00016701884934694331, 1, False], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0001, 0.5034569, 0.003431681098723333, 0.5872975916666666, 0.0024153896281242366, 0.6325691666666666, 0.0009407010758583313, 0.6529282583333333, 0.0003332702327464578, 1, False], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.5107748833333334, 0.00393708274780722, 0.59446325, 0.0066784857913527815, 0.6425968583333332, 0.005732356678715345, 0.6456814916666667, 0.0065310753849200705, 5, False], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.5173537083333334, 0.0022526992240934052, 0.6359311083333334, 0.004476537728588958, 0.6938841583333332, 0.0005489597358617357, 0.6992523416666667, 0.0006338158390714579, 5, False], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0001, 0.4752682666666666, 0.0036491741141844447, 0.5812931916666667, 0.003674047351020904, 0.6275144249999999, 0.002350103590647847, 0.6460231583333332, 0.002776426888178403, 5, False], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.4874747499999999, 0.0029950247461652773, 0.6298919083333334, 0.00628966544519646, 0.67627435, 0.0037923109600122218, 0.6783317666666666, 0.003907020212358053, 10, False], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.5231453083333333, 0.005859620823100072, 0.6166474250000001, 0.005674915752845066, 0.6678425000000001, 0.001037340569418056, 0.6465253583333332, 0.0035237023797139617, 10, False], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0001, 0.5004006583333334, 0.0043541650395367395, 0.6138275583333332, 0.0036345186368584017, 0.6471654583333333, 0.003144468695657289, 0.6666770666666666, 0.00193550742399, 10, False]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f5ecca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "294d600b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0053888258934020996, AUC: 0.5083743333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011163065830866496, AUC: 0.5555713333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011519194841384888, AUC: 0.6339973333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012265472412109374, AUC: 0.6482106666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004785251299540202, AUC: 0.45894758333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010515480041503906, AUC: 0.6808616666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097139040629069, AUC: 0.6735478333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001158262848854065, AUC: 0.6645963333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002081297715504964, AUC: 0.5468329166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992099046707154, AUC: 0.6545398333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001181641697883606, AUC: 0.6227119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012087234656016032, AUC: 0.6747756666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029571340878804526, AUC: 0.42823191666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010765260457992554, AUC: 0.6793964166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011122363011042277, AUC: 0.6777794166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011849076747894288, AUC: 0.6750039166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020178468624750773, AUC: 0.5219041666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001072571277618408, AUC: 0.6878450833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011367605924606323, AUC: 0.6763965000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011956455707550048, AUC: 0.6798735833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0066022364298502605, AUC: 0.5727894166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103708823521932, AUC: 0.7188979999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009921568830808004, AUC: 0.7163568333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012731473445892333, AUC: 0.7144595833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016108662684758504, AUC: 0.49719683333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010739359855651855, AUC: 0.6935228333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010923791726430256, AUC: 0.7211683333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010441219011942545, AUC: 0.7205261666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008686558087666829, AUC: 0.5756996666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087567130724589, AUC: 0.6568675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011722314755121866, AUC: 0.6350644166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012100796699523926, AUC: 0.6632841666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020558752218882244, AUC: 0.5304973333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010855903228123983, AUC: 0.6664822499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011296828190485637, AUC: 0.6900355833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011508344411849975, AUC: 0.6842542500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023987669944763182, AUC: 0.47386600000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011190526485443115, AUC: 0.5874691666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011887950499852497, AUC: 0.5807331666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012476621468861897, AUC: 0.6282875833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026117592652638755, AUC: 0.5501220833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010784562031428018, AUC: 0.646719\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011126607259114582, AUC: 0.6314551666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011253907283147175, AUC: 0.6645647499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00907833480834961, AUC: 0.4095369166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103119929631551, AUC: 0.5183336666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00108243723710378, AUC: 0.6521908333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011252542734146117, AUC: 0.6145240833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012144622166951497, AUC: 0.4181985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010980870326360067, AUC: 0.5616944166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882917245229086, AUC: 0.6519211666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011063965956370036, AUC: 0.66767875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009048165639241537, AUC: 0.43410375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882413387298584, AUC: 0.641269\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011007899045944213, AUC: 0.649048\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001128840168317159, AUC: 0.6556144166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007734829584757487, AUC: 0.4437409166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010419075886408489, AUC: 0.7244224166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001304646611213684, AUC: 0.7183474166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014163004557291668, AUC: 0.7187720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020479775667190552, AUC: 0.51335725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101297616958618, AUC: 0.5413485833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010999488433202109, AUC: 0.6217745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011075912714004516, AUC: 0.6400816666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017265344858169555, AUC: 0.46495474999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010841899712880452, AUC: 0.6420518333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010756450494130453, AUC: 0.6767504166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011000306208928427, AUC: 0.6903722499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012104656219482422, AUC: 0.41221791666666663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101318875948588, AUC: 0.5439462500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011140114068984984, AUC: 0.5820619166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011396301587422689, AUC: 0.5939751666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013818121910095215, AUC: 0.3908481666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010969403187433878, AUC: 0.5741503333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011325390338897704, AUC: 0.5341461666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011663690408070882, AUC: 0.5428661666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004127991437911987, AUC: 0.5320692499999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011001667579015096, AUC: 0.565351\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010823936859766642, AUC: 0.66097125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011145733197530112, AUC: 0.6432240833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021704901059468587, AUC: 0.5081183333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010876192649205525, AUC: 0.6033494166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010726250012715658, AUC: 0.6338876666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010898060003916423, AUC: 0.60157925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0061941939989725745, AUC: 0.5234804166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010763348340988158, AUC: 0.6378586666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010690643787384033, AUC: 0.6724069166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010695849259694417, AUC: 0.6631999166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029249339898427327, AUC: 0.44276099999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010818012952804566, AUC: 0.6249320000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001167305072148641, AUC: 0.7006170833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010593249003092449, AUC: 0.7213223333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035500851472218833, AUC: 0.50164675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011043848991394043, AUC: 0.54231\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098570664723714, AUC: 0.5695655000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094109813372294, AUC: 0.5949230833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028190720081329346, AUC: 0.47897783333333327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010734304587046305, AUC: 0.6374557500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010634453694025674, AUC: 0.6531299166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010553747018178304, AUC: 0.6659868333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008131365617116292, AUC: 0.44895408333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011458585262298583, AUC: 0.6904523333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001035565733909607, AUC: 0.7257168333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012226609388987224, AUC: 0.702193\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009203200022379556, AUC: 0.5448211666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010926599899927776, AUC: 0.6043731666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011341997782389323, AUC: 0.6254171666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010712933540344238, AUC: 0.66773725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021905073165893553, AUC: 0.5471773333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010562962690989176, AUC: 0.7030220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010428677399953207, AUC: 0.7076155833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010711555480957032, AUC: 0.7094441666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003333326896031698, AUC: 0.5377636666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010806766748428344, AUC: 0.642129\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010924222469329834, AUC: 0.5946263333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010711589256922403, AUC: 0.6532606666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008774649620056153, AUC: 0.6241281666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010897631247838338, AUC: 0.5906111666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010884263515472412, AUC: 0.596979\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010742882092793782, AUC: 0.6337864999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036368446350097654, AUC: 0.5375485833333333\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011079335610071819, AUC: 0.6403586666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011695301135381062, AUC: 0.602681\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012256291309992473, AUC: 0.6285350833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005145518620808919, AUC: 0.44634475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010701054732004801, AUC: 0.6592354166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011435633500417074, AUC: 0.6172679166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011791830857594808, AUC: 0.64276475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005740523020426432, AUC: 0.41456291666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011070729494094848, AUC: 0.545332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011548689603805542, AUC: 0.5669795000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001200473427772522, AUC: 0.6261900833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022632593313852947, AUC: 0.4380240833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010723592837651571, AUC: 0.6887599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011568965911865235, AUC: 0.6591684999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012437734206517538, AUC: 0.60485875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018457919359207153, AUC: 0.55856275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010602103074391683, AUC: 0.676076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011733896334966023, AUC: 0.6035581666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011962558031082154, AUC: 0.6873893333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00838488229115804, AUC: 0.5265839166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010944061279296875, AUC: 0.654685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012505316734313964, AUC: 0.7014080833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013044424454371135, AUC: 0.7064013333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007856205145517985, AUC: 0.5915576666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010972687403361004, AUC: 0.6041635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00114812699953715, AUC: 0.62198325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012080289920171103, AUC: 0.6327571666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002285668134689331, AUC: 0.5247015833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011085167725880941, AUC: 0.5773756666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011094851891199748, AUC: 0.67543275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011885340611139934, AUC: 0.6666003333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036948370933532716, AUC: 0.43958058333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001069793423016866, AUC: 0.6709383333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011645712455113728, AUC: 0.6009698333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011933152675628663, AUC: 0.6677953333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003929627418518066, AUC: 0.4183385833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001110571026802063, AUC: 0.5317219999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001152188777923584, AUC: 0.5732484999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001216336210568746, AUC: 0.5800096666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003980309009552002, AUC: 0.5070290833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010700532595316569, AUC: 0.6445938333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011160261233647665, AUC: 0.5381821666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984551111857096, AUC: 0.6573060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007836485385894775, AUC: 0.4103730833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010970458189646403, AUC: 0.5674521666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010961037079493204, AUC: 0.6252298333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001122948169708252, AUC: 0.6204729166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010923314412434895, AUC: 0.41686466666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099669059117635, AUC: 0.5706851666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011077654361724854, AUC: 0.61126075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011014473040898642, AUC: 0.6663798333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004605100154876709, AUC: 0.4316365833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001077207366625468, AUC: 0.6230364999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010845094124476114, AUC: 0.6518664166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010816977818806966, AUC: 0.6803232499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007020300070444743, AUC: 0.5918789166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010466662247975668, AUC: 0.68247\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010511811971664429, AUC: 0.6724024166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010570799509684245, AUC: 0.6767935833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028751110235850016, AUC: 0.45622691666666676\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010817949771881104, AUC: 0.602141\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011074643135070802, AUC: 0.5737399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010939449071884154, AUC: 0.6590863333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005461203416188558, AUC: 0.45323550000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010623817046483358, AUC: 0.6461289166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010850675900777181, AUC: 0.6511835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011046991348266603, AUC: 0.6535490833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023248490492502848, AUC: 0.51568475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001300006628036499, AUC: 0.7089076666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010798428455988567, AUC: 0.7372998333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012731024424235027, AUC: 0.7253934166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002557363589604696, AUC: 0.5463335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101215124130249, AUC: 0.5372181666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001085688312848409, AUC: 0.6449231666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011124345461527507, AUC: 0.6414555000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017286035219828287, AUC: 0.4455651666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010931458473205566, AUC: 0.60530275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010769670804341633, AUC: 0.6755840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011087607542673748, AUC: 0.6672878333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034246060053507487, AUC: 0.46436649999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001090443213780721, AUC: 0.5917323333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00108868678410848, AUC: 0.60868325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010782825549443564, AUC: 0.6326221666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004270084063212077, AUC: 0.47567216666666673\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010902762413024902, AUC: 0.5802461666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088633894920349, AUC: 0.5930140833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010953359603881835, AUC: 0.54499625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004291628837585449, AUC: 0.43709708333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010911602179209392, AUC: 0.572385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001089426358540853, AUC: 0.5912680833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001077050010363261, AUC: 0.6359901666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003900085687637329, AUC: 0.4815301666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011072588761647543, AUC: 0.48698325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099105715751648, AUC: 0.5862985833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010957812070846557, AUC: 0.5476917500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028812230428059897, AUC: 0.55280275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010783047676086426, AUC: 0.6177990833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010747024218241373, AUC: 0.6332463333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010614176193873088, AUC: 0.6451901666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004155467033386231, AUC: 0.53043275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010899144411087036, AUC: 0.606247\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010819032192230225, AUC: 0.6305116666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010890053908030192, AUC: 0.6113765833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020048028230667115, AUC: 0.5931780000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010772932370503743, AUC: 0.6419350833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010667002995808918, AUC: 0.6577113333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001060768206914266, AUC: 0.6619066666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007765159130096435, AUC: 0.5444235833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010996445814768472, AUC: 0.5426753333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010889208714167278, AUC: 0.6047218333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010936522881189981, AUC: 0.5875358333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005751923243204752, AUC: 0.44032116666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095639705657959, AUC: 0.58367475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010873432556788127, AUC: 0.6055861666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010804917414983114, AUC: 0.6198759166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007940491994222006, AUC: 0.5980770833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094688336054484, AUC: 0.5689736666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001085538903872172, AUC: 0.6112655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010963680346806844, AUC: 0.5705725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005407075881958007, AUC: 0.4677414166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010836641391118367, AUC: 0.6448572499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011251831849416096, AUC: 0.6361315833333335\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011914784510930379, AUC: 0.6240734166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023554405212402343, AUC: 0.4700285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010850404898325602, AUC: 0.6233955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011104260285695394, AUC: 0.6535688333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011573739051818847, AUC: 0.6540090833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00530566676457723, AUC: 0.4907729166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011098883152008056, AUC: 0.5446788333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145491639773051, AUC: 0.5830515833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011320642630259195, AUC: 0.6684396666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012915862401326498, AUC: 0.5365515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011070706844329833, AUC: 0.5431425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011233591238657633, AUC: 0.6475516666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011704811652501425, AUC: 0.6520456666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009043893178304037, AUC: 0.3958669166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010925205945968629, AUC: 0.6063278333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001122375210126241, AUC: 0.6276160833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012131238381067912, AUC: 0.5614534999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004727090040842692, AUC: 0.42868008333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010698118209838867, AUC: 0.6596433333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095819632212321, AUC: 0.6503713333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011717450221379597, AUC: 0.6223735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004732184092203776, AUC: 0.5233229166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010391622384389241, AUC: 0.6818479166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010653945604960123, AUC: 0.6792489166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010980259577433268, AUC: 0.68595425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016474919716517131, AUC: 0.47314133333333325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010652266343434652, AUC: 0.6558399166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001108786185582479, AUC: 0.6665774166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011450514396031698, AUC: 0.6801645833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031354562441507977, AUC: 0.42186033333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011128941774368286, AUC: 0.5227439166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001136849522590637, AUC: 0.5913285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001227043310801188, AUC: 0.5117476666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003512922763824463, AUC: 0.42164399999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001093176325162252, AUC: 0.6176624166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011098206837972006, AUC: 0.6645506666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011351662874221802, AUC: 0.6799634999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003787134091059367, AUC: 0.46579808333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010777191718419393, AUC: 0.650869\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011082991361618041, AUC: 0.5412195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001116129438082377, AUC: 0.602544\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009494578043619792, AUC: 0.5785944166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086069623629252, AUC: 0.6067176666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010645600954691569, AUC: 0.6524258333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001119981328646342, AUC: 0.5740205833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014893545309702556, AUC: 0.5147845833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010881072680155436, AUC: 0.5916234166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001052786707878113, AUC: 0.6934054999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00108830463886261, AUC: 0.6655008333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033571554025014242, AUC: 0.43533875000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010616891781489053, AUC: 0.6435039166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010459538300832112, AUC: 0.6678250833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001065412680308024, AUC: 0.6670287500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005959226608276367, AUC: 0.36945425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086418628692627, AUC: 0.6006046666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010508412520090738, AUC: 0.6594155833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001061234712600708, AUC: 0.6830227500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002466677904129028, AUC: 0.5022106666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010860243638356527, AUC: 0.5979315000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010745192368825276, AUC: 0.6619666666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011291266282399495, AUC: 0.6359860833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010658900578816731, AUC: 0.5182304999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010712224245071412, AUC: 0.6549818333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010742261409759522, AUC: 0.6570839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011202009916305542, AUC: 0.6024794166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01710516611735026, AUC: 0.47007916666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010952344338099162, AUC: 0.5796060833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011034897168477377, AUC: 0.5976064166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011379482746124267, AUC: 0.572879\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003692817767461141, AUC: 0.4503040833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010954463481903077, AUC: 0.5600168333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097853382428487, AUC: 0.5904741666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011293626626332602, AUC: 0.5443664999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005870925744374593, AUC: 0.41144374999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011055472294489543, AUC: 0.5378953333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010633336305618285, AUC: 0.6512815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011010560194651285, AUC: 0.6409848333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015494970083236694, AUC: 0.5676933333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010837071339289347, AUC: 0.6027206666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010784968137741088, AUC: 0.6306749166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010775943597157796, AUC: 0.6433308333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006495138645172119, AUC: 0.4843408333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010748443206151327, AUC: 0.6262219166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010918902158737183, AUC: 0.5597046666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010834360122680665, AUC: 0.6138046666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004927356084187826, AUC: 0.44561124999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010812255541483562, AUC: 0.6187255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010861013730367026, AUC: 0.6047561666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010757153034210206, AUC: 0.6378210833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004236334005991618, AUC: 0.53376225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001053187330563863, AUC: 0.6802904999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010549397865931194, AUC: 0.707322\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001049498160680135, AUC: 0.7173719166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019901039600372316, AUC: 0.5079928333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010722840229670206, AUC: 0.6268785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001065795143445333, AUC: 0.6511400833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010672556956609091, AUC: 0.6540048333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004992599646250407, AUC: 0.3883905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001113569974899292, AUC: 0.5305151666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101508339246114, AUC: 0.5553133333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010981414715449016, AUC: 0.5683530833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004242204030354818, AUC: 0.44027075000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010937814315160116, AUC: 0.5913730833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010776551564534505, AUC: 0.6300409166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010835390090942383, AUC: 0.62701025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029406830469767254, AUC: 0.5880721666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010836275815963746, AUC: 0.6296732500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010813354651133219, AUC: 0.6267249166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010722055435180664, AUC: 0.6438299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004063597440719604, AUC: 0.514617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010883302688598632, AUC: 0.5808056666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010941217343012491, AUC: 0.5523899166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010946215391159058, AUC: 0.5514673333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015948575337727865, AUC: 0.5340653333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010999303261439006, AUC: 0.5856963333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011032426357269287, AUC: 0.6176274166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001080282966295878, AUC: 0.6403633333333333\n",
      "\n",
      "[['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.5114340166666667, 0.002092150513974722, 0.6581454083333332, 0.002239285218479788, 0.6627791416666666, 0.0017482602106209013, 0.6753271916666668, 0.0006883278604778474, 1, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.45691495, 0.0028386302317697193, 0.59592865, 0.0037556808618108345, 0.6378666833333334, 0.002322282924477501, 0.6431673333333332, 0.002246811227576389, 1, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0001, 0.515782875, 0.002552141191850348, 0.62764935, 0.001982959012012226, 0.6479962, 0.002529749873798888, 0.6613433, 0.0016453860869933355, 1, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.4895805416666666, 0.003779200803835072, 0.6248646583333334, 0.002882340665858958, 0.6222697499999998, 0.0017123422166319447, 0.6443301833333333, 0.0013163547528552775, 5, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.47748281666666664, 0.0032497889435233324, 0.6187936166666665, 0.002567033028440556, 0.6381672083333332, 0.0027841285161003455, 0.664804775, 0.0006790309580834018, 5, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0001, 0.5117901250000001, 0.003231998553382294, 0.5792651666666667, 0.0016324257262222228, 0.6122306833333334, 0.0004439571837886108, 0.6057758, 0.001520786319021109, 5, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.46296099166666665, 0.0019082223064422932, 0.6100139416666666, 0.002754494862472292, 0.6399996583333334, 0.0008978640343978477, 0.6340224833333332, 0.002929749149266391, 10, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.471623825, 0.0032281022349631245, 0.602375025, 0.0013438334068820146, 0.6372704249999999, 0.0018992524104297903, 0.618881275, 0.0019603903308861854, 10, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0001, 0.500481625, 0.003414226224967011, 0.6072900583333333, 0.001403928877082013, 0.6135694333333334, 0.0021070331995913867, 0.6297357333333332, 0.001898588301142776, 10, False]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class euclidean distance capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['distance'] = 'euclidean'\n",
    "\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"distance_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70f1ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bac108e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.009307427088419596, AUC: 0.5663711666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010381158590316772, AUC: 0.6928776666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010394632418950398, AUC: 0.7083165833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001089628259340922, AUC: 0.7163693333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003905921379725138, AUC: 0.4067528333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106173316637675, AUC: 0.5916745833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011440603733062745, AUC: 0.6577873333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011451536019643149, AUC: 0.7221175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003082738240559896, AUC: 0.40130899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010982749462127685, AUC: 0.6372173333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011223978598912557, AUC: 0.6647081666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011434684197107951, AUC: 0.6945667499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009841351509094239, AUC: 0.44450175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010697311162948609, AUC: 0.6591475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145783543586731, AUC: 0.6237505833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00112550421555837, AUC: 0.7095135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015699257850646973, AUC: 0.5543661666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011026999155680339, AUC: 0.6353009166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011459863583246866, AUC: 0.6578324166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012483625411987304, AUC: 0.6440775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027165615558624265, AUC: 0.45634749999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010871538718541464, AUC: 0.6273918333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001148347536722819, AUC: 0.6041458333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012063084046045939, AUC: 0.6527393333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008669881820678712, AUC: 0.5927104166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014313968420028686, AUC: 0.708514\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001006326953570048, AUC: 0.7806156666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009739565253257752, AUC: 0.78898725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023654577732086183, AUC: 0.42974358333333323\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010918844938278198, AUC: 0.6345458333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011173750162124633, AUC: 0.6971070833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086344043413798, AUC: 0.7525048333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0057791283925374345, AUC: 0.5036635833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010832340717315673, AUC: 0.6702354166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010967893997828166, AUC: 0.7136516666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011412729024887086, AUC: 0.7268528333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002480293353398641, AUC: 0.4885828333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010803773403167724, AUC: 0.6681533333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010607144832611084, AUC: 0.7306670833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001108709176381429, AUC: 0.7388393333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007506568908691406, AUC: 0.5356166666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010982218583424886, AUC: 0.5250188333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010778247912724814, AUC: 0.6451798333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011128324270248413, AUC: 0.6225453333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016701342900594076, AUC: 0.42722350000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010850432713826498, AUC: 0.5972164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010690858364105224, AUC: 0.664523\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011254624923070272, AUC: 0.6221204166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023381519317626953, AUC: 0.5116879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010644834438959757, AUC: 0.67191775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107664155960083, AUC: 0.6692058333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001065787672996521, AUC: 0.7117065833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004264139652252198, AUC: 0.3738314166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010483994483947753, AUC: 0.667424\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010356021722157797, AUC: 0.6923219999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010311555067698162, AUC: 0.706198\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036865827242533364, AUC: 0.5290393333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010735597610473633, AUC: 0.6589265833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010707809527715046, AUC: 0.6894758333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011547438700993855, AUC: 0.6374955833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004337542057037354, AUC: 0.46744066666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010801050265630086, AUC: 0.6348425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010579396883646647, AUC: 0.6872302499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001081854740778605, AUC: 0.7067206666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006562276522318522, AUC: 0.47032775000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010590982834498088, AUC: 0.6609269166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010789066950480143, AUC: 0.6310016666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001116389552752177, AUC: 0.5949518333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002736317237218221, AUC: 0.43491658333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010627652406692504, AUC: 0.6848278333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010859239101409911, AUC: 0.6665951666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011109973986943562, AUC: 0.6803804999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004880438327789307, AUC: 0.51087675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010854655504226685, AUC: 0.6468545833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010903065204620362, AUC: 0.6861354166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010511157115300495, AUC: 0.71697625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011261920611063639, AUC: 0.4038078333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010582979520161948, AUC: 0.6748376666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010762415726979573, AUC: 0.6752030833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010911909341812134, AUC: 0.6900136666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027665245532989503, AUC: 0.5040969166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010878295103708903, AUC: 0.60295125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010826247930526734, AUC: 0.6258310833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010953646103541057, AUC: 0.5572198333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002852704683939616, AUC: 0.5063758333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010880125761032105, AUC: 0.6131395000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001085563023885091, AUC: 0.6246570833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087370793024699, AUC: 0.6223289166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0070868239402771, AUC: 0.4662568333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010692745049794515, AUC: 0.65096375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010721711715062459, AUC: 0.6562806666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010683088302612304, AUC: 0.6689235833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016482744216918944, AUC: 0.5245193333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010860065619150798, AUC: 0.6040925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010820563634236654, AUC: 0.6250715833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010824129184087118, AUC: 0.6272235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01616198412577311, AUC: 0.519884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010902215242385865, AUC: 0.5756524166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001082411487897237, AUC: 0.6126565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010903645356496175, AUC: 0.5869915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031519697507222492, AUC: 0.445286\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001067394495010376, AUC: 0.6421843333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010639657179514568, AUC: 0.6384843333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010810608069101969, AUC: 0.7027599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0045092264811197914, AUC: 0.47959925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010877254009246827, AUC: 0.6087140833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010893857876459758, AUC: 0.6089168333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010884941418965659, AUC: 0.6192116666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004866736888885498, AUC: 0.4874373333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010892486969629924, AUC: 0.587225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010790967543919882, AUC: 0.6325878333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010761571327845255, AUC: 0.6304633333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004073219140370687, AUC: 0.446753\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001092194398244222, AUC: 0.5900568333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010906160672505697, AUC: 0.5814438333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010775267283121745, AUC: 0.6302179166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007165931065877279, AUC: 0.463745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010760011275609334, AUC: 0.6363281666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010631863276163737, AUC: 0.6618095833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010588233868281047, AUC: 0.6731375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010254356066385905, AUC: 0.4761089166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001080254316329956, AUC: 0.6450380833333333\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011165897448857625, AUC: 0.6467489999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001058436155319214, AUC: 0.7070731666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007202654838562012, AUC: 0.5093048333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010309611161549886, AUC: 0.6877974166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010366814136505127, AUC: 0.7073982500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010591098070144654, AUC: 0.7130885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006620848178863525, AUC: 0.5186609166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010601287682851155, AUC: 0.6800023333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011205731630325318, AUC: 0.6398495833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011403633753458659, AUC: 0.6908668333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022953606446584067, AUC: 0.4950433333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010856351852416993, AUC: 0.6545500833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011217998266220093, AUC: 0.6681236666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011196550925572713, AUC: 0.7141925000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025443352858225505, AUC: 0.5500075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010670501391092937, AUC: 0.6760150833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011259879271189371, AUC: 0.6464321666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010950273672739664, AUC: 0.704392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005891632239023844, AUC: 0.5343106666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011004143953323364, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010962204933166503, AUC: 0.616543\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001063297430674235, AUC: 0.6808071666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030544833342234295, AUC: 0.5390959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010972550710042317, AUC: 0.545636\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011179236968358358, AUC: 0.6223395833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001132046898206075, AUC: 0.6880726666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01784600575764974, AUC: 0.45312366666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010688005288441976, AUC: 0.6797004999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010850247144699096, AUC: 0.6948933333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010267422596613566, AUC: 0.7312673333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004708751678466797, AUC: 0.5378645833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010926309029261271, AUC: 0.6217843333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001013020674387614, AUC: 0.7238407499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011631203095118204, AUC: 0.6896146666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016822316487630207, AUC: 0.5660324999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010990248521169027, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994722843170165, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010987895727157593, AUC: 0.5085208333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008892767270406087, AUC: 0.6485014166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001075467864672343, AUC: 0.6421730833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010956129233042399, AUC: 0.5997845833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011172842582066855, AUC: 0.6112955833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030479044119517008, AUC: 0.5688559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986307859420777, AUC: 0.5271575833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001077911138534546, AUC: 0.6662336666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010691362222035726, AUC: 0.6984761666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0043939528465271, AUC: 0.5186005833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010978002945582072, AUC: 0.5252745000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010652739206949869, AUC: 0.6636294166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011329166889190673, AUC: 0.5348341666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002119073947270711, AUC: 0.5551196666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010829243262608847, AUC: 0.6154715000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011056837240854898, AUC: 0.5542634999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001111623207728068, AUC: 0.6232960833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006415510018666585, AUC: 0.4585190833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010947363376617432, AUC: 0.5719650000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994587341944377, AUC: 0.6181165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011283134619394937, AUC: 0.5988735833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016095528602600097, AUC: 0.5001016666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010756532351175944, AUC: 0.6766456666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010658512910207113, AUC: 0.6980575833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001055336038271586, AUC: 0.7068993333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006483989874521891, AUC: 0.4768765833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984715620676675, AUC: 0.5071676666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010791263182957968, AUC: 0.6400528333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010872536500295003, AUC: 0.6536346666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004176324049631755, AUC: 0.59201425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001069385051727295, AUC: 0.6641923333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010631552934646606, AUC: 0.6827873333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010476031700770061, AUC: 0.7029880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005902893225351969, AUC: 0.4702345833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010631628036499024, AUC: 0.6705648333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011049431562423705, AUC: 0.6049161666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011251726150512695, AUC: 0.609145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006689687569936116, AUC: 0.4555991666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010606642961502074, AUC: 0.6678731666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098002274831136, AUC: 0.578623\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001109260320663452, AUC: 0.6041565000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013810444513956705, AUC: 0.5945279166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010980782906214396, AUC: 0.5166393333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010940787792205811, AUC: 0.5562711666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010852585633595784, AUC: 0.6054055833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010326968510945639, AUC: 0.46391075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00108814537525177, AUC: 0.6051797499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010778623024622598, AUC: 0.6576635000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107601002852122, AUC: 0.6642584166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00810356299082438, AUC: 0.40166900000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010781443913777668, AUC: 0.6406595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010945734977722168, AUC: 0.549913\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010662610530853271, AUC: 0.6691830833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004108323733011882, AUC: 0.4966600833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010647641817728678, AUC: 0.6815454166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001071482022603353, AUC: 0.6720305833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001058586557706197, AUC: 0.6914581666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010969357172648112, AUC: 0.38187841666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010811973412831624, AUC: 0.62675225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001070385217666626, AUC: 0.6639953333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010919309457143149, AUC: 0.5793328333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012069509188334148, AUC: 0.4826971666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010934518575668336, AUC: 0.5577550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010902401606241862, AUC: 0.5817373333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010898268222808837, AUC: 0.5957196666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00349670680363973, AUC: 0.47968425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010983569224675496, AUC: 0.5015083333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010928091208140054, AUC: 0.5538816666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010870020786921182, AUC: 0.5976769166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001877047816912333, AUC: 0.5284498333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010840619802474976, AUC: 0.6311115833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010724639495213826, AUC: 0.6682761666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001065284013748169, AUC: 0.6910443333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001934542973836263, AUC: 0.536873\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010676891009012857, AUC: 0.649258\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010652023553848267, AUC: 0.6728703333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010744198560714722, AUC: 0.6623664166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007490639368693034, AUC: 0.5084459166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010828644434611003, AUC: 0.6369933333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010756362676620484, AUC: 0.6575118333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010852906703948974, AUC: 0.6414653333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002945788224538167, AUC: 0.48746316666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001062063217163086, AUC: 0.6717909166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011464081207911174, AUC: 0.5963015\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001116778572400411, AUC: 0.6900645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01625508181254069, AUC: 0.4664030833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010741564830144246, AUC: 0.6418016666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010800315936406453, AUC: 0.6800916666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011884698470433553, AUC: 0.63988025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008342554728190104, AUC: 0.42451666666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011109998226165772, AUC: 0.5107818333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011286404530207316, AUC: 0.6413061666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012048654556274414, AUC: 0.6312376666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01494838809967041, AUC: 0.4806551666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010929774443308512, AUC: 0.5900256666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010437474648157755, AUC: 0.6801426666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010652991930643718, AUC: 0.6801737500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00754862642288208, AUC: 0.48777675000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010817113320032755, AUC: 0.6697704166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010635205904642741, AUC: 0.7178928333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011229722499847413, AUC: 0.7375216666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027584671179453533, AUC: 0.5582151666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010694115161895752, AUC: 0.6481705833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010754584074020386, AUC: 0.6796188333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102502703666687, AUC: 0.6889550833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004639325300852458, AUC: 0.5744726666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010643746455510457, AUC: 0.6653024166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010460499127705892, AUC: 0.6779089166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011653558015823364, AUC: 0.68429975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037261667251586913, AUC: 0.5546785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010888798236846923, AUC: 0.6112012500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001124220649401347, AUC: 0.5931224166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011153006156285603, AUC: 0.6812201666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005664628664652506, AUC: 0.5717695833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010323719580968222, AUC: 0.71657\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011105549335479737, AUC: 0.7014670833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011508906682332356, AUC: 0.7332465833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036683813730875653, AUC: 0.47780733333333325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010862116813659668, AUC: 0.5967595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989519357681274, AUC: 0.6453134166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011621027787526448, AUC: 0.6411509999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016094333330790203, AUC: 0.5455816666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010940646727879842, AUC: 0.5560634999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010912297169367471, AUC: 0.6183153333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010924818515777588, AUC: 0.6553369166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004154547214508057, AUC: 0.5658715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010786510308583577, AUC: 0.6347021666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010856610933939615, AUC: 0.6517498333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010600257317225138, AUC: 0.6989579166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0070316877365112305, AUC: 0.581247\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010822877883911134, AUC: 0.6392471666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010774699052174886, AUC: 0.6544375833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010936934153238933, AUC: 0.6626444166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009202286084493002, AUC: 0.6229930833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001069573958714803, AUC: 0.6736977500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010940964619318644, AUC: 0.6132519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973670879999796, AUC: 0.6461046666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00918920358022054, AUC: 0.46712441666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010958014329274495, AUC: 0.5666346666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010858547687530517, AUC: 0.6466649166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001117258667945862, AUC: 0.6234403333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015238554000854491, AUC: 0.46595224999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010689182281494142, AUC: 0.6686160833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010755481719970704, AUC: 0.66703875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011423951387405395, AUC: 0.5604398333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038874141375223794, AUC: 0.4054255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010630921522776285, AUC: 0.6802650833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010437423388163249, AUC: 0.7077526666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010724194049835205, AUC: 0.6917408333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007150370438893636, AUC: 0.4294099166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010717049837112426, AUC: 0.6789174166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010702789624532064, AUC: 0.68963275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106743335723877, AUC: 0.6608636666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003355928341547648, AUC: 0.5150579166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010520528554916382, AUC: 0.6919333333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010491923888524372, AUC: 0.6978359166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010494410196940104, AUC: 0.7144828333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006835258642832438, AUC: 0.5572606666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010848799149195353, AUC: 0.6142598333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001083342671394348, AUC: 0.6384822500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001083211859067281, AUC: 0.663072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004312897523244222, AUC: 0.5263136666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010921010573705038, AUC: 0.5952860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010864551067352294, AUC: 0.6308811666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001083032210667928, AUC: 0.6460286666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031025232474009195, AUC: 0.42957533333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010743730862935385, AUC: 0.6355126666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010683009227116904, AUC: 0.6595160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010649169683456421, AUC: 0.6812145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019405757586161295, AUC: 0.49275916666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087343692779541, AUC: 0.6018481666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010804861386617024, AUC: 0.637628\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010803983211517335, AUC: 0.6367157499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028807759284973145, AUC: 0.5455578333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010654911994934081, AUC: 0.6596728333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001059812625249227, AUC: 0.6691358333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001056305726369222, AUC: 0.6706499166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0073672135670979815, AUC: 0.5630151666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010917633771896362, AUC: 0.5607159166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088225245475769, AUC: 0.5812029166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010757554769515992, AUC: 0.6322244166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029347774187723796, AUC: 0.5617626666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010927528142929076, AUC: 0.5773246666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010857496658960978, AUC: 0.6159069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087834596633911, AUC: 0.6102225833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0052178146044413246, AUC: 0.4976745833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099079966545105, AUC: 0.49966683333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010990656614303588, AUC: 0.5003318333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991636117299397, AUC: 0.49950066666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004764893372853597, AUC: 0.3621400833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001082196831703186, AUC: 0.6245364999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010677456061045328, AUC: 0.66712125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010747713645299275, AUC: 0.661007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004353288173675537, AUC: 0.5649864166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010889420906702678, AUC: 0.6012222500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010753500858942668, AUC: 0.6632700833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010847838719685871, AUC: 0.6354239166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002503709554672241, AUC: 0.4752878333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989478826522827, AUC: 0.5191613333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010834298928578696, AUC: 0.6427473333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010844480196634928, AUC: 0.6350068333333333\n",
      "\n",
      "[['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.4844348833333333, 0.0042033586334808345, 0.6525058416666666, 0.0010553556739909026, 0.6838582416666666, 0.002490521724000628, 0.7146568166666666, 0.0016994896297594462, 1, False], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.4664768500000001, 0.0027772994033025, 0.6422793166666667, 0.002086289643358057, 0.6706872083333332, 0.0003636294273197909, 0.6689108833333333, 0.0018334513915891637, 1, False], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0001, 0.48439535000000006, 0.0007474858444788881, 0.6111307833333334, 0.000558652625400277, 0.6267739333333334, 0.00048503429102055636, 0.631847775, 0.00159656526268479, 1, False], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.5179552916666668, 0.0011004114626531245, 0.6190523833333333, 0.005104343141653056, 0.6466169333333334, 0.0035487241627511117, 0.6827895666666667, 0.0035823878498733343, 5, False], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.5244422999999999, 0.003778312396394719, 0.6068485333333333, 0.004142400986939166, 0.6306464583333333, 0.0020018035081864624, 0.6343599083333333, 0.0027821255341464552, 5, False], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0001, 0.48747963333333333, 0.0035255676627197184, 0.6047402500000001, 0.0032017699976597223, 0.6234150916666666, 0.002728689716353407, 0.639791075, 0.0015824827392700712, 5, False], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.5083758083333333, 0.002437405699387569, 0.632217425, 0.002975831440928406, 0.66131655, 0.0015664061309391647, 0.6807750416666666, 0.0011836032693684035, 10, False], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.5155923916666667, 0.004538676360261179, 0.6404337, 0.0020904449133905584, 0.6585162, 0.0009280331694391655, 0.6577083416666667, 0.0016921898612270148, 10, False], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0001, 0.501907275, 0.003937383953704236, 0.5874947166666666, 0.002254639096960001, 0.6267741416666668, 0.0024398690516736826, 0.630799425, 0.0023013060926117363, 10, False]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class cosine distance capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['distance'] = 'cosine'\n",
    "\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"cosine_distance_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "77f9b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdf4c5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010990204016367595, AUC: 0.5136136666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019028458992640176, AUC: 0.7512252500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002086396058400472, AUC: 0.7691906666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002416961034138997, AUC: 0.7808670000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010924596786499024, AUC: 0.5750945000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001673059900601705, AUC: 0.7794949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002390830198923747, AUC: 0.7884782499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002485786517461141, AUC: 0.7885251666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973639885584513, AUC: 0.5168527500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001288370688756307, AUC: 0.7993951666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001963376998901367, AUC: 0.8154784999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018575411240259806, AUC: 0.8272758333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002889474232991, AUC: 0.5800430833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014540077050526937, AUC: 0.7614485000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024033334255218505, AUC: 0.7844169166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026617908477783205, AUC: 0.8052283333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010971461534500123, AUC: 0.5515258333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013851773341496787, AUC: 0.778522\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001976533651351929, AUC: 0.7745744999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002331717650095622, AUC: 0.7970936666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103301207224528, AUC: 0.47176391666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013020078738530476, AUC: 0.7888039166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024007017612457274, AUC: 0.7778662500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001970576842625936, AUC: 0.8121091666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973190863927205, AUC: 0.5521948333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001569384256998698, AUC: 0.7577094999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002028331756591797, AUC: 0.7819775833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024892863432566326, AUC: 0.7912178333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011189500093460084, AUC: 0.35523316666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001377241571744283, AUC: 0.7758425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002085300286610921, AUC: 0.78223675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002421713908513387, AUC: 0.8017280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098294973373413, AUC: 0.4996409166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016685739755630494, AUC: 0.7695623333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002051869591077169, AUC: 0.7821711666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002364952882130941, AUC: 0.7924216666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011055078109105429, AUC: 0.39565374999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014359800020853678, AUC: 0.7632740833333335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class triplet loss capped smote\n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2]\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "loss_caps = [0.5]\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = loss_cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                _, _ = train.train_triplet_capped_loss(epoch, train_loader_tripletloss_smote, network, optimizer, verbose=False, cap_calc=loss_fns.TripletLoss,loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"triplet_loss_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f59a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11bd9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea18543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
