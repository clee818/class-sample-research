{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ad7e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "457a1df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops \n",
    "\n",
    "NUM_CLASSES = 10\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "          #   \"mean_40\", \"variance_40\",\n",
    "          #   \"mean_50\", \"variance_50\",\n",
    "             \"cap\", \"normalization\", \"other\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c61a5021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 3 classes\n",
    "\n",
    "NUM_CLASSES_REDUCED = 3\n",
    "nums = (0, 3, 1)\n",
    "ratio = (200, 20, 1)\n",
    "\n",
    "norm=True\n",
    "\n",
    "if norm:\n",
    "    transform=torchvision.transforms.Compose([torchvision.transforms.Normalize(mean=[134.1855, 122.7346, 118.3749], std=[70.5125, 64.4848, 66.5604])])\n",
    "else:\n",
    "    transform=None\n",
    "\n",
    "    \n",
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "\n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums, transform=transform)\n",
    "targets = ratio_train_CIFAR10.labels \n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED, transform=transform)\n",
    "\n",
    "triplet_train_CIFAR10 = class_sampling.ForTripletLoss(reduced_train_CIFAR10, smote=False, transform=transform, num_classes=3)\n",
    "triplet_ratio_train_CIFAR10 = class_sampling.ForTripletLoss(ratio_train_CIFAR10, smote=False, transform=transform, num_classes=3)\n",
    "triplet_smote_train_CIFAR10 = class_sampling.ForTripletLoss(smote_train_CIFAR10, smote=True, transform=transform, num_classes=3)\n",
    "\n",
    "weight = 1. / class_count\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= max(class_count)\n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss = DataLoader(triplet_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_ratio = DataLoader(triplet_ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_smote = DataLoader(triplet_smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "# to be used in distance capped smote - get average tensor for the entire class \n",
    "dataset = train_loader_ratio.dataset\n",
    "class0 = dataset.images[dataset.labels==0]\n",
    "class1 = dataset.images[dataset.labels==1]\n",
    "class2 = dataset.images[dataset.labels==1]\n",
    "class0_avg = torch.mean(class0.float(), 0)\n",
    "class1_avg = torch.mean(class1.float(), 0)\n",
    "class2_avg = torch.mean(class2.float(), 0)\n",
    "avg_tensors_list = [class0_avg, class1_avg, class2_avg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170fce52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3 class normal\n",
    "\n",
    "learning_rates = [5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 3, nums, (1, 1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97f0654",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf6798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  3 class ratio\n",
    "\n",
    "learning_rates =  [1e-4, 1e-3, 5e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e7d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8774b790",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0033108042081197104, AUC: 0.5012746666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010891745090484618, AUC: 0.5691605000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010772271553675333, AUC: 0.6205455833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010581616560618083, AUC: 0.66594325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009650338172912597, AUC: 0.64924925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989543994267782, AUC: 0.5006625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988156000773111, AUC: 0.5036341666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098307967185974, AUC: 0.511603\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014643325408299763, AUC: 0.5624135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988226731618246, AUC: 0.5028216666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010955985387166342, AUC: 0.5535775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010860007206598917, AUC: 0.6126334166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017339369455973306, AUC: 0.6287725000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010934834877649943, AUC: 0.5562849166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010735339721043905, AUC: 0.6279554166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010635928312937419, AUC: 0.6537821666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00311078421274821, AUC: 0.6122645000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010816566149393718, AUC: 0.6115289166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010676841735839844, AUC: 0.6443601666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010676517089207966, AUC: 0.6437641666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003010539134343465, AUC: 0.5604719166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010484061241149902, AUC: 0.6771929166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001044447143872579, AUC: 0.6828719166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001045499285062154, AUC: 0.6731064999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005095264434814453, AUC: 0.5934418333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010727895100911457, AUC: 0.6352476666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010556917985280355, AUC: 0.6731788333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010515360434850058, AUC: 0.6720729999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004967935721079508, AUC: 0.5115388333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989669561386108, AUC: 0.501477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097963293393453, AUC: 0.5139119166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010826340913772583, AUC: 0.5988807500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008962241172790528, AUC: 0.46783916666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010985263188680013, AUC: 0.50874\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010896445512771607, AUC: 0.5841320833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010682722727457681, AUC: 0.6465361666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017491921583811442, AUC: 0.4792893333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010711934169133504, AUC: 0.6451561666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010578383207321166, AUC: 0.68593325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010505107641220093, AUC: 0.6982249166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010706811587015789, AUC: 0.5282234166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001061619480450948, AUC: 0.6390623333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001114656170209249, AUC: 0.6740934166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012691501379013062, AUC: 0.6652653333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007427863756815593, AUC: 0.4185044166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001084286650021871, AUC: 0.58175425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010580129226048787, AUC: 0.6456232500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011062675714492798, AUC: 0.6641103333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030471408367156983, AUC: 0.46239433333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010617636442184447, AUC: 0.6655880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011149996916453044, AUC: 0.6446978333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013898284037907919, AUC: 0.6695008333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020981840292612713, AUC: 0.5315558333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010490548610687257, AUC: 0.6604576666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00106292720635732, AUC: 0.64541675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011092890501022338, AUC: 0.6473134166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025615330537160238, AUC: 0.4643255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010793258746465046, AUC: 0.6003605833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010611701806386312, AUC: 0.6487610833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011049154996871948, AUC: 0.6225078333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002963742653528849, AUC: 0.4861299166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010549907684326172, AUC: 0.6784993333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013114278713862101, AUC: 0.65652\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001763128360112508, AUC: 0.6614195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003440234581629435, AUC: 0.4392371666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010772697528203328, AUC: 0.6156298333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011034450133641561, AUC: 0.6357530833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011656708717346191, AUC: 0.6330305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004977382183074951, AUC: 0.5888548333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010707391103108724, AUC: 0.6659326666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012835449775060019, AUC: 0.6431731666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015637336572011313, AUC: 0.6489561666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0063886931737263996, AUC: 0.399383\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010752424001693726, AUC: 0.6106143333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012818395296732585, AUC: 0.5992483333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013339670499165853, AUC: 0.5723510833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026855950355529784, AUC: 0.5174875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010493956804275512, AUC: 0.6744605833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001089785893758138, AUC: 0.6140959166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012206356525421142, AUC: 0.6147953333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009377074877421062, AUC: 0.5566096666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010837449232737224, AUC: 0.5934846666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012367159525553386, AUC: 0.6239264166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001931144952774048, AUC: 0.67264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004204644838968913, AUC: 0.6017256666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012657182216644287, AUC: 0.5986390833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019427632490793864, AUC: 0.6046090833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023304985364278157, AUC: 0.5837676666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029421582221984864, AUC: 0.6314905000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010638542572657267, AUC: 0.6172354166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015519940455754598, AUC: 0.6331893333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021608004570007324, AUC: 0.6480911666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002372199853261312, AUC: 0.51225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010411208868026734, AUC: 0.6773319166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013682610193888347, AUC: 0.6756165833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021713258425394696, AUC: 0.7029964999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004619466463724772, AUC: 0.5606215833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010885966618855794, AUC: 0.5866075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011480294863382976, AUC: 0.6525846666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014715615113576254, AUC: 0.643338\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007221521695454916, AUC: 0.4941961666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010529818932215372, AUC: 0.6523379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011516759792963664, AUC: 0.6579788333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018557409842809042, AUC: 0.6827570833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009308429718017578, AUC: 0.539823\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011011182467142741, AUC: 0.49850533333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011037826935450237, AUC: 0.5013376666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011058456500371296, AUC: 0.49884533333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0118248504002889, AUC: 0.47009975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014740148385365805, AUC: 0.7365466666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023439241250356037, AUC: 0.7258684999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003214358568191528, AUC: 0.7193210833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008746652921040854, AUC: 0.5803128333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991198619206746, AUC: 0.5003306666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010824837287267048, AUC: 0.6544369999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014401022990544637, AUC: 0.6453408333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011464783032735189, AUC: 0.40880383333333326\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010881396929423015, AUC: 0.6685733333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015919443766276042, AUC: 0.6202325833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002839865287144979, AUC: 0.6500708333333333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class oversampled \n",
    "\n",
    "learning_rates = [1e-4, 5e-4, 1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "923429dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8aa19a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.006379682540893555, AUC: 0.5555167499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010770999590555828, AUC: 0.7082001666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011475319465001424, AUC: 0.7300091666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012237483263015747, AUC: 0.7375256666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021883630752563476, AUC: 0.5853621666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011258338292439779, AUC: 0.6908154166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012150108416875203, AUC: 0.7028374166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011469882726669312, AUC: 0.7570853333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002026644229888916, AUC: 0.49485124999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001054565707842509, AUC: 0.7094285833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001168736457824707, AUC: 0.707386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012017017602920532, AUC: 0.6683456666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018511176109313964, AUC: 0.54186175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011469180583953858, AUC: 0.6809189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006299654642742, AUC: 0.7146174166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011957310835520427, AUC: 0.7583339166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011058505694071451, AUC: 0.5906061666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011384323438008625, AUC: 0.68790075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001181691845258077, AUC: 0.7294569166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012818557818730673, AUC: 0.723963\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007394368330637614, AUC: 0.4380160833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010780050754547119, AUC: 0.7321193333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011058851877848307, AUC: 0.7453125000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012145222028096517, AUC: 0.75858625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02268055788675944, AUC: 0.4862148333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098679780960083, AUC: 0.49933466666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098562757174174, AUC: 0.499338\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984772046407063, AUC: 0.5006695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023918784459431965, AUC: 0.4272140833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103639284769694, AUC: 0.7132798333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010598060290018718, AUC: 0.7375085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012684359947840373, AUC: 0.7315633333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006638091723124186, AUC: 0.5599561666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010386726061503092, AUC: 0.723674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011951104799906413, AUC: 0.7302891666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011742955048878987, AUC: 0.7556646666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00785024356842041, AUC: 0.4896606666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010985156695048014, AUC: 0.5013401666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010983107884724936, AUC: 0.503194\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010976864099502563, AUC: 0.5053355\n",
      "\n",
      "[['smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.5169259916666666, 0.0030523780412756237, 0.6647011916666667, 0.006979168440701458, 0.6799949083333332, 0.008141787962282567, 0.6897072833333333, 0.009374064240398888, None, False, None]]\n"
     ]
    }
   ],
   "source": [
    "#  3 class SMOTE\n",
    "\n",
    "learning_rates = [5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)\n",
    "    \n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e34ddade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dc17d94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004620168209075928, AUC: 0.5224699166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032700355052947996, AUC: 0.65412675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023182222843170165, AUC: 0.6341258333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016645215352376302, AUC: 0.7060236666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00591958220799764, AUC: 0.505117\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004119347174962362, AUC: 0.5946488333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001755180795987447, AUC: 0.7035093333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001964009126027425, AUC: 0.7295488333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0043713115056355795, AUC: 0.4085865833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016593863169352214, AUC: 0.722733\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018062065442403157, AUC: 0.7048511666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019965092738469443, AUC: 0.6980991666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013644606590270995, AUC: 0.4427893333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019738929669062297, AUC: 0.7059536666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017942049900690714, AUC: 0.6927988333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019231324195861817, AUC: 0.72229975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032147006988525392, AUC: 0.4984538333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022280614376068114, AUC: 0.6882096666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001923011541366577, AUC: 0.7115016666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002189170281092326, AUC: 0.6765593333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0071949974695841475, AUC: 0.43524974999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025574404398600262, AUC: 0.6834674166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002559229294459025, AUC: 0.6989545833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002253734270731608, AUC: 0.71202\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006388411362965901, AUC: 0.57681125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022331839402516683, AUC: 0.6843386666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029782606760660807, AUC: 0.719218\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018446124792098999, AUC: 0.7140474166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004625419139862061, AUC: 0.4220980000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022333877086639403, AUC: 0.6549976666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001749162236849467, AUC: 0.7071013333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002016409397125244, AUC: 0.7174317499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034746370315551756, AUC: 0.47518649999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016891684532165528, AUC: 0.6898971666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018666686216990153, AUC: 0.7266434166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019854795535405477, AUC: 0.7084774166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012103025436401366, AUC: 0.48356933333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015317246516545614, AUC: 0.6839028333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018535057306289673, AUC: 0.6980058333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018716199398040772, AUC: 0.689493\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031785619258880614, AUC: 0.4493183333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010253608226776123, AUC: 0.6914396666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011740447282791138, AUC: 0.7359080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001194417993227641, AUC: 0.738433\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002602392276128133, AUC: 0.47475366666666674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011079551378885904, AUC: 0.6902058333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011395405928293864, AUC: 0.7308109166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012364893754323323, AUC: 0.7567433333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002468026876449585, AUC: 0.44136291666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011118733485539754, AUC: 0.711592\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001142312208811442, AUC: 0.7309383333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011314800182978312, AUC: 0.7357541666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010019526481628419, AUC: 0.49331450000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010481293598810831, AUC: 0.7135536666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011577494541803995, AUC: 0.724225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145140012105306, AUC: 0.7312375000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003724007765452067, AUC: 0.45395416666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010740491151809692, AUC: 0.7242934166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011932284434636434, AUC: 0.7489648333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012084437211354574, AUC: 0.73174175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0051693522135416664, AUC: 0.5283039166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010928740104039511, AUC: 0.5582463333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011459138393402099, AUC: 0.6059868333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001169200619061788, AUC: 0.6760423333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033050804932912192, AUC: 0.4868890833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010502624909083048, AUC: 0.6591195833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011096171935399374, AUC: 0.7369954166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012287641366322834, AUC: 0.7242468333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009105114301045736, AUC: 0.42352191666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00102720574537913, AUC: 0.71345475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011696877479553222, AUC: 0.72304425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013579951524734497, AUC: 0.7022021666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006798748334248861, AUC: 0.5948063333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002205610275268, AUC: 0.7085406666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010114536086718241, AUC: 0.7337970833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001104941487312317, AUC: 0.7632221666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004611253102620443, AUC: 0.44240583333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011016714175542195, AUC: 0.5003333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010795877774556477, AUC: 0.7098338333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012670063972473145, AUC: 0.715192\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010228350321451822, AUC: 0.41775074999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010509448051452637, AUC: 0.6990739166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010669530232747396, AUC: 0.7227665833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011352572043736777, AUC: 0.7475705833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011651581446329752, AUC: 0.40751641666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096131126085917, AUC: 0.7384790833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011417748133341471, AUC: 0.7328101666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011841011842091877, AUC: 0.7482245000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006454404354095459, AUC: 0.4783680833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986293156941731, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986181497573853, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098614017168681, AUC: 0.49983333333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005513421694437663, AUC: 0.5092850833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086468736330668, AUC: 0.7061335833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001193962812423706, AUC: 0.718693\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003832817077636, AUC: 0.7211851666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018346519867579142, AUC: 0.4715146666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010832481781641642, AUC: 0.6684931666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010695111751556395, AUC: 0.7238518333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001064674695332845, AUC: 0.7530567499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024959243138631183, AUC: 0.5835906666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010908070802688598, AUC: 0.6615429166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011001924673716228, AUC: 0.7075508333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012157375415166218, AUC: 0.7182206666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006374645074208578, AUC: 0.489673\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010034387707710266, AUC: 0.7022106666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001242663621902466, AUC: 0.6784239166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011921960512797038, AUC: 0.759859\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005791068394978841, AUC: 0.57925825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107000998655955, AUC: 0.6864105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011323394378026327, AUC: 0.72307675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011958227554957073, AUC: 0.7431989166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0061645299593607586, AUC: 0.5436045833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010343753894170124, AUC: 0.6722659999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011665374437967936, AUC: 0.7285349166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012842472791671754, AUC: 0.7274453333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00378561798731486, AUC: 0.5657540833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986368258794148, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011124326785405478, AUC: 0.5003333333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011208449999491373, AUC: 0.5\n",
      "\n",
      "[['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.47703315, 0.0023935143808247222, 0.6762275666666666, 0.0011148184608552787, 0.6996709999999999, 0.0005693019507458329, 0.7074000333333333, 0.00022401099147388955, 1, False], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.4788630666666666, 0.002346876533395558, 0.6670779250000001, 0.0052124882764603515, 0.71805045, 0.0014899216534586131, 0.7274815250000001, 0.0005800807215722915, 5, False], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.5046315583333334, 0.003607008163059792, 0.6534609833333332, 0.006329068172458058, 0.6736041333333332, 0.0077281842878433316, 0.691859425, 0.009379088293504789, 10, False]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-3]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm, None]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f5ecca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "294d600b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0053888258934020996, AUC: 0.5083743333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011163065830866496, AUC: 0.5555713333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011519194841384888, AUC: 0.6339973333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012265472412109374, AUC: 0.6482106666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004785251299540202, AUC: 0.45894758333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010515480041503906, AUC: 0.6808616666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097139040629069, AUC: 0.6735478333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001158262848854065, AUC: 0.6645963333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002081297715504964, AUC: 0.5468329166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992099046707154, AUC: 0.6545398333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001181641697883606, AUC: 0.6227119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012087234656016032, AUC: 0.6747756666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029571340878804526, AUC: 0.42823191666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010765260457992554, AUC: 0.6793964166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011122363011042277, AUC: 0.6777794166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011849076747894288, AUC: 0.6750039166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020178468624750773, AUC: 0.5219041666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001072571277618408, AUC: 0.6878450833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011367605924606323, AUC: 0.6763965000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011956455707550048, AUC: 0.6798735833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0066022364298502605, AUC: 0.5727894166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103708823521932, AUC: 0.7188979999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009921568830808004, AUC: 0.7163568333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012731473445892333, AUC: 0.7144595833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016108662684758504, AUC: 0.49719683333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010739359855651855, AUC: 0.6935228333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010923791726430256, AUC: 0.7211683333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010441219011942545, AUC: 0.7205261666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008686558087666829, AUC: 0.5756996666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087567130724589, AUC: 0.6568675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011722314755121866, AUC: 0.6350644166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012100796699523926, AUC: 0.6632841666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020558752218882244, AUC: 0.5304973333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010855903228123983, AUC: 0.6664822499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011296828190485637, AUC: 0.6900355833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011508344411849975, AUC: 0.6842542500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023987669944763182, AUC: 0.47386600000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011190526485443115, AUC: 0.5874691666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011887950499852497, AUC: 0.5807331666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012476621468861897, AUC: 0.6282875833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026117592652638755, AUC: 0.5501220833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010784562031428018, AUC: 0.646719\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011126607259114582, AUC: 0.6314551666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011253907283147175, AUC: 0.6645647499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00907833480834961, AUC: 0.4095369166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103119929631551, AUC: 0.5183336666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00108243723710378, AUC: 0.6521908333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011252542734146117, AUC: 0.6145240833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012144622166951497, AUC: 0.4181985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010980870326360067, AUC: 0.5616944166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882917245229086, AUC: 0.6519211666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011063965956370036, AUC: 0.66767875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009048165639241537, AUC: 0.43410375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882413387298584, AUC: 0.641269\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011007899045944213, AUC: 0.649048\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001128840168317159, AUC: 0.6556144166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007734829584757487, AUC: 0.4437409166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010419075886408489, AUC: 0.7244224166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001304646611213684, AUC: 0.7183474166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014163004557291668, AUC: 0.7187720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020479775667190552, AUC: 0.51335725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101297616958618, AUC: 0.5413485833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010999488433202109, AUC: 0.6217745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011075912714004516, AUC: 0.6400816666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017265344858169555, AUC: 0.46495474999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010841899712880452, AUC: 0.6420518333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010756450494130453, AUC: 0.6767504166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011000306208928427, AUC: 0.6903722499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012104656219482422, AUC: 0.41221791666666663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101318875948588, AUC: 0.5439462500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011140114068984984, AUC: 0.5820619166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011396301587422689, AUC: 0.5939751666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013818121910095215, AUC: 0.3908481666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010969403187433878, AUC: 0.5741503333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011325390338897704, AUC: 0.5341461666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011663690408070882, AUC: 0.5428661666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004127991437911987, AUC: 0.5320692499999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011001667579015096, AUC: 0.565351\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010823936859766642, AUC: 0.66097125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011145733197530112, AUC: 0.6432240833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021704901059468587, AUC: 0.5081183333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010876192649205525, AUC: 0.6033494166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010726250012715658, AUC: 0.6338876666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010898060003916423, AUC: 0.60157925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0061941939989725745, AUC: 0.5234804166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010763348340988158, AUC: 0.6378586666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010690643787384033, AUC: 0.6724069166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010695849259694417, AUC: 0.6631999166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029249339898427327, AUC: 0.44276099999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010818012952804566, AUC: 0.6249320000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001167305072148641, AUC: 0.7006170833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010593249003092449, AUC: 0.7213223333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035500851472218833, AUC: 0.50164675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011043848991394043, AUC: 0.54231\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098570664723714, AUC: 0.5695655000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094109813372294, AUC: 0.5949230833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028190720081329346, AUC: 0.47897783333333327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010734304587046305, AUC: 0.6374557500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010634453694025674, AUC: 0.6531299166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010553747018178304, AUC: 0.6659868333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008131365617116292, AUC: 0.44895408333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011458585262298583, AUC: 0.6904523333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001035565733909607, AUC: 0.7257168333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012226609388987224, AUC: 0.702193\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009203200022379556, AUC: 0.5448211666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010926599899927776, AUC: 0.6043731666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011341997782389323, AUC: 0.6254171666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010712933540344238, AUC: 0.66773725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021905073165893553, AUC: 0.5471773333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010562962690989176, AUC: 0.7030220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010428677399953207, AUC: 0.7076155833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010711555480957032, AUC: 0.7094441666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003333326896031698, AUC: 0.5377636666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010806766748428344, AUC: 0.642129\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010924222469329834, AUC: 0.5946263333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010711589256922403, AUC: 0.6532606666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008774649620056153, AUC: 0.6241281666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010897631247838338, AUC: 0.5906111666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010884263515472412, AUC: 0.596979\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010742882092793782, AUC: 0.6337864999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036368446350097654, AUC: 0.5375485833333333\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011079335610071819, AUC: 0.6403586666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011695301135381062, AUC: 0.602681\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012256291309992473, AUC: 0.6285350833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005145518620808919, AUC: 0.44634475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010701054732004801, AUC: 0.6592354166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011435633500417074, AUC: 0.6172679166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011791830857594808, AUC: 0.64276475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005740523020426432, AUC: 0.41456291666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011070729494094848, AUC: 0.545332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011548689603805542, AUC: 0.5669795000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001200473427772522, AUC: 0.6261900833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022632593313852947, AUC: 0.4380240833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010723592837651571, AUC: 0.6887599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011568965911865235, AUC: 0.6591684999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012437734206517538, AUC: 0.60485875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018457919359207153, AUC: 0.55856275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010602103074391683, AUC: 0.676076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011733896334966023, AUC: 0.6035581666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011962558031082154, AUC: 0.6873893333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00838488229115804, AUC: 0.5265839166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010944061279296875, AUC: 0.654685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012505316734313964, AUC: 0.7014080833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013044424454371135, AUC: 0.7064013333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007856205145517985, AUC: 0.5915576666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010972687403361004, AUC: 0.6041635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00114812699953715, AUC: 0.62198325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012080289920171103, AUC: 0.6327571666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002285668134689331, AUC: 0.5247015833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011085167725880941, AUC: 0.5773756666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011094851891199748, AUC: 0.67543275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011885340611139934, AUC: 0.6666003333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036948370933532716, AUC: 0.43958058333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001069793423016866, AUC: 0.6709383333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011645712455113728, AUC: 0.6009698333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011933152675628663, AUC: 0.6677953333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003929627418518066, AUC: 0.4183385833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001110571026802063, AUC: 0.5317219999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001152188777923584, AUC: 0.5732484999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001216336210568746, AUC: 0.5800096666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003980309009552002, AUC: 0.5070290833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010700532595316569, AUC: 0.6445938333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011160261233647665, AUC: 0.5381821666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984551111857096, AUC: 0.6573060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007836485385894775, AUC: 0.4103730833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010970458189646403, AUC: 0.5674521666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010961037079493204, AUC: 0.6252298333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001122948169708252, AUC: 0.6204729166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010923314412434895, AUC: 0.41686466666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099669059117635, AUC: 0.5706851666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011077654361724854, AUC: 0.61126075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011014473040898642, AUC: 0.6663798333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004605100154876709, AUC: 0.4316365833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001077207366625468, AUC: 0.6230364999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010845094124476114, AUC: 0.6518664166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010816977818806966, AUC: 0.6803232499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007020300070444743, AUC: 0.5918789166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010466662247975668, AUC: 0.68247\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010511811971664429, AUC: 0.6724024166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010570799509684245, AUC: 0.6767935833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028751110235850016, AUC: 0.45622691666666676\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010817949771881104, AUC: 0.602141\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011074643135070802, AUC: 0.5737399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010939449071884154, AUC: 0.6590863333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005461203416188558, AUC: 0.45323550000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010623817046483358, AUC: 0.6461289166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010850675900777181, AUC: 0.6511835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011046991348266603, AUC: 0.6535490833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023248490492502848, AUC: 0.51568475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001300006628036499, AUC: 0.7089076666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010798428455988567, AUC: 0.7372998333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012731024424235027, AUC: 0.7253934166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002557363589604696, AUC: 0.5463335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101215124130249, AUC: 0.5372181666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001085688312848409, AUC: 0.6449231666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011124345461527507, AUC: 0.6414555000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017286035219828287, AUC: 0.4455651666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010931458473205566, AUC: 0.60530275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010769670804341633, AUC: 0.6755840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011087607542673748, AUC: 0.6672878333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034246060053507487, AUC: 0.46436649999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001090443213780721, AUC: 0.5917323333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00108868678410848, AUC: 0.60868325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010782825549443564, AUC: 0.6326221666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004270084063212077, AUC: 0.47567216666666673\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010902762413024902, AUC: 0.5802461666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088633894920349, AUC: 0.5930140833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010953359603881835, AUC: 0.54499625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004291628837585449, AUC: 0.43709708333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010911602179209392, AUC: 0.572385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001089426358540853, AUC: 0.5912680833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001077050010363261, AUC: 0.6359901666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003900085687637329, AUC: 0.4815301666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011072588761647543, AUC: 0.48698325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099105715751648, AUC: 0.5862985833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010957812070846557, AUC: 0.5476917500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028812230428059897, AUC: 0.55280275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010783047676086426, AUC: 0.6177990833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010747024218241373, AUC: 0.6332463333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010614176193873088, AUC: 0.6451901666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004155467033386231, AUC: 0.53043275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010899144411087036, AUC: 0.606247\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010819032192230225, AUC: 0.6305116666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010890053908030192, AUC: 0.6113765833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020048028230667115, AUC: 0.5931780000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010772932370503743, AUC: 0.6419350833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010667002995808918, AUC: 0.6577113333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001060768206914266, AUC: 0.6619066666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007765159130096435, AUC: 0.5444235833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010996445814768472, AUC: 0.5426753333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010889208714167278, AUC: 0.6047218333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010936522881189981, AUC: 0.5875358333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005751923243204752, AUC: 0.44032116666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095639705657959, AUC: 0.58367475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010873432556788127, AUC: 0.6055861666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010804917414983114, AUC: 0.6198759166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007940491994222006, AUC: 0.5980770833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094688336054484, AUC: 0.5689736666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001085538903872172, AUC: 0.6112655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010963680346806844, AUC: 0.5705725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005407075881958007, AUC: 0.4677414166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010836641391118367, AUC: 0.6448572499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011251831849416096, AUC: 0.6361315833333335\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011914784510930379, AUC: 0.6240734166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023554405212402343, AUC: 0.4700285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010850404898325602, AUC: 0.6233955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011104260285695394, AUC: 0.6535688333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011573739051818847, AUC: 0.6540090833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00530566676457723, AUC: 0.4907729166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011098883152008056, AUC: 0.5446788333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145491639773051, AUC: 0.5830515833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011320642630259195, AUC: 0.6684396666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012915862401326498, AUC: 0.5365515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011070706844329833, AUC: 0.5431425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011233591238657633, AUC: 0.6475516666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011704811652501425, AUC: 0.6520456666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009043893178304037, AUC: 0.3958669166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010925205945968629, AUC: 0.6063278333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001122375210126241, AUC: 0.6276160833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012131238381067912, AUC: 0.5614534999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004727090040842692, AUC: 0.42868008333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010698118209838867, AUC: 0.6596433333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095819632212321, AUC: 0.6503713333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011717450221379597, AUC: 0.6223735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004732184092203776, AUC: 0.5233229166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010391622384389241, AUC: 0.6818479166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010653945604960123, AUC: 0.6792489166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010980259577433268, AUC: 0.68595425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016474919716517131, AUC: 0.47314133333333325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010652266343434652, AUC: 0.6558399166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001108786185582479, AUC: 0.6665774166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011450514396031698, AUC: 0.6801645833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031354562441507977, AUC: 0.42186033333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011128941774368286, AUC: 0.5227439166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001136849522590637, AUC: 0.5913285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001227043310801188, AUC: 0.5117476666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003512922763824463, AUC: 0.42164399999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001093176325162252, AUC: 0.6176624166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011098206837972006, AUC: 0.6645506666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011351662874221802, AUC: 0.6799634999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003787134091059367, AUC: 0.46579808333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010777191718419393, AUC: 0.650869\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011082991361618041, AUC: 0.5412195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001116129438082377, AUC: 0.602544\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009494578043619792, AUC: 0.5785944166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086069623629252, AUC: 0.6067176666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010645600954691569, AUC: 0.6524258333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001119981328646342, AUC: 0.5740205833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014893545309702556, AUC: 0.5147845833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010881072680155436, AUC: 0.5916234166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001052786707878113, AUC: 0.6934054999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00108830463886261, AUC: 0.6655008333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033571554025014242, AUC: 0.43533875000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010616891781489053, AUC: 0.6435039166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010459538300832112, AUC: 0.6678250833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001065412680308024, AUC: 0.6670287500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005959226608276367, AUC: 0.36945425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086418628692627, AUC: 0.6006046666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010508412520090738, AUC: 0.6594155833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001061234712600708, AUC: 0.6830227500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002466677904129028, AUC: 0.5022106666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010860243638356527, AUC: 0.5979315000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010745192368825276, AUC: 0.6619666666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011291266282399495, AUC: 0.6359860833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010658900578816731, AUC: 0.5182304999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010712224245071412, AUC: 0.6549818333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010742261409759522, AUC: 0.6570839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011202009916305542, AUC: 0.6024794166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01710516611735026, AUC: 0.47007916666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010952344338099162, AUC: 0.5796060833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011034897168477377, AUC: 0.5976064166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011379482746124267, AUC: 0.572879\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003692817767461141, AUC: 0.4503040833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010954463481903077, AUC: 0.5600168333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097853382428487, AUC: 0.5904741666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011293626626332602, AUC: 0.5443664999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005870925744374593, AUC: 0.41144374999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011055472294489543, AUC: 0.5378953333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010633336305618285, AUC: 0.6512815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011010560194651285, AUC: 0.6409848333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015494970083236694, AUC: 0.5676933333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010837071339289347, AUC: 0.6027206666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010784968137741088, AUC: 0.6306749166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010775943597157796, AUC: 0.6433308333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006495138645172119, AUC: 0.4843408333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010748443206151327, AUC: 0.6262219166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010918902158737183, AUC: 0.5597046666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010834360122680665, AUC: 0.6138046666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004927356084187826, AUC: 0.44561124999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010812255541483562, AUC: 0.6187255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010861013730367026, AUC: 0.6047561666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010757153034210206, AUC: 0.6378210833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004236334005991618, AUC: 0.53376225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001053187330563863, AUC: 0.6802904999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010549397865931194, AUC: 0.707322\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001049498160680135, AUC: 0.7173719166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019901039600372316, AUC: 0.5079928333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010722840229670206, AUC: 0.6268785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001065795143445333, AUC: 0.6511400833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010672556956609091, AUC: 0.6540048333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004992599646250407, AUC: 0.3883905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001113569974899292, AUC: 0.5305151666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101508339246114, AUC: 0.5553133333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010981414715449016, AUC: 0.5683530833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004242204030354818, AUC: 0.44027075000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010937814315160116, AUC: 0.5913730833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010776551564534505, AUC: 0.6300409166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010835390090942383, AUC: 0.62701025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029406830469767254, AUC: 0.5880721666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010836275815963746, AUC: 0.6296732500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010813354651133219, AUC: 0.6267249166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010722055435180664, AUC: 0.6438299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004063597440719604, AUC: 0.514617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010883302688598632, AUC: 0.5808056666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010941217343012491, AUC: 0.5523899166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010946215391159058, AUC: 0.5514673333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015948575337727865, AUC: 0.5340653333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010999303261439006, AUC: 0.5856963333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011032426357269287, AUC: 0.6176274166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001080282966295878, AUC: 0.6403633333333333\n",
      "\n",
      "[['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.5114340166666667, 0.002092150513974722, 0.6581454083333332, 0.002239285218479788, 0.6627791416666666, 0.0017482602106209013, 0.6753271916666668, 0.0006883278604778474, 1, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.45691495, 0.0028386302317697193, 0.59592865, 0.0037556808618108345, 0.6378666833333334, 0.002322282924477501, 0.6431673333333332, 0.002246811227576389, 1, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0001, 0.515782875, 0.002552141191850348, 0.62764935, 0.001982959012012226, 0.6479962, 0.002529749873798888, 0.6613433, 0.0016453860869933355, 1, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.4895805416666666, 0.003779200803835072, 0.6248646583333334, 0.002882340665858958, 0.6222697499999998, 0.0017123422166319447, 0.6443301833333333, 0.0013163547528552775, 5, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.47748281666666664, 0.0032497889435233324, 0.6187936166666665, 0.002567033028440556, 0.6381672083333332, 0.0027841285161003455, 0.664804775, 0.0006790309580834018, 5, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0001, 0.5117901250000001, 0.003231998553382294, 0.5792651666666667, 0.0016324257262222228, 0.6122306833333334, 0.0004439571837886108, 0.6057758, 0.001520786319021109, 5, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.46296099166666665, 0.0019082223064422932, 0.6100139416666666, 0.002754494862472292, 0.6399996583333334, 0.0008978640343978477, 0.6340224833333332, 0.002929749149266391, 10, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.471623825, 0.0032281022349631245, 0.602375025, 0.0013438334068820146, 0.6372704249999999, 0.0018992524104297903, 0.618881275, 0.0019603903308861854, 10, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0001, 0.500481625, 0.003414226224967011, 0.6072900583333333, 0.001403928877082013, 0.6135694333333334, 0.0021070331995913867, 0.6297357333333332, 0.001898588301142776, 10, False]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class euclidean distance capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['distance'] = 'euclidean'\n",
    "\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"distance_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm, None]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70f1ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bac108e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0022090151309967043, AUC: 0.482327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00109909454981486, AUC: 0.5001666666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986895163853964, AUC: 0.5001666666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010929495890935263, AUC: 0.5460208333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001519055922826131, AUC: 0.4967521666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010438531239827474, AUC: 0.6816228333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010392544269561767, AUC: 0.6899668333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001037814458211263, AUC: 0.6980486666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005044939676920573, AUC: 0.6125878333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010688435236612956, AUC: 0.6570639166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010312530597050985, AUC: 0.697778\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010634669462839762, AUC: 0.6862102499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007575435161590576, AUC: 0.5238689166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107310418287913, AUC: 0.6662355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001072569489479065, AUC: 0.6746176666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001085254430770874, AUC: 0.6771743333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028289421399434406, AUC: 0.4885859166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010410460233688354, AUC: 0.7036060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009533187349637349, AUC: 0.7297900833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015043843189875286, AUC: 0.6886336666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014473302841186524, AUC: 0.5911926666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098763108253479, AUC: 0.5004998333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098947803179423, AUC: 0.5001666666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010997874736785888, AUC: 0.5001666666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025886443456013997, AUC: 0.4500475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010358128547668457, AUC: 0.6953330000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010308589537938435, AUC: 0.7041645833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010385271708170574, AUC: 0.7070993333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022589210510253907, AUC: 0.46242933333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001078564206759135, AUC: 0.6206535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010257769425710043, AUC: 0.7118113333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010619584321975707, AUC: 0.6875585000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008191946665445964, AUC: 0.5690634999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001083682338396708, AUC: 0.6152215833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010341012477874756, AUC: 0.6949535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001081222176551819, AUC: 0.6787115833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008309387842814127, AUC: 0.4769141666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010737499396006266, AUC: 0.6571966666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001075956702232361, AUC: 0.6781427500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010619895855585734, AUC: 0.6984168333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004305452505747477, AUC: 0.5750994166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011089092095692952, AUC: 0.7151464999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012463964621225992, AUC: 0.7201233333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013027705351511638, AUC: 0.7324314166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003910218000411987, AUC: 0.544082\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010496389865875244, AUC: 0.72604325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105039079984029, AUC: 0.7358148333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011878447532653808, AUC: 0.7490896666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005690385818481445, AUC: 0.49387066666666657\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010647862354914348, AUC: 0.6563656666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001203441341718038, AUC: 0.7176055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001217331329981486, AUC: 0.74703475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003779361804326375, AUC: 0.5448866666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011112850507100423, AUC: 0.7071536666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011028506358464558, AUC: 0.7401751666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013160616159439086, AUC: 0.7339251666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006351720174153645, AUC: 0.5132059166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010897442897160848, AUC: 0.7115326666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001162826657295227, AUC: 0.7270785000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011613900264104207, AUC: 0.7405481666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023979299863179523, AUC: 0.4796194999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001573287049929301, AUC: 0.4465569166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00119786270459493, AUC: 0.7350076666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013269410133361817, AUC: 0.7094534166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001604472041130066, AUC: 0.5758475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010587514638900756, AUC: 0.704365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011456350485483805, AUC: 0.7303601666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012554979721705118, AUC: 0.7339838333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008675639470418293, AUC: 0.4127057499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010791893800099692, AUC: 0.7108243333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011739135185877482, AUC: 0.72049275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001234118382136027, AUC: 0.730929\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0067029595375061035, AUC: 0.3974898333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010041511456171672, AUC: 0.7091404999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011437125205993653, AUC: 0.7278484999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011817816495895386, AUC: 0.7456888333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001600554903348287, AUC: 0.41348500000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011192431449890137, AUC: 0.7084774166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001034453272819519, AUC: 0.7212149999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011828157901763916, AUC: 0.7366116666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002877264340718587, AUC: 0.532575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010806144078572592, AUC: 0.6646406666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102540135383606, AUC: 0.6857524999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011551042397816976, AUC: 0.6766275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034697853724161786, AUC: 0.47786075000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010743688344955445, AUC: 0.6145121666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009478311737378439, AUC: 0.7400715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009958248933156332, AUC: 0.7294850833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004176090399424235, AUC: 0.52501275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989094177881876, AUC: 0.5001668333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010521700382232666, AUC: 0.6860750833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010794554551442465, AUC: 0.6952201666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005168417771657308, AUC: 0.4449969166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010801597038904826, AUC: 0.6529916666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011456027428309124, AUC: 0.5554916666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011782394647598267, AUC: 0.628238\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004230804284413655, AUC: 0.5408441666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010700388352076213, AUC: 0.6747928333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010877378384272257, AUC: 0.7073590833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010978849331537883, AUC: 0.72440575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008166804472605387, AUC: 0.41604424999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102625608444214, AUC: 0.5005001666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011034855842590333, AUC: 0.5757086666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001142662247021993, AUC: 0.6202011666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006057216326395671, AUC: 0.5505013333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011012241840362548, AUC: 0.6509624166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009897990624109904, AUC: 0.7101106666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012075997591018677, AUC: 0.6727653333333331\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002164623498916626, AUC: 0.45542900000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010173428455988566, AUC: 0.7310378333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010583391586939494, AUC: 0.7474667500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009997122883796691, AUC: 0.7504073333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0104118808110555, AUC: 0.4433228333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010787751277287802, AUC: 0.6722560833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010502953926722208, AUC: 0.7249629999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010755687952041626, AUC: 0.7402831666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021465911865234374, AUC: 0.4524713333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010445525248845419, AUC: 0.6952054166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011094758113225302, AUC: 0.6579646666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011546998421351116, AUC: 0.6694478333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00572831122080485, AUC: 0.4994456666666666\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011008405685424805, AUC: 0.5001766666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011000505685806274, AUC: 0.5063233333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010945692857106527, AUC: 0.5684515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002201924006144206, AUC: 0.5847720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010677912235260009, AUC: 0.6394275833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010587460597356161, AUC: 0.6679229166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010557381709416708, AUC: 0.6791149166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005102400302886963, AUC: 0.4865385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010583461920420328, AUC: 0.6661760833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001045379360516866, AUC: 0.6912495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010401338736216227, AUC: 0.7031131666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004726005872090658, AUC: 0.5488045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001022370219230652, AUC: 0.6852906666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010080135663350423, AUC: 0.6974095833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000960826595624288, AUC: 0.7158985000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015556090672810873, AUC: 0.4842736666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010861006180445353, AUC: 0.6344921666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010785254637400309, AUC: 0.67159175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001044673760732015, AUC: 0.6866560833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017733447949091594, AUC: 0.48636725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011027741432189941, AUC: 0.4995001666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011019036372502645, AUC: 0.49983333333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110119632879893, AUC: 0.49983333333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034442949295043945, AUC: 0.4704391666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010883770386377971, AUC: 0.6132594166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010845179557800292, AUC: 0.6388421666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010747142235438029, AUC: 0.6595682500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020034749110539756, AUC: 0.43065016666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011019164721171061, AUC: 0.5006778333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011016519467035928, AUC: 0.5023274999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101508895556132, AUC: 0.5043019999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009930801709493002, AUC: 0.51239875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010687155723571777, AUC: 0.6460704166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010543562968571981, AUC: 0.6869537500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001047417720158895, AUC: 0.6997224166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002457318067550659, AUC: 0.5285763333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001014769474665324, AUC: 0.705224\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009913925925890605, AUC: 0.7157886666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009702782233556111, AUC: 0.727957\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034706683158874513, AUC: 0.3988265833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006752252578735, AUC: 0.5001666666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992457071940104, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100489377975464, AUC: 0.5001666666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028488404750823975, AUC: 0.43082833333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010952140092849731, AUC: 0.583875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010827640295028686, AUC: 0.6875030833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010670712391535441, AUC: 0.7076433333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005266226609547933, AUC: 0.5004606666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994198719660442, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989980697631836, AUC: 0.5003333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010995082060496013, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016563217639923096, AUC: 0.573664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001041029135386149, AUC: 0.7060684166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010298808813095093, AUC: 0.7129080833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010873279968897501, AUC: 0.7060911666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006314749558766683, AUC: 0.4843851666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988652706146241, AUC: 0.5084810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010772671302159628, AUC: 0.6652418333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011133761008580525, AUC: 0.5865530833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005791969458262125, AUC: 0.5090520833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011013295650482177, AUC: 0.502834\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001030515948931376, AUC: 0.7073949166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010761269330978394, AUC: 0.6858559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005709685961405436, AUC: 0.44724224999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010861074129740397, AUC: 0.6213524166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001063911239306132, AUC: 0.67445925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001090489387512207, AUC: 0.6539531666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0069627947807312015, AUC: 0.5821179166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010684135754903158, AUC: 0.6757653333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010584844350814819, AUC: 0.6836916666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010926491419474284, AUC: 0.6504676666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006695546627044678, AUC: 0.5350720833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009797421495119731, AUC: 0.7214695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011337598959604899, AUC: 0.7239596666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009805366198221842, AUC: 0.7433275833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007619559288024902, AUC: 0.48702716666666673\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010920956532160441, AUC: 0.5631842499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010689677000045776, AUC: 0.6611494166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010905439058939616, AUC: 0.65381925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007556658903757731, AUC: 0.5752039166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010786496003468832, AUC: 0.6079970833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010936845541000366, AUC: 0.7194765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011245852708816528, AUC: 0.7220306666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011269750912984213, AUC: 0.46792125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011278841098149618, AUC: 0.6768278333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011887166897455851, AUC: 0.7021645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011056175629297893, AUC: 0.7144\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01459325917561849, AUC: 0.5081552500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010601043303807576, AUC: 0.7084429166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010873599449793497, AUC: 0.7278953333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011564232110977174, AUC: 0.7335751666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003921881039937338, AUC: 0.4750769166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011191973686218261, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010166149139404297, AUC: 0.7005333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010840984582901, AUC: 0.713018\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003322593371073405, AUC: 0.4477309166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102803627649943, AUC: 0.6791748333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012248289187749228, AUC: 0.7071253333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012829253276189169, AUC: 0.728382\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018393969138463338, AUC: 0.5175529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087230086326599, AUC: 0.7093696666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011375640233357747, AUC: 0.7284576666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001197864294052124, AUC: 0.7413935833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005226225217183431, AUC: 0.5937110833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010917723576227825, AUC: 0.7122641666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011479159593582153, AUC: 0.7408241666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001248996655146281, AUC: 0.6998175833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006062014738718669, AUC: 0.4775778333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010966151158014934, AUC: 0.5714571666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011218514839808145, AUC: 0.6405858333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011022345622380574, AUC: 0.6664464166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007324358304341634, AUC: 0.5459616666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010716644525527953, AUC: 0.6934288333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011885886192321777, AUC: 0.7100834166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001212502876917521, AUC: 0.7229\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004353785673777262, AUC: 0.49921525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102708657582601, AUC: 0.6925908333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001052934686342875, AUC: 0.68595775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011812189817428588, AUC: 0.7284591666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022327703634897867, AUC: 0.42304858333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010655484596888225, AUC: 0.6957322499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001039730707804362, AUC: 0.7123454166666668\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010980440775553385, AUC: 0.7122550833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029827739397684733, AUC: 0.49234633333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010995362997055053, AUC: 0.5006681666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001045853098233541, AUC: 0.701257\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010761226812998454, AUC: 0.6972056666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004204115390777588, AUC: 0.5671574166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010754772822062174, AUC: 0.6596719166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011111609141031902, AUC: 0.6449171666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001150728146235148, AUC: 0.6672355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003342944860458374, AUC: 0.51839325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010622838338216146, AUC: 0.6769604166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973263184229532, AUC: 0.674305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011038169860839843, AUC: 0.6984254166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004540504614512126, AUC: 0.4444675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010318257808685303, AUC: 0.7069801666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010223760604858398, AUC: 0.7141586666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010839794476826985, AUC: 0.6707595000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002260911226272583, AUC: 0.5900266666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001044940710067749, AUC: 0.6839614166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001049545447031657, AUC: 0.6981123333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010589594841003418, AUC: 0.7040466666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010725399653116863, AUC: 0.48886375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010609846512476602, AUC: 0.7044275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010654333432515463, AUC: 0.7139842500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001214697003364563, AUC: 0.6539589166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002706189791361491, AUC: 0.5600708333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010271739959716796, AUC: 0.7158112500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011107821861902872, AUC: 0.7027941666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012388704617818196, AUC: 0.6423309166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003714893976847331, AUC: 0.544871\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010538500944773355, AUC: 0.6967611666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010431599219640096, AUC: 0.7184723333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011901019016901653, AUC: 0.6583985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021029463211695355, AUC: 0.44257216666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008500655492147, AUC: 0.49983316666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011069763501485189, AUC: 0.5001666666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001049164613087972, AUC: 0.6919865000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019739827712376913, AUC: 0.44729733333333327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010990289052327474, AUC: 0.5019999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984016259511311, AUC: 0.5023399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010982683897018432, AUC: 0.5024973333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012587910334269205, AUC: 0.5871418333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010709030628204346, AUC: 0.6714015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010609379609425862, AUC: 0.6918846666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009759029348691304, AUC: 0.71105275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005323325634002686, AUC: 0.42927883333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001022761344909668, AUC: 0.68700925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010283805131912232, AUC: 0.6979777500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010275872151056925, AUC: 0.7032234166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021762319405873617, AUC: 0.4504154166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010181482235590616, AUC: 0.6839735833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009829556147257488, AUC: 0.7068795833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010452215671539306, AUC: 0.7131650833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003230017264684041, AUC: 0.4305717499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002097924550374, AUC: 0.49933350000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099769671758016, AUC: 0.4998333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994075139363606, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024634336630503335, AUC: 0.490211\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001077680190404256, AUC: 0.6457984166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001076430837313334, AUC: 0.6570032499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010700429677963257, AUC: 0.6672855000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002607105255126953, AUC: 0.5552651666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010801562468210856, AUC: 0.6541130833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010730937321980795, AUC: 0.6720155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001072399655977885, AUC: 0.6781817499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006407520929972331, AUC: 0.5678588333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010802853504816691, AUC: 0.6966570833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009711410800615946, AUC: 0.7147993333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001065695842107137, AUC: 0.71696075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005250612258911133, AUC: 0.5860303333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010703077713648478, AUC: 0.64817925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010628116925557455, AUC: 0.6647990833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010587199926376343, AUC: 0.6749936666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018748664458592733, AUC: 0.4706449166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005863348642986, AUC: 0.49920950000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010798596938451132, AUC: 0.6650127499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010773632129033406, AUC: 0.6854173333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008146313985188803, AUC: 0.5808761666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010526421864827473, AUC: 0.6612855000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001135362466176351, AUC: 0.7051951666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009759151339530944, AUC: 0.7416966666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023454844951629637, AUC: 0.5297878333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010794730186462402, AUC: 0.62822475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011133111715316772, AUC: 0.5278122499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010976545810699463, AUC: 0.6513575833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014223788181940715, AUC: 0.5224622499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010454503297805786, AUC: 0.6780916666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010353075663248699, AUC: 0.69372\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010583542585372926, AUC: 0.6862388333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024028308391571044, AUC: 0.49006358333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010693954626719156, AUC: 0.6727655833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010808497667312623, AUC: 0.6940559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010732080936431884, AUC: 0.7097349999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013988483111063639, AUC: 0.4455411666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010927412509918212, AUC: 0.6053450833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011043146053949992, AUC: 0.6217509166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106628894805908, AUC: 0.6559947500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004422988891601563, AUC: 0.5503729166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010433575709660847, AUC: 0.6786419166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008089383443196, AUC: 0.6162728333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010845898389816285, AUC: 0.6851226666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005087791760762532, AUC: 0.4093319166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006309986114503, AUC: 0.500665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011059788465499877, AUC: 0.5013236666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010738699436187743, AUC: 0.6729732500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018893845081329346, AUC: 0.4291089166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010481082598368327, AUC: 0.6983983333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010514597495396931, AUC: 0.70839325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001050307313601176, AUC: 0.7131291666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004436244964599609, AUC: 0.545117\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010938462813695272, AUC: 0.5673266666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010605715910593668, AUC: 0.6882204166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010707963307698568, AUC: 0.6869113333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004023472229639689, AUC: 0.43386450000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010817637046178182, AUC: 0.6372235833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010908825397491456, AUC: 0.6264553333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010877640644709268, AUC: 0.6743955833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008992623329162598, AUC: 0.6229528333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010750957727432252, AUC: 0.7155950000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012093165715535482, AUC: 0.7241813333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012676773468653362, AUC: 0.7282110833333334\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.003582649310429891, AUC: 0.4832521666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009886308113733928, AUC: 0.7111300000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011806331078211467, AUC: 0.7298126666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010766961177190145, AUC: 0.7630954166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034387753009796143, AUC: 0.523011\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010720754861831665, AUC: 0.6975636666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012911065816879272, AUC: 0.727802\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014056069453557331, AUC: 0.7328014999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037713797092437744, AUC: 0.5320789166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011224740346272787, AUC: 0.7285950833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012205634514490763, AUC: 0.7380878333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00104001251856486, AUC: 0.6944266666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021871374448140463, AUC: 0.5663385000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010685537258783976, AUC: 0.72108675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011838592290878296, AUC: 0.7358033333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001234346826871236, AUC: 0.7399564999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022392065525054933, AUC: 0.5637098333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010774617592493694, AUC: 0.7234612500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012630499601364135, AUC: 0.7272101666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012684125105539958, AUC: 0.7443641666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006307536760965983, AUC: 0.46285083333333327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011411002079645793, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011476464271545411, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011250032583872476, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002916451613108317, AUC: 0.55075875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010871668656667075, AUC: 0.7219167500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012449927727381388, AUC: 0.7308326666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012526632150014242, AUC: 0.741138\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003477248271306356, AUC: 0.44715774999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010187264283498128, AUC: 0.6990361666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012461460034052531, AUC: 0.7337696666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013189704418182372, AUC: 0.7424593333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013559090932210286, AUC: 0.622406\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011489016612370808, AUC: 0.6868693333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011830063660939534, AUC: 0.7165126666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012111798524856567, AUC: 0.7208798333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005678338686625163, AUC: 0.5876929999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011007721026738485, AUC: 0.4996683333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011206634044647217, AUC: 0.5415136666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011439387798309326, AUC: 0.64068925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002687066316604614, AUC: 0.38380525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010915251970291138, AUC: 0.627567\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008321444193523, AUC: 0.6857365833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011123105684916177, AUC: 0.7107096666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006227914492289225, AUC: 0.5584809166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00108717676003774, AUC: 0.6578560000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001129621386528015, AUC: 0.6594529166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011291395823160807, AUC: 0.6967664166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00573676856358846, AUC: 0.59110075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001074197769165039, AUC: 0.6770234999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001069149136543274, AUC: 0.71725525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011871747573216757, AUC: 0.6798985833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006779763221740722, AUC: 0.5304124166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984635353088379, AUC: 0.5250699999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010798592567443847, AUC: 0.6616996666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001121928890546163, AUC: 0.67675025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005193284511566162, AUC: 0.44713866666666663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000984079897403717, AUC: 0.727186\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010334900816281636, AUC: 0.7380258333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010064286986986796, AUC: 0.74431875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005023056825002034, AUC: 0.5036661666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110491144657135, AUC: 0.5842776666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010871562560399374, AUC: 0.6829763333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012332028945287068, AUC: 0.618509\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005822939078013102, AUC: 0.47437124999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010655659834543864, AUC: 0.6875300000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011390140056610108, AUC: 0.6300491666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011582982142766317, AUC: 0.6765664166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004415445963541667, AUC: 0.41078600000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010303716659545898, AUC: 0.6984268333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086552341779073, AUC: 0.6708329166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012135977347691855, AUC: 0.6160311666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027525893052419027, AUC: 0.470472\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010359586477279663, AUC: 0.7017764999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011351085106531779, AUC: 0.625438\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010887155930201212, AUC: 0.7025426666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011480908075968424, AUC: 0.56979375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010956135988235474, AUC: 0.531176\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010831988255182903, AUC: 0.6123044166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010814192692438762, AUC: 0.6351195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029432780742645265, AUC: 0.5034464166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010247449874877929, AUC: 0.6952205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010497469902038573, AUC: 0.7028243333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011509758234024047, AUC: 0.6942979166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029635618527730305, AUC: 0.5617679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010802778800328573, AUC: 0.6182026666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001078430692354838, AUC: 0.6311623333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010841381947199503, AUC: 0.6207576666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003696163336435954, AUC: 0.4816928333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010666314363479614, AUC: 0.65036825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010667692422866822, AUC: 0.6698179166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001073760747909546, AUC: 0.6751929166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006191806634267171, AUC: 0.40005508333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010883626540501912, AUC: 0.5903658333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010806504090627035, AUC: 0.6145495833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010868995984395346, AUC: 0.5983451666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01627521006266276, AUC: 0.4131719166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010253814458847045, AUC: 0.7048333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010136052767435709, AUC: 0.7077755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009697354038556416, AUC: 0.7139413333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009026017506917318, AUC: 0.52319975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001078840692838033, AUC: 0.63663875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010707332690556843, AUC: 0.6604795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001073810577392578, AUC: 0.6541121666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00378770915667216, AUC: 0.48412575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010903554360071818, AUC: 0.5763249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010900805791219075, AUC: 0.5834705833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010849279165267944, AUC: 0.609808\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01173632558186849, AUC: 0.49675125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010905423164367675, AUC: 0.6045753333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001082624912261963, AUC: 0.6175529166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010762329498926798, AUC: 0.6349049999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004105034510294596, AUC: 0.6023314166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001042192260424296, AUC: 0.6804012500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010043429732322694, AUC: 0.7004837500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010323208967844646, AUC: 0.7005606666666667\n",
      "\n",
      "[['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.5153769, 0.0028860051946886112, 0.62975995, 0.004906845680226668, 0.6581558083333333, 0.006465472997693128, 0.6568040666666667, 0.004648912236905273, 10, False, 'start_epoch=2'], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.49502922499999996, 0.004134451048782011, 0.6795605916666667, 0.00633431368355201, 0.7275721416666665, 5.3550810475069206e-05, 0.7359695916666666, 0.0001159570057909026, 10, False, 'start_epoch=2'], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.48390583333333337, 0.00213370798420278, 0.6357066083333334, 0.005396145725451457, 0.6790963583333334, 0.0038763585928903435, 0.6907081333333334, 0.0018527176645975012, 10, False, 'start_epoch=2'], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0001, 0.5032266, 0.0016732328508080572, 0.6090295, 0.005688288977949996, 0.62782425, 0.0070637390695375045, 0.6444617166666667, 0.006811749858091946, 10, False, 'start_epoch=2'], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.49486762500000003, 0.0031434805177295126, 0.5883196583333334, 0.007023642449704789, 0.6516641249999999, 0.006109054124046182, 0.6387877916666668, 0.0064213499167197945, 5, False, 'start_epoch=2'], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.5108107083333333, 0.002064140553667012, 0.6551553333333333, 0.0046255551528180585, 0.7063103833333334, 0.0007120368829266684, 0.7170422583333333, 0.00040642617863534674, 5, False, 'start_epoch=2'], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.50718175, 0.003054529709987499, 0.6540807416666666, 0.006147559578967293, 0.6780513, 0.003971287255033608, 0.6796602666666665, 0.0005208727481372208, 5, False, 'start_epoch=2'], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0001, 0.5014715416666666, 0.0038647835658809026, 0.6187675166666666, 0.006278417142590001, 0.6472545249999999, 0.005674668072636186, 0.6552777583333333, 0.006198429991803403, 5, False, 'start_epoch=2'], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.493652625, 0.0032787553049017336, 0.6327968083333333, 0.003340531824280626, 0.6383199833333333, 0.004992960211424721, 0.6877554833333333, 0.0006777071414858309, 1, False, 'start_epoch=2'], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.5374516583333333, 0.003312196799465904, 0.6905254000000001, 0.004194178357384447, 0.7064012333333334, 0.004767339207451112, 0.71073325, 0.0052227669365666664, 1, False, 'start_epoch=2'], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.4957926416666666, 0.004595417763372288, 0.6386381833333333, 0.005492300063408057, 0.6612980333333334, 0.002679076472829444, 0.6762782166666668, 0.001517160350432223, 1, False, 'start_epoch=2'], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0001, 0.5036336166666666, 0.0037702047651002753, 0.6288106916666666, 0.0027842467983347934, 0.6500420833333334, 0.0017691102866583344, 0.6537040333333334, 0.0014709595913475007, 1, False, 'start_epoch=2']]\n"
     ]
    }
   ],
   "source": [
    "# 3 class cosine distance capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-4, 5e-3, 1e-3, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [10, 5, 1]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['distance'] = 'cosine'\n",
    "\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    #loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                loss_fn_args['print_capped'] = False\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['print_capped'] = False\n",
    "                loss_fn_args['loss_cap'] = cap\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"cosine_distance_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm, 'start_epoch=2']\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84c63d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d17d841c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011114914417266846, AUC: 0.36460475000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010337845484415691, AUC: 0.6661265833333333\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m     _, avg_tensor \u001b[38;5;241m=\u001b[39m network(avg_tensors_list[k])\n\u001b[1;32m     34\u001b[0m     loss_fn_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_tensors\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(avg_tensor)\n\u001b[0;32m---> 35\u001b[0m _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_softmax_with_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCappedCELossAvgDistance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     37\u001b[0m     _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_softmax(test_loader_reduced, network, embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:346\u001b[0m, in \u001b[0;36mtrain_softmax_with_embeddings\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m    344\u001b[0m output, embeds \u001b[38;5;241m=\u001b[39m network(data)\n\u001b[1;32m    345\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze(), target, smote_target, embeds\u001b[38;5;241m=\u001b[39membeds)\n\u001b[0;32m--> 346\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3 class cosine distance capped smote w/ average tensor \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['distance'] = 'cosine'\n",
    "\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                loss_fn_args['avg_tensors'] = []\n",
    "                for k in range(NUM_CLASSES_REDUCED):\n",
    "                    _, avg_tensor = network(avg_tensors_list[k])\n",
    "                    loss_fn_args['avg_tensors'].append(avg_tensor)\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELossAvgDistance, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"cosine_distance_capped_smote_avg\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm, None]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f9b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a83179ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010977579752604167, AUC: 0.5471043333333333\n",
      "\n",
      "Train loss: 0.22670290546214328\n",
      "Train loss: 0.24609810189997897\n",
      "Train loss: 0.2162285436975195\n",
      "Train loss: 0.21934155656936322\n",
      "Train loss: 0.2423914039388616\n",
      "\n",
      "Test set: Avg. loss: 0.001086990475654602, AUC: 0.6655497499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010696454842885335, AUC: 0.6809255833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010534924666086832, AUC: 0.6828300833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096967101097107, AUC: 0.5540400833333333\n",
      "\n",
      "Train loss: 0.3043331108194716\n",
      "Train loss: 0.30202328118872135\n",
      "Train loss: 0.3150767704273792\n",
      "Train loss: 0.3039932352431277\n",
      "Train loss: 0.31126081943511963\n",
      "\n",
      "Test set: Avg. loss: 0.0010940040747324626, AUC: 0.6071353333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010892706712086996, AUC: 0.6661772500000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010844313303629558, AUC: 0.6989626666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011072221994400024, AUC: 0.4322525\n",
      "\n",
      "Train loss: 0.25621940759902306\n",
      "Train loss: 0.25718629537744725\n",
      "Train loss: 0.24687633691950048\n",
      "Train loss: 0.24878853939949197\n",
      "Train loss: 0.23803025661630833\n",
      "\n",
      "Test set: Avg. loss: 0.0011008050044377644, AUC: 0.5103728333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010904593070348103, AUC: 0.5859034166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010789347092310588, AUC: 0.6437806666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110916268825531, AUC: 0.35976708333333335\n",
      "\n",
      "Train loss: 0.27164848129800023\n",
      "Train loss: 0.2985845631741463\n",
      "Train loss: 0.2833634480516961\n",
      "Train loss: 0.28820335078746717\n",
      "Train loss: 0.28930677606704386\n",
      "\n",
      "Test set: Avg. loss: 0.0011035532156626383, AUC: 0.4263143333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098180095354716, AUC: 0.5362820833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010945879220962524, AUC: 0.6033534166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010996756156285603, AUC: 0.4939538333333333\n",
      "\n",
      "Train loss: 0.26684618402034677\n",
      "Train loss: 0.2655425150343712\n",
      "Train loss: 0.2637613278754214\n",
      "Train loss: 0.2625646515095488\n",
      "Train loss: 0.27259465379917874\n",
      "\n",
      "Test set: Avg. loss: 0.0010959258874257405, AUC: 0.55239525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010899982849756876, AUC: 0.6162824166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010836057662963867, AUC: 0.6608156666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097235878308614, AUC: 0.53987625\n",
      "\n",
      "Train loss: 0.24906214221994927\n",
      "Train loss: 0.2228831618390185\n",
      "Train loss: 0.22117721501817095\n",
      "Train loss: 0.21119252316495205\n",
      "Train loss: 0.25207892859235725\n",
      "\n",
      "Test set: Avg. loss: 0.0010920849641164144, AUC: 0.6228047500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010840352773666382, AUC: 0.6778507500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001074101726214091, AUC: 0.6965210833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994513034820557, AUC: 0.5689089166666667\n",
      "\n",
      "Train loss: 0.26565324245615207\n",
      "Train loss: 0.2756113288250375\n",
      "Train loss: 0.25993522126623925\n",
      "Train loss: 0.25192763982935157\n",
      "Train loss: 0.2564177710959252\n",
      "\n",
      "Test set: Avg. loss: 0.0010962652762730916, AUC: 0.6156206666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010907398462295533, AUC: 0.6662098333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010842584371566772, AUC: 0.6882850833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011036330858866373, AUC: 0.41170016666666664\n",
      "\n",
      "Train loss: 0.20256761287121064\n",
      "Train loss: 0.1990937205071145\n",
      "Train loss: 0.19113869185143328\n",
      "Train loss: 0.19698455206891324\n",
      "Train loss: 0.18415525187837317\n",
      "\n",
      "Test set: Avg. loss: 0.001095029592514038, AUC: 0.5597735833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010829739173253377, AUC: 0.6359120833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010736297766367594, AUC: 0.65424\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001109179695447286, AUC: 0.492542\n",
      "\n",
      "Train loss: 0.2726488838804529\n",
      "Train loss: 0.2667148891915666\n",
      "Train loss: 0.2623840659222704\n",
      "Train loss: 0.27433423919880645\n",
      "Train loss: 0.2656566891264408\n",
      "\n",
      "Test set: Avg. loss: 0.0011033058563868206, AUC: 0.5623021666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010951735178629557, AUC: 0.6491028333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010884065628051758, AUC: 0.6865855000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001114657998085022, AUC: 0.37763758333333336\n",
      "\n",
      "Train loss: 0.1593544194038878\n",
      "Train loss: 0.14676678687968153\n",
      "Train loss: 0.15056192113998088\n",
      "Train loss: 0.13230974217678637\n",
      "Train loss: 0.13317370085006064\n",
      "\n",
      "Test set: Avg. loss: 0.0011077995697657268, AUC: 0.44353291666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010979957580566407, AUC: 0.6209344166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001089155356089274, AUC: 0.6674314166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006116072336833, AUC: 0.55615925\n",
      "\n",
      "Train loss: 0.1739715467108057\n",
      "Train loss: 0.15653977926741255\n",
      "Train loss: 0.13294198386212613\n",
      "Train loss: 0.1429826609631802\n",
      "Train loss: 0.1290777140475334\n",
      "\n",
      "Test set: Avg. loss: 0.0010937082767486573, AUC: 0.6334323333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001082950194676717, AUC: 0.6721998333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107327930132548, AUC: 0.683088\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011057693163553874, AUC: 0.3858436666666667\n",
      "\n",
      "Train loss: 0.24064553113693887\n",
      "Train loss: 0.2064819376519386\n",
      "Train loss: 0.21588386855226882\n",
      "Train loss: 0.17982669155648415\n",
      "Train loss: 0.17986807696362758\n",
      "\n",
      "Test set: Avg. loss: 0.0011015419960021973, AUC: 0.43795041666666673\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001093891183535258, AUC: 0.5749518333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088539759318034, AUC: 0.6279371666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011024139722188315, AUC: 0.4978894166666667\n",
      "\n",
      "Train loss: 0.26839645771270104\n",
      "Train loss: 0.27835660589502215\n",
      "Train loss: 0.27949503812384097\n",
      "Train loss: 0.2606491989277779\n",
      "Train loss: 0.26408321299451465\n",
      "\n",
      "Test set: Avg. loss: 0.0011005994081497192, AUC: 0.5241239166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010970083077748616, AUC: 0.5584187500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001093760093053182, AUC: 0.5759050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096644639968872, AUC: 0.611678\n",
      "\n",
      "Train loss: 0.25404399050042986\n",
      "Train loss: 0.25992841568398983\n",
      "Train loss: 0.25177958518900767\n",
      "Train loss: 0.23935310384060474\n",
      "Train loss: 0.23407640431789642\n",
      "\n",
      "Test set: Avg. loss: 0.0010928446054458618, AUC: 0.6405930000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087654709815979, AUC: 0.6676282499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010823326508204141, AUC: 0.6846369999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011078151861826579, AUC: 0.40171341666666666\n",
      "\n",
      "Train loss: 0.24430899417146723\n",
      "Train loss: 0.20867254987676093\n",
      "Train loss: 0.21637885748071872\n",
      "Train loss: 0.19242410659790038\n",
      "Train loss: 0.18122758307355516\n",
      "\n",
      "Test set: Avg. loss: 0.0011029911835988362, AUC: 0.4495942499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010952550570170084, AUC: 0.5830144166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010898833672205606, AUC: 0.6362035833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011047050555547078, AUC: 0.40920141666666665\n",
      "\n",
      "Train loss: 0.2819150163772258\n",
      "Train loss: 0.272119491911949\n",
      "Train loss: 0.2729741164978514\n",
      "Train loss: 0.26554992934490773\n",
      "Train loss: 0.2751561129346807\n",
      "\n",
      "Test set: Avg. loss: 0.001103366494178772, AUC: 0.4202754166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006934642791748, AUC: 0.4608425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010985374053319296, AUC: 0.5144795833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010993908246358236, AUC: 0.55114475\n",
      "\n",
      "Train loss: 0.22695925768385541\n",
      "Train loss: 0.22883726180867947\n",
      "Train loss: 0.2434353143610853\n",
      "Train loss: 0.21246468843297756\n",
      "Train loss: 0.22805507538166453\n",
      "\n",
      "Test set: Avg. loss: 0.001092501481374105, AUC: 0.5754229166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010821839173634847, AUC: 0.6382609166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001072401523590088, AUC: 0.6612424999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010997863213221232, AUC: 0.5442776666666668\n",
      "\n",
      "Train loss: 0.22948297642646953\n",
      "Train loss: 0.22146393441139384\n",
      "Train loss: 0.20313969926631198\n",
      "Train loss: 0.19421831039672202\n",
      "Train loss: 0.21769214767090816\n",
      "\n",
      "Test set: Avg. loss: 0.0010959505240122477, AUC: 0.5862051666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010904568831125894, AUC: 0.6294255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010858723322550456, AUC: 0.642788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001113751530647278, AUC: 0.46476741666666666\n",
      "\n",
      "Train loss: 0.27520943169898177\n",
      "Train loss: 0.2590181026052921\n",
      "Train loss: 0.2655629381220391\n",
      "Train loss: 0.27385654627008643\n",
      "Train loss: 0.2541143531494952\n",
      "\n",
      "Test set: Avg. loss: 0.001107380469640096, AUC: 0.50114775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100283662478129, AUC: 0.5446071666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010961368481318156, AUC: 0.5829728333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011004146337509155, AUC: 0.5335329166666667\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.22865817369298733\n",
      "Train loss: 0.2219101591313139\n",
      "Train loss: 0.21008287921864935\n",
      "Train loss: 0.2049694005479204\n",
      "Train loss: 0.18186203621803446\n",
      "\n",
      "Test set: Avg. loss: 0.0010946365594863893, AUC: 0.5833615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001084994633992513, AUC: 0.6356315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010774826606114705, AUC: 0.65485025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106550971666972, AUC: 0.39227341666666665\n",
      "\n",
      "Train loss: 0.12493679219103875\n",
      "Train loss: 0.03730655477402058\n",
      "Train loss: 0.027834622657045404\n",
      "Train loss: 0.017303673003582244\n",
      "Train loss: 0.01561295656447715\n",
      "\n",
      "Test set: Avg. loss: 0.0009611998597780863, AUC: 0.7801376666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015269558827082316, AUC: 0.7485891666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014714843034744263, AUC: 0.8214801666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001093711773554484, AUC: 0.6077783333333334\n",
      "\n",
      "Train loss: 0.06810512289087824\n",
      "Train loss: 0.025651184548722936\n",
      "Train loss: 0.019604093977745544\n",
      "Train loss: 0.012258205261636288\n",
      "Train loss: 0.025885360798937208\n",
      "\n",
      "Test set: Avg. loss: 0.001018695871035258, AUC: 0.7407453333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013246962229410807, AUC: 0.7607241666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001485967993736267, AUC: 0.7876276666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010990929206212362, AUC: 0.5330248333333333\n",
      "\n",
      "Train loss: 0.10847122796038364\n",
      "Train loss: 0.030408769972780917\n",
      "Train loss: 0.019496633651408742\n",
      "Train loss: 0.012604763152751516\n",
      "Train loss: 0.009509397313949909\n",
      "\n",
      "Test set: Avg. loss: 0.0009992846846580505, AUC: 0.7192255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013318212429682414, AUC: 0.7517246666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001703771471977234, AUC: 0.7664048333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011020609140396118, AUC: 0.5047115000000001\n",
      "\n",
      "Train loss: 0.15471354722976685\n",
      "Train loss: 0.03197397779911122\n",
      "Train loss: 0.018870899778731326\n",
      "Train loss: 0.009544168888254368\n",
      "Train loss: 0.012674916297831434\n",
      "\n",
      "Test set: Avg. loss: 0.001023888905843099, AUC: 0.7342110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014814379215240478, AUC: 0.7642451666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012584259112675986, AUC: 0.8143523333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010892382860183715, AUC: 0.68919125\n",
      "\n",
      "Train loss: 0.09897994335661543\n",
      "Train loss: 0.04299450250382119\n",
      "Train loss: 0.020189229985500903\n",
      "Train loss: 0.011350483082710429\n",
      "Train loss: 0.012185671481680363\n",
      "\n",
      "Test set: Avg. loss: 0.0011022953589757284, AUC: 0.7407045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001411835233370463, AUC: 0.7827315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001859504222869873, AUC: 0.8090241666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010974677801132203, AUC: 0.56591775\n",
      "\n",
      "Train loss: 0.10203534111063531\n",
      "Train loss: 0.013854247712074443\n",
      "Train loss: 0.009581445886733684\n",
      "Train loss: 0.013009760481245974\n",
      "Train loss: 0.01197948506537904\n",
      "\n",
      "Test set: Avg. loss: 0.0011253231366475424, AUC: 0.7317013333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001439905842145284, AUC: 0.7914833333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011200330257415772, AUC: 0.8202013333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011116668780644734, AUC: 0.46080175000000007\n",
      "\n",
      "Train loss: 0.10501240567958102\n",
      "Train loss: 0.03158625744758768\n",
      "Train loss: 0.017933064825991368\n",
      "Train loss: 0.01465698480606079\n",
      "Train loss: 0.00786929409554664\n",
      "\n",
      "Test set: Avg. loss: 0.0010122893651326498, AUC: 0.7283588333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010053748488426208, AUC: 0.7936705000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012782777945200602, AUC: 0.8203048333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011055029233296711, AUC: 0.4468463333333333\n",
      "\n",
      "Train loss: 0.08537752247871237\n",
      "Train loss: 0.01983051781958722\n",
      "Train loss: 0.016945930998376074\n",
      "Train loss: 0.007339880060642324\n",
      "Train loss: 0.012510779056143253\n",
      "\n",
      "Test set: Avg. loss: 0.0011336512565612792, AUC: 0.7277228333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001267892042795817, AUC: 0.7582572499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002077299197514852, AUC: 0.7583445833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011126453479131063, AUC: 0.37238991666666665\n",
      "\n",
      "Train loss: 0.17616940787497987\n",
      "Train loss: 0.07189575332276364\n",
      "Train loss: 0.0314451240478678\n",
      "Train loss: 0.015458220370272373\n",
      "Train loss: 0.01322019708917496\n",
      "\n",
      "Test set: Avg. loss: 0.0010672011375427246, AUC: 0.7357616666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015026928186416626, AUC: 0.7719559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016274982293446858, AUC: 0.8032056666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011193834543228149, AUC: 0.42663658333333326\n",
      "\n",
      "Train loss: 0.09559693209668424\n",
      "Train loss: 0.0319166196153519\n",
      "Train loss: 0.03533677090989783\n",
      "Train loss: 0.009858934169119976\n",
      "Train loss: 0.014506559676312385\n",
      "\n",
      "Test set: Avg. loss: 0.0009400815963745117, AUC: 0.7786783333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012287856737772623, AUC: 0.8029461666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001775221069653829, AUC: 0.7985331666666667\n",
      "\n",
      "[['cosine_distance_capped_smote_with_smote_triplet_loss', 3, (0, 3, 1), (200, 20, 1), (5e-06, 0.0001), 0.47777827500000003, 0.005365624646358401, 0.47777827500000003, 0.005365624646358401, 0.47777827500000003, 0.005365624646358401, 0.47777827500000003, 0.005365624646358401, None, True, 'start_epoch=5'], ['cosine_distance_capped_smote_with_smote_triplet_loss', 3, (0, 3, 1), (200, 20, 1), (1e-05, 5e-05), 0.4956207916666667, 0.00533169092356146, 0.4956207916666667, 0.00533169092356146, 0.4956207916666667, 0.00533169092356146, 0.4956207916666667, 0.00533169092356146, None, True, 'start_epoch=5'], ['cosine_distance_capped_smote_with_smote_triplet_loss', 3, (0, 3, 1), (200, 20, 1), (0.001, 0.01), 0.49995716666666673, 0.009050082359293058, 0.49995716666666673, 0.009050082359293058, 0.49995716666666673, 0.009050082359293058, 0.49995716666666673, 0.009050082359293058, None, True, 'start_epoch=5']]\n"
     ]
    }
   ],
   "source": [
    "# 3 class triplet loss + cosine distance capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(5e-6, 1e-4), (1e-5, 5e-5), (1e-3, 1e-2)]\n",
    "\n",
    "cap_aucs = []\n",
    "loss_caps = [1]\n",
    "\n",
    "start_epoch = 5\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['distance'] = 'cosine'\n",
    "\n",
    "\n",
    "\n",
    "for cap in loss_caps:\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            embed_network = models.ConvNetOnlyEmbeddings(3)\n",
    "            linear_probe = models.ConvNetLinearProbe(3)\n",
    "            complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "            embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "            linear_optimizer = optim.SGD(complete_network.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, complete_network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                _, train_losses = train.train_triplet_loss_smote(epoch, train_loader_tripletloss_smote, embed_network, embed_optimizer, verbose=False)\n",
    "                print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "            for epoch in range(start_epoch, n_epochs+1):\n",
    "                loss_fn_args['loss_cap'] = cap\n",
    "                loss_fn_args['avg_tensors'] = []\n",
    "                for k in range(3):\n",
    "                    avg_tensor = embed_network(avg_tensors_list[k])\n",
    "                    loss_fn_args['avg_tensors'].append(avg_tensor)\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, complete_network, linear_optimizer, verbose=False, loss_fn=loss_fns.CappedCELossAvgDistance, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, complete_network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"cosine_distance_capped_smote_with_smote_triplet_loss\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], None, norm, \"start_epoch=5\"]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "831546cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdf4c5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010990204016367595, AUC: 0.5136136666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019028458992640176, AUC: 0.7512252500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002086396058400472, AUC: 0.7691906666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002416961034138997, AUC: 0.7808670000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010924596786499024, AUC: 0.5750945000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001673059900601705, AUC: 0.7794949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002390830198923747, AUC: 0.7884782499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002485786517461141, AUC: 0.7885251666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973639885584513, AUC: 0.5168527500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001288370688756307, AUC: 0.7993951666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001963376998901367, AUC: 0.8154784999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018575411240259806, AUC: 0.8272758333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002889474232991, AUC: 0.5800430833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014540077050526937, AUC: 0.7614485000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024033334255218505, AUC: 0.7844169166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026617908477783205, AUC: 0.8052283333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010971461534500123, AUC: 0.5515258333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013851773341496787, AUC: 0.778522\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001976533651351929, AUC: 0.7745744999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002331717650095622, AUC: 0.7970936666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103301207224528, AUC: 0.47176391666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013020078738530476, AUC: 0.7888039166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024007017612457274, AUC: 0.7778662500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001970576842625936, AUC: 0.8121091666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973190863927205, AUC: 0.5521948333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001569384256998698, AUC: 0.7577094999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002028331756591797, AUC: 0.7819775833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024892863432566326, AUC: 0.7912178333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011189500093460084, AUC: 0.35523316666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001377241571744283, AUC: 0.7758425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002085300286610921, AUC: 0.78223675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002421713908513387, AUC: 0.8017280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098294973373413, AUC: 0.4996409166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016685739755630494, AUC: 0.7695623333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002051869591077169, AUC: 0.7821711666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002364952882130941, AUC: 0.7924216666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011055078109105429, AUC: 0.39565374999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014359800020853678, AUC: 0.7632740833333335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class triplet loss capped smote\n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2]\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "loss_caps = [1, 5, 10]\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = loss_cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                _, _ = train.train_triplet_capped_loss(epoch, train_loader_tripletloss_smote, network, optimizer, verbose=False, cap_calc=loss_fns.TripletLoss,loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"triplet_loss_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f59a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11bd9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea18543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
