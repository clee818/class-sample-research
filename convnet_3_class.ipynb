{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ad7e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "457a1df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops \n",
    "\n",
    "NUM_CLASSES = 10\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "          #   \"mean_40\", \"variance_40\",\n",
    "          #   \"mean_50\", \"variance_50\",\n",
    "             \"cap\", \"normalization\", \"other\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c61a5021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 3 classes\n",
    "\n",
    "NUM_CLASSES_REDUCED = 3\n",
    "nums = (0, 3, 1)\n",
    "ratio = (200, 20, 1)\n",
    "\n",
    "norm=False\n",
    "\n",
    "if norm:\n",
    "    transform=torchvision.transforms.Compose([torchvision.transforms.Normalize(mean=[134.1855, 122.7346, 118.3749], std=[70.5125, 64.4848, 66.5604])])\n",
    "else:\n",
    "    transform=None\n",
    "\n",
    "    \n",
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "\n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums, transform=transform)\n",
    "targets = ratio_train_CIFAR10.labels \n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED, transform=transform)\n",
    "\n",
    "triplet_train_CIFAR10 = class_sampling.ForTripletLoss(reduced_train_CIFAR10, smote=False, transform=transform, num_classes=3)\n",
    "triplet_ratio_train_CIFAR10 = class_sampling.ForTripletLoss(ratio_train_CIFAR10, smote=False, transform=transform, num_classes=3)\n",
    "triplet_smote_train_CIFAR10 = class_sampling.ForTripletLoss(smote_train_CIFAR10, smote=True, transform=transform, num_classes=3)\n",
    "\n",
    "weight = 1. / class_count\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= max(class_count)\n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss = DataLoader(triplet_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_ratio = DataLoader(triplet_ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_smote = DataLoader(triplet_smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "# to be used in distance capped smote - get average tensor for the entire class \n",
    "dataset = train_loader_ratio.dataset\n",
    "class0 = dataset.images[dataset.labels==0]\n",
    "class1 = dataset.images[dataset.labels==1]\n",
    "class2 = dataset.images[dataset.labels==1]\n",
    "class0_avg = torch.mean(class0.float(), 0)\n",
    "class1_avg = torch.mean(class1.float(), 0)\n",
    "class2_avg = torch.mean(class2.float(), 0)\n",
    "avg_tensors_list = [class0_avg, class1_avg, class2_avg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170fce52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3 class normal\n",
    "\n",
    "learning_rates = [5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 3, nums, (1, 1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97f0654",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf6798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  3 class ratio\n",
    "\n",
    "learning_rates =  [1e-4, 1e-3, 5e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e7d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8774b790",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0033108042081197104, AUC: 0.5012746666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010891745090484618, AUC: 0.5691605000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010772271553675333, AUC: 0.6205455833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010581616560618083, AUC: 0.66594325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009650338172912597, AUC: 0.64924925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989543994267782, AUC: 0.5006625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988156000773111, AUC: 0.5036341666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098307967185974, AUC: 0.511603\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014643325408299763, AUC: 0.5624135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988226731618246, AUC: 0.5028216666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010955985387166342, AUC: 0.5535775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010860007206598917, AUC: 0.6126334166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017339369455973306, AUC: 0.6287725000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010934834877649943, AUC: 0.5562849166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010735339721043905, AUC: 0.6279554166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010635928312937419, AUC: 0.6537821666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00311078421274821, AUC: 0.6122645000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010816566149393718, AUC: 0.6115289166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010676841735839844, AUC: 0.6443601666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010676517089207966, AUC: 0.6437641666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003010539134343465, AUC: 0.5604719166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010484061241149902, AUC: 0.6771929166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001044447143872579, AUC: 0.6828719166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001045499285062154, AUC: 0.6731064999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005095264434814453, AUC: 0.5934418333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010727895100911457, AUC: 0.6352476666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010556917985280355, AUC: 0.6731788333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010515360434850058, AUC: 0.6720729999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004967935721079508, AUC: 0.5115388333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989669561386108, AUC: 0.501477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097963293393453, AUC: 0.5139119166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010826340913772583, AUC: 0.5988807500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008962241172790528, AUC: 0.46783916666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010985263188680013, AUC: 0.50874\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010896445512771607, AUC: 0.5841320833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010682722727457681, AUC: 0.6465361666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017491921583811442, AUC: 0.4792893333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010711934169133504, AUC: 0.6451561666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010578383207321166, AUC: 0.68593325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010505107641220093, AUC: 0.6982249166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010706811587015789, AUC: 0.5282234166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001061619480450948, AUC: 0.6390623333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001114656170209249, AUC: 0.6740934166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012691501379013062, AUC: 0.6652653333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007427863756815593, AUC: 0.4185044166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001084286650021871, AUC: 0.58175425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010580129226048787, AUC: 0.6456232500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011062675714492798, AUC: 0.6641103333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030471408367156983, AUC: 0.46239433333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010617636442184447, AUC: 0.6655880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011149996916453044, AUC: 0.6446978333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013898284037907919, AUC: 0.6695008333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020981840292612713, AUC: 0.5315558333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010490548610687257, AUC: 0.6604576666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00106292720635732, AUC: 0.64541675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011092890501022338, AUC: 0.6473134166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025615330537160238, AUC: 0.4643255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010793258746465046, AUC: 0.6003605833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010611701806386312, AUC: 0.6487610833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011049154996871948, AUC: 0.6225078333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002963742653528849, AUC: 0.4861299166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010549907684326172, AUC: 0.6784993333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013114278713862101, AUC: 0.65652\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001763128360112508, AUC: 0.6614195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003440234581629435, AUC: 0.4392371666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010772697528203328, AUC: 0.6156298333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011034450133641561, AUC: 0.6357530833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011656708717346191, AUC: 0.6330305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004977382183074951, AUC: 0.5888548333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010707391103108724, AUC: 0.6659326666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012835449775060019, AUC: 0.6431731666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015637336572011313, AUC: 0.6489561666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0063886931737263996, AUC: 0.399383\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010752424001693726, AUC: 0.6106143333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012818395296732585, AUC: 0.5992483333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013339670499165853, AUC: 0.5723510833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026855950355529784, AUC: 0.5174875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010493956804275512, AUC: 0.6744605833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001089785893758138, AUC: 0.6140959166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012206356525421142, AUC: 0.6147953333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009377074877421062, AUC: 0.5566096666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010837449232737224, AUC: 0.5934846666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012367159525553386, AUC: 0.6239264166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001931144952774048, AUC: 0.67264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004204644838968913, AUC: 0.6017256666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012657182216644287, AUC: 0.5986390833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019427632490793864, AUC: 0.6046090833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023304985364278157, AUC: 0.5837676666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029421582221984864, AUC: 0.6314905000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010638542572657267, AUC: 0.6172354166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015519940455754598, AUC: 0.6331893333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021608004570007324, AUC: 0.6480911666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002372199853261312, AUC: 0.51225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010411208868026734, AUC: 0.6773319166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013682610193888347, AUC: 0.6756165833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021713258425394696, AUC: 0.7029964999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004619466463724772, AUC: 0.5606215833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010885966618855794, AUC: 0.5866075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011480294863382976, AUC: 0.6525846666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014715615113576254, AUC: 0.643338\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007221521695454916, AUC: 0.4941961666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010529818932215372, AUC: 0.6523379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011516759792963664, AUC: 0.6579788333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018557409842809042, AUC: 0.6827570833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009308429718017578, AUC: 0.539823\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011011182467142741, AUC: 0.49850533333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011037826935450237, AUC: 0.5013376666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011058456500371296, AUC: 0.49884533333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0118248504002889, AUC: 0.47009975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014740148385365805, AUC: 0.7365466666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023439241250356037, AUC: 0.7258684999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003214358568191528, AUC: 0.7193210833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008746652921040854, AUC: 0.5803128333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991198619206746, AUC: 0.5003306666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010824837287267048, AUC: 0.6544369999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014401022990544637, AUC: 0.6453408333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011464783032735189, AUC: 0.40880383333333326\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010881396929423015, AUC: 0.6685733333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015919443766276042, AUC: 0.6202325833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002839865287144979, AUC: 0.6500708333333333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class oversampled \n",
    "\n",
    "learning_rates = [1e-4, 5e-4, 1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "923429dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8aa19a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.006379682540893555, AUC: 0.5555167499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010770999590555828, AUC: 0.7082001666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011475319465001424, AUC: 0.7300091666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012237483263015747, AUC: 0.7375256666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021883630752563476, AUC: 0.5853621666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011258338292439779, AUC: 0.6908154166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012150108416875203, AUC: 0.7028374166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011469882726669312, AUC: 0.7570853333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002026644229888916, AUC: 0.49485124999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001054565707842509, AUC: 0.7094285833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001168736457824707, AUC: 0.707386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012017017602920532, AUC: 0.6683456666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018511176109313964, AUC: 0.54186175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011469180583953858, AUC: 0.6809189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006299654642742, AUC: 0.7146174166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011957310835520427, AUC: 0.7583339166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011058505694071451, AUC: 0.5906061666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011384323438008625, AUC: 0.68790075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001181691845258077, AUC: 0.7294569166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012818557818730673, AUC: 0.723963\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007394368330637614, AUC: 0.4380160833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010780050754547119, AUC: 0.7321193333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011058851877848307, AUC: 0.7453125000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012145222028096517, AUC: 0.75858625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02268055788675944, AUC: 0.4862148333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098679780960083, AUC: 0.49933466666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098562757174174, AUC: 0.499338\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984772046407063, AUC: 0.5006695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023918784459431965, AUC: 0.4272140833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103639284769694, AUC: 0.7132798333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010598060290018718, AUC: 0.7375085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012684359947840373, AUC: 0.7315633333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006638091723124186, AUC: 0.5599561666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010386726061503092, AUC: 0.723674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011951104799906413, AUC: 0.7302891666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011742955048878987, AUC: 0.7556646666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00785024356842041, AUC: 0.4896606666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010985156695048014, AUC: 0.5013401666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010983107884724936, AUC: 0.503194\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010976864099502563, AUC: 0.5053355\n",
      "\n",
      "[['smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.5169259916666666, 0.0030523780412756237, 0.6647011916666667, 0.006979168440701458, 0.6799949083333332, 0.008141787962282567, 0.6897072833333333, 0.009374064240398888, None, False, None]]\n"
     ]
    }
   ],
   "source": [
    "#  3 class SMOTE\n",
    "\n",
    "learning_rates = [5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)\n",
    "    \n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e34ddade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dc17d94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004620168209075928, AUC: 0.5224699166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032700355052947996, AUC: 0.65412675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023182222843170165, AUC: 0.6341258333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016645215352376302, AUC: 0.7060236666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00591958220799764, AUC: 0.505117\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004119347174962362, AUC: 0.5946488333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001755180795987447, AUC: 0.7035093333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001964009126027425, AUC: 0.7295488333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0043713115056355795, AUC: 0.4085865833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016593863169352214, AUC: 0.722733\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018062065442403157, AUC: 0.7048511666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019965092738469443, AUC: 0.6980991666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013644606590270995, AUC: 0.4427893333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019738929669062297, AUC: 0.7059536666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017942049900690714, AUC: 0.6927988333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019231324195861817, AUC: 0.72229975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032147006988525392, AUC: 0.4984538333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022280614376068114, AUC: 0.6882096666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001923011541366577, AUC: 0.7115016666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002189170281092326, AUC: 0.6765593333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0071949974695841475, AUC: 0.43524974999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025574404398600262, AUC: 0.6834674166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002559229294459025, AUC: 0.6989545833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002253734270731608, AUC: 0.71202\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006388411362965901, AUC: 0.57681125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022331839402516683, AUC: 0.6843386666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029782606760660807, AUC: 0.719218\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018446124792098999, AUC: 0.7140474166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004625419139862061, AUC: 0.4220980000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022333877086639403, AUC: 0.6549976666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001749162236849467, AUC: 0.7071013333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002016409397125244, AUC: 0.7174317499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034746370315551756, AUC: 0.47518649999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016891684532165528, AUC: 0.6898971666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018666686216990153, AUC: 0.7266434166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019854795535405477, AUC: 0.7084774166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012103025436401366, AUC: 0.48356933333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015317246516545614, AUC: 0.6839028333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018535057306289673, AUC: 0.6980058333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018716199398040772, AUC: 0.689493\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031785619258880614, AUC: 0.4493183333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010253608226776123, AUC: 0.6914396666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011740447282791138, AUC: 0.7359080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001194417993227641, AUC: 0.738433\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002602392276128133, AUC: 0.47475366666666674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011079551378885904, AUC: 0.6902058333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011395405928293864, AUC: 0.7308109166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012364893754323323, AUC: 0.7567433333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002468026876449585, AUC: 0.44136291666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011118733485539754, AUC: 0.711592\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001142312208811442, AUC: 0.7309383333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011314800182978312, AUC: 0.7357541666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010019526481628419, AUC: 0.49331450000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010481293598810831, AUC: 0.7135536666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011577494541803995, AUC: 0.724225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145140012105306, AUC: 0.7312375000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003724007765452067, AUC: 0.45395416666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010740491151809692, AUC: 0.7242934166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011932284434636434, AUC: 0.7489648333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012084437211354574, AUC: 0.73174175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0051693522135416664, AUC: 0.5283039166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010928740104039511, AUC: 0.5582463333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011459138393402099, AUC: 0.6059868333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001169200619061788, AUC: 0.6760423333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033050804932912192, AUC: 0.4868890833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010502624909083048, AUC: 0.6591195833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011096171935399374, AUC: 0.7369954166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012287641366322834, AUC: 0.7242468333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009105114301045736, AUC: 0.42352191666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00102720574537913, AUC: 0.71345475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011696877479553222, AUC: 0.72304425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013579951524734497, AUC: 0.7022021666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006798748334248861, AUC: 0.5948063333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002205610275268, AUC: 0.7085406666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010114536086718241, AUC: 0.7337970833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001104941487312317, AUC: 0.7632221666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004611253102620443, AUC: 0.44240583333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011016714175542195, AUC: 0.5003333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010795877774556477, AUC: 0.7098338333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012670063972473145, AUC: 0.715192\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010228350321451822, AUC: 0.41775074999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010509448051452637, AUC: 0.6990739166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010669530232747396, AUC: 0.7227665833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011352572043736777, AUC: 0.7475705833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011651581446329752, AUC: 0.40751641666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096131126085917, AUC: 0.7384790833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011417748133341471, AUC: 0.7328101666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011841011842091877, AUC: 0.7482245000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006454404354095459, AUC: 0.4783680833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986293156941731, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986181497573853, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098614017168681, AUC: 0.49983333333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005513421694437663, AUC: 0.5092850833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086468736330668, AUC: 0.7061335833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001193962812423706, AUC: 0.718693\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003832817077636, AUC: 0.7211851666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018346519867579142, AUC: 0.4715146666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010832481781641642, AUC: 0.6684931666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010695111751556395, AUC: 0.7238518333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001064674695332845, AUC: 0.7530567499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024959243138631183, AUC: 0.5835906666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010908070802688598, AUC: 0.6615429166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011001924673716228, AUC: 0.7075508333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012157375415166218, AUC: 0.7182206666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006374645074208578, AUC: 0.489673\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010034387707710266, AUC: 0.7022106666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001242663621902466, AUC: 0.6784239166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011921960512797038, AUC: 0.759859\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005791068394978841, AUC: 0.57925825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107000998655955, AUC: 0.6864105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011323394378026327, AUC: 0.72307675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011958227554957073, AUC: 0.7431989166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0061645299593607586, AUC: 0.5436045833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010343753894170124, AUC: 0.6722659999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011665374437967936, AUC: 0.7285349166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012842472791671754, AUC: 0.7274453333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00378561798731486, AUC: 0.5657540833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986368258794148, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011124326785405478, AUC: 0.5003333333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011208449999491373, AUC: 0.5\n",
      "\n",
      "[['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.47703315, 0.0023935143808247222, 0.6762275666666666, 0.0011148184608552787, 0.6996709999999999, 0.0005693019507458329, 0.7074000333333333, 0.00022401099147388955, 1, False], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.4788630666666666, 0.002346876533395558, 0.6670779250000001, 0.0052124882764603515, 0.71805045, 0.0014899216534586131, 0.7274815250000001, 0.0005800807215722915, 5, False], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.5046315583333334, 0.003607008163059792, 0.6534609833333332, 0.006329068172458058, 0.6736041333333332, 0.0077281842878433316, 0.691859425, 0.009379088293504789, 10, False]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-3]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm, None]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f5ecca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "294d600b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0053888258934020996, AUC: 0.5083743333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011163065830866496, AUC: 0.5555713333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011519194841384888, AUC: 0.6339973333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012265472412109374, AUC: 0.6482106666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004785251299540202, AUC: 0.45894758333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010515480041503906, AUC: 0.6808616666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097139040629069, AUC: 0.6735478333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001158262848854065, AUC: 0.6645963333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002081297715504964, AUC: 0.5468329166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992099046707154, AUC: 0.6545398333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001181641697883606, AUC: 0.6227119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012087234656016032, AUC: 0.6747756666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029571340878804526, AUC: 0.42823191666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010765260457992554, AUC: 0.6793964166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011122363011042277, AUC: 0.6777794166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011849076747894288, AUC: 0.6750039166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020178468624750773, AUC: 0.5219041666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001072571277618408, AUC: 0.6878450833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011367605924606323, AUC: 0.6763965000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011956455707550048, AUC: 0.6798735833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0066022364298502605, AUC: 0.5727894166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103708823521932, AUC: 0.7188979999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009921568830808004, AUC: 0.7163568333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012731473445892333, AUC: 0.7144595833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016108662684758504, AUC: 0.49719683333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010739359855651855, AUC: 0.6935228333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010923791726430256, AUC: 0.7211683333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010441219011942545, AUC: 0.7205261666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008686558087666829, AUC: 0.5756996666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087567130724589, AUC: 0.6568675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011722314755121866, AUC: 0.6350644166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012100796699523926, AUC: 0.6632841666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020558752218882244, AUC: 0.5304973333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010855903228123983, AUC: 0.6664822499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011296828190485637, AUC: 0.6900355833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011508344411849975, AUC: 0.6842542500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023987669944763182, AUC: 0.47386600000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011190526485443115, AUC: 0.5874691666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011887950499852497, AUC: 0.5807331666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012476621468861897, AUC: 0.6282875833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026117592652638755, AUC: 0.5501220833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010784562031428018, AUC: 0.646719\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011126607259114582, AUC: 0.6314551666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011253907283147175, AUC: 0.6645647499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00907833480834961, AUC: 0.4095369166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103119929631551, AUC: 0.5183336666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00108243723710378, AUC: 0.6521908333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011252542734146117, AUC: 0.6145240833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012144622166951497, AUC: 0.4181985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010980870326360067, AUC: 0.5616944166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882917245229086, AUC: 0.6519211666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011063965956370036, AUC: 0.66767875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009048165639241537, AUC: 0.43410375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882413387298584, AUC: 0.641269\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011007899045944213, AUC: 0.649048\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001128840168317159, AUC: 0.6556144166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007734829584757487, AUC: 0.4437409166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010419075886408489, AUC: 0.7244224166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001304646611213684, AUC: 0.7183474166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014163004557291668, AUC: 0.7187720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020479775667190552, AUC: 0.51335725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101297616958618, AUC: 0.5413485833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010999488433202109, AUC: 0.6217745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011075912714004516, AUC: 0.6400816666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017265344858169555, AUC: 0.46495474999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010841899712880452, AUC: 0.6420518333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010756450494130453, AUC: 0.6767504166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011000306208928427, AUC: 0.6903722499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012104656219482422, AUC: 0.41221791666666663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101318875948588, AUC: 0.5439462500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011140114068984984, AUC: 0.5820619166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011396301587422689, AUC: 0.5939751666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013818121910095215, AUC: 0.3908481666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010969403187433878, AUC: 0.5741503333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011325390338897704, AUC: 0.5341461666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011663690408070882, AUC: 0.5428661666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004127991437911987, AUC: 0.5320692499999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011001667579015096, AUC: 0.565351\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010823936859766642, AUC: 0.66097125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011145733197530112, AUC: 0.6432240833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021704901059468587, AUC: 0.5081183333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010876192649205525, AUC: 0.6033494166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010726250012715658, AUC: 0.6338876666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010898060003916423, AUC: 0.60157925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0061941939989725745, AUC: 0.5234804166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010763348340988158, AUC: 0.6378586666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010690643787384033, AUC: 0.6724069166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010695849259694417, AUC: 0.6631999166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029249339898427327, AUC: 0.44276099999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010818012952804566, AUC: 0.6249320000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001167305072148641, AUC: 0.7006170833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010593249003092449, AUC: 0.7213223333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035500851472218833, AUC: 0.50164675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011043848991394043, AUC: 0.54231\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098570664723714, AUC: 0.5695655000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094109813372294, AUC: 0.5949230833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028190720081329346, AUC: 0.47897783333333327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010734304587046305, AUC: 0.6374557500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010634453694025674, AUC: 0.6531299166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010553747018178304, AUC: 0.6659868333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008131365617116292, AUC: 0.44895408333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011458585262298583, AUC: 0.6904523333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001035565733909607, AUC: 0.7257168333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012226609388987224, AUC: 0.702193\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009203200022379556, AUC: 0.5448211666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010926599899927776, AUC: 0.6043731666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011341997782389323, AUC: 0.6254171666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010712933540344238, AUC: 0.66773725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021905073165893553, AUC: 0.5471773333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010562962690989176, AUC: 0.7030220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010428677399953207, AUC: 0.7076155833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010711555480957032, AUC: 0.7094441666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003333326896031698, AUC: 0.5377636666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010806766748428344, AUC: 0.642129\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010924222469329834, AUC: 0.5946263333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010711589256922403, AUC: 0.6532606666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008774649620056153, AUC: 0.6241281666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010897631247838338, AUC: 0.5906111666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010884263515472412, AUC: 0.596979\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010742882092793782, AUC: 0.6337864999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036368446350097654, AUC: 0.5375485833333333\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011079335610071819, AUC: 0.6403586666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011695301135381062, AUC: 0.602681\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012256291309992473, AUC: 0.6285350833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005145518620808919, AUC: 0.44634475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010701054732004801, AUC: 0.6592354166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011435633500417074, AUC: 0.6172679166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011791830857594808, AUC: 0.64276475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005740523020426432, AUC: 0.41456291666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011070729494094848, AUC: 0.545332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011548689603805542, AUC: 0.5669795000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001200473427772522, AUC: 0.6261900833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022632593313852947, AUC: 0.4380240833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010723592837651571, AUC: 0.6887599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011568965911865235, AUC: 0.6591684999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012437734206517538, AUC: 0.60485875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018457919359207153, AUC: 0.55856275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010602103074391683, AUC: 0.676076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011733896334966023, AUC: 0.6035581666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011962558031082154, AUC: 0.6873893333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00838488229115804, AUC: 0.5265839166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010944061279296875, AUC: 0.654685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012505316734313964, AUC: 0.7014080833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013044424454371135, AUC: 0.7064013333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007856205145517985, AUC: 0.5915576666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010972687403361004, AUC: 0.6041635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00114812699953715, AUC: 0.62198325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012080289920171103, AUC: 0.6327571666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002285668134689331, AUC: 0.5247015833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011085167725880941, AUC: 0.5773756666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011094851891199748, AUC: 0.67543275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011885340611139934, AUC: 0.6666003333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036948370933532716, AUC: 0.43958058333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001069793423016866, AUC: 0.6709383333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011645712455113728, AUC: 0.6009698333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011933152675628663, AUC: 0.6677953333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003929627418518066, AUC: 0.4183385833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001110571026802063, AUC: 0.5317219999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001152188777923584, AUC: 0.5732484999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001216336210568746, AUC: 0.5800096666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003980309009552002, AUC: 0.5070290833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010700532595316569, AUC: 0.6445938333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011160261233647665, AUC: 0.5381821666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984551111857096, AUC: 0.6573060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007836485385894775, AUC: 0.4103730833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010970458189646403, AUC: 0.5674521666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010961037079493204, AUC: 0.6252298333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001122948169708252, AUC: 0.6204729166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010923314412434895, AUC: 0.41686466666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099669059117635, AUC: 0.5706851666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011077654361724854, AUC: 0.61126075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011014473040898642, AUC: 0.6663798333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004605100154876709, AUC: 0.4316365833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001077207366625468, AUC: 0.6230364999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010845094124476114, AUC: 0.6518664166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010816977818806966, AUC: 0.6803232499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007020300070444743, AUC: 0.5918789166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010466662247975668, AUC: 0.68247\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010511811971664429, AUC: 0.6724024166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010570799509684245, AUC: 0.6767935833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028751110235850016, AUC: 0.45622691666666676\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010817949771881104, AUC: 0.602141\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011074643135070802, AUC: 0.5737399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010939449071884154, AUC: 0.6590863333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005461203416188558, AUC: 0.45323550000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010623817046483358, AUC: 0.6461289166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010850675900777181, AUC: 0.6511835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011046991348266603, AUC: 0.6535490833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023248490492502848, AUC: 0.51568475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001300006628036499, AUC: 0.7089076666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010798428455988567, AUC: 0.7372998333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012731024424235027, AUC: 0.7253934166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002557363589604696, AUC: 0.5463335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101215124130249, AUC: 0.5372181666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001085688312848409, AUC: 0.6449231666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011124345461527507, AUC: 0.6414555000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017286035219828287, AUC: 0.4455651666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010931458473205566, AUC: 0.60530275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010769670804341633, AUC: 0.6755840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011087607542673748, AUC: 0.6672878333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034246060053507487, AUC: 0.46436649999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001090443213780721, AUC: 0.5917323333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00108868678410848, AUC: 0.60868325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010782825549443564, AUC: 0.6326221666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004270084063212077, AUC: 0.47567216666666673\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010902762413024902, AUC: 0.5802461666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088633894920349, AUC: 0.5930140833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010953359603881835, AUC: 0.54499625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004291628837585449, AUC: 0.43709708333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010911602179209392, AUC: 0.572385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001089426358540853, AUC: 0.5912680833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001077050010363261, AUC: 0.6359901666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003900085687637329, AUC: 0.4815301666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011072588761647543, AUC: 0.48698325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099105715751648, AUC: 0.5862985833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010957812070846557, AUC: 0.5476917500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028812230428059897, AUC: 0.55280275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010783047676086426, AUC: 0.6177990833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010747024218241373, AUC: 0.6332463333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010614176193873088, AUC: 0.6451901666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004155467033386231, AUC: 0.53043275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010899144411087036, AUC: 0.606247\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010819032192230225, AUC: 0.6305116666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010890053908030192, AUC: 0.6113765833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020048028230667115, AUC: 0.5931780000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010772932370503743, AUC: 0.6419350833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010667002995808918, AUC: 0.6577113333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001060768206914266, AUC: 0.6619066666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007765159130096435, AUC: 0.5444235833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010996445814768472, AUC: 0.5426753333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010889208714167278, AUC: 0.6047218333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010936522881189981, AUC: 0.5875358333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005751923243204752, AUC: 0.44032116666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095639705657959, AUC: 0.58367475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010873432556788127, AUC: 0.6055861666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010804917414983114, AUC: 0.6198759166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007940491994222006, AUC: 0.5980770833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094688336054484, AUC: 0.5689736666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001085538903872172, AUC: 0.6112655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010963680346806844, AUC: 0.5705725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005407075881958007, AUC: 0.4677414166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010836641391118367, AUC: 0.6448572499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011251831849416096, AUC: 0.6361315833333335\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011914784510930379, AUC: 0.6240734166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023554405212402343, AUC: 0.4700285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010850404898325602, AUC: 0.6233955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011104260285695394, AUC: 0.6535688333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011573739051818847, AUC: 0.6540090833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00530566676457723, AUC: 0.4907729166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011098883152008056, AUC: 0.5446788333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145491639773051, AUC: 0.5830515833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011320642630259195, AUC: 0.6684396666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012915862401326498, AUC: 0.5365515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011070706844329833, AUC: 0.5431425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011233591238657633, AUC: 0.6475516666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011704811652501425, AUC: 0.6520456666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009043893178304037, AUC: 0.3958669166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010925205945968629, AUC: 0.6063278333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001122375210126241, AUC: 0.6276160833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012131238381067912, AUC: 0.5614534999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004727090040842692, AUC: 0.42868008333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010698118209838867, AUC: 0.6596433333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095819632212321, AUC: 0.6503713333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011717450221379597, AUC: 0.6223735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004732184092203776, AUC: 0.5233229166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010391622384389241, AUC: 0.6818479166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010653945604960123, AUC: 0.6792489166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010980259577433268, AUC: 0.68595425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016474919716517131, AUC: 0.47314133333333325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010652266343434652, AUC: 0.6558399166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001108786185582479, AUC: 0.6665774166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011450514396031698, AUC: 0.6801645833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031354562441507977, AUC: 0.42186033333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011128941774368286, AUC: 0.5227439166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001136849522590637, AUC: 0.5913285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001227043310801188, AUC: 0.5117476666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003512922763824463, AUC: 0.42164399999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001093176325162252, AUC: 0.6176624166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011098206837972006, AUC: 0.6645506666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011351662874221802, AUC: 0.6799634999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003787134091059367, AUC: 0.46579808333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010777191718419393, AUC: 0.650869\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011082991361618041, AUC: 0.5412195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001116129438082377, AUC: 0.602544\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009494578043619792, AUC: 0.5785944166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086069623629252, AUC: 0.6067176666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010645600954691569, AUC: 0.6524258333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001119981328646342, AUC: 0.5740205833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014893545309702556, AUC: 0.5147845833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010881072680155436, AUC: 0.5916234166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001052786707878113, AUC: 0.6934054999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00108830463886261, AUC: 0.6655008333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033571554025014242, AUC: 0.43533875000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010616891781489053, AUC: 0.6435039166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010459538300832112, AUC: 0.6678250833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001065412680308024, AUC: 0.6670287500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005959226608276367, AUC: 0.36945425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086418628692627, AUC: 0.6006046666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010508412520090738, AUC: 0.6594155833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001061234712600708, AUC: 0.6830227500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002466677904129028, AUC: 0.5022106666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010860243638356527, AUC: 0.5979315000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010745192368825276, AUC: 0.6619666666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011291266282399495, AUC: 0.6359860833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010658900578816731, AUC: 0.5182304999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010712224245071412, AUC: 0.6549818333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010742261409759522, AUC: 0.6570839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011202009916305542, AUC: 0.6024794166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01710516611735026, AUC: 0.47007916666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010952344338099162, AUC: 0.5796060833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011034897168477377, AUC: 0.5976064166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011379482746124267, AUC: 0.572879\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003692817767461141, AUC: 0.4503040833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010954463481903077, AUC: 0.5600168333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097853382428487, AUC: 0.5904741666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011293626626332602, AUC: 0.5443664999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005870925744374593, AUC: 0.41144374999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011055472294489543, AUC: 0.5378953333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010633336305618285, AUC: 0.6512815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011010560194651285, AUC: 0.6409848333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015494970083236694, AUC: 0.5676933333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010837071339289347, AUC: 0.6027206666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010784968137741088, AUC: 0.6306749166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010775943597157796, AUC: 0.6433308333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006495138645172119, AUC: 0.4843408333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010748443206151327, AUC: 0.6262219166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010918902158737183, AUC: 0.5597046666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010834360122680665, AUC: 0.6138046666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004927356084187826, AUC: 0.44561124999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010812255541483562, AUC: 0.6187255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010861013730367026, AUC: 0.6047561666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010757153034210206, AUC: 0.6378210833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004236334005991618, AUC: 0.53376225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001053187330563863, AUC: 0.6802904999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010549397865931194, AUC: 0.707322\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001049498160680135, AUC: 0.7173719166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019901039600372316, AUC: 0.5079928333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010722840229670206, AUC: 0.6268785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001065795143445333, AUC: 0.6511400833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010672556956609091, AUC: 0.6540048333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004992599646250407, AUC: 0.3883905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001113569974899292, AUC: 0.5305151666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101508339246114, AUC: 0.5553133333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010981414715449016, AUC: 0.5683530833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004242204030354818, AUC: 0.44027075000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010937814315160116, AUC: 0.5913730833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010776551564534505, AUC: 0.6300409166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010835390090942383, AUC: 0.62701025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029406830469767254, AUC: 0.5880721666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010836275815963746, AUC: 0.6296732500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010813354651133219, AUC: 0.6267249166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010722055435180664, AUC: 0.6438299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004063597440719604, AUC: 0.514617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010883302688598632, AUC: 0.5808056666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010941217343012491, AUC: 0.5523899166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010946215391159058, AUC: 0.5514673333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015948575337727865, AUC: 0.5340653333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010999303261439006, AUC: 0.5856963333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011032426357269287, AUC: 0.6176274166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001080282966295878, AUC: 0.6403633333333333\n",
      "\n",
      "[['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.5114340166666667, 0.002092150513974722, 0.6581454083333332, 0.002239285218479788, 0.6627791416666666, 0.0017482602106209013, 0.6753271916666668, 0.0006883278604778474, 1, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.45691495, 0.0028386302317697193, 0.59592865, 0.0037556808618108345, 0.6378666833333334, 0.002322282924477501, 0.6431673333333332, 0.002246811227576389, 1, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0001, 0.515782875, 0.002552141191850348, 0.62764935, 0.001982959012012226, 0.6479962, 0.002529749873798888, 0.6613433, 0.0016453860869933355, 1, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.4895805416666666, 0.003779200803835072, 0.6248646583333334, 0.002882340665858958, 0.6222697499999998, 0.0017123422166319447, 0.6443301833333333, 0.0013163547528552775, 5, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.47748281666666664, 0.0032497889435233324, 0.6187936166666665, 0.002567033028440556, 0.6381672083333332, 0.0027841285161003455, 0.664804775, 0.0006790309580834018, 5, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0001, 0.5117901250000001, 0.003231998553382294, 0.5792651666666667, 0.0016324257262222228, 0.6122306833333334, 0.0004439571837886108, 0.6057758, 0.001520786319021109, 5, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.46296099166666665, 0.0019082223064422932, 0.6100139416666666, 0.002754494862472292, 0.6399996583333334, 0.0008978640343978477, 0.6340224833333332, 0.002929749149266391, 10, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.471623825, 0.0032281022349631245, 0.602375025, 0.0013438334068820146, 0.6372704249999999, 0.0018992524104297903, 0.618881275, 0.0019603903308861854, 10, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0001, 0.500481625, 0.003414226224967011, 0.6072900583333333, 0.001403928877082013, 0.6135694333333334, 0.0021070331995913867, 0.6297357333333332, 0.001898588301142776, 10, False]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class euclidean distance capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['distance'] = 'euclidean'\n",
    "\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"distance_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm, None]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70f1ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bac108e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.002080826759338379, AUC: 0.4106796666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010747290054957072, AUC: 0.6596691666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010710937182108562, AUC: 0.6816675833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088242252667745, AUC: 0.6701527500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011719092686971029, AUC: 0.5359823333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010914045174916585, AUC: 0.6056128333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010783994595209758, AUC: 0.6660287500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011116851965586344, AUC: 0.6275164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005634347756703695, AUC: 0.42221525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010647549231847127, AUC: 0.6730455833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010821845134099325, AUC: 0.669699\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001085131565729777, AUC: 0.6831138333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008083182175954183, AUC: 0.45520724999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010794227520624796, AUC: 0.6404148333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010604687929153442, AUC: 0.67681325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001063960075378418, AUC: 0.6907799166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003445685704549154, AUC: 0.5034734166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010437694390614826, AUC: 0.6770529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001034997542699178, AUC: 0.693224\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010355735619862875, AUC: 0.7038030000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035304884910583494, AUC: 0.4391979166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010879130760828653, AUC: 0.6249463333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088425318400065, AUC: 0.6532287499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011039398113886515, AUC: 0.6622996666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004751717885335287, AUC: 0.4452259166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010887971719106039, AUC: 0.6313303333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010740042130152384, AUC: 0.6764085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010883527994155883, AUC: 0.6705335833333331\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002428386608759562, AUC: 0.47807825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011061731974283855, AUC: 0.5128271666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011060840288798013, AUC: 0.63047275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001149980902671814, AUC: 0.5648070000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00187968377272288, AUC: 0.4941233333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010638130108515421, AUC: 0.6464171666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010654526551564535, AUC: 0.6865925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011118123133977255, AUC: 0.6585474166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029297688007354737, AUC: 0.5066098333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010610750118891397, AUC: 0.6622346666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010768486261367798, AUC: 0.6526853333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010969711542129516, AUC: 0.650579\n",
      "\n",
      "[['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.46907931666666663, 0.0015036767862900019, 0.6333551083333333, 0.0020603180937695106, 0.6686820416666667, 0.0003180254267684027, 0.6582132666666668, 0.0013795504648094442, 0.75, False, 'start_epoch=2']]\n"
     ]
    }
   ],
   "source": [
    "# 3 class cosine distance capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [0.75]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['distance'] = 'cosine'\n",
    "loss_fn_args['print_capped'] = False\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    #loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = cap\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"cosine_distance_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm, 'start_epoch=2']\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84c63d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d841c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3 class cosine distance capped smote w/ average tensor \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['distance'] = 'cosine'\n",
    "\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                loss_fn_args['avg_tensors'] = []\n",
    "                for k in range(NUM_CLASSES_REDUCED):\n",
    "                    _, avg_tensor = network(avg_tensors_list[k])\n",
    "                    loss_fn_args['avg_tensors'].append(avg_tensor)\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELossAvgDistance, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"cosine_distance_capped_smote_avg\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm, None]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77f9b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdf4c5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010990204016367595, AUC: 0.5136136666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019028458992640176, AUC: 0.7512252500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002086396058400472, AUC: 0.7691906666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002416961034138997, AUC: 0.7808670000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010924596786499024, AUC: 0.5750945000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001673059900601705, AUC: 0.7794949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002390830198923747, AUC: 0.7884782499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002485786517461141, AUC: 0.7885251666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973639885584513, AUC: 0.5168527500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001288370688756307, AUC: 0.7993951666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001963376998901367, AUC: 0.8154784999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018575411240259806, AUC: 0.8272758333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002889474232991, AUC: 0.5800430833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014540077050526937, AUC: 0.7614485000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024033334255218505, AUC: 0.7844169166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026617908477783205, AUC: 0.8052283333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010971461534500123, AUC: 0.5515258333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013851773341496787, AUC: 0.778522\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001976533651351929, AUC: 0.7745744999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002331717650095622, AUC: 0.7970936666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103301207224528, AUC: 0.47176391666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013020078738530476, AUC: 0.7888039166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024007017612457274, AUC: 0.7778662500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001970576842625936, AUC: 0.8121091666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973190863927205, AUC: 0.5521948333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001569384256998698, AUC: 0.7577094999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002028331756591797, AUC: 0.7819775833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024892863432566326, AUC: 0.7912178333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011189500093460084, AUC: 0.35523316666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001377241571744283, AUC: 0.7758425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002085300286610921, AUC: 0.78223675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002421713908513387, AUC: 0.8017280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098294973373413, AUC: 0.4996409166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016685739755630494, AUC: 0.7695623333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002051869591077169, AUC: 0.7821711666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002364952882130941, AUC: 0.7924216666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011055078109105429, AUC: 0.39565374999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014359800020853678, AUC: 0.7632740833333335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class triplet loss capped smote\n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2]\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "loss_caps = [1, 5, 10]\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = loss_cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                _, _ = train.train_triplet_capped_loss(epoch, train_loader_tripletloss_smote, network, optimizer, verbose=False, cap_calc=loss_fns.TripletLoss,loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"triplet_loss_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f59a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11bd9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea18543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
