{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5afbdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  \n",
    "\n",
    "\n",
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops \n",
    "\n",
    "NUM_CLASSES = 10\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "          #   \"mean_40\", \"variance_40\",\n",
    "          #   \"mean_50\", \"variance_50\",\n",
    "             \"cap\", \"normalization\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e360758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 3 classes\n",
    "\n",
    "NUM_CLASSES_REDUCED = 3\n",
    "nums = (0, 3, 1)\n",
    "ratio = (20, 2, 1)\n",
    "\n",
    "norm=True\n",
    "\n",
    "if norm:\n",
    "    transform=torchvision.transforms.Compose([torchvision.transforms.Normalize(mean=[134.1855, 122.7346, 118.3749], std=[70.5125, 64.4848, 66.5604])])\n",
    "else:\n",
    "    transform=None\n",
    "\n",
    "    \n",
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "\n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums, transform=transform)\n",
    "targets = ratio_train_CIFAR10.labels \n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED, transform=transform)\n",
    "\n",
    "triplet_train_CIFAR10 = class_sampling.ForTripletLoss(reduced_train_CIFAR10, smote=False, transform=transform, num_classes=3)\n",
    "triplet_ratio_train_CIFAR10 = class_sampling.ForTripletLoss(ratio_train_CIFAR10, smote=False, transform=transform, num_classes=3)\n",
    "triplet_smote_train_CIFAR10 = class_sampling.ForTripletLoss(smote_train_CIFAR10, smote=True, transform=transform, num_classes=3)\n",
    "\n",
    "weight = 1. / class_count\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= max(class_count)\n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss = DataLoader(triplet_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_ratio = DataLoader(triplet_ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_smote = DataLoader(triplet_smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09ea390",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011114914417266846, AUC: 0.36460475000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011025424003601075, AUC: 0.48520416666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010982316732406616, AUC: 0.5714016666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010944233338038127, AUC: 0.62808525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011040406624476114, AUC: 0.5100708333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010912309487660725, AUC: 0.6503089166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010792030096054078, AUC: 0.7089203333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010646488666534424, AUC: 0.7345886666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011084489425023398, AUC: 0.3779119166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010993793805440268, AUC: 0.5041459166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094974120457967, AUC: 0.60830675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010907506942749022, AUC: 0.65444475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011058311859766642, AUC: 0.4225466666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094127337137858, AUC: 0.5760265000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001082828164100647, AUC: 0.6762239166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010691068569819132, AUC: 0.71691625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998145739237468, AUC: 0.5322780833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010970675945281983, AUC: 0.5826410000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010939324696858724, AUC: 0.6129826666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010893264214197796, AUC: 0.64225275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010938932498296102, AUC: 0.59929025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010803812742233277, AUC: 0.66752625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00106389852364858, AUC: 0.6768195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010473333994547526, AUC: 0.6820699166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010968128045399984, AUC: 0.5933673333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010902556975682578, AUC: 0.6711284166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010823647578557332, AUC: 0.7117222499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010708037614822388, AUC: 0.7232466666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011017668644587198, AUC: 0.4566325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00109535551071167, AUC: 0.5678875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010894284248352051, AUC: 0.6471428333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010817763010660807, AUC: 0.6925782500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011013665199279786, AUC: 0.4917800833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088576118151347, AUC: 0.6863039166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010741681257883707, AUC: 0.7100995833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010558685461680094, AUC: 0.7118560833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001107686479886373, AUC: 0.43057283333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989142656326295, AUC: 0.5222048333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095370928446452, AUC: 0.5880641666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010921854575475056, AUC: 0.645378\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010981613000233968, AUC: 0.5263778333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000983055810133616, AUC: 0.7512443333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009194654822349548, AUC: 0.7894514166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008784446318944295, AUC: 0.8155882500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011022710005442302, AUC: 0.4892674166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010172497828801472, AUC: 0.7197129166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009511158863703409, AUC: 0.7666473333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008988935550053915, AUC: 0.7997960833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001110364317893982, AUC: 0.41568733333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010140114625295004, AUC: 0.75491625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009485764106114705, AUC: 0.7919393333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008997021118799845, AUC: 0.8180593333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011092983484268187, AUC: 0.41575041666666673\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010098774830500285, AUC: 0.729498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009517418543497721, AUC: 0.7794161666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008967744906743368, AUC: 0.8139239166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008447408676148, AUC: 0.48446324999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010277336438496907, AUC: 0.7344560000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009332393209139506, AUC: 0.7776555000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008805685838063558, AUC: 0.8059038333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011048526763916015, AUC: 0.4352424166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001009353200594584, AUC: 0.7355573333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009356552163759868, AUC: 0.7861181666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008924335638682047, AUC: 0.8097348333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011028556426366171, AUC: 0.5073796666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009959317843119303, AUC: 0.7132923333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009411011735598247, AUC: 0.7794420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008814009626706441, AUC: 0.8147506666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010971651474634807, AUC: 0.5345293333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009944749077161153, AUC: 0.7320365833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009276374578475953, AUC: 0.7874376666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008744420607884725, AUC: 0.8190953333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010948770840962729, AUC: 0.5501870833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009823816021283468, AUC: 0.76500175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009165100852648417, AUC: 0.7930344166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008789356350898743, AUC: 0.808912\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010952338774998982, AUC: 0.6054512500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009843429724375407, AUC: 0.7717355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008968397577603658, AUC: 0.8075248333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008513313134511311, AUC: 0.8301531666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011014574368794759, AUC: 0.5218728333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010787601470947266, AUC: 0.6564960833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010347105264663697, AUC: 0.6794130833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010074029962221782, AUC: 0.7053959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001110546867052714, AUC: 0.4020902499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010399429798126222, AUC: 0.68385825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001011730909347534, AUC: 0.7048479166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009922597606976827, AUC: 0.7322212499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011055397987365723, AUC: 0.4680790833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010302927096684773, AUC: 0.7148125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009755022525787353, AUC: 0.7523270000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009339791933695475, AUC: 0.7880956666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095810612042745, AUC: 0.6056743333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001068042000134786, AUC: 0.7141154166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010325841903686523, AUC: 0.74126025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009906755685806273, AUC: 0.7562658333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998433828353883, AUC: 0.5940195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001029565652211507, AUC: 0.7268811666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009820104837417601, AUC: 0.755742\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009469510912895202, AUC: 0.7741475833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011040873130162556, AUC: 0.42735900000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010806246598561605, AUC: 0.6723030833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010337572892506917, AUC: 0.7248777500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009944018125534059, AUC: 0.7569161666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098974307378133, AUC: 0.50805375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010392080942789714, AUC: 0.6887404166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010029820203781127, AUC: 0.7128736666666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class normal\n",
    "\n",
    "learning_rates = [1e-4, 1e-3, 5e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 3, nums, (1, 1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301365f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a777aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  3 class ratio\n",
    "\n",
    "learning_rates =  [1e-4, 1e-3, 5e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a60e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc9195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 class oversampled \n",
    "\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b5b1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2d65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  3 class SMOTE\n",
    "\n",
    "learning_rates = [1e-4, 5e-4, 1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eada1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aa136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 class capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c903db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70f4846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 class euclidean distance capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['distance'] = 'euclidean'\n",
    "\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"distance_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c7d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef44df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
