{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ad7e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  \n",
    "\n",
    "\n",
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops \n",
    "\n",
    "NUM_CLASSES = 10\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "          #   \"mean_40\", \"variance_40\",\n",
    "          #   \"mean_50\", \"variance_50\",\n",
    "             \"cap\", \"normalization\", \"other\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c61a5021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 3 classes\n",
    "\n",
    "NUM_CLASSES_REDUCED = 3\n",
    "nums = (0, 3, 1)\n",
    "ratio = (200, 20, 1)\n",
    "\n",
    "norm=False\n",
    "\n",
    "if norm:\n",
    "    transform=torchvision.transforms.Compose([torchvision.transforms.Normalize(mean=[134.1855, 122.7346, 118.3749], std=[70.5125, 64.4848, 66.5604])])\n",
    "else:\n",
    "    transform=None\n",
    "\n",
    "    \n",
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "\n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums, transform=transform)\n",
    "targets = ratio_train_CIFAR10.labels \n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED, transform=transform)\n",
    "\n",
    "triplet_train_CIFAR10 = class_sampling.ForTripletLoss(reduced_train_CIFAR10, smote=False, transform=transform, num_classes=3)\n",
    "triplet_ratio_train_CIFAR10 = class_sampling.ForTripletLoss(ratio_train_CIFAR10, smote=False, transform=transform, num_classes=3)\n",
    "triplet_smote_train_CIFAR10 = class_sampling.ForTripletLoss(smote_train_CIFAR10, smote=True, transform=transform, num_classes=3)\n",
    "\n",
    "weight = 1. / class_count\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= max(class_count)\n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss = DataLoader(triplet_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_ratio = DataLoader(triplet_ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_smote = DataLoader(triplet_smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170fce52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3 class normal\n",
    "\n",
    "learning_rates = [1e-4, 1e-3, 5e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 3, nums, (1, 1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b97f0654",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf6798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  3 class ratio\n",
    "\n",
    "learning_rates =  [1e-4, 1e-3, 5e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e7d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8774b790",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0033108042081197104, AUC: 0.5012746666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010891745090484618, AUC: 0.5691605000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010772271553675333, AUC: 0.6205455833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010581616560618083, AUC: 0.66594325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009650338172912597, AUC: 0.64924925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989543994267782, AUC: 0.5006625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988156000773111, AUC: 0.5036341666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098307967185974, AUC: 0.511603\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014643325408299763, AUC: 0.5624135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988226731618246, AUC: 0.5028216666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010955985387166342, AUC: 0.5535775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010860007206598917, AUC: 0.6126334166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017339369455973306, AUC: 0.6287725000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010934834877649943, AUC: 0.5562849166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010735339721043905, AUC: 0.6279554166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010635928312937419, AUC: 0.6537821666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00311078421274821, AUC: 0.6122645000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010816566149393718, AUC: 0.6115289166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010676841735839844, AUC: 0.6443601666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010676517089207966, AUC: 0.6437641666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003010539134343465, AUC: 0.5604719166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010484061241149902, AUC: 0.6771929166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001044447143872579, AUC: 0.6828719166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001045499285062154, AUC: 0.6731064999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005095264434814453, AUC: 0.5934418333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010727895100911457, AUC: 0.6352476666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010556917985280355, AUC: 0.6731788333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010515360434850058, AUC: 0.6720729999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004967935721079508, AUC: 0.5115388333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989669561386108, AUC: 0.501477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097963293393453, AUC: 0.5139119166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010826340913772583, AUC: 0.5988807500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008962241172790528, AUC: 0.46783916666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010985263188680013, AUC: 0.50874\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010896445512771607, AUC: 0.5841320833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010682722727457681, AUC: 0.6465361666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017491921583811442, AUC: 0.4792893333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010711934169133504, AUC: 0.6451561666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010578383207321166, AUC: 0.68593325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010505107641220093, AUC: 0.6982249166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010706811587015789, AUC: 0.5282234166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001061619480450948, AUC: 0.6390623333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001114656170209249, AUC: 0.6740934166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012691501379013062, AUC: 0.6652653333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007427863756815593, AUC: 0.4185044166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001084286650021871, AUC: 0.58175425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010580129226048787, AUC: 0.6456232500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011062675714492798, AUC: 0.6641103333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030471408367156983, AUC: 0.46239433333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010617636442184447, AUC: 0.6655880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011149996916453044, AUC: 0.6446978333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013898284037907919, AUC: 0.6695008333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020981840292612713, AUC: 0.5315558333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010490548610687257, AUC: 0.6604576666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00106292720635732, AUC: 0.64541675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011092890501022338, AUC: 0.6473134166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025615330537160238, AUC: 0.4643255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010793258746465046, AUC: 0.6003605833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010611701806386312, AUC: 0.6487610833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011049154996871948, AUC: 0.6225078333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002963742653528849, AUC: 0.4861299166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010549907684326172, AUC: 0.6784993333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013114278713862101, AUC: 0.65652\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001763128360112508, AUC: 0.6614195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003440234581629435, AUC: 0.4392371666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010772697528203328, AUC: 0.6156298333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011034450133641561, AUC: 0.6357530833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011656708717346191, AUC: 0.6330305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004977382183074951, AUC: 0.5888548333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010707391103108724, AUC: 0.6659326666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012835449775060019, AUC: 0.6431731666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015637336572011313, AUC: 0.6489561666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0063886931737263996, AUC: 0.399383\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010752424001693726, AUC: 0.6106143333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012818395296732585, AUC: 0.5992483333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013339670499165853, AUC: 0.5723510833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026855950355529784, AUC: 0.5174875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010493956804275512, AUC: 0.6744605833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001089785893758138, AUC: 0.6140959166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012206356525421142, AUC: 0.6147953333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009377074877421062, AUC: 0.5566096666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010837449232737224, AUC: 0.5934846666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012367159525553386, AUC: 0.6239264166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001931144952774048, AUC: 0.67264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004204644838968913, AUC: 0.6017256666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012657182216644287, AUC: 0.5986390833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019427632490793864, AUC: 0.6046090833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023304985364278157, AUC: 0.5837676666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029421582221984864, AUC: 0.6314905000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010638542572657267, AUC: 0.6172354166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015519940455754598, AUC: 0.6331893333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021608004570007324, AUC: 0.6480911666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002372199853261312, AUC: 0.51225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010411208868026734, AUC: 0.6773319166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013682610193888347, AUC: 0.6756165833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021713258425394696, AUC: 0.7029964999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004619466463724772, AUC: 0.5606215833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010885966618855794, AUC: 0.5866075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011480294863382976, AUC: 0.6525846666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014715615113576254, AUC: 0.643338\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007221521695454916, AUC: 0.4941961666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010529818932215372, AUC: 0.6523379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011516759792963664, AUC: 0.6579788333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018557409842809042, AUC: 0.6827570833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009308429718017578, AUC: 0.539823\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011011182467142741, AUC: 0.49850533333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011037826935450237, AUC: 0.5013376666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011058456500371296, AUC: 0.49884533333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0118248504002889, AUC: 0.47009975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014740148385365805, AUC: 0.7365466666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023439241250356037, AUC: 0.7258684999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003214358568191528, AUC: 0.7193210833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008746652921040854, AUC: 0.5803128333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991198619206746, AUC: 0.5003306666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010824837287267048, AUC: 0.6544369999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014401022990544637, AUC: 0.6453408333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011464783032735189, AUC: 0.40880383333333326\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010881396929423015, AUC: 0.6685733333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015919443766276042, AUC: 0.6202325833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002839865287144979, AUC: 0.6500708333333333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class oversampled \n",
    "\n",
    "learning_rates = [1e-4, 5e-4, 1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "923429dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8aa19a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0033108042081197104, AUC: 0.5012746666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010981606642405193, AUC: 0.5078064999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010916711489359539, AUC: 0.5696267500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010836415688196819, AUC: 0.6216942499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001436800241470337, AUC: 0.4174843333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010810662508010864, AUC: 0.6466885833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010712401469548543, AUC: 0.6677557500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010669514338175455, AUC: 0.6858748333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0074207375844319665, AUC: 0.47449125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00108564559618632, AUC: 0.5965848333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010810999075571697, AUC: 0.6220325833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010828034083048502, AUC: 0.6182540833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00392035977045695, AUC: 0.5374605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010892696777979533, AUC: 0.6160569166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001085516333580017, AUC: 0.635571\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010770933230717977, AUC: 0.6572729166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003082060734430949, AUC: 0.468488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010616727272669475, AUC: 0.6893074166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010660279591878255, AUC: 0.6954115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010759052435557047, AUC: 0.6803870833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01163977591196696, AUC: 0.5361313333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003444194793701, AUC: 0.4998263333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011000251372655233, AUC: 0.4995103333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010999047756195068, AUC: 0.5033138333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004088732322057088, AUC: 0.4640233333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010990289847056071, AUC: 0.5016721666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989937782287598, AUC: 0.5053153333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010807150999704996, AUC: 0.6376016666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021960195700327554, AUC: 0.5265366666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010824960072835287, AUC: 0.6212809166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010773621400197346, AUC: 0.64585625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010770439306894938, AUC: 0.6508668333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005359292507171631, AUC: 0.430348\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010946938594182333, AUC: 0.5429879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010867762565612794, AUC: 0.5882766666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010833756526311238, AUC: 0.6119659166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014664870103200276, AUC: 0.49845633333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001089019219080607, AUC: 0.5921311666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001083534081776937, AUC: 0.6395850833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010725709597269695, AUC: 0.6774855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00193717888991038, AUC: 0.4401167499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001063861568768819, AUC: 0.68116725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010761324167251588, AUC: 0.68896825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010579609473546346, AUC: 0.7018405833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004381368478139242, AUC: 0.54471875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001085672895113627, AUC: 0.6230288333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010620373090108235, AUC: 0.6963876666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010602217117945353, AUC: 0.7073420833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004591920534769694, AUC: 0.4814635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010859460830688476, AUC: 0.6386932500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010747756560643513, AUC: 0.6711570833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011403684218724568, AUC: 0.5640758333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020066167116165163, AUC: 0.4551304166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010832845369974772, AUC: 0.6657140833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010627134243647257, AUC: 0.7081200833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010761298338572184, AUC: 0.7058136666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004199365615844727, AUC: 0.474165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010772217909495035, AUC: 0.6611595833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001062719742457072, AUC: 0.6992881666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011040577093760172, AUC: 0.6875382499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003312690258026123, AUC: 0.5127585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010876209735870362, AUC: 0.6017182500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882527430852254, AUC: 0.62834825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010908667643864949, AUC: 0.6612998333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017094500064849855, AUC: 0.5264639166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010848230918248495, AUC: 0.6182003333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001071896235148112, AUC: 0.6730391666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011185373862584432, AUC: 0.6151333333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017649821440378825, AUC: 0.5723455833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010987479289372762, AUC: 0.4996666666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010987189213434855, AUC: 0.4996666666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010997829834620157, AUC: 0.4998333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00608832597732544, AUC: 0.5086716666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010920181671778362, AUC: 0.5601531666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010409244696299234, AUC: 0.6902645000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011068669160207113, AUC: 0.6028140000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021818832556406657, AUC: 0.5789070833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010776280562082925, AUC: 0.6493408333333331\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011081376473108927, AUC: 0.5832308333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010898412466049195, AUC: 0.6675905000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021191367308298747, AUC: 0.542856\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010503672361373902, AUC: 0.7180411666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011078071991602579, AUC: 0.7179821666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011496917804082235, AUC: 0.7184414166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026698588530222576, AUC: 0.5463678333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010320513645807902, AUC: 0.7134408333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001108221968015035, AUC: 0.7109823333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001041821599006653, AUC: 0.7197072499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005746569474538167, AUC: 0.4532845833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001078433394432068, AUC: 0.6482778333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001126816471417745, AUC: 0.6100678333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011936333179473878, AUC: 0.5948250000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009807568550109863, AUC: 0.47465483333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010308910608291626, AUC: 0.7073885833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010718892415364583, AUC: 0.7265860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001066834052403768, AUC: 0.7409858333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009487020174662271, AUC: 0.6259245833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099141518274943, AUC: 0.5001666666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010993936856587728, AUC: 0.5005000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011007195313771565, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036891055901845295, AUC: 0.4669228333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010305541356404623, AUC: 0.7047171666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001035597284634908, AUC: 0.715627\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010791491667429605, AUC: 0.7220846666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002094646135965983, AUC: 0.46421533333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010728196303049723, AUC: 0.6705396666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011331758896509806, AUC: 0.6012943333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001163550853729248, AUC: 0.6642346666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003460930347442627, AUC: 0.5495174166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095219651858012, AUC: 0.6098751666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011400548617045085, AUC: 0.5993753333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001179930845896403, AUC: 0.6403596666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003280318816502889, AUC: 0.5327590833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099272648493449, AUC: 0.5246611666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011349796454111734, AUC: 0.5247438333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010822640260060629, AUC: 0.6992065833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021930465698242188, AUC: 0.4549478333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010677061478296915, AUC: 0.6927625833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011027512550354005, AUC: 0.6951028333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011327549219131469, AUC: 0.7108076666666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class SMOTE\n",
    "\n",
    "learning_rates = [1e-4, 5e-4, 1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e34ddade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dc17d94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004812867800394694, AUC: 0.5409411666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023861385981241864, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002552573919296265, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025949869950612388, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009052912712097167, AUC: 0.6076364166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002305452028910319, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002475385268529256, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025396854877471923, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008161910692850749, AUC: 0.4842375833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020253818829854327, AUC: 0.5001665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002539922555287679, AUC: 0.5001665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026366058190663657, AUC: 0.4998333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038154197533925376, AUC: 0.6093339166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023435554504394533, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024914178053538003, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025269065697987874, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031156800587972006, AUC: 0.5093163333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024133938948313395, AUC: 0.5000003333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026747779846191407, AUC: 0.4998335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002615092674891154, AUC: 0.499334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01446598211924235, AUC: 0.4437715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025545570055643717, AUC: 0.4998333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00261655330657959, AUC: 0.4998333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027922404607137043, AUC: 0.4998333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004291361967722575, AUC: 0.4879761666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003064807971318563, AUC: 0.5001666666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00256652307510376, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026002572377522787, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008738385836283365, AUC: 0.48364425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026279996236165367, AUC: 0.5003333333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002617308855056763, AUC: 0.5000003333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002647848924001058, AUC: 0.5000003333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005825160980224609, AUC: 0.4536914166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025450984636942544, AUC: 0.5005000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026282122135162355, AUC: 0.5001665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027145199775695803, AUC: 0.5001665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02267486572265625, AUC: 0.5673269166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00232442577679952, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002533121188481649, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026267924308776857, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004002079804738362, AUC: 0.5646723333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988486210505167, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986137787501018, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098699688911438, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010303960164388021, AUC: 0.4616\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001160082737604777, AUC: 0.49867\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986574490865072, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989943345387776, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002344720760981242, AUC: 0.4810855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986892382303874, AUC: 0.5003333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010987078348795574, AUC: 0.5003333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986342430114747, AUC: 0.5003333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00428933842976888, AUC: 0.5795606666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098675847053528, AUC: 0.4996666666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988184213638306, AUC: 0.4996666666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010987897316614788, AUC: 0.4996666666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006336372534434, AUC: 0.5590700833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986384948094687, AUC: 0.4998333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988965034484863, AUC: 0.4998333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988253752390543, AUC: 0.4998333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005059037049611409, AUC: 0.4420678333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011142648458480834, AUC: 0.49933425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011091484626134236, AUC: 0.49933425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011601476669311523, AUC: 0.4985009166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021111454168955485, AUC: 0.5645010833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986537138621012, AUC: 0.5001666666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010985951423645019, AUC: 0.5001666666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098590612411499, AUC: 0.5001666666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019826226631800333, AUC: 0.60002025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986967086791992, AUC: 0.5003333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986363490422567, AUC: 0.5003333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010987685124079387, AUC: 0.5003333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00324357008934021, AUC: 0.5801936666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098755160967509, AUC: 0.4998333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010987016757329304, AUC: 0.5001668333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986311833063762, AUC: 0.4998333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004217862606048584, AUC: 0.47700908333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010983821948369343, AUC: 0.5003333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010983309745788575, AUC: 0.5003333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010983136892318726, AUC: 0.5003333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015770610173543294, AUC: 0.5417658333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010987476507822672, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986540714899699, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986196994781493, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00413735024134318, AUC: 0.5327050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 23897210.261333335, AUC: 0.4991666666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 23897208.597333334, AUC: 0.4991666666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 23897210.133333333, AUC: 0.4991666666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004730463663736979, AUC: 0.5393131666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986175537109374, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988799730936687, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986205339431764, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002141329367955526, AUC: 0.4344065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986199378967285, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989187558492024, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988926490147909, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00344099489847819, AUC: 0.5803881666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010987171332041423, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010987287362416586, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986515283584594, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004296266237894694, AUC: 0.47261016666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988916556040446, AUC: 0.5000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098651647567749, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010987575848897299, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009753436724344889, AUC: 0.5919727499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989516178766885, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098662813504537, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986290375391643, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004094748735427856, AUC: 0.36678275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986746549606324, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986935297648112, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010987517833709716, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004551428476969401, AUC: 0.48637966666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988281965255736, AUC: 0.4996666666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098726232846578, AUC: 0.4996666666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988351504007975, AUC: 0.4996666666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035684934457143147, AUC: 0.5548563333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098883867263794, AUC: 0.4998333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988272031148274, AUC: 0.4998333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989216566085816, AUC: 0.4998333333333333\n",
      "\n",
      "[['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.05, 0.5187875666666666, 0.0032272329824469467, 0.5001000166666666, 3.443556916667559e-08, 0.5000000166666666, 1.1094463611110032e-08, 0.49991674999999997, 4.5752829166664745e-08, 1, False], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.05, 0.5309780500000001, 0.0030717790135002766, 0.499850425, 2.5161740340277357e-07, 0.500016775, 9.68242417361053e-08, 0.49990009166666666, 2.6752118673610135e-07, 5, False], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.05, 0.5101180333333334, 0.00442513017263639, 0.49986671666666666, 6.556891138888147e-08, 0.4998666666666667, 6.555555555554924e-08, 0.4998666666666667, 6.555555555554924e-08, 10, False]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f5ecca8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "16 columns passed, passed data had 15 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:934\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 934\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_or_indexify_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:981\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    982\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    983\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    984\u001b[0m     )\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 16 columns passed, passed data had 15 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/convnet_aucs.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m df2 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcol_names\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df1, df2])\n\u001b[1;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/convnet_aucs.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:781\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    780\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 781\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    783\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    789\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    790\u001b[0m         arrays,\n\u001b[1;32m    791\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    794\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    795\u001b[0m     )\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:498\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 498\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:840\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    837\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    838\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 840\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:937\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    934\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[0;32m--> 937\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[1;32m    940\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: 16 columns passed, passed data had 15 columns"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "294d600b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0053888258934020996, AUC: 0.5083743333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011163065830866496, AUC: 0.5555713333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011519194841384888, AUC: 0.6339973333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012265472412109374, AUC: 0.6482106666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004785251299540202, AUC: 0.45894758333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010515480041503906, AUC: 0.6808616666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097139040629069, AUC: 0.6735478333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001158262848854065, AUC: 0.6645963333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002081297715504964, AUC: 0.5468329166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992099046707154, AUC: 0.6545398333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001181641697883606, AUC: 0.6227119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012087234656016032, AUC: 0.6747756666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029571340878804526, AUC: 0.42823191666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010765260457992554, AUC: 0.6793964166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011122363011042277, AUC: 0.6777794166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011849076747894288, AUC: 0.6750039166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020178468624750773, AUC: 0.5219041666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001072571277618408, AUC: 0.6878450833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011367605924606323, AUC: 0.6763965000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011956455707550048, AUC: 0.6798735833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0066022364298502605, AUC: 0.5727894166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103708823521932, AUC: 0.7188979999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009921568830808004, AUC: 0.7163568333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012731473445892333, AUC: 0.7144595833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016108662684758504, AUC: 0.49719683333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010739359855651855, AUC: 0.6935228333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010923791726430256, AUC: 0.7211683333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010441219011942545, AUC: 0.7205261666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008686558087666829, AUC: 0.5756996666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087567130724589, AUC: 0.6568675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011722314755121866, AUC: 0.6350644166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012100796699523926, AUC: 0.6632841666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020558752218882244, AUC: 0.5304973333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010855903228123983, AUC: 0.6664822499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011296828190485637, AUC: 0.6900355833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011508344411849975, AUC: 0.6842542500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023987669944763182, AUC: 0.47386600000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011190526485443115, AUC: 0.5874691666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011887950499852497, AUC: 0.5807331666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012476621468861897, AUC: 0.6282875833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026117592652638755, AUC: 0.5501220833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010784562031428018, AUC: 0.646719\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011126607259114582, AUC: 0.6314551666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011253907283147175, AUC: 0.6645647499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00907833480834961, AUC: 0.4095369166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103119929631551, AUC: 0.5183336666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00108243723710378, AUC: 0.6521908333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011252542734146117, AUC: 0.6145240833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012144622166951497, AUC: 0.4181985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010980870326360067, AUC: 0.5616944166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882917245229086, AUC: 0.6519211666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011063965956370036, AUC: 0.66767875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009048165639241537, AUC: 0.43410375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882413387298584, AUC: 0.641269\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011007899045944213, AUC: 0.649048\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001128840168317159, AUC: 0.6556144166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007734829584757487, AUC: 0.4437409166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010419075886408489, AUC: 0.7244224166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001304646611213684, AUC: 0.7183474166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014163004557291668, AUC: 0.7187720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020479775667190552, AUC: 0.51335725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101297616958618, AUC: 0.5413485833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010999488433202109, AUC: 0.6217745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011075912714004516, AUC: 0.6400816666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017265344858169555, AUC: 0.46495474999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010841899712880452, AUC: 0.6420518333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010756450494130453, AUC: 0.6767504166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011000306208928427, AUC: 0.6903722499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012104656219482422, AUC: 0.41221791666666663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101318875948588, AUC: 0.5439462500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011140114068984984, AUC: 0.5820619166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011396301587422689, AUC: 0.5939751666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013818121910095215, AUC: 0.3908481666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010969403187433878, AUC: 0.5741503333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011325390338897704, AUC: 0.5341461666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011663690408070882, AUC: 0.5428661666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004127991437911987, AUC: 0.5320692499999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011001667579015096, AUC: 0.565351\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010823936859766642, AUC: 0.66097125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011145733197530112, AUC: 0.6432240833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021704901059468587, AUC: 0.5081183333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010876192649205525, AUC: 0.6033494166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010726250012715658, AUC: 0.6338876666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010898060003916423, AUC: 0.60157925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0061941939989725745, AUC: 0.5234804166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010763348340988158, AUC: 0.6378586666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010690643787384033, AUC: 0.6724069166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010695849259694417, AUC: 0.6631999166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029249339898427327, AUC: 0.44276099999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010818012952804566, AUC: 0.6249320000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001167305072148641, AUC: 0.7006170833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010593249003092449, AUC: 0.7213223333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035500851472218833, AUC: 0.50164675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011043848991394043, AUC: 0.54231\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098570664723714, AUC: 0.5695655000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094109813372294, AUC: 0.5949230833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028190720081329346, AUC: 0.47897783333333327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010734304587046305, AUC: 0.6374557500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010634453694025674, AUC: 0.6531299166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010553747018178304, AUC: 0.6659868333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008131365617116292, AUC: 0.44895408333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011458585262298583, AUC: 0.6904523333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001035565733909607, AUC: 0.7257168333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012226609388987224, AUC: 0.702193\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009203200022379556, AUC: 0.5448211666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010926599899927776, AUC: 0.6043731666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011341997782389323, AUC: 0.6254171666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010712933540344238, AUC: 0.66773725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021905073165893553, AUC: 0.5471773333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010562962690989176, AUC: 0.7030220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010428677399953207, AUC: 0.7076155833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010711555480957032, AUC: 0.7094441666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003333326896031698, AUC: 0.5377636666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010806766748428344, AUC: 0.642129\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010924222469329834, AUC: 0.5946263333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010711589256922403, AUC: 0.6532606666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008774649620056153, AUC: 0.6241281666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010897631247838338, AUC: 0.5906111666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010884263515472412, AUC: 0.596979\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010742882092793782, AUC: 0.6337864999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036368446350097654, AUC: 0.5375485833333333\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011079335610071819, AUC: 0.6403586666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011695301135381062, AUC: 0.602681\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012256291309992473, AUC: 0.6285350833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005145518620808919, AUC: 0.44634475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010701054732004801, AUC: 0.6592354166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011435633500417074, AUC: 0.6172679166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011791830857594808, AUC: 0.64276475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005740523020426432, AUC: 0.41456291666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011070729494094848, AUC: 0.545332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011548689603805542, AUC: 0.5669795000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001200473427772522, AUC: 0.6261900833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022632593313852947, AUC: 0.4380240833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010723592837651571, AUC: 0.6887599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011568965911865235, AUC: 0.6591684999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012437734206517538, AUC: 0.60485875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018457919359207153, AUC: 0.55856275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010602103074391683, AUC: 0.676076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011733896334966023, AUC: 0.6035581666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011962558031082154, AUC: 0.6873893333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00838488229115804, AUC: 0.5265839166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010944061279296875, AUC: 0.654685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012505316734313964, AUC: 0.7014080833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013044424454371135, AUC: 0.7064013333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007856205145517985, AUC: 0.5915576666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010972687403361004, AUC: 0.6041635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00114812699953715, AUC: 0.62198325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012080289920171103, AUC: 0.6327571666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002285668134689331, AUC: 0.5247015833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011085167725880941, AUC: 0.5773756666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011094851891199748, AUC: 0.67543275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011885340611139934, AUC: 0.6666003333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036948370933532716, AUC: 0.43958058333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001069793423016866, AUC: 0.6709383333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011645712455113728, AUC: 0.6009698333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011933152675628663, AUC: 0.6677953333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003929627418518066, AUC: 0.4183385833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001110571026802063, AUC: 0.5317219999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001152188777923584, AUC: 0.5732484999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001216336210568746, AUC: 0.5800096666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003980309009552002, AUC: 0.5070290833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010700532595316569, AUC: 0.6445938333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011160261233647665, AUC: 0.5381821666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984551111857096, AUC: 0.6573060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007836485385894775, AUC: 0.4103730833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010970458189646403, AUC: 0.5674521666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010961037079493204, AUC: 0.6252298333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001122948169708252, AUC: 0.6204729166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010923314412434895, AUC: 0.41686466666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099669059117635, AUC: 0.5706851666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011077654361724854, AUC: 0.61126075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011014473040898642, AUC: 0.6663798333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004605100154876709, AUC: 0.4316365833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001077207366625468, AUC: 0.6230364999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010845094124476114, AUC: 0.6518664166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010816977818806966, AUC: 0.6803232499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007020300070444743, AUC: 0.5918789166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010466662247975668, AUC: 0.68247\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010511811971664429, AUC: 0.6724024166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010570799509684245, AUC: 0.6767935833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028751110235850016, AUC: 0.45622691666666676\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010817949771881104, AUC: 0.602141\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011074643135070802, AUC: 0.5737399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010939449071884154, AUC: 0.6590863333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005461203416188558, AUC: 0.45323550000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010623817046483358, AUC: 0.6461289166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010850675900777181, AUC: 0.6511835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011046991348266603, AUC: 0.6535490833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023248490492502848, AUC: 0.51568475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001300006628036499, AUC: 0.7089076666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010798428455988567, AUC: 0.7372998333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012731024424235027, AUC: 0.7253934166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002557363589604696, AUC: 0.5463335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101215124130249, AUC: 0.5372181666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001085688312848409, AUC: 0.6449231666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011124345461527507, AUC: 0.6414555000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017286035219828287, AUC: 0.4455651666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010931458473205566, AUC: 0.60530275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010769670804341633, AUC: 0.6755840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011087607542673748, AUC: 0.6672878333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034246060053507487, AUC: 0.46436649999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001090443213780721, AUC: 0.5917323333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00108868678410848, AUC: 0.60868325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010782825549443564, AUC: 0.6326221666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004270084063212077, AUC: 0.47567216666666673\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010902762413024902, AUC: 0.5802461666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088633894920349, AUC: 0.5930140833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010953359603881835, AUC: 0.54499625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004291628837585449, AUC: 0.43709708333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010911602179209392, AUC: 0.572385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001089426358540853, AUC: 0.5912680833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001077050010363261, AUC: 0.6359901666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003900085687637329, AUC: 0.4815301666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011072588761647543, AUC: 0.48698325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099105715751648, AUC: 0.5862985833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010957812070846557, AUC: 0.5476917500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028812230428059897, AUC: 0.55280275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010783047676086426, AUC: 0.6177990833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010747024218241373, AUC: 0.6332463333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010614176193873088, AUC: 0.6451901666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004155467033386231, AUC: 0.53043275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010899144411087036, AUC: 0.606247\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010819032192230225, AUC: 0.6305116666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010890053908030192, AUC: 0.6113765833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020048028230667115, AUC: 0.5931780000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010772932370503743, AUC: 0.6419350833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010667002995808918, AUC: 0.6577113333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001060768206914266, AUC: 0.6619066666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007765159130096435, AUC: 0.5444235833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010996445814768472, AUC: 0.5426753333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010889208714167278, AUC: 0.6047218333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010936522881189981, AUC: 0.5875358333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005751923243204752, AUC: 0.44032116666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095639705657959, AUC: 0.58367475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010873432556788127, AUC: 0.6055861666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010804917414983114, AUC: 0.6198759166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007940491994222006, AUC: 0.5980770833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094688336054484, AUC: 0.5689736666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001085538903872172, AUC: 0.6112655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010963680346806844, AUC: 0.5705725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005407075881958007, AUC: 0.4677414166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010836641391118367, AUC: 0.6448572499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011251831849416096, AUC: 0.6361315833333335\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011914784510930379, AUC: 0.6240734166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023554405212402343, AUC: 0.4700285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010850404898325602, AUC: 0.6233955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011104260285695394, AUC: 0.6535688333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011573739051818847, AUC: 0.6540090833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00530566676457723, AUC: 0.4907729166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011098883152008056, AUC: 0.5446788333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145491639773051, AUC: 0.5830515833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011320642630259195, AUC: 0.6684396666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012915862401326498, AUC: 0.5365515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011070706844329833, AUC: 0.5431425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011233591238657633, AUC: 0.6475516666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011704811652501425, AUC: 0.6520456666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009043893178304037, AUC: 0.3958669166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010925205945968629, AUC: 0.6063278333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001122375210126241, AUC: 0.6276160833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012131238381067912, AUC: 0.5614534999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004727090040842692, AUC: 0.42868008333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010698118209838867, AUC: 0.6596433333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095819632212321, AUC: 0.6503713333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011717450221379597, AUC: 0.6223735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004732184092203776, AUC: 0.5233229166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010391622384389241, AUC: 0.6818479166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010653945604960123, AUC: 0.6792489166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010980259577433268, AUC: 0.68595425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016474919716517131, AUC: 0.47314133333333325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010652266343434652, AUC: 0.6558399166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001108786185582479, AUC: 0.6665774166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011450514396031698, AUC: 0.6801645833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031354562441507977, AUC: 0.42186033333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011128941774368286, AUC: 0.5227439166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001136849522590637, AUC: 0.5913285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001227043310801188, AUC: 0.5117476666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003512922763824463, AUC: 0.42164399999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001093176325162252, AUC: 0.6176624166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011098206837972006, AUC: 0.6645506666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011351662874221802, AUC: 0.6799634999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003787134091059367, AUC: 0.46579808333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010777191718419393, AUC: 0.650869\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011082991361618041, AUC: 0.5412195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001116129438082377, AUC: 0.602544\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009494578043619792, AUC: 0.5785944166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086069623629252, AUC: 0.6067176666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010645600954691569, AUC: 0.6524258333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001119981328646342, AUC: 0.5740205833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014893545309702556, AUC: 0.5147845833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010881072680155436, AUC: 0.5916234166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001052786707878113, AUC: 0.6934054999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00108830463886261, AUC: 0.6655008333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033571554025014242, AUC: 0.43533875000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010616891781489053, AUC: 0.6435039166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010459538300832112, AUC: 0.6678250833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001065412680308024, AUC: 0.6670287500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005959226608276367, AUC: 0.36945425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086418628692627, AUC: 0.6006046666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010508412520090738, AUC: 0.6594155833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001061234712600708, AUC: 0.6830227500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002466677904129028, AUC: 0.5022106666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010860243638356527, AUC: 0.5979315000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010745192368825276, AUC: 0.6619666666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011291266282399495, AUC: 0.6359860833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010658900578816731, AUC: 0.5182304999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010712224245071412, AUC: 0.6549818333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010742261409759522, AUC: 0.6570839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011202009916305542, AUC: 0.6024794166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01710516611735026, AUC: 0.47007916666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010952344338099162, AUC: 0.5796060833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011034897168477377, AUC: 0.5976064166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011379482746124267, AUC: 0.572879\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003692817767461141, AUC: 0.4503040833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010954463481903077, AUC: 0.5600168333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097853382428487, AUC: 0.5904741666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011293626626332602, AUC: 0.5443664999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005870925744374593, AUC: 0.41144374999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011055472294489543, AUC: 0.5378953333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010633336305618285, AUC: 0.6512815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011010560194651285, AUC: 0.6409848333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015494970083236694, AUC: 0.5676933333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010837071339289347, AUC: 0.6027206666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010784968137741088, AUC: 0.6306749166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010775943597157796, AUC: 0.6433308333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006495138645172119, AUC: 0.4843408333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010748443206151327, AUC: 0.6262219166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010918902158737183, AUC: 0.5597046666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010834360122680665, AUC: 0.6138046666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004927356084187826, AUC: 0.44561124999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010812255541483562, AUC: 0.6187255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010861013730367026, AUC: 0.6047561666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010757153034210206, AUC: 0.6378210833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004236334005991618, AUC: 0.53376225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001053187330563863, AUC: 0.6802904999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010549397865931194, AUC: 0.707322\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001049498160680135, AUC: 0.7173719166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019901039600372316, AUC: 0.5079928333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010722840229670206, AUC: 0.6268785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001065795143445333, AUC: 0.6511400833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010672556956609091, AUC: 0.6540048333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004992599646250407, AUC: 0.3883905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001113569974899292, AUC: 0.5305151666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101508339246114, AUC: 0.5553133333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010981414715449016, AUC: 0.5683530833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004242204030354818, AUC: 0.44027075000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010937814315160116, AUC: 0.5913730833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010776551564534505, AUC: 0.6300409166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010835390090942383, AUC: 0.62701025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029406830469767254, AUC: 0.5880721666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010836275815963746, AUC: 0.6296732500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010813354651133219, AUC: 0.6267249166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010722055435180664, AUC: 0.6438299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004063597440719604, AUC: 0.514617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010883302688598632, AUC: 0.5808056666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010941217343012491, AUC: 0.5523899166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010946215391159058, AUC: 0.5514673333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015948575337727865, AUC: 0.5340653333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010999303261439006, AUC: 0.5856963333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011032426357269287, AUC: 0.6176274166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001080282966295878, AUC: 0.6403633333333333\n",
      "\n",
      "[['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.5114340166666667, 0.002092150513974722, 0.6581454083333332, 0.002239285218479788, 0.6627791416666666, 0.0017482602106209013, 0.6753271916666668, 0.0006883278604778474, 1, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.45691495, 0.0028386302317697193, 0.59592865, 0.0037556808618108345, 0.6378666833333334, 0.002322282924477501, 0.6431673333333332, 0.002246811227576389, 1, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0001, 0.515782875, 0.002552141191850348, 0.62764935, 0.001982959012012226, 0.6479962, 0.002529749873798888, 0.6613433, 0.0016453860869933355, 1, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.4895805416666666, 0.003779200803835072, 0.6248646583333334, 0.002882340665858958, 0.6222697499999998, 0.0017123422166319447, 0.6443301833333333, 0.0013163547528552775, 5, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.47748281666666664, 0.0032497889435233324, 0.6187936166666665, 0.002567033028440556, 0.6381672083333332, 0.0027841285161003455, 0.664804775, 0.0006790309580834018, 5, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0001, 0.5117901250000001, 0.003231998553382294, 0.5792651666666667, 0.0016324257262222228, 0.6122306833333334, 0.0004439571837886108, 0.6057758, 0.001520786319021109, 5, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.46296099166666665, 0.0019082223064422932, 0.6100139416666666, 0.002754494862472292, 0.6399996583333334, 0.0008978640343978477, 0.6340224833333332, 0.002929749149266391, 10, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0005, 0.471623825, 0.0032281022349631245, 0.602375025, 0.0013438334068820146, 0.6372704249999999, 0.0018992524104297903, 0.618881275, 0.0019603903308861854, 10, False], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.0001, 0.500481625, 0.003414226224967011, 0.6072900583333333, 0.001403928877082013, 0.6135694333333334, 0.0021070331995913867, 0.6297357333333332, 0.001898588301142776, 10, False]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class euclidean distance capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['distance'] = 'euclidean'\n",
    "\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"distance_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70f1ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac108e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0033108042081197104, AUC: 0.5012746666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010315173864364625, AUC: 0.6993076666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001050257444381714, AUC: 0.7155940833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001184686779975891, AUC: 0.6812269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001436800241470337, AUC: 0.4174843333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010893462498982748, AUC: 0.6803560833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010367254416147867, AUC: 0.7091099166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011729620297749837, AUC: 0.7189865000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0074207375844319665, AUC: 0.47449125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001107176661491394, AUC: 0.5924970833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011400168736775717, AUC: 0.6406568333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012474682728449502, AUC: 0.6116938333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00392035977045695, AUC: 0.5374605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011524693171183268, AUC: 0.6903105833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011620090007781983, AUC: 0.7247786666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011772128343582153, AUC: 0.7492855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003082060734430949, AUC: 0.468488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011091880003611246, AUC: 0.6556036666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010824592113494872, AUC: 0.7240209166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012759371201197306, AUC: 0.6724420833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01163977591196696, AUC: 0.5361313333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010737388531366983, AUC: 0.7034769166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010427471001942953, AUC: 0.7443435833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001048147996266683, AUC: 0.7418451666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004088732322057088, AUC: 0.4640233333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010713371833165487, AUC: 0.6811145833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011560667355855305, AUC: 0.6580728333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011437164942423503, AUC: 0.7069555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021960195700327554, AUC: 0.5265366666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010082492431004843, AUC: 0.7259933333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010503043333689372, AUC: 0.7066401666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010594391425450642, AUC: 0.72776725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005359292507171631, AUC: 0.430348\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010997316042582195, AUC: 0.6226203333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010799174308776856, AUC: 0.710871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class cosine distance capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['distance'] = 'cosine'\n",
    "\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"cosine_distance_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f9b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdf4c5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010990204016367595, AUC: 0.5136136666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019028458992640176, AUC: 0.7512252500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002086396058400472, AUC: 0.7691906666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002416961034138997, AUC: 0.7808670000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010924596786499024, AUC: 0.5750945000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001673059900601705, AUC: 0.7794949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002390830198923747, AUC: 0.7884782499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002485786517461141, AUC: 0.7885251666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973639885584513, AUC: 0.5168527500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001288370688756307, AUC: 0.7993951666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001963376998901367, AUC: 0.8154784999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018575411240259806, AUC: 0.8272758333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002889474232991, AUC: 0.5800430833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014540077050526937, AUC: 0.7614485000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024033334255218505, AUC: 0.7844169166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026617908477783205, AUC: 0.8052283333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010971461534500123, AUC: 0.5515258333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013851773341496787, AUC: 0.778522\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001976533651351929, AUC: 0.7745744999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002331717650095622, AUC: 0.7970936666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103301207224528, AUC: 0.47176391666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013020078738530476, AUC: 0.7888039166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024007017612457274, AUC: 0.7778662500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001970576842625936, AUC: 0.8121091666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973190863927205, AUC: 0.5521948333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001569384256998698, AUC: 0.7577094999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002028331756591797, AUC: 0.7819775833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024892863432566326, AUC: 0.7912178333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011189500093460084, AUC: 0.35523316666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001377241571744283, AUC: 0.7758425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002085300286610921, AUC: 0.78223675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002421713908513387, AUC: 0.8017280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098294973373413, AUC: 0.4996409166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016685739755630494, AUC: 0.7695623333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002051869591077169, AUC: 0.7821711666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002364952882130941, AUC: 0.7924216666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011055078109105429, AUC: 0.39565374999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014359800020853678, AUC: 0.7632740833333335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class triplet loss capped smote\n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2]\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "loss_caps = [1, 5, 10]\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = loss_cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                _, _ = train.train_triplet_capped_loss(epoch, train_loader_tripletloss_smote, network, optimizer, verbose=False, cap_calc=loss_fns.TripletLoss,loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"triplet_loss_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f59a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11bd9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea18543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
