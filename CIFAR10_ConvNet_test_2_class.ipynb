{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c3c2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "378c3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91dda12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "NUM_CLASSES_REDUCED = 2\n",
    "nums = (0, 1)\n",
    "ratio = (100, 1)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b57e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "          #   \"mean_40\", \"variance_40\",\n",
    "          #   \"mean_50\", \"variance_50\",\n",
    "             \"cap\", \"normalization\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c2a1ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "norm=False\n",
    "\n",
    "if norm:\n",
    "    transform=torchvision.transforms.Compose([torchvision.transforms.Normalize(mean=[143.8888, 127.1705, 117.5357], std=[69.8313, 64.5137, 66.9933])])\n",
    "else:\n",
    "   # transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "    transform=None\n",
    "\n",
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "    \n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums, transform=transform)\n",
    "\n",
    "triplet_train_CIFAR10 = class_sampling.ForTripletLoss(reduced_train_CIFAR10, smote=False, transform=transform, num_classes=2)\n",
    "triplet_ratio_train_CIFAR10 = class_sampling.ForTripletLoss(ratio_train_CIFAR10, smote=False, transform=transform, num_classes=2)\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED, transform=transform)\n",
    "triplet_smote_train_CIFAR10 = class_sampling.ForTripletLoss(smote_train_CIFAR10, smote=True, transform=transform, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13159c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000   50]\n"
     ]
    }
   ],
   "source": [
    "targets = ratio_train_CIFAR10.labels \n",
    "\n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "print(class_count)\n",
    "\n",
    "weight = 1. / class_count\n",
    "\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "undersampler_smote = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * 50 * NUM_CLASSES_REDUCED), replacement=False)\n",
    "weight *= class_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af8cd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_smote_undersampled = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler_smote)\n",
    "\n",
    "train_loader_tripletloss = DataLoader(triplet_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_ratio = DataLoader(triplet_ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_smote = DataLoader(triplet_smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97b7ae03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006989752054214478, AUC: 0.306551\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 18\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     20\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:25\u001b[0m, in \u001b[0;36mtrain_sigmoid\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     24\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2 CLASS normal\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2, 1e-2, 1e-3, 5e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8f1d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d211694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006964986324310303, AUC: 0.36889099999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020484524965286256, AUC: 0.7839530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008004228307837897, AUC: 0.795284\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019669443964958192, AUC: 0.8054380000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007447379432839922, AUC: 0.845868\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001964762210845947, AUC: 0.8220825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006831325510387668, AUC: 0.8975879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007002675533294678, AUC: 0.29741149999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002268509268760681, AUC: 0.7684610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000813441066587768, AUC: 0.779452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001961479306221008, AUC: 0.8012969999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007347470497952239, AUC: 0.8548\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001975426435470581, AUC: 0.8130399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006815541962548943, AUC: 0.888204\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006969135999679566, AUC: 0.396233\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019460168480873107, AUC: 0.77575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008138178086074272, AUC: 0.786416\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018313562273979188, AUC: 0.7987635000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007782826017551493, AUC: 0.83284\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019774017333984376, AUC: 0.8090884999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007228488456642273, AUC: 0.8668359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006919987499713898, AUC: 0.590628\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025043778419494627, AUC: 0.7681610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000824363611656988, AUC: 0.7828759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001947782337665558, AUC: 0.812111\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007398436336529137, AUC: 0.8474480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018197112679481505, AUC: 0.825343\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006961135962738259, AUC: 0.8962600000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007070669531822204, AUC: 0.324542\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022070237398147583, AUC: 0.777849\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007834381481982989, AUC: 0.8004519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017794649600982666, AUC: 0.804418\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007518135676953462, AUC: 0.869884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018613035678863524, AUC: 0.821823\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006768840757927092, AUC: 0.905208\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006920409202575684, AUC: 0.5614465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019371829628944397, AUC: 0.7898210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007722149164148486, AUC: 0.814532\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019019254446029663, AUC: 0.8066430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007144288805377956, AUC: 0.8728039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019621884822845458, AUC: 0.8117289999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006771543613717993, AUC: 0.8938480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006985616087913513, AUC: 0.402626\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022849451303482054, AUC: 0.7709100000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007966125646772066, AUC: 0.7914239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019928234815597533, AUC: 0.8113159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007288691906793283, AUC: 0.850916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021587210893630982, AUC: 0.8282889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006852457171656412, AUC: 0.8815680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006967081129550934, AUC: 0.490978\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002309197545051575, AUC: 0.7767790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008197864576584042, AUC: 0.787224\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016714023947715759, AUC: 0.796902\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008205616820861798, AUC: 0.868436\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001921132743358612, AUC: 0.814014\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935946207198471, AUC: 0.882064\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006814700961112976, AUC: 0.6930545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017630945444107055, AUC: 0.797293\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007966739678810728, AUC: 0.826472\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018730688691139222, AUC: 0.8231759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006986736052549712, AUC: 0.8817240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001585574746131897, AUC: 0.8531839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006644394956897981, AUC: 0.9270039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006954295635223388, AUC: 0.4636135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002018611252307892, AUC: 0.751193\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007984958703417589, AUC: 0.774552\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017926790118217469, AUC: 0.7881119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007826962842061968, AUC: 0.847824\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001984425067901611, AUC: 0.8044880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006981643691363901, AUC: 0.886428\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006964466571807861, AUC: 0.408804\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002466729164123535, AUC: 0.7191185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008746101727506311, AUC: 0.7285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023755837678909302, AUC: 0.7475769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000830049450407819, AUC: 0.7640520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002291519284248352, AUC: 0.769038\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008008942412821078, AUC: 0.790772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006984891295433044, AUC: 0.2990615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024695838689804076, AUC: 0.702512\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000903661599899135, AUC: 0.703584\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002255721092224121, AUC: 0.733269\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008449238319281895, AUC: 0.7426200000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002216058373451233, AUC: 0.745498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00081563748529937, AUC: 0.76258\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007006736695766449, AUC: 0.27604300000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002342958450317383, AUC: 0.709482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008892802150901591, AUC: 0.716492\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023358142375946044, AUC: 0.7353419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008375033682747053, AUC: 0.748888\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002135712385177612, AUC: 0.761386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008083367728545229, AUC: 0.779436\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929198205471039, AUC: 0.617357\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023249754905700685, AUC: 0.702419\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008810730036901365, AUC: 0.707332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022707459926605226, AUC: 0.723219\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008420162158326643, AUC: 0.737148\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022084383964538572, AUC: 0.751656\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008105676031053657, AUC: 0.7706759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006976652443408966, AUC: 0.31618199999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00242640483379364, AUC: 0.708145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008835200035004038, AUC: 0.717596\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002294458746910095, AUC: 0.7325029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000837876483761143, AUC: 0.745504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022259188890457154, AUC: 0.7540900000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008166247896739457, AUC: 0.76966\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948408782482148, AUC: 0.4378925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025573383569717405, AUC: 0.713814\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009001902463191216, AUC: 0.7248199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002298957109451294, AUC: 0.7394749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008347357915585289, AUC: 0.75346\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00230900776386261, AUC: 0.7645065000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00081061886752596, AUC: 0.7800199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006877087652683258, AUC: 0.691464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024140385389328, AUC: 0.698046\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008903174916522043, AUC: 0.70268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002340320587158203, AUC: 0.7352985000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008448387669528475, AUC: 0.7441639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002200485348701477, AUC: 0.756923\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008142537423547837, AUC: 0.7689999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988822519779206, AUC: 0.43333200000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002531559944152832, AUC: 0.7074045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008929316163763846, AUC: 0.7121\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023130635023117067, AUC: 0.7368629999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008417200209517586, AUC: 0.746704\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021978071928024294, AUC: 0.754598\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008112246306188921, AUC: 0.7669400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006983757019042968, AUC: 0.3209125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002440286636352539, AUC: 0.7050354999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008984846457617707, AUC: 0.706744\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0022208908796310426, AUC: 0.738314\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008432061213188537, AUC: 0.747452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021523762941360472, AUC: 0.7596109999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008099005716198152, AUC: 0.772908\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932830810546875, AUC: 0.540304\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002492153882980347, AUC: 0.69481\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008906742870475691, AUC: 0.70906\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002333818554878235, AUC: 0.721683\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008443353652474608, AUC: 0.739868\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002207614302635193, AUC: 0.743633\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000816104515907493, AUC: 0.7657440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693915456533432, AUC: 0.470027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019597501158714293, AUC: 0.6219365000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010087698723862666, AUC: 0.6026600000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025770732164382935, AUC: 0.649457\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009415403474583336, AUC: 0.63322\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026130679845809937, AUC: 0.668488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009363414068697112, AUC: 0.6562439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952630281448364, AUC: 0.44003899999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002121282696723938, AUC: 0.501671\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011203794852636829, AUC: 0.462036\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002640716314315796, AUC: 0.5757490000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001002348891763699, AUC: 0.545164\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026532732248306275, AUC: 0.618418\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009733501744299831, AUC: 0.596864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006954418122768402, AUC: 0.4471715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019567978382110597, AUC: 0.5443659999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001200668271446582, AUC: 0.5197800000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002592808127403259, AUC: 0.6007549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010081923288283961, AUC: 0.584496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026791830062866213, AUC: 0.635221\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009875340786495127, AUC: 0.6246160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946364641189576, AUC: 0.437509\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018418793678283692, AUC: 0.44055199999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001196526876180479, AUC: 0.44020000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002501891493797302, AUC: 0.569048\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009530799596173928, AUC: 0.5647760000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026047686338424685, AUC: 0.612415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009452534362784412, AUC: 0.610228\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918039619922638, AUC: 0.601547\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002175205945968628, AUC: 0.531949\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000998632746684079, AUC: 0.516276\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002629355549812317, AUC: 0.607276\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009485418508105939, AUC: 0.598236\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002678037881851196, AUC: 0.643075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009430882653756307, AUC: 0.638256\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006879782378673554, AUC: 0.6277915000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022157384157180785, AUC: 0.5750029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010036093595963303, AUC: 0.56244\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027285748720169067, AUC: 0.6079819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009884885016201747, AUC: 0.595452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027752447128295897, AUC: 0.633263\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009839284465196405, AUC: 0.6226480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000696372777223587, AUC: 0.395862\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020386130809783936, AUC: 0.5864360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009720208211848051, AUC: 0.590588\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025379515886306764, AUC: 0.6347349999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009204692729186303, AUC: 0.637304\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026244809627532957, AUC: 0.657347\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009247063012738334, AUC: 0.6611159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943791210651398, AUC: 0.45462250000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020633881092071535, AUC: 0.576015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009746211608595187, AUC: 0.5702480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002581100583076477, AUC: 0.637748\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009229023426328555, AUC: 0.6324919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002632107138633728, AUC: 0.6657649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009244124916852406, AUC: 0.662368\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952486932277679, AUC: 0.4182815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018663351535797119, AUC: 0.545241\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011194773346628294, AUC: 0.552808\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025204519033432005, AUC: 0.6163200000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009360123755834481, AUC: 0.623728\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026386983394622804, AUC: 0.6517930000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009299963736666901, AUC: 0.6604\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006928751170635223, AUC: 0.4950935000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022486521005630495, AUC: 0.5687530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009940252438335134, AUC: 0.555052\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002693565011024475, AUC: 0.616304\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009704065832798138, AUC: 0.6050719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027260791063308714, AUC: 0.6472209999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009642241993863689, AUC: 0.638994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870812177658081, AUC: 0.739843\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008490297496318817, AUC: 0.5857295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004328208394569925, AUC: 0.552808\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017542111873626708, AUC: 0.5989365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001168501495503553, AUC: 0.5705520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002265569806098938, AUC: 0.616207\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009791869404587414, AUC: 0.591996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000696898102760315, AUC: 0.5182770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008060028553009033, AUC: 0.278211\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006648399871174652, AUC: 0.2956880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017908037900924683, AUC: 0.4555199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012544574736073465, AUC: 0.43443200000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023184224367141723, AUC: 0.5466409999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009757834696902497, AUC: 0.52138\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006959914863109588, AUC: 0.4148805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011752314567565919, AUC: 0.576725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002258190761993427, AUC: 0.568176\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020100303888320923, AUC: 0.5972729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010602153155325662, AUC: 0.590644\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023770898580551146, AUC: 0.610203\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009749452314238147, AUC: 0.60552\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006949910223484039, AUC: 0.47264950000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007239996790885926, AUC: 0.6477555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006259306055484432, AUC: 0.641904\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001465491473674774, AUC: 0.6645909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014327549595053834, AUC: 0.660544\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020677268505096434, AUC: 0.663818\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010180411716498952, AUC: 0.658852\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006855813264846802, AUC: 0.704147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001160222053527832, AUC: 0.594452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002175763076779866, AUC: 0.582432\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002008086562156677, AUC: 0.611847\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001003693709942964, AUC: 0.60094\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023846813440322875, AUC: 0.62987\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009382291516233788, AUC: 0.6193000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006942566931247711, AUC: 0.46113950000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012747212052345277, AUC: 0.6141890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001783648311796755, AUC: 0.604588\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00202965784072876, AUC: 0.6359250000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009888098204489982, AUC: 0.62552\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002390943884849548, AUC: 0.6458510000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009440167406850522, AUC: 0.636112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006907444298267365, AUC: 0.6638065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001061254382133484, AUC: 0.559679\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027392128050917446, AUC: 0.5395599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019682028889656065, AUC: 0.5875830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010773851526993336, AUC: 0.571\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002398018717765808, AUC: 0.605855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982346836993895, AUC: 0.59148\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947148740291595, AUC: 0.491396\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010332340002059937, AUC: 0.639335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002572380872351108, AUC: 0.644908\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018369460701942444, AUC: 0.637613\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010801145913872387, AUC: 0.641616\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022674994468688966, AUC: 0.640206\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009589608891470597, AUC: 0.643248\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006972042322158814, AUC: 0.3433105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014003113508224488, AUC: 0.540345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001675022819844803, AUC: 0.536408\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002195003390312195, AUC: 0.5878695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009807692990075833, AUC: 0.581788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025299243927001955, AUC: 0.612595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009592226822630013, AUC: 0.60672\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006968080401420593, AUC: 0.42184750000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014318109154701233, AUC: 0.500686\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016474896108750069, AUC: 0.46987599999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002215456962585449, AUC: 0.5417299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010021077599929702, AUC: 0.5135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025008474588394166, AUC: 0.5724260000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009738422585374648, AUC: 0.5467920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929619610309601, AUC: 0.517834\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025628254413604737, AUC: 0.660749\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009607309042153382, AUC: 0.6539680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024684345722198486, AUC: 0.688854\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009149152947829502, AUC: 0.685716\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024153801202774047, AUC: 0.700696\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008824188794276797, AUC: 0.7013880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963621377944946, AUC: 0.39932549999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026565759181976316, AUC: 0.6451220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000960259028793414, AUC: 0.6375200000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002558474898338318, AUC: 0.6828970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009217527456175867, AUC: 0.682304\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023991605043411253, AUC: 0.703199\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008820832003424368, AUC: 0.706716\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006975791752338409, AUC: 0.339162\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026126396656036377, AUC: 0.658784\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009396092526356477, AUC: 0.658856\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024959851503372193, AUC: 0.6913860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009068760096030955, AUC: 0.698356\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023628296852111815, AUC: 0.710028\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000874362751140748, AUC: 0.7199519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935542821884155, AUC: 0.5349695000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026821036338806154, AUC: 0.6905810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009409535158941947, AUC: 0.6890879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025097055435180666, AUC: 0.7111405000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008983603248022275, AUC: 0.712144\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024522416591644287, AUC: 0.7268490000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008782912564860417, AUC: 0.7291799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902917921543121, AUC: 0.633925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002716934323310852, AUC: 0.656775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009502239078098889, AUC: 0.657456\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002546200513839722, AUC: 0.6893039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009040189766795329, AUC: 0.696444\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025260263681411745, AUC: 0.707875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008834564439620417, AUC: 0.7185640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006962230801582336, AUC: 0.4351415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002600910544395447, AUC: 0.678147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009459915709229979, AUC: 0.665432\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025221219062805176, AUC: 0.707263\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009089078047856837, AUC: 0.7019759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002376657962799072, AUC: 0.7258259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008787278932436268, AUC: 0.724556\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006895989775657653, AUC: 0.6795534999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026365914344787596, AUC: 0.6996745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000932357641084507, AUC: 0.696256\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024846131801605225, AUC: 0.721959\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008952125390567402, AUC: 0.72334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024358515739440916, AUC: 0.732541\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008663736498628808, AUC: 0.738012\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007075443267822266, AUC: 0.3425195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025817182064056397, AUC: 0.648787\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009326574619453733, AUC: 0.654364\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002425007462501526, AUC: 0.6846749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008935547655097919, AUC: 0.696696\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023255358934402464, AUC: 0.7044335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008635037645955782, AUC: 0.7195079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006972502768039703, AUC: 0.4017025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025414057970046997, AUC: 0.688927\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009161692572812928, AUC: 0.690264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024763214588165283, AUC: 0.713159\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008872966284835988, AUC: 0.717964\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002337065577507019, AUC: 0.726836\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008584197800569606, AUC: 0.7334040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947490572929382, AUC: 0.45900799999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025774240493774414, AUC: 0.678329\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009367089254763162, AUC: 0.674748\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025151158571243286, AUC: 0.703386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000898137059486886, AUC: 0.706268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002368255615234375, AUC: 0.7219849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008681333497498591, AUC: 0.727964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS ratio\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2, 1e-2, 1e-3, 5e-4, 5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "learning_rate_train_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                _, train_auc = metric_utils.auc_sigmoid(train_loader_ratio, network) \n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f2d0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f1ccb0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006984597146511078, AUC: 0.3728135\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 18\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_oversampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     20\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:25\u001b[0m, in \u001b[0;36mtrain_sigmoid\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     24\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2 CLASS oversampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2, 1e-2, 1e-3, 5e-3, 1e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "52d8c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b1541b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006928627490997315, AUC: 0.5610565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006705484390258789, AUC: 0.731087\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006308182775974274, AUC: 0.7689\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006022096574306488, AUC: 0.792463\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006950775682926178, AUC: 0.4702115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006825314462184907, AUC: 0.7292394999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006524708569049835, AUC: 0.754748\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006467648446559906, AUC: 0.7728849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006955726444721221, AUC: 0.3766905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007012077569961548, AUC: 0.601467\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006722729504108429, AUC: 0.7478565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006584925651550293, AUC: 0.764774\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918634176254272, AUC: 0.548633\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006640854477882385, AUC: 0.7368910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006435896456241608, AUC: 0.772193\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006132080554962159, AUC: 0.805193\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006965889036655426, AUC: 0.3598795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006798864006996155, AUC: 0.722772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006425350904464722, AUC: 0.760243\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006240421831607819, AUC: 0.7786550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006957835257053375, AUC: 0.4375025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006834719181060791, AUC: 0.711951\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006513985991477967, AUC: 0.7628889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006210027635097504, AUC: 0.7961360000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007116021811962127, AUC: 0.2701275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006652469336986542, AUC: 0.749688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006440185308456421, AUC: 0.780153\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006193165481090545, AUC: 0.7989299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007008937299251557, AUC: 0.3809365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006463136076927186, AUC: 0.7635879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005850833356380462, AUC: 0.7921960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005856163203716278, AUC: 0.8098865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006968954503536225, AUC: 0.411403\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943140625953674, AUC: 0.7122989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006632572412490845, AUC: 0.767749\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006245843768119812, AUC: 0.795972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006954509615898132, AUC: 0.42410250000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006669250130653382, AUC: 0.768473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006450056135654449, AUC: 0.786346\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006585321128368377, AUC: 0.7965205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006984304785728454, AUC: 0.3510465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006993257403373718, AUC: 0.445579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007002671062946319, AUC: 0.557559\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006986504197120666, AUC: 0.624322\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006875935792922974, AUC: 0.6208015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006796917617321014, AUC: 0.7305775000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006728697121143341, AUC: 0.732001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000661188006401062, AUC: 0.737393\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006826050281524658, AUC: 0.708147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006837069392204285, AUC: 0.688508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006812298893928528, AUC: 0.6901515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006734901666641236, AUC: 0.704798\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932272017002106, AUC: 0.5695859999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006879149675369263, AUC: 0.6111989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006847475469112396, AUC: 0.640729\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006875899136066436, AUC: 0.6689890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902289688587189, AUC: 0.6300385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006905774176120758, AUC: 0.638779\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006883034110069275, AUC: 0.677482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006861826181411743, AUC: 0.7036560000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006971223056316376, AUC: 0.4343925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006849060654640197, AUC: 0.6515434999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006698879599571228, AUC: 0.6940439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006588464975357056, AUC: 0.713968\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927119195461273, AUC: 0.519376\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000688279390335083, AUC: 0.6079015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000678226500749588, AUC: 0.666508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006683720946311951, AUC: 0.6944840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006899041533470154, AUC: 0.6205875000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006839993894100189, AUC: 0.6695689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006716804206371307, AUC: 0.7078499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006677893996238709, AUC: 0.720974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930917203426361, AUC: 0.534002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947171986103058, AUC: 0.5545225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963235139846802, AUC: 0.5801274999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006964735686779023, AUC: 0.6224324999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932185888290405, AUC: 0.5061089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940591931343078, AUC: 0.546071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943828165531158, AUC: 0.6247775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941763460636139, AUC: 0.6806765000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000687966525554657, AUC: 0.7050644999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006887478828430176, AUC: 0.6989965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894939541816712, AUC: 0.690177\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006904767751693726, AUC: 0.6740535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929472386837006, AUC: 0.520876\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930617392063141, AUC: 0.510615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929692327976226, AUC: 0.5129855000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930001080036163, AUC: 0.509326\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000698872059583664, AUC: 0.252758\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988219320774078, AUC: 0.25494649999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988555192947388, AUC: 0.258632\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988353133201599, AUC: 0.26737\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007016754448413849, AUC: 0.3065925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007013347446918487, AUC: 0.3021595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007006541788578033, AUC: 0.3044395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006997955143451691, AUC: 0.3110185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947731077671051, AUC: 0.600099\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941960752010345, AUC: 0.5978035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693750262260437, AUC: 0.5939255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934162974357605, AUC: 0.5842645000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006975911855697632, AUC: 0.3350575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006968346834182739, AUC: 0.35933800000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006962250173091889, AUC: 0.392072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006956771612167358, AUC: 0.43028\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948958039283752, AUC: 0.42235900000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947318315505981, AUC: 0.42986450000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946011483669281, AUC: 0.4361265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945200264453888, AUC: 0.44007999999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006920962631702423, AUC: 0.5704765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006916036903858184, AUC: 0.600501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006912074089050293, AUC: 0.621478\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006909322440624237, AUC: 0.6336729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006977652311325074, AUC: 0.3840535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973227560520172, AUC: 0.3848955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006969808340072631, AUC: 0.38405150000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000696704238653183, AUC: 0.383508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006994488835334778, AUC: 0.386849\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006997025310993194, AUC: 0.392687\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006996612548828125, AUC: 0.40394399999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006992768943309784, AUC: 0.42428699999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988563239574432, AUC: 0.30963050000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006964642107486725, AUC: 0.4378635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936416327953339, AUC: 0.530629\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006915313899517059, AUC: 0.585804\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936706900596618, AUC: 0.5052385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943995654582978, AUC: 0.5241214999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006942475736141204, AUC: 0.5752839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930316984653473, AUC: 0.618519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006896259486675263, AUC: 0.604004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006859594881534576, AUC: 0.636279\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.000681488573551178, AUC: 0.661619\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006757974326610565, AUC: 0.6790075000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006896138191223144, AUC: 0.610887\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893069446086884, AUC: 0.613855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006885486543178558, AUC: 0.633282\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006888088881969452, AUC: 0.6441379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000698607712984085, AUC: 0.47644650000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006970592141151428, AUC: 0.5706005000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006956733465194702, AUC: 0.6301249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951669156551361, AUC: 0.6572\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006925795376300812, AUC: 0.575475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951481699943542, AUC: 0.5105139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006970000267028808, AUC: 0.4651495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000699210911989212, AUC: 0.424679\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938398480415344, AUC: 0.49060099999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006900518834590912, AUC: 0.6081639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006852288544178009, AUC: 0.6609965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006791809201240539, AUC: 0.6885735000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006768987178802491, AUC: 0.7593004999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000679383397102356, AUC: 0.7485544999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006822780966758728, AUC: 0.7235449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006835326850414276, AUC: 0.703198\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000697054535150528, AUC: 0.412941\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006958455443382264, AUC: 0.4202635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951872408390046, AUC: 0.4582435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693785697221756, AUC: 0.5277975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936173141002655, AUC: 0.49884150000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000692090779542923, AUC: 0.5442275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006922757029533387, AUC: 0.548847\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006904305517673493, AUC: 0.5985255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006974986791610717, AUC: 0.47294749999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006974535882472992, AUC: 0.47196799999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006974373161792755, AUC: 0.470164\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006974062919616699, AUC: 0.4685745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945805549621582, AUC: 0.511066\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945959925651551, AUC: 0.512351\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946030259132385, AUC: 0.513831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946027278900146, AUC: 0.5153995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007035338878631592, AUC: 0.3031575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000703522503376007, AUC: 0.3047515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000703513115644455, AUC: 0.306492\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000703542560338974, AUC: 0.307971\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006903063356876373, AUC: 0.6405325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902830302715302, AUC: 0.6414055000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902560889720917, AUC: 0.642237\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902223825454712, AUC: 0.6432424999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973913013935089, AUC: 0.3781635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973600089550018, AUC: 0.37925449999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973490417003632, AUC: 0.37986600000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973181366920472, AUC: 0.3809785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006964112520217896, AUC: 0.37526950000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963879466056823, AUC: 0.3755745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000696378231048584, AUC: 0.3756815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963653862476349, AUC: 0.375803\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952761113643647, AUC: 0.4577645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952200829982758, AUC: 0.45907450000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695160061120987, AUC: 0.46040699999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006950892806053162, AUC: 0.462167\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000690339595079422, AUC: 0.6339035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006903114020824433, AUC: 0.634817\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902790367603302, AUC: 0.6358309999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902419328689575, AUC: 0.6368320000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006928563117980957, AUC: 0.5400750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927987933158875, AUC: 0.5429109999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927626430988312, AUC: 0.544478\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927131414413453, AUC: 0.5468745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936696171760559, AUC: 0.48374\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936245560646057, AUC: 0.48604149999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935936212539673, AUC: 0.4878675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693551242351532, AUC: 0.48999699999999996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS undersampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2, 1e-2, 1e-3, 5e-3, 1e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fda3918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cdf3dafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006929865777492523, AUC: 0.6035210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941011548042297, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01152815311261923, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933517456054688, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010536302033037242, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934399306774139, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01047680665950964, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006829564273357391, AUC: 0.7606660000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007110981047153472, AUC: 0.513034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008160775073684089, AUC: 0.4939\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006991162598133087, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009259643141586001, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932158470153809, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011024211881184342, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006900024712085724, AUC: 0.6275345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006032556593418122, AUC: 0.6915875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009716101306499822, AUC: 0.67254\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006006793975830078, AUC: 0.703085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010080904476713426, AUC: 0.68551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693379133939743, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010516728127356803, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006867653727531433, AUC: 0.6620455000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941483020782471, AUC: 0.5020020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0101606553025765, AUC: 0.5024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947364509105683, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011732829313467044, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932136416435242, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011021087063421118, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946891844272614, AUC: 0.45950349999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940916180610656, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010191532998982043, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006905839145183564, AUC: 0.5271815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010548212835104159, AUC: 0.585972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006954963207244873, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009828539650038918, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695802628993988, AUC: 0.5831175000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006958652138710022, AUC: 0.594743\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008937520402492863, AUC: 0.6741199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006913960874080657, AUC: 0.532716\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009897223864451493, AUC: 0.5484240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006926067471504212, AUC: 0.5197149999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009974045635450004, AUC: 0.543572\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007032238245010376, AUC: 0.26280099999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963876187801361, AUC: 0.5065189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009458458411811602, AUC: 0.5388360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007120804488658906, AUC: 0.567307\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008191924195478458, AUC: 0.6288479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007221527099609375, AUC: 0.5354529999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007242459070564497, AUC: 0.6090860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007022532224655151, AUC: 0.309856\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940309107303619, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01150185514204573, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932080686092377, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011013430510417069, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936182677745819, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011321447296897963, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007022981643676758, AUC: 0.279428\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933487057685852, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010538602512661773, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937902569770813, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011403333208348491, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006939776837825775, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011481297110566998, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007030169069766998, AUC: 0.26083599999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941924095153809, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010158426478357598, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933222115039825, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011132937882206227, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947585344314575, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011739103463616701, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947209239006043, AUC: 0.503085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004985811412334442, AUC: 0.8705590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008741397662918166, AUC: 0.910452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00048722681403160097, AUC: 0.8629110000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007839573692567277, AUC: 0.911974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005121163129806519, AUC: 0.831754\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006779720865853942, AUC: 0.9525359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931937634944916, AUC: 0.6319214999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005152012705802917, AUC: 0.8624559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010805733227493739, AUC: 0.8948520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005664720535278321, AUC: 0.842025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006664018843433645, AUC: 0.9242239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005163897573947907, AUC: 0.8882985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004746445078070801, AUC: 0.95862\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006993449330329895, AUC: 0.308698\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005618657171726227, AUC: 0.837395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007188617935275087, AUC: 0.8682\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005276458859443665, AUC: 0.85389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0053951541858144325, AUC: 0.9311919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005647776126861573, AUC: 0.8183910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0056284768156485985, AUC: 0.942612\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006987028419971466, AUC: 0.419359\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000473314568400383, AUC: 0.8638839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007454072691426419, AUC: 0.884772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005346084535121918, AUC: 0.8605094999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007051103150490487, AUC: 0.9267599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005636484026908874, AUC: 0.8834069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004816935280762096, AUC: 0.9464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006926956474781036, AUC: 0.62321\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005920693576335907, AUC: 0.872232\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00950404413855902, AUC: 0.898072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000488565668463707, AUC: 0.86691\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005895164248966934, AUC: 0.8858999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005560655891895294, AUC: 0.870534\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004082345602535966, AUC: 0.9216799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00069858717918396, AUC: 0.3564225000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005433701276779174, AUC: 0.8413849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007599443150038766, AUC: 0.8711\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005662823319435119, AUC: 0.8394060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0061812017282637036, AUC: 0.9136279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006344362497329712, AUC: 0.863313\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004362780357351398, AUC: 0.933392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006920470595359802, AUC: 0.626553\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005774954557418823, AUC: 0.8625749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005773622346396493, AUC: 0.8950400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005901984572410584, AUC: 0.8413899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005424396850094937, AUC: 0.9351479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005415751934051514, AUC: 0.844387\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005204110599980496, AUC: 0.9476119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006909263730049133, AUC: 0.608957\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005072842240333557, AUC: 0.848906\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006716722314900691, AUC: 0.88254\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000501204401254654, AUC: 0.8605349999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005464349566119732, AUC: 0.916816\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005297437012195587, AUC: 0.852255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006041818249343646, AUC: 0.951712\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929771304130555, AUC: 0.6099144999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005701894760131835, AUC: 0.810224\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007652453461495957, AUC: 0.9052879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005346051752567291, AUC: 0.865758\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004242133191316434, AUC: 0.94032\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005565256476402282, AUC: 0.8667680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006119210065001308, AUC: 0.932944\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973126232624054, AUC: 0.389163\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0005637988150119782, AUC: 0.828475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00714115304521995, AUC: 0.9039240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005115714669227601, AUC: 0.872054\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0058704357690150194, AUC: 0.9506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005990622639656067, AUC: 0.854916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004999826568188054, AUC: 0.9478759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893006265163422, AUC: 0.592972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005413131713867187, AUC: 0.838742\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009401522638774154, AUC: 0.873108\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005082505941390992, AUC: 0.8643450000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007597315765843533, AUC: 0.93034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005349463224411011, AUC: 0.819852\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005960197472336269, AUC: 0.952184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006917398273944855, AUC: 0.5357335000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005320677161216736, AUC: 0.872421\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007882021895729669, AUC: 0.885336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047688519954681394, AUC: 0.8764339999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006409961439595364, AUC: 0.9162440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004914150685071945, AUC: 0.8588819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005879358218447997, AUC: 0.94408\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951043009757996, AUC: 0.4402385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005084710121154785, AUC: 0.8479289999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007602337244713661, AUC: 0.887208\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004919102936983108, AUC: 0.8564430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006338312826534309, AUC: 0.9233879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005057655572891235, AUC: 0.839117\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006557884688424592, AUC: 0.94724\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006957819759845733, AUC: 0.554511\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005380403399467468, AUC: 0.8291689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009521948453223352, AUC: 0.906452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004899816066026687, AUC: 0.8586830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00712057093582531, AUC: 0.9333940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005030784308910369, AUC: 0.850487\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0057055700031837615, AUC: 0.94844\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007004860043525696, AUC: 0.3607595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00053057861328125, AUC: 0.8529720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0070074458405523015, AUC: 0.886212\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004892897009849548, AUC: 0.868004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005604852668129571, AUC: 0.920404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000458210751414299, AUC: 0.875929\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006420788363655015, AUC: 0.946916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940570473670959, AUC: 0.4731449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005776671469211579, AUC: 0.8586539999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012373634175498888, AUC: 0.8687280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00044246050715446473, AUC: 0.889369\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0070867665805438955, AUC: 0.9181079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005300732553005219, AUC: 0.8762265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005396853927338478, AUC: 0.960264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929945051670075, AUC: 0.564638\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005563854575157166, AUC: 0.8648770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009520969921999639, AUC: 0.923976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005157967507839203, AUC: 0.8525540000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006059927674803403, AUC: 0.939404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000465511754155159, AUC: 0.877534\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006717685177774713, AUC: 0.948132\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006980034112930298, AUC: 0.392889\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005355749130249024, AUC: 0.8342959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007846096653749447, AUC: 0.8555839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004474954456090927, AUC: 0.879305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00643831486159032, AUC: 0.898184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046650290489196777, AUC: 0.887073\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00500908629138871, AUC: 0.929724\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006906140744686127, AUC: 0.6270205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005572724044322967, AUC: 0.85573\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008027610229973746, AUC: 0.898772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005564309954643249, AUC: 0.8470209999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004956791843518172, AUC: 0.911748\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004649658203125, AUC: 0.866714\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006907176440305049, AUC: 0.956392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938843429088593, AUC: 0.47562899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000519424855709076, AUC: 0.8551110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007493531125606877, AUC: 0.8831279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000504085659980774, AUC: 0.8609840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006028514882125476, AUC: 0.9416640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004914458394050599, AUC: 0.867466\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006038065062891139, AUC: 0.96608\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963664293289185, AUC: 0.38524\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006432399749755859, AUC: 0.7625959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008959613684380408, AUC: 0.7848919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005628331005573272, AUC: 0.826678\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00864995011598757, AUC: 0.84258\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005056944489479065, AUC: 0.8590329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00824941654016476, AUC: 0.871856\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006910701990127564, AUC: 0.599355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000616879791021347, AUC: 0.7707120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01059920307433251, AUC: 0.781044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005687816441059113, AUC: 0.799205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009738183611690408, AUC: 0.8164119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005393176674842834, AUC: 0.8172925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008101473804747704, AUC: 0.842912\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006939460039138794, AUC: 0.54028\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006021830439567566, AUC: 0.7970139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010496961374093991, AUC: 0.803984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005550967454910278, AUC: 0.828073\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009545001192848282, AUC: 0.839732\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005240070223808289, AUC: 0.844389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0091742208452508, AUC: 0.8625879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006914302408695221, AUC: 0.6094385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005888742804527283, AUC: 0.824057\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01029107907030842, AUC: 0.83066\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005443206429481506, AUC: 0.8330359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009114989762259001, AUC: 0.849788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005262280106544494, AUC: 0.853632\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008392372603463654, AUC: 0.878884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006837622821331024, AUC: 0.691576\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005895999670028686, AUC: 0.782758\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009808489296695973, AUC: 0.80072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005589506924152375, AUC: 0.806367\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009164169448437077, AUC: 0.8281080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005357769429683685, AUC: 0.8292949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00911985157149853, AUC: 0.8574439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945233047008514, AUC: 0.48311099999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006134748458862304, AUC: 0.806477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01052536406139336, AUC: 0.808596\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005562895834445953, AUC: 0.822084\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009412364829884898, AUC: 0.833688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005309949219226837, AUC: 0.8396819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007839170440588848, AUC: 0.857212\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937495470046997, AUC: 0.48171200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006372389495372772, AUC: 0.819923\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0120068094163838, AUC: 0.8215159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005695891678333283, AUC: 0.822279\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009782834808425149, AUC: 0.840788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005112173855304718, AUC: 0.8443940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008199451105429394, AUC: 0.868456\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006921443343162536, AUC: 0.5599354999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005984731018543243, AUC: 0.8115479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010888017746481566, AUC: 0.82364\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005443423092365265, AUC: 0.8276000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009130165565131914, AUC: 0.8468919999999999\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0005055390298366547, AUC: 0.849865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008350928868397627, AUC: 0.8736079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963136792182922, AUC: 0.39968349999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005984225273132324, AUC: 0.771389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009001542176350508, AUC: 0.78562\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000558992862701416, AUC: 0.8067799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008595412568290635, AUC: 0.829488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000538530021905899, AUC: 0.82298\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007287021987509019, AUC: 0.8586119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006968409717082977, AUC: 0.38355749999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006436823904514313, AUC: 0.803137\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01135210377154964, AUC: 0.7909520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000583594560623169, AUC: 0.8077380000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01014432730060993, AUC: 0.820932\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005280269980430603, AUC: 0.833491\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008627873015875864, AUC: 0.8455999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870892643928528, AUC: 0.656811\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006438588798046112, AUC: 0.785184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011279767680876325, AUC: 0.77802\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005855887830257415, AUC: 0.790768\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009658517200167816, AUC: 0.799412\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005556738078594208, AUC: 0.8051299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008975181662210143, AUC: 0.823952\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007047882378101349, AUC: 0.3414305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006809703409671784, AUC: 0.7605970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010495741154887888, AUC: 0.7505620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006465067863464356, AUC: 0.7901370000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010764568288727561, AUC: 0.7835139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006005278527736664, AUC: 0.7971060000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010074336410749077, AUC: 0.8006639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952670216560364, AUC: 0.435129\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006432079970836639, AUC: 0.7717779999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010464417521316226, AUC: 0.7932920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005900460183620453, AUC: 0.7872790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009503405046935129, AUC: 0.809028\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005679526627063752, AUC: 0.8025730000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008954270877460441, AUC: 0.825492\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006970075964927673, AUC: 0.3704055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006620379984378815, AUC: 0.8133829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010983516284734896, AUC: 0.824464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005913313031196594, AUC: 0.8132949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009973079898569843, AUC: 0.826076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005637726485729218, AUC: 0.8200820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009417588061625414, AUC: 0.837888\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006897422671318054, AUC: 0.598884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006093161702156067, AUC: 0.7742269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00976294757115959, AUC: 0.785188\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005706937611103058, AUC: 0.7944450000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008533435092113986, AUC: 0.8078\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005440973341464996, AUC: 0.820147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008820098668041796, AUC: 0.837528\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000686633974313736, AUC: 0.66552\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006343769133090973, AUC: 0.7708459999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00998953613904443, AUC: 0.77996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000589023768901825, AUC: 0.781485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009906717373593018, AUC: 0.791872\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005734374821186065, AUC: 0.7928010000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009384710352019508, AUC: 0.8104199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006928159296512603, AUC: 0.5240104999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006608406603336335, AUC: 0.7565659999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010737750010915321, AUC: 0.761488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006106067299842835, AUC: 0.7730439999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010074301509573908, AUC: 0.781744\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005812602043151855, AUC: 0.7897865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009442508126249407, AUC: 0.805176\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943551301956176, AUC: 0.5027984999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006626200675964356, AUC: 0.8112789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010643097292078603, AUC: 0.82374\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005914508402347564, AUC: 0.8046445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010367770584503022, AUC: 0.8259\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005529929101467133, AUC: 0.8113150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009098475008907885, AUC: 0.837008\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006970823407173157, AUC: 0.3750015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006655546128749848, AUC: 0.788638\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011142519086894422, AUC: 0.803384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005962426960468292, AUC: 0.7917019999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010091654510781317, AUC: 0.800416\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005699568390846253, AUC: 0.8046564999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009837563828666612, AUC: 0.8201919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006922467052936554, AUC: 0.5355885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006529574692249298, AUC: 0.762027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011207672263136004, AUC: 0.763824\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006132130324840545, AUC: 0.778235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010650047831016012, AUC: 0.7884599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005724292099475861, AUC: 0.790807\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00923902967188618, AUC: 0.8058000000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 Class Weighted Loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2, 1e-2, 5e-3, 1e-3, 5e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['pos_weight'] = torch.tensor([weight[1]])\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                _, train_auc = metric_utils.auc_sigmoid(train_loader_ratio, network) \n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9eed2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7c5a840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001968830943107605, AUC: 0.6361665000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931888163089752, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931563913822174, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931565403938293, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032240989208221437, AUC: 0.38664699999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693162739276886, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931797564029694, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931637823581696, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006873347997665405, AUC: 0.683021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693148136138916, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931990087032318, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931540668010712, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001451125681400299, AUC: 0.47390000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931474804878234, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931490898132324, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931617856025696, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031172895431518556, AUC: 0.6896909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693187415599823, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931720674037933, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931543052196503, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00219118869304657, AUC: 0.37103100000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931504905223847, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931714117527009, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931474804878234, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006955941677093506, AUC: 0.3322345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931925714015961, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931772828102111, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931811273097992, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028511343002319335, AUC: 0.5939905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931495368480683, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931488215923309, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931621432304382, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011438738107681274, AUC: 0.351393\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693156749010086, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931765377521515, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931516230106354, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015125332474708558, AUC: 0.429251\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930655539035797, AUC: 0.49700500000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007237447500228882, AUC: 0.502838\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007046388685703278, AUC: 0.47184400000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034982739686965944, AUC: 0.618384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932589411735535, AUC: 0.49600900000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000694739580154419, AUC: 0.495022\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006974195539951324, AUC: 0.490041\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011465153098106384, AUC: 0.401948\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006579118371009827, AUC: 0.8357714999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007065658271312714, AUC: 0.8619270000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008777901530265808, AUC: 0.8454470000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009435354471206665, AUC: 0.577937\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693109542131424, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930815577507019, AUC: 0.5000005000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930619776248932, AUC: 0.500499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029533997774124146, AUC: 0.545541\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934868097305298, AUC: 0.49650900000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952846646308899, AUC: 0.49300000000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005754931569099427, AUC: 0.8135389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038374502658843996, AUC: 0.565988\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963256597518921, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945149898529053, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936972737312316, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001448841392993927, AUC: 0.5810709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007005001306533813, AUC: 0.499501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006956695616245269, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948554813861847, AUC: 0.4989985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004549663305282592, AUC: 0.44000300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006500706076622009, AUC: 0.8463595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000791742742061615, AUC: 0.7998700000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007664673924446106, AUC: 0.7890570000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028102716207504272, AUC: 0.5378565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007185445129871368, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007116530537605286, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006978726089000702, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018956378698349, AUC: 0.37419199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007894859313964844, AUC: 0.7529689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008713241219520569, AUC: 0.7998595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000786967545747757, AUC: 0.8392839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001775826096534729, AUC: 0.554507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007811388969421386, AUC: 0.500399\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006824729740619659, AUC: 0.828609\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008813585042953491, AUC: 0.8618694999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019287337064743042, AUC: 0.518386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000670422375202179, AUC: 0.734654\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007510415315628052, AUC: 0.8405705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009540053308010101, AUC: 0.743062\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003408196210861206, AUC: 0.7021970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006586644351482392, AUC: 0.832709\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007639351189136505, AUC: 0.8214729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007232868075370788, AUC: 0.8304604999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033094873428344725, AUC: 0.46274150000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006908712387084961, AUC: 0.7841234999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008197993040084839, AUC: 0.8188309999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000855831652879715, AUC: 0.7749320000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007158293724060058, AUC: 0.573209\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000609256386756897, AUC: 0.8331350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006620108485221863, AUC: 0.732453\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009972791373729706, AUC: 0.5589255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004602427005767822, AUC: 0.536975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000682371735572815, AUC: 0.811897\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008557241559028625, AUC: 0.6945314999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008623737096786499, AUC: 0.857157\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023244422674179076, AUC: 0.5137155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006493400633335114, AUC: 0.8368385000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008260475099086762, AUC: 0.7857859999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008859169483184814, AUC: 0.8146289999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004095608711242676, AUC: 0.30450750000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000710114985704422, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007200100123882293, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007078570127487183, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006330698013305664, AUC: 0.49924100000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005744315683841705, AUC: 0.843899\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006924572587013244, AUC: 0.850835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007589767277240753, AUC: 0.8768189999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020437593460083006, AUC: 0.4564695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005823137760162353, AUC: 0.8514384999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006698321998119354, AUC: 0.7442280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008827683627605439, AUC: 0.7737239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025592957735061646, AUC: 0.6610935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000622205376625061, AUC: 0.8423610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007380467653274537, AUC: 0.867303\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000803507387638092, AUC: 0.8693059999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002686577558517456, AUC: 0.574317\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008157544732093811, AUC: 0.819151\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005929317772388459, AUC: 0.860327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006713431775569916, AUC: 0.859534\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010352564454078675, AUC: 0.479156\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006612022221088409, AUC: 0.7615710000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006444250345230102, AUC: 0.8049535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007023713588714599, AUC: 0.7501019999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035049201250076296, AUC: 0.364836\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006851175129413605, AUC: 0.6413059999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006361819505691528, AUC: 0.8489644999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006566484868526459, AUC: 0.8274655000000002\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.00241806423664093, AUC: 0.3173455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006629388034343719, AUC: 0.7816734999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005770286917686462, AUC: 0.8724530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005904681980609894, AUC: 0.892585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001278232514858246, AUC: 0.507354\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930362582206726, AUC: 0.5019979999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006615914404392243, AUC: 0.7755829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007022977471351624, AUC: 0.6703819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020855687856674195, AUC: 0.4122905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006579199731349945, AUC: 0.7659765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000646492600440979, AUC: 0.8105619999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940586864948272, AUC: 0.7869459999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013735867142677307, AUC: 0.58304\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000610756278038025, AUC: 0.8491740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006886270344257355, AUC: 0.737814\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006371495127677917, AUC: 0.8547920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001305916428565979, AUC: 0.44187750000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005949230194091797, AUC: 0.863308\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006304492950439453, AUC: 0.84153\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006957325339317322, AUC: 0.806642\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008709692656993866, AUC: 0.5386139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006063607931137085, AUC: 0.8584470000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007980087101459503, AUC: 0.8418700000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000669499397277832, AUC: 0.8283939999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008707275390625, AUC: 0.354607\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006290667951107025, AUC: 0.8206869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005803967118263245, AUC: 0.8815959999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000599628746509552, AUC: 0.873222\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034300466775894164, AUC: 0.40457550000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006403252482414246, AUC: 0.789991\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006331656873226166, AUC: 0.8034855000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006379285752773284, AUC: 0.7980840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011988989710807801, AUC: 0.38362300000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938813030719758, AUC: 0.49477600000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936386823654174, AUC: 0.495012\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935105323791504, AUC: 0.501972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008709073662757873, AUC: 0.554262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006604278087615966, AUC: 0.7741944999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006455783247947693, AUC: 0.8053085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000651368260383606, AUC: 0.810346\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007912777185440064, AUC: 0.4069285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006583632528781891, AUC: 0.729901\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006464119553565979, AUC: 0.7933275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006274025738239288, AUC: 0.823512\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004647176265716552, AUC: 0.440377\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006442047655582428, AUC: 0.7825475000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006391406357288361, AUC: 0.7995915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006263911128044129, AUC: 0.8200305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013046321272850037, AUC: 0.646519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005313718318939209, AUC: 0.8365239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005058334320783615, AUC: 0.8507679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004936377257108688, AUC: 0.8587339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007593387961387634, AUC: 0.6597930000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006295811831951142, AUC: 0.799296\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000618862122297287, AUC: 0.8181650000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006132758855819702, AUC: 0.8227334999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012480967044830322, AUC: 0.413415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006244701743125915, AUC: 0.8253189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006149193346500397, AUC: 0.8396085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005953423976898193, AUC: 0.8439519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010108822345733643, AUC: 0.684472\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006323348879814148, AUC: 0.752054\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005877191126346588, AUC: 0.8231439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005664858222007752, AUC: 0.843323\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014604349136352539, AUC: 0.61742\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006674048900604248, AUC: 0.7258069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006546805799007416, AUC: 0.801582\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006493621170520782, AUC: 0.8238329999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS SMOTE\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2, 1e-2, 5e-3, 1e-3, 1e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66655269",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cfecae8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006927153766155242, AUC: 0.520336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006058035492897034, AUC: 0.7684874999999999\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 27\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid_with_smote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCappedBCELoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     29\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:56\u001b[0m, in \u001b[0;36mtrain_sigmoid_with_smote\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mfloat(), smote_target)\n\u001b[1;32m     55\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m---> 56\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2 Class Capped SMOTE \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3,1e-2, 5e-3]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_sigmoid_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606da0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = (col_names))\n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "563f22d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0012810109853744506, AUC: 0.4544585\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'Tensor' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 21\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSigmoidFocalLoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     23\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:23\u001b[0m, in \u001b[0;36mtrain_sigmoid\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m output \u001b[38;5;241m=\u001b[39m network(data)\n\u001b[0;32m---> 23\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/loss_fns.py:20\u001b[0m, in \u001b[0;36mSigmoidFocalLoss.forward\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, targets):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torchvision\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39msigmoid_focal_loss(\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m, targets, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'Tensor' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# 2 CLASS Focal Loss\n",
    "# this code might not work \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SigmoidFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d438d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5f6eb22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf2\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7924e1f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.005698421239852905, AUC: 0.3140615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006708012223243713, AUC: 0.7778695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006707127690315246, AUC: 0.787516\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005773176550865173, AUC: 0.849029\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004301182508468628, AUC: 0.722232\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006169197559356689, AUC: 0.8504210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000606573611497879, AUC: 0.857934\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006615423262119293, AUC: 0.846602\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013060729503631592, AUC: 0.33644399999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006230704486370087, AUC: 0.8254634999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006484315097332001, AUC: 0.8357224999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006770155727863312, AUC: 0.8425440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001565246045589447, AUC: 0.39420900000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006965601146221161, AUC: 0.859681\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006382490396499633, AUC: 0.871605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010520057082176208, AUC: 0.8457330000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00158281010389328, AUC: 0.548578\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006666460335254669, AUC: 0.7720600000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944780349731445, AUC: 0.7442995000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007583337128162384, AUC: 0.63171\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022197554111480712, AUC: 0.35996700000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005065007209777832, AUC: 0.85887\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008205614984035492, AUC: 0.8404170000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004839869886636734, AUC: 0.852982\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011423288583755494, AUC: 0.5575330000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006301871240139008, AUC: 0.8206169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006632060110569, AUC: 0.8165115000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006974429786205292, AUC: 0.7883384999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016642996072769164, AUC: 0.484873\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006471936404705048, AUC: 0.8204565000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006263204514980317, AUC: 0.7809514999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006439337134361268, AUC: 0.864905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001160304307937622, AUC: 0.377119\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006235313415527344, AUC: 0.8434600000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006380892097949982, AUC: 0.8298150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007070594727993011, AUC: 0.7519194999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0067447471618652345, AUC: 0.349441\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006633142530918122, AUC: 0.7950739999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006555951833724976, AUC: 0.822144\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007357679605484009, AUC: 0.7437509999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008558807075023651, AUC: 0.559912\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006556170880794525, AUC: 0.8044765000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000798446774482727, AUC: 0.801143\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008967866897583008, AUC: 0.822384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00369672691822052, AUC: 0.6232645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007838921844959259, AUC: 0.51291\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008929863274097443, AUC: 0.7916525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007719839513301849, AUC: 0.799328\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006254876375198364, AUC: 0.473279\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006401098072528839, AUC: 0.77892\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007755281329154968, AUC: 0.7755100000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000843909353017807, AUC: 0.8346814999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017196095585823058, AUC: 0.34696800000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005537250638008118, AUC: 0.8383275000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005859960913658142, AUC: 0.865712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# distance + capped loss\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_cap = 5.0\n",
    "loss_fn_args['distance'] = 'euclidean'\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(start_epoch):\n",
    "            loss_fn_args['loss_cap'] = None\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "        for epoch in range(start_epoch, n_epochs + 1):\n",
    "            loss_fn_args['loss_cap'] = loss_cap\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"distance_capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], loss_fn_args['loss_cap'], norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f921d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cosine distance + capped loss\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-3, 5e-4, 1e-4, 5e-5]\n",
    "\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = 5.0\n",
    "loss_fn_args['distance'] = 'cosine'\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(start_epoch):\n",
    "            loss_fn_args['loss_cap'] = None\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "        for epoch in range(start_epoch, n_epochs + 1):\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"cosine_distance_capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], loss_fn_args['loss_cap'], norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab098399",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17af0e21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004998538017272949, AUC: 0.5778245\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m15\u001b[39m):\n\u001b[0;32m---> 24\u001b[0m     _, train_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_triplet_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_tripletloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(train_losses))))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:128\u001b[0m, in \u001b[0;36mtrain_triplet_loss\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn_args)\u001b[0m\n\u001b[1;32m    126\u001b[0m neg_embeds \u001b[38;5;241m=\u001b[39m network(neg_data\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m    127\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(anchor_embeds, pos_embeds, neg_embeds)\n\u001b[0;32m--> 128\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "# 2 class triplet loss no ratio \n",
    "# no smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(5e-6, 1e-3), (1e-6, 5e-4), (1e-6, 1e-4), (5e-6, 5e-4)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "    \n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10): \n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(complete_network.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(15):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss, embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_reduced, complete_network, linear_optimizer, verbose=False)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "    \n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"triplet_loss\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], 0.0, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8afa13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46d043a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.000679101675748825, AUC: 0.7387214999999999\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m15\u001b[39m):\n\u001b[0;32m---> 27\u001b[0m     _, train_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_triplet_loss_smote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_tripletloss_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(train_losses))))\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:201\u001b[0m, in \u001b[0;36mtrain_triplet_loss_smote\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn_args)\u001b[0m\n\u001b[1;32m    199\u001b[0m neg_embeds \u001b[38;5;241m=\u001b[39m network(neg_data)\n\u001b[1;32m    200\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(anchor_embeds, pos_embeds, neg_embeds)\n\u001b[0;32m--> 201\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "# 2 class triplet loss with capped SMOTE \n",
    "\n",
    "# note: sometimes can get a very high accuracy but may diverge\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(5e-6, 1e-3), (1e-6, 5e-4), (1e-6, 1e-4), (5e-6, 5e-4)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = 5.0\n",
    "loss_fn_args['distance'] = 'euclidean'\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10): \n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(complete_network.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(15):\n",
    "            _, train_losses = train.train_triplet_loss_smote(epoch, train_loader_tripletloss_smote, embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_reduced, complete_network, linear_optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"triplet_loss\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], 0.0, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f34cc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0007769797742366791, AUC: 0.581019\n",
      "\n",
      "Train loss: 16.096790712090986\n",
      "Train loss: 7.95101783849016\n",
      "Train loss: 1.6742434924161886\n",
      "Train loss: 0.0\n",
      "Train loss: 6.369154097158698\n",
      "Train loss: 1.0685905746266813\n",
      "Train loss: 0.9919730077815961\n",
      "Train loss: 4.3510085480122624\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 1.729099128819719\n",
      "Train loss: 0.0\n",
      "Train loss: 2.9074361294130737\n",
      "Train loss: 0.0\n",
      "\n",
      "Test set: Avg. loss: 0.0007081505060195923, AUC: 0.595766\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918745040893554, AUC: 0.599685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006822091042995453, AUC: 0.606827\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006758022308349609, AUC: 0.614831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006717154681682586, AUC: 0.620797\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006670982539653778, AUC: 0.6299465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006633270978927612, AUC: 0.641555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006615970730781555, AUC: 0.647514\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006594435572624207, AUC: 0.6563729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006575994789600372, AUC: 0.666123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006559013724327088, AUC: 0.6735810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006542154252529145, AUC: 0.6845415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006518358290195465, AUC: 0.698683\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006498202979564666, AUC: 0.7103269999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006485094726085662, AUC: 0.717825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006470210254192352, AUC: 0.7272419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000646136075258255, AUC: 0.7334495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000645109623670578, AUC: 0.7394789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006443145573139191, AUC: 0.7454485000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000643904983997345, AUC: 0.746941\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006429647505283355, AUC: 0.7538564999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000642709881067276, AUC: 0.7577389999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006426226794719696, AUC: 0.760882\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006425316333770752, AUC: 0.764992\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006421145796775818, AUC: 0.768756\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006414653062820435, AUC: 0.773288\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006413009464740754, AUC: 0.774419\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006411257684230805, AUC: 0.7756689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000640982300043106, AUC: 0.7791325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006405025720596314, AUC: 0.780958\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006400455534458161, AUC: 0.7839640000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006398386657238006, AUC: 0.7868439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006397152841091156, AUC: 0.7885960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006394894123077393, AUC: 0.791209\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006395555436611175, AUC: 0.7915060000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006393527686595916, AUC: 0.7925110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006391496956348419, AUC: 0.794049\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006389279961585999, AUC: 0.795107\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006387557983398437, AUC: 0.7956540000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000638495922088623, AUC: 0.79725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006383010745048523, AUC: 0.797925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006380480825901032, AUC: 0.7989930000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006377894580364227, AUC: 0.8008280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006376721858978271, AUC: 0.8018989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006373582482337952, AUC: 0.8024609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006369325518608093, AUC: 0.803453\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006367611885070801, AUC: 0.8036590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006366349756717682, AUC: 0.8035285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006367577910423278, AUC: 0.8037650000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006367309987545014, AUC: 0.8036895000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023003580570220947, AUC: 0.5429729999999999\n",
      "\n",
      "Train loss: 136.23961547658413\n",
      "Train loss: 17.986870584608635\n",
      "Train loss: 15.341035212142557\n",
      "Train loss: 4.752364243133159\n",
      "Train loss: 4.356625907028778\n",
      "Train loss: 0.7004882715925386\n",
      "Train loss: 1.1259002685546875\n",
      "Train loss: 6.454442579534989\n",
      "Train loss: 4.747905163825313\n",
      "Train loss: 0.3334539147871959\n",
      "Train loss: 3.8129760645612887\n",
      "Train loss: 0.0\n",
      "Train loss: 3.7372857588755934\n",
      "Train loss: 3.130930067617682\n",
      "Train loss: 3.3125161279605915\n",
      "\n",
      "Test set: Avg. loss: 0.0007151560485363006, AUC: 0.45533100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007060944736003875, AUC: 0.4688575000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007020347714424133, AUC: 0.4808285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007004459500312805, AUC: 0.4868415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006991749107837677, AUC: 0.49186949999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006983735859394073, AUC: 0.49412800000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006977417469024659, AUC: 0.5011685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006969517469406128, AUC: 0.511384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006968002021312714, AUC: 0.5048075000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006965127289295197, AUC: 0.5116275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006962951421737671, AUC: 0.514332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006961811780929565, AUC: 0.5129704999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006960669755935669, AUC: 0.505204\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695921242237091, AUC: 0.5012044999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006958386301994324, AUC: 0.4976050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006957364678382874, AUC: 0.49709449999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006956190168857574, AUC: 0.500152\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006955111622810364, AUC: 0.493364\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006954317688941955, AUC: 0.48901300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006953255832195282, AUC: 0.48931600000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952438652515411, AUC: 0.4857020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951936483383179, AUC: 0.48956999999999984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951353847980499, AUC: 0.495209\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695084810256958, AUC: 0.49530450000000015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006950275301933289, AUC: 0.49494200000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006949717700481415, AUC: 0.49411399999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006949259042739868, AUC: 0.49331900000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948856711387634, AUC: 0.493639\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948323547840119, AUC: 0.49492200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947891116142273, AUC: 0.498717\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00069475057721138, AUC: 0.503362\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947134435176849, AUC: 0.505495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946909725666046, AUC: 0.508237\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946579813957214, AUC: 0.504954\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946228444576263, AUC: 0.5067439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945843696594238, AUC: 0.5100250000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945613324642182, AUC: 0.506382\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000694526344537735, AUC: 0.5097929999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944917440414429, AUC: 0.507986\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944626867771149, AUC: 0.506527\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944490969181061, AUC: 0.506454\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000694442331790924, AUC: 0.5044559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944300532341003, AUC: 0.502201\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944151818752288, AUC: 0.5003179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943970620632171, AUC: 0.49821999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943894028663635, AUC: 0.4986275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943775415420532, AUC: 0.49697849999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943643987178803, AUC: 0.500112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943606734275818, AUC: 0.50076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943549513816834, AUC: 0.49980600000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001080885648727417, AUC: 0.41414299999999993\n",
      "\n",
      "Train loss: 24.389437904840783\n",
      "Train loss: 0.8292029658450356\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 1.7853070995475673\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 3.6756908561609967\n",
      "Train loss: 0.0\n",
      "\n",
      "Test set: Avg. loss: 0.0007218130528926849, AUC: 0.518904\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006989753544330597, AUC: 0.540902\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946681439876556, AUC: 0.5388165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931599974632263, AUC: 0.5372144999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918653249740601, AUC: 0.5308125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006907552778720856, AUC: 0.5248035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006905039250850678, AUC: 0.5236075\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006901566684246063, AUC: 0.5230110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006899235546588897, AUC: 0.5261325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006896623373031616, AUC: 0.5253125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894396543502808, AUC: 0.5309490000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006897644698619842, AUC: 0.52842\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006899001598358154, AUC: 0.525083\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006898137331008911, AUC: 0.5242764999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006899007260799408, AUC: 0.5234755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006897071897983551, AUC: 0.5284414999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894522905349732, AUC: 0.5316019999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894703507423401, AUC: 0.5321385000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006892255246639252, AUC: 0.5359525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006892721951007843, AUC: 0.5328990000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006891983449459076, AUC: 0.537638\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006890941560268402, AUC: 0.5369265000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006891008913516998, AUC: 0.53901\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893084347248077, AUC: 0.5359465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893291175365448, AUC: 0.5363020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893750429153442, AUC: 0.535502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000689245194196701, AUC: 0.536058\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006888734996318817, AUC: 0.5422319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006889899671077729, AUC: 0.538168\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006888663470745087, AUC: 0.5418495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006884230673313141, AUC: 0.5464029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006882088780403137, AUC: 0.550612\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006883909702301025, AUC: 0.546473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006878928244113922, AUC: 0.5528475000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006881430745124817, AUC: 0.5479885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000688222199678421, AUC: 0.546259\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006877389848232269, AUC: 0.5549315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006872588694095611, AUC: 0.5582575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870459020137787, AUC: 0.5611119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006872363984584809, AUC: 0.5597035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870978772640228, AUC: 0.5620409999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006871461272239685, AUC: 0.561431\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870918273925781, AUC: 0.561923\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006871341466903687, AUC: 0.5619075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000687317818403244, AUC: 0.551667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870152950286865, AUC: 0.5539374999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006868258118629455, AUC: 0.5586725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006866075992584228, AUC: 0.560983\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006869733929634094, AUC: 0.5588695000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006868163049221039, AUC: 0.5607644999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 class triplet loss with ratio \n",
    "# no smote \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(1e-7, 1e-5)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(3): \n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(complete_network.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(15):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss_ratio, embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "        for epoch in range(50):\n",
    "            _, _ = train.train_linear_probe(epoch, train_loader_reduced, complete_network, linear_optimizer, verbose=False)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"triplet_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], \n",
    "           auc_mean[i][4], auc_variance[i][4],\n",
    "           auc_mean[i][5], auc_variance[i][5],\n",
    "           None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10048fce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "14 columns passed, passed data had 18 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:934\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 934\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_or_indexify_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:981\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    982\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    983\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    984\u001b[0m     )\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 14 columns passed, passed data had 18 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/convnet_aucs.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m df2 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcol_names\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df1, df2])\n\u001b[1;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/convnet_aucs.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:781\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    780\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 781\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    783\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    789\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    790\u001b[0m         arrays,\n\u001b[1;32m    791\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    794\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    795\u001b[0m     )\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:498\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 498\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:840\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    837\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    838\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 840\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:937\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    934\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[0;32m--> 937\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[1;32m    940\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: 14 columns passed, passed data had 18 columns"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de718f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "041af005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006910498738288879, AUC: 0.616086\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007207183837890625, AUC: 0.832752\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000947897344827652, AUC: 0.865763\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000987816333770752, AUC: 0.872393\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006940085291862488, AUC: 0.5015175000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007139445841312409, AUC: 0.83234\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000874194860458374, AUC: 0.866993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001173335611820221, AUC: 0.856846\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006919935345649719, AUC: 0.6351335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007935252189636231, AUC: 0.8241814999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012041029930114746, AUC: 0.8379819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011449605822563172, AUC: 0.8504\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006936784386634827, AUC: 0.5426825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007189079523086548, AUC: 0.841718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009975289702415465, AUC: 0.854266\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008743861317634582, AUC: 0.876096\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006892592906951905, AUC: 0.6796914999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007791438400745392, AUC: 0.822926\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009577655792236328, AUC: 0.8439690000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006392468810081482, AUC: 0.8397245\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006995988488197327, AUC: 0.298342\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006817654967308045, AUC: 0.821243\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008468112051486969, AUC: 0.8586940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010628915429115296, AUC: 0.861876\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006901213526725769, AUC: 0.618081\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007482103705406189, AUC: 0.8229569999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008806695938110351, AUC: 0.8501390000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009523346722126007, AUC: 0.8514910000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006949737370014191, AUC: 0.47735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007385897636413574, AUC: 0.831748\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000977450281381607, AUC: 0.864357\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012061765193939209, AUC: 0.8679840000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006908837556838989, AUC: 0.56853\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007839757204055786, AUC: 0.823591\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000968728631734848, AUC: 0.8550849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013508934378623962, AUC: 0.8454849999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006951073706150055, AUC: 0.4753805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008466125726699829, AUC: 0.819769\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009607299268245697, AUC: 0.849471\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010104178488254548, AUC: 0.85276\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006896412074565888, AUC: 0.6992855000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000653426468372345, AUC: 0.748837\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006123585999011994, AUC: 0.757056\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006542311012744903, AUC: 0.7757289999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006934636533260345, AUC: 0.4778895000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006704036593437195, AUC: 0.7816799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006365029215812683, AUC: 0.7708070000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006107737421989441, AUC: 0.7615860000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006904133856296539, AUC: 0.617076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006338779628276825, AUC: 0.738035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006527813076972961, AUC: 0.779309\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006777863502502442, AUC: 0.7926740000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000693917840719223, AUC: 0.473646\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000660376638174057, AUC: 0.7121365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006292278468608857, AUC: 0.742271\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006548523604869843, AUC: 0.768243\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006952302753925324, AUC: 0.3976435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006171909272670746, AUC: 0.7735144999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005779665112495422, AUC: 0.7829489999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005622219145298004, AUC: 0.810346\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006928829550743104, AUC: 0.560443\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006379791796207428, AUC: 0.735864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000623079001903534, AUC: 0.770255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006333324909210205, AUC: 0.805311\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006952957510948181, AUC: 0.404599\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006268816292285919, AUC: 0.7446729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006408754289150238, AUC: 0.787983\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006428678631782531, AUC: 0.8137810000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006911108493804931, AUC: 0.6018955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006251320838928223, AUC: 0.7610389999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006433751881122589, AUC: 0.791401\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006798151135444642, AUC: 0.8039784999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006922091841697693, AUC: 0.593274\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006042839288711547, AUC: 0.771814\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006405251622200012, AUC: 0.7865740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000660250186920166, AUC: 0.805793\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006939904093742371, AUC: 0.468337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006171875894069672, AUC: 0.7386130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006442972719669342, AUC: 0.780391\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006432374119758606, AUC: 0.809107\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006958833634853363, AUC: 0.3942525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010030234158039093, AUC: 0.841799\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010634756088256836, AUC: 0.860072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087903916835785, AUC: 0.8761540000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006890489757061005, AUC: 0.6505865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001307406187057495, AUC: 0.8250130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009768723249435425, AUC: 0.8553999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00077918741106987, AUC: 0.8570869999999998\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006897425651550292, AUC: 0.6602600000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000989554613828659, AUC: 0.8301284999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012175801992416382, AUC: 0.8751460000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012226909399032592, AUC: 0.8760739999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006891187131404877, AUC: 0.6889075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009736456274986267, AUC: 0.847331\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145780086517334, AUC: 0.8599190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014009369611740112, AUC: 0.8435935\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006868513226509094, AUC: 0.6442665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008077155947685241, AUC: 0.850448\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013922181725502013, AUC: 0.839272\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007639396190643311, AUC: 0.780933\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006907799541950226, AUC: 0.5921905000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008817166090011597, AUC: 0.8511010000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011775738596916199, AUC: 0.86387\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009674123227596283, AUC: 0.881146\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.00069780433177948, AUC: 0.3219115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000997807115316391, AUC: 0.8197989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012135251760482788, AUC: 0.850824\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001227737009525299, AUC: 0.8640049999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006900607645511628, AUC: 0.586266\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001126256823539734, AUC: 0.845559\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001341320037841797, AUC: 0.8553310000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013008238077163697, AUC: 0.863486\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006944347620010376, AUC: 0.543937\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012643110752105712, AUC: 0.8121779999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00111706280708313, AUC: 0.857944\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012020502090454102, AUC: 0.8523690000000002\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006912509799003601, AUC: 0.5795595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010476632118225097, AUC: 0.824136\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010052161514759064, AUC: 0.866595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011390247344970704, AUC: 0.867624\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006910772621631623, AUC: 0.5907955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007230232954025268, AUC: 0.8336870000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009344089329242707, AUC: 0.846666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009925714135169982, AUC: 0.864591\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006974345147609711, AUC: 0.343945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006383246183395385, AUC: 0.8408779999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008807825148105621, AUC: 0.862138\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009696088433265686, AUC: 0.8683449999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006918532848358154, AUC: 0.5718395000000001\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0007955302298069, AUC: 0.821545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008325413167476654, AUC: 0.8549479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000945974737405777, AUC: 0.860855\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006943807899951935, AUC: 0.45884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007042157351970673, AUC: 0.823237\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010037005841732026, AUC: 0.8432679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001122573137283325, AUC: 0.8510929999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006971780955791474, AUC: 0.4324415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008017181158065796, AUC: 0.8170700000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011431435942649841, AUC: 0.8431079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011760561466217041, AUC: 0.8444419999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.000689770132303238, AUC: 0.664884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008555488288402558, AUC: 0.8221830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011109410524368286, AUC: 0.830791\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011780904531478883, AUC: 0.85151\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006930331885814666, AUC: 0.512229\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007910500466823578, AUC: 0.8102750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001182625710964203, AUC: 0.8152240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001377645492553711, AUC: 0.8289019999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006983460485935211, AUC: 0.41495250000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006540019810199738, AUC: 0.8364845000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007671346068382263, AUC: 0.8702159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010498945415019988, AUC: 0.8620790000000002\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006883287131786346, AUC: 0.619265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006750421226024627, AUC: 0.843852\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000974831610918045, AUC: 0.851053\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006665010154247283, AUC: 0.797388\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006911867558956146, AUC: 0.6087604999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007447440028190613, AUC: 0.846307\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008746879696846008, AUC: 0.8788595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009635629951953888, AUC: 0.862806\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006994609534740448, AUC: 0.40575300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006221111714839935, AUC: 0.765823\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006088358461856842, AUC: 0.7891429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000688678115606308, AUC: 0.808627\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006847473084926605, AUC: 0.6958875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005859143435955048, AUC: 0.772304\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006295728683471679, AUC: 0.784506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006763483881950379, AUC: 0.802004\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006882085204124451, AUC: 0.690016\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00062450310587883, AUC: 0.7758369999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005862591564655304, AUC: 0.7873049999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006046504378318787, AUC: 0.8156930000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006861849129199981, AUC: 0.724416\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005884796380996704, AUC: 0.7697799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006449683308601379, AUC: 0.7824730000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000680989921092987, AUC: 0.795188\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006866821646690369, AUC: 0.6922385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005866250097751617, AUC: 0.7775730000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006253832876682282, AUC: 0.7940070000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006726661622524262, AUC: 0.8128149999999998\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0007026048600673676, AUC: 0.294347\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005987475514411926, AUC: 0.762648\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006338472068309784, AUC: 0.7864419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006526747643947602, AUC: 0.8073714999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006935766935348511, AUC: 0.47192850000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005993907451629639, AUC: 0.763018\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006392341554164886, AUC: 0.7839240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006605783104896546, AUC: 0.8012\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006888934969902039, AUC: 0.645678\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005944704413414001, AUC: 0.7782695000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006071470081806183, AUC: 0.788897\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006248910427093506, AUC: 0.8100765\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006950117945671081, AUC: 0.5001975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005853878557682037, AUC: 0.7849440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006056124269962311, AUC: 0.814585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988736689090729, AUC: 0.8146450000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006886630356311798, AUC: 0.6701675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006175990998744965, AUC: 0.747234\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006523481607437133, AUC: 0.7853610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006481782197952271, AUC: 0.8215109999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006969042122364044, AUC: 0.43839449999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007808294594287873, AUC: 0.8517539999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009704014956951142, AUC: 0.866062\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012301408052444458, AUC: 0.8562569999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006909841895103454, AUC: 0.6119459999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000898714303970337, AUC: 0.8428109999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013289505243301392, AUC: 0.834411\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013072332739830017, AUC: 0.862791\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006901565492153168, AUC: 0.58633\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007984386384487152, AUC: 0.846918\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007340492904186249, AUC: 0.8671599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014116488099098206, AUC: 0.8641530000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006939886510372162, AUC: 0.5981464999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008006298542022705, AUC: 0.8770120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010236947536468506, AUC: 0.870592\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016239920258522033, AUC: 0.8418890000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006916697919368744, AUC: 0.5669725000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008830125331878662, AUC: 0.854479\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012331462502479552, AUC: 0.8604154999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001279185652732849, AUC: 0.87662\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006853259801864624, AUC: 0.7408770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009251608550548553, AUC: 0.860464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010159789323806762, AUC: 0.877815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00131690913438797, AUC: 0.864201\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0007031798958778381, AUC: 0.331358\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009263357222080231, AUC: 0.8631039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012688028216362, AUC: 0.8624249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006690696775913239, AUC: 0.765047\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006951743066310883, AUC: 0.404082\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007166216373443603, AUC: 0.84267\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012340849637985229, AUC: 0.8525100000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008956596553325653, AUC: 0.884409\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.000692009150981903, AUC: 0.645405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000899756669998169, AUC: 0.8586159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012483698725700377, AUC: 0.856895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011511145830154419, AUC: 0.872717\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006921719312667847, AUC: 0.5720585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010166949033737183, AUC: 0.8461209999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001135564684867859, AUC: 0.847106\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011074504852294923, AUC: 0.8520930000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006952035427093506, AUC: 0.46676549999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009449301362037659, AUC: 0.7884340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009611445963382721, AUC: 0.8419909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010328314900398255, AUC: 0.852017\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006870959997177125, AUC: 0.721527\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933919191360473, AUC: 0.82532\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009421987533569336, AUC: 0.839611\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013948305249214172, AUC: 0.8322229999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006963384449481965, AUC: 0.38264100000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006147408485412597, AUC: 0.8376509999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009407485127449035, AUC: 0.8533900000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015488449335098267, AUC: 0.830544\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006945516467094421, AUC: 0.44271649999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007115994393825531, AUC: 0.838678\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007588164210319519, AUC: 0.86482\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010575720071792603, AUC: 0.847473\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0007015173435211182, AUC: 0.2960235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008149043023586274, AUC: 0.8215784999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009621008634567261, AUC: 0.8420889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001291257917881012, AUC: 0.838169\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006914902627468109, AUC: 0.5626935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894059777259827, AUC: 0.82719\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007271471023559571, AUC: 0.86903\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000951990693807602, AUC: 0.8576270000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000693727970123291, AUC: 0.519144\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006186171770095826, AUC: 0.8489500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010045465230941772, AUC: 0.8496060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010542113780975342, AUC: 0.8580730000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006983237862586975, AUC: 0.305317\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007065722346305847, AUC: 0.8331059999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006564630568027496, AUC: 0.867629\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001091837763786316, AUC: 0.848458\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006952480375766754, AUC: 0.43551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000639549195766449, AUC: 0.8447629999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009017195105552674, AUC: 0.856239\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008066090941429139, AUC: 0.8835649999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006867781579494477, AUC: 0.6648525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000856195867061615, AUC: 0.8239000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008963901698589325, AUC: 0.853935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001068742334842682, AUC: 0.8582560000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000690276950597763, AUC: 0.645324\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005964024662971497, AUC: 0.757014\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006015383899211883, AUC: 0.7960529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006246768534183502, AUC: 0.8249219999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006964195966720581, AUC: 0.43680349999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006243427395820618, AUC: 0.7663089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005850922763347625, AUC: 0.7805069999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006391020417213439, AUC: 0.808804\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006911260485649109, AUC: 0.626563\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005906570851802826, AUC: 0.769106\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006242986619472504, AUC: 0.7930980000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006722333431243896, AUC: 0.803169\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006961284875869751, AUC: 0.458764\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006045307815074921, AUC: 0.7535339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006210446655750275, AUC: 0.7877329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006421128809452057, AUC: 0.803418\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006937628388404846, AUC: 0.48386049999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006230002343654633, AUC: 0.7706850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005857810378074646, AUC: 0.783764\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000631675660610199, AUC: 0.80701\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006877063810825348, AUC: 0.7066534999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005937637686729431, AUC: 0.774143\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000626932442188263, AUC: 0.800898\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006429665088653565, AUC: 0.8116749999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000698445349931717, AUC: 0.37047850000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006124925911426544, AUC: 0.754975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000612223207950592, AUC: 0.7881020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006545252203941346, AUC: 0.80801\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006954845488071442, AUC: 0.451551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006025398969650268, AUC: 0.75983\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006489477455615997, AUC: 0.782936\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006728507280349731, AUC: 0.794284\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006972964704036712, AUC: 0.3498825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006090272963047027, AUC: 0.744376\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006339028477668762, AUC: 0.7646840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006539447605609893, AUC: 0.7911630000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006937877833843231, AUC: 0.476089\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005802241265773773, AUC: 0.7756670000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00061830073595047, AUC: 0.789547\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006414312720298767, AUC: 0.803375\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006992079019546509, AUC: 0.3216085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000974680095911026, AUC: 0.8396560000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012805145382881165, AUC: 0.864967\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019251338243484496, AUC: 0.849728\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006924118399620056, AUC: 0.5419670000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006719506978988647, AUC: 0.8794839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008100633919239044, AUC: 0.8823050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011670985817909241, AUC: 0.8774809999999998\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006922790110111236, AUC: 0.575252\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008964536786079406, AUC: 0.8662559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010962303280830383, AUC: 0.866342\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011286607384681702, AUC: 0.876034\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006910220086574554, AUC: 0.5805339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000736018568277359, AUC: 0.8188399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015612427592277527, AUC: 0.823455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001128779947757721, AUC: 0.8759729999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0007067374587059021, AUC: 0.30995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006396883726119995, AUC: 0.864826\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012463725805282593, AUC: 0.84067\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015206063985824586, AUC: 0.8386899999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006988278925418854, AUC: 0.35999850000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007679277956485749, AUC: 0.863103\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008974720239639282, AUC: 0.846678\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012884286642074584, AUC: 0.8605009999999997\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000692645937204361, AUC: 0.561404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010343928337097168, AUC: 0.8383590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009694491624832153, AUC: 0.865714\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001324811637401581, AUC: 0.862055\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006948121488094329, AUC: 0.41014\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008284163475036621, AUC: 0.8439479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001125046968460083, AUC: 0.8655590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008775015175342559, AUC: 0.8835139999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006928246319293976, AUC: 0.528832\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009751957654953003, AUC: 0.831459\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001141067624092102, AUC: 0.853624\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013789694905281067, AUC: 0.845602\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006983065009117126, AUC: 0.360371\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106187641620636, AUC: 0.835091\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012018346190452575, AUC: 0.85331\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012700953483581543, AUC: 0.8716679999999999\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m cap \u001b[38;5;241m=\u001b[39m loss_caps[c]\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(learning_rates)): \n\u001b[0;32m---> 57\u001b[0m     row \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtriplet_loss_capped_smote\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m, nums, ratio, \u001b[43mlearning_rates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     58\u001b[0m             auc_mean[i][\u001b[38;5;241m0\u001b[39m], auc_variance[i][\u001b[38;5;241m0\u001b[39m], \n\u001b[1;32m     59\u001b[0m             auc_mean[i][\u001b[38;5;241m1\u001b[39m], auc_variance[i][\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     60\u001b[0m             auc_mean[i][\u001b[38;5;241m2\u001b[39m], auc_variance[i][\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m     61\u001b[0m             auc_mean[i][\u001b[38;5;241m3\u001b[39m], auc_variance[i][\u001b[38;5;241m3\u001b[39m], cap, norm]\n\u001b[1;32m     62\u001b[0m     rows\u001b[38;5;241m.\u001b[39mappend(row)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# capped smote using triplet loss\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-3, 1e-3, 1e-2]\n",
    "\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "loss_caps = [1, 5, 10]\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10): \n",
    "            print(loss_cap)\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(2)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                _, _ = train.train_triplet_capped_loss(epoch, train_loader_tripletloss_smote, network, optimizer, verbose=False, cap_calc=loss_fns.TripletLoss,loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "    \n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "        \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"triplet_loss_capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "29f1f9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c6b13234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006964887678623199, AUC: 0.3482715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00073517307639122, AUC: 0.841398\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010314007997512817, AUC: 0.826527\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009717654287815094, AUC: 0.8537300000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006961984634399415, AUC: 0.47539049999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007382735610008239, AUC: 0.825611\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009567088186740875, AUC: 0.831588\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015623956918716432, AUC: 0.8068925\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006931769251823425, AUC: 0.509471\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006451610028743743, AUC: 0.8464630000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012270017266273498, AUC: 0.8370259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013078323006629944, AUC: 0.864949\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006944571435451508, AUC: 0.445826\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008669923841953278, AUC: 0.8259850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009103761315345764, AUC: 0.8520369999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009062850177288056, AUC: 0.8787030000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006883836388587952, AUC: 0.6776545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007697112262248993, AUC: 0.839264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011312421560287476, AUC: 0.836353\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015704461336135864, AUC: 0.812537\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000699282020330429, AUC: 0.30072699999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007561590373516083, AUC: 0.8454740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006398472785949707, AUC: 0.782464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001031688392162323, AUC: 0.865548\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006867203414440155, AUC: 0.7741625000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007571307122707367, AUC: 0.8070809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008658646643161774, AUC: 0.851773\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001038480818271637, AUC: 0.8632489999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006985587179660798, AUC: 0.4241545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007187469601631165, AUC: 0.8148850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008026745319366455, AUC: 0.847086\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012267091274261474, AUC: 0.839636\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006906501948833466, AUC: 0.6274955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006881701350212097, AUC: 0.8297220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000837628573179245, AUC: 0.8645340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010912498831748961, AUC: 0.8570349999999998\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006931628286838532, AUC: 0.517671\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000721651703119278, AUC: 0.84447\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012107672691345215, AUC: 0.854232\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001070845365524292, AUC: 0.8673435\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006956841349601746, AUC: 0.4125635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00058995121717453, AUC: 0.7727959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006098356246948242, AUC: 0.80277\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006457704603672027, AUC: 0.810598\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006939020454883576, AUC: 0.519663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006275418400764466, AUC: 0.793617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005780983865261078, AUC: 0.807573\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005679846704006195, AUC: 0.817343\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006914187371730805, AUC: 0.576268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006320301592350006, AUC: 0.7865439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005776986479759216, AUC: 0.799199\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006131931841373443, AUC: 0.8158160000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0007014695405960083, AUC: 0.2262525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006065096855163574, AUC: 0.7679549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006076382398605346, AUC: 0.782258\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000623035877943039, AUC: 0.7969729999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0007048844397068023, AUC: 0.2643715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006356420814990997, AUC: 0.7727170000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00057607901096344, AUC: 0.7863129999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005922649502754212, AUC: 0.805336\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006923337280750275, AUC: 0.5283145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00060067218542099, AUC: 0.7647740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006089491248130798, AUC: 0.7924229999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006275667548179627, AUC: 0.8096549999999998\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0007001697123050689, AUC: 0.291281\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005993172526359558, AUC: 0.768286\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005809304416179657, AUC: 0.796477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005988768637180329, AUC: 0.8078699999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006949147582054139, AUC: 0.418945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006285334825515747, AUC: 0.768832\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005816346108913422, AUC: 0.794821\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006180701851844787, AUC: 0.807817\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006933906674385071, AUC: 0.5035585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005996799767017364, AUC: 0.8007094999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005933422446250915, AUC: 0.804215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006146812736988067, AUC: 0.8228629999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006925413310527801, AUC: 0.5801085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005885217487812042, AUC: 0.7805460000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005701355338096619, AUC: 0.8124170000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005963357090950012, AUC: 0.8258099999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006931113302707672, AUC: 0.5052620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006038292646408081, AUC: 0.8442369999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009694813191890717, AUC: 0.871629\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016370036602020265, AUC: 0.834625\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000694307565689087, AUC: 0.5696005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012186483740806579, AUC: 0.832325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00117341548204422, AUC: 0.863417\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014034479260444641, AUC: 0.851683\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006916224658489227, AUC: 0.5674680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010546446442604065, AUC: 0.846184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010310733914375306, AUC: 0.8756970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012629610896110535, AUC: 0.8824119999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000692455530166626, AUC: 0.542134\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010450857281684876, AUC: 0.844635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011799923181533814, AUC: 0.8730639999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001465002477169037, AUC: 0.872712\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000696535885334015, AUC: 0.43334100000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011155704259872437, AUC: 0.830578\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008758077919483185, AUC: 0.85151\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014336732625961303, AUC: 0.872911\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006861005127429962, AUC: 0.753309\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008633033633232116, AUC: 0.867355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012579433917999268, AUC: 0.8600350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011751952171325684, AUC: 0.863558\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006959941685199737, AUC: 0.4150395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009759220480918884, AUC: 0.849207\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010458824634552001, AUC: 0.8547909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008734932541847229, AUC: 0.774424\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006905612051486969, AUC: 0.618195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010072756111621857, AUC: 0.8289580000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012035175561904907, AUC: 0.8549620000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011508205533027648, AUC: 0.8504689999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006850793659687042, AUC: 0.7759304999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107933908700943, AUC: 0.8351950000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012307174801826476, AUC: 0.8374840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001549645185470581, AUC: 0.8352599999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006924456655979156, AUC: 0.5842160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006634254157543183, AUC: 0.874015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000829733669757843, AUC: 0.874533\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013509661555290223, AUC: 0.861971\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006873879432678223, AUC: 0.668617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007288475334644317, AUC: 0.83084\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000798613578081131, AUC: 0.8603899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010380207896232604, AUC: 0.861265\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006963967382907867, AUC: 0.34070750000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007012554407119751, AUC: 0.813416\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008462599813938141, AUC: 0.8528180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006368292272090912, AUC: 0.868417\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006908078789710999, AUC: 0.6734819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006440731883049011, AUC: 0.8425530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001008029878139496, AUC: 0.8353280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021165251731873, AUC: 0.8578000000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006938815414905548, AUC: 0.4701365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006398897767066955, AUC: 0.823039\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009389999508857727, AUC: 0.8418840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012309846878051757, AUC: 0.839351\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006928956806659699, AUC: 0.5152815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006695924997329711, AUC: 0.836996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000713811069726944, AUC: 0.869234\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009120012223720551, AUC: 0.884378\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006952646076679229, AUC: 0.454125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007137651145458221, AUC: 0.819804\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005585785508155823, AUC: 0.794235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010339073836803437, AUC: 0.8582479999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006922257542610168, AUC: 0.593245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006762655973434448, AUC: 0.840071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009531191289424896, AUC: 0.8400770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009395413696765899, AUC: 0.8632420000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006843304336071014, AUC: 0.754032\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007121754586696625, AUC: 0.8400759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007208648920059204, AUC: 0.879004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008298051655292511, AUC: 0.8793030000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006933661103248597, AUC: 0.539899\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007351889312267304, AUC: 0.838316\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008324334919452667, AUC: 0.855024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009299099445343017, AUC: 0.856663\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0007029947936534882, AUC: 0.2404185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007611706256866455, AUC: 0.838189\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000869436115026474, AUC: 0.852241\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012457705140113831, AUC: 0.848675\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006937456727027893, AUC: 0.48097750000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006296309232711792, AUC: 0.8038609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005941517353057861, AUC: 0.827549\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005852521359920502, AUC: 0.8455324999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006981613039970398, AUC: 0.35741700000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006273975074291229, AUC: 0.731024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006015610098838806, AUC: 0.759389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006258268058300018, AUC: 0.7851140000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006868114173412323, AUC: 0.6472709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006055816113948822, AUC: 0.7690710000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006437261402606965, AUC: 0.7876675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006609744429588318, AUC: 0.796758\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006866777837276458, AUC: 0.678727\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000609272301197052, AUC: 0.7902450000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005601440370082855, AUC: 0.8025740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005625659823417663, AUC: 0.8274110000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.000690476804971695, AUC: 0.639091\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006030465364456176, AUC: 0.76774\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005807923972606659, AUC: 0.8022050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006262666881084442, AUC: 0.8161539999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0007070432305335999, AUC: 0.2567175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006206088662147522, AUC: 0.770519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005693963170051575, AUC: 0.794623\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005815098285675049, AUC: 0.8173400000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006920393109321594, AUC: 0.585516\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006010463535785675, AUC: 0.7736589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005691076517105102, AUC: 0.7999640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005890714228153228, AUC: 0.8220719999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006970245838165283, AUC: 0.39544100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006008437871932984, AUC: 0.7631329999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005794459283351898, AUC: 0.790374\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005711245238780975, AUC: 0.8116605\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006910800039768219, AUC: 0.584476\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005670577585697174, AUC: 0.800147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005715482532978058, AUC: 0.82222\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006515784859657288, AUC: 0.8217369999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006903544068336487, AUC: 0.6095495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006156922280788422, AUC: 0.7991219999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000538288563489914, AUC: 0.8264360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005560230016708374, AUC: 0.842511\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006966787576675415, AUC: 0.3983555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007222783267498016, AUC: 0.869363\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010071758329868318, AUC: 0.8470485000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012444838881492614, AUC: 0.867406\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006997887194156646, AUC: 0.35265399999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009627985954284668, AUC: 0.836147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008971806466579437, AUC: 0.853533\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018746265172958374, AUC: 0.829051\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006894789934158325, AUC: 0.6815665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009019197821617126, AUC: 0.855526\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008880127966403961, AUC: 0.875398\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011336479783058166, AUC: 0.887485\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006959426403045654, AUC: 0.37305950000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008238074481487274, AUC: 0.8587229999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009622399508953095, AUC: 0.867203\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014156073331832886, AUC: 0.8529399999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006926114559173584, AUC: 0.5274565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008006506264209747, AUC: 0.837974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010012348294258118, AUC: 0.8614569999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001378297448158264, AUC: 0.868375\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0007036321759223938, AUC: 0.276433\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006420859694480897, AUC: 0.8778259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007337818443775177, AUC: 0.8765740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013255744576454163, AUC: 0.847181\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006944135427474975, AUC: 0.474867\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009478649497032165, AUC: 0.805874\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001002079963684082, AUC: 0.872723\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017056990265846253, AUC: 0.824271\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006942294836044311, AUC: 0.467659\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006910409927368164, AUC: 0.8614120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001030386745929718, AUC: 0.8703960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001048705518245697, AUC: 0.8718370000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006894182562828064, AUC: 0.66474\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005513716340065002, AUC: 0.8781209999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007959308028221131, AUC: 0.8807670000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012150654196739197, AUC: 0.872276\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006941581666469574, AUC: 0.4895235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009898362159729004, AUC: 0.8593590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012273207306861878, AUC: 0.8441259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014980888962745666, AUC: 0.8405285\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006946149468421936, AUC: 0.431736\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007163270711898804, AUC: 0.8414879999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001023943543434143, AUC: 0.852204\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000873123973608017, AUC: 0.8733000000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006913607120513916, AUC: 0.5953550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006977505087852478, AUC: 0.830875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009022049903869629, AUC: 0.8551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008309317231178283, AUC: 0.869514\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006998177468776703, AUC: 0.32885600000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006329614818096161, AUC: 0.844735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007288149893283845, AUC: 0.8794120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008934969007968902, AUC: 0.880472\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006871189177036285, AUC: 0.647358\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006863475143909454, AUC: 0.836021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008715020120143891, AUC: 0.861773\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0008677043914794922, AUC: 0.873766\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006948215365409851, AUC: 0.44974300000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006209433376789094, AUC: 0.859944\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011180386543273926, AUC: 0.8403849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008133378028869629, AUC: 0.876189\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006891251504421235, AUC: 0.614881\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007206940054893493, AUC: 0.822441\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000845925122499466, AUC: 0.8559945000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010710387229919433, AUC: 0.867974\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006960428357124329, AUC: 0.439736\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005857323706150055, AUC: 0.841833\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009211321473121643, AUC: 0.846067\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010535757541656494, AUC: 0.8533420000000002\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006958933174610138, AUC: 0.37558400000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007239218056201935, AUC: 0.830268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007279396951198578, AUC: 0.863995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001042657732963562, AUC: 0.8694850000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006966108083724976, AUC: 0.4870975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007084191739559173, AUC: 0.813611\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001110625147819519, AUC: 0.820113\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008412255644798279, AUC: 0.878032\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006922284066677093, AUC: 0.5504545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006489351093769074, AUC: 0.831129\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011578829884529114, AUC: 0.8231729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010449389815330506, AUC: 0.857415\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006993962824344635, AUC: 0.307477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005656020939350128, AUC: 0.8104359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005565700232982635, AUC: 0.817501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005500772297382354, AUC: 0.8309770000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006847118139266968, AUC: 0.717801\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006017836928367614, AUC: 0.755253\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006175267100334168, AUC: 0.784343\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006482227146625518, AUC: 0.799193\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006961579024791718, AUC: 0.41039400000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006349701881408691, AUC: 0.7525470000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005696538984775543, AUC: 0.798821\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005563447773456573, AUC: 0.827493\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0007002742290496826, AUC: 0.36772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005938598811626435, AUC: 0.7970849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005595190823078156, AUC: 0.8116250000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005830189883708954, AUC: 0.823024\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006892826557159424, AUC: 0.685903\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006198147535324096, AUC: 0.7771329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005677554905414581, AUC: 0.791501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005808756649494172, AUC: 0.8144760000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006931638419628144, AUC: 0.506908\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005789222419261932, AUC: 0.8014009999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005544031262397766, AUC: 0.8228660000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005623725652694702, AUC: 0.83096\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006979638040065765, AUC: 0.33687649999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005897727608680726, AUC: 0.8049669999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005656885504722595, AUC: 0.8137570000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005767394602298737, AUC: 0.82622\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000693691611289978, AUC: 0.49163749999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000623263955116272, AUC: 0.729361\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006053521037101745, AUC: 0.777497\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006119861006736756, AUC: 0.8077430000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006916729807853699, AUC: 0.665225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000604123443365097, AUC: 0.7826569999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000572631984949112, AUC: 0.804999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005838558971881867, AUC: 0.8272949999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000702115386724472, AUC: 0.256207\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006290761232376099, AUC: 0.772369\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005777866840362549, AUC: 0.7912410000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000571781724691391, AUC: 0.817149\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000689954161643982, AUC: 0.6224354999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006696927547454834, AUC: 0.8391649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010561627745628358, AUC: 0.8624040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011525577306747436, AUC: 0.8789560000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000687600165605545, AUC: 0.6463235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001127226710319519, AUC: 0.830285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011954184770584107, AUC: 0.8479339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013237202167510986, AUC: 0.8752679999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006991429030895233, AUC: 0.27942599999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008747574388980866, AUC: 0.847294\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010428960919380188, AUC: 0.8798949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001121375322341919, AUC: 0.8715649999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006881274878978729, AUC: 0.7000775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008347879946231842, AUC: 0.8535305000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009024002850055694, AUC: 0.8643719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001316437244415283, AUC: 0.8608529999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006976059079170227, AUC: 0.4164485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000802900105714798, AUC: 0.859806\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010028988122940063, AUC: 0.871806\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001041642963886261, AUC: 0.869907\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006893242597579956, AUC: 0.5960265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007124507427215576, AUC: 0.847674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001251802146434784, AUC: 0.8570409999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013170126080513, AUC: 0.8638170000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006925864517688752, AUC: 0.5448594999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009432995319366455, AUC: 0.8400119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012971652150154114, AUC: 0.8591899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013174834847450257, AUC: 0.8662359999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006914799213409424, AUC: 0.601997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009482887387275696, AUC: 0.8457029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013148759603500365, AUC: 0.8533959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011871318817138672, AUC: 0.869723\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006934074759483337, AUC: 0.49730749999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009023742377758026, AUC: 0.849256\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006607531905174255, AUC: 0.8573689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001261540174484253, AUC: 0.8524685000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000690977931022644, AUC: 0.595788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001017330527305603, AUC: 0.828994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014169846773147584, AUC: 0.815164\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00099807870388031, AUC: 0.8697720000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# capped smote using triplet loss w/ average \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-3, 1e-3, 1e-2]\n",
    "\n",
    "loss_fn_args = {}\n",
    "cap_aucs = []\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "loss_caps = [1, 5, 10]\n",
    "cap_calc = loss_fns.TripletLossWithAverage\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10): \n",
    "            print(loss_cap)\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(2)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                _, _ = train.train_triplet_capped_loss(epoch, train_loader_tripletloss_smote, network, optimizer, verbose=False, cap_calc=cap_calc, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "        \n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "        \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "\n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"triplet_loss_capped_smote_average\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7278387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "97fbf2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006953137814998627, AUC: 0.41351000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012100113034248352, AUC: 0.751849\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001347662389278412, AUC: 0.779907\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013431642651557923, AUC: 0.8068489999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000698688566684723, AUC: 0.35026250000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011922909617424011, AUC: 0.7699495000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001296019196510315, AUC: 0.8160130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014232771992683411, AUC: 0.8320620000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006938562393188477, AUC: 0.47384050000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011157945990562439, AUC: 0.7417819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013826609253883362, AUC: 0.776867\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012448267340660095, AUC: 0.806332\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000687132328748703, AUC: 0.732907\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013641536831855774, AUC: 0.7088535000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014020829200744628, AUC: 0.7531479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014276145100593567, AUC: 0.771426\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0007051105201244354, AUC: 0.322938\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012302497029304505, AUC: 0.7643880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013913211822509765, AUC: 0.8029550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013240280747413636, AUC: 0.8284419999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006952855587005615, AUC: 0.485296\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012617493271827698, AUC: 0.7613989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001308695673942566, AUC: 0.806971\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014748266935348511, AUC: 0.815221\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006956429183483123, AUC: 0.476747\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011212791800498962, AUC: 0.774918\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011230679154396058, AUC: 0.825895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001202311098575592, AUC: 0.838972\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006950486600399017, AUC: 0.48097199999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011566267609596252, AUC: 0.79585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001257402002811432, AUC: 0.818791\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012450679540634156, AUC: 0.844643\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0007013601660728454, AUC: 0.330824\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011272501945495605, AUC: 0.7583740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013851812481880188, AUC: 0.788767\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013489682078361512, AUC: 0.8201449999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0007009153962135315, AUC: 0.24535099999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012266036868095397, AUC: 0.7551239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014567877650260926, AUC: 0.787479\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014886544346809387, AUC: 0.8071490000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006931928098201751, AUC: 0.5002415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014012029767036438, AUC: 0.649479\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013114124536514281, AUC: 0.693584\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012299007773399354, AUC: 0.7121620000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0007101229131221771, AUC: 0.2119795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014262165427207946, AUC: 0.616976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014142146110534667, AUC: 0.6947409999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011929526925086975, AUC: 0.720495\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006943486928939819, AUC: 0.47322600000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001498607873916626, AUC: 0.60515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013810985684394837, AUC: 0.6721889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012916117310523988, AUC: 0.6959690000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006934674978256225, AUC: 0.49733550000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015681596398353576, AUC: 0.549493\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001625352680683136, AUC: 0.65735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001388828694820404, AUC: 0.699155\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006907029747962952, AUC: 0.5882499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014448300004005431, AUC: 0.607761\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013494152426719666, AUC: 0.6913860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011988258361816406, AUC: 0.715993\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006981097161769867, AUC: 0.360881\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014119338393211364, AUC: 0.656449\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013790208697319031, AUC: 0.687104\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013106749653816223, AUC: 0.7021760000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006989958882331848, AUC: 0.4668105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001303554654121399, AUC: 0.682885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012563939094543458, AUC: 0.707837\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012131325006484986, AUC: 0.7160259999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006810896396636963, AUC: 0.7282065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017621628642082215, AUC: 0.254372\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019957852959632872, AUC: 0.526392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016850491166114808, AUC: 0.6527689999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000695532888174057, AUC: 0.4225245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001594462215900421, AUC: 0.567238\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001725798487663269, AUC: 0.651816\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001516685664653778, AUC: 0.686813\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006976980865001679, AUC: 0.3722295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001732854187488556, AUC: 0.531613\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018189577460289, AUC: 0.639831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015425624847412109, AUC: 0.685395\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006946506798267365, AUC: 0.45809099999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010704345107078552, AUC: 0.827189\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012367932200431824, AUC: 0.8512869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011861377358436584, AUC: 0.8723659999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006960716247558594, AUC: 0.40448150000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010685625076293946, AUC: 0.821029\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015051649808883666, AUC: 0.8280540000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012095825672149658, AUC: 0.859088\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006914547681808471, AUC: 0.5743914999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013932805061340331, AUC: 0.806651\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013861716389656066, AUC: 0.843584\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011066436171531678, AUC: 0.8695819999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006953510642051697, AUC: 0.44617850000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001377259373664856, AUC: 0.813183\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011593254208564759, AUC: 0.8457060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015679771900177003, AUC: 0.840825\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006900211274623871, AUC: 0.6015820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014498870372772216, AUC: 0.80487\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016269005537033082, AUC: 0.8309340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001314681053161621, AUC: 0.862953\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000698762983083725, AUC: 0.298201\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010476409792900085, AUC: 0.8333949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015019388198852538, AUC: 0.857112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012248939275741578, AUC: 0.8620990000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006974970698356628, AUC: 0.30638350000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012795483469963073, AUC: 0.8072629999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016676595211029053, AUC: 0.8269390000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014754130244255066, AUC: 0.8483880000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006972302198410034, AUC: 0.3896045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013244966864585875, AUC: 0.810084\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015768024921417236, AUC: 0.822819\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010522279143333436, AUC: 0.865938\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006987023949623108, AUC: 0.3143995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010556061267852784, AUC: 0.829197\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001492241322994232, AUC: 0.8176049999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001694005012512207, AUC: 0.8349770000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006853301227092743, AUC: 0.7056475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014562657475471496, AUC: 0.7940020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001479567587375641, AUC: 0.808318\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013639997839927673, AUC: 0.8473469999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006961835324764252, AUC: 0.38450400000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012881333231925965, AUC: 0.775539\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014020306468009948, AUC: 0.789855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013349701166152953, AUC: 0.805526\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006926476657390595, AUC: 0.5199125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001334111452102661, AUC: 0.740914\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001407979667186737, AUC: 0.787825\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0012890581488609314, AUC: 0.818852\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006957947611808777, AUC: 0.4403390000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013087846636772155, AUC: 0.7569419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014033986926078796, AUC: 0.7918499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011560410261154176, AUC: 0.82701\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006878388524055481, AUC: 0.7391975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001277011215686798, AUC: 0.733312\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013133590817451476, AUC: 0.7746850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013910502791404725, AUC: 0.7918069999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006964699029922485, AUC: 0.3996565000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001165420174598694, AUC: 0.743013\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013641301393508911, AUC: 0.789986\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014104530215263366, AUC: 0.8106280000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006948442161083221, AUC: 0.46871799999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012675276398658752, AUC: 0.740739\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013235929608345033, AUC: 0.790343\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00137513667345047, AUC: 0.805365\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006947003304958344, AUC: 0.424541\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00122288978099823, AUC: 0.748015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013375158905982972, AUC: 0.782779\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001588438630104065, AUC: 0.798579\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006849124431610108, AUC: 0.694234\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012758437395095825, AUC: 0.774427\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012920546531677247, AUC: 0.8121820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012923317551612854, AUC: 0.8296669999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006977393925189972, AUC: 0.36523649999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001291788637638092, AUC: 0.737293\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013498828411102296, AUC: 0.7949120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014673247933387756, AUC: 0.8174250000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006943438947200775, AUC: 0.4504785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011255612969398498, AUC: 0.768319\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012107766270637512, AUC: 0.80548\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013772826790809632, AUC: 0.813829\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006917154788970947, AUC: 0.5690235000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013541932702064515, AUC: 0.598478\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012371926307678223, AUC: 0.688711\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010923356413841247, AUC: 0.723258\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006968607604503632, AUC: 0.3831825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015744782090187073, AUC: 0.5828990000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014835953712463378, AUC: 0.665832\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012981435656547547, AUC: 0.6980459999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006955974400043488, AUC: 0.43256\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014091846942901611, AUC: 0.645457\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001369967758655548, AUC: 0.683918\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013020054697990416, AUC: 0.7027850000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0007000738978385925, AUC: 0.3881845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016745031476020813, AUC: 0.57461\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016251876950263977, AUC: 0.659213\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013794299364089965, AUC: 0.6950369999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006960032284259796, AUC: 0.4766595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001597978949546814, AUC: 0.5385105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015363540053367614, AUC: 0.6627500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012637528777122498, AUC: 0.704556\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006979741156101227, AUC: 0.32655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001522919237613678, AUC: 0.553407\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015850431323051453, AUC: 0.659846\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013161882162094115, AUC: 0.6960270000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006918782293796539, AUC: 0.559968\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012581725716590882, AUC: 0.679128\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001229236662387848, AUC: 0.710427\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011915088295936585, AUC: 0.724267\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006963012814521789, AUC: 0.36674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014103291034698487, AUC: 0.610051\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013099029064178468, AUC: 0.704035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011947248578071594, AUC: 0.7228699999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006930817067623138, AUC: 0.55144\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00126514995098114, AUC: 0.663859\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001268758773803711, AUC: 0.6927850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011619875431060792, AUC: 0.7084585000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006888798475265503, AUC: 0.6461855000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012820826768875121, AUC: 0.685115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012846903204917907, AUC: 0.709021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012085999846458435, AUC: 0.723304\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006979137063026429, AUC: 0.32278750000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00133148592710495, AUC: 0.820056\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013782886266708375, AUC: 0.858347\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00192118239402771, AUC: 0.85137\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006899331212043762, AUC: 0.657114\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012319467067718505, AUC: 0.8096279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012445133924484254, AUC: 0.847517\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001337042450904846, AUC: 0.847077\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006820880770683289, AUC: 0.710675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012821065187454224, AUC: 0.816725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012086466550827026, AUC: 0.8384739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012879958152770996, AUC: 0.850204\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006903654634952545, AUC: 0.593124\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001118379533290863, AUC: 0.8279394999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014810474514961242, AUC: 0.8571609999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013770051002502442, AUC: 0.854878\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0007069114446640014, AUC: 0.242749\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014144913554191589, AUC: 0.8204989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001513637661933899, AUC: 0.8455180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001306992769241333, AUC: 0.8610800000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006921878755092621, AUC: 0.5970345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013711398243904114, AUC: 0.7955110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015437301397323608, AUC: 0.8171569999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012172499895095825, AUC: 0.8463309999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006925551295280457, AUC: 0.5933379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014085525274276733, AUC: 0.78868\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013914011716842651, AUC: 0.826568\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012870569229125977, AUC: 0.854255\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006961119771003723, AUC: 0.38871350000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008103238046169282, AUC: 0.858504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000931158035993576, AUC: 0.8774884999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013895972967147827, AUC: 0.8711614999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006905738711357116, AUC: 0.644471\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012654099464416504, AUC: 0.7992060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011641114950180053, AUC: 0.843349\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015687112212181091, AUC: 0.84473\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006835061609745026, AUC: 0.7417645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013023046255111693, AUC: 0.825409\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014262330532073975, AUC: 0.8375239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012669787406921387, AUC: 0.852042\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000697836846113205, AUC: 0.37008850000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011396661400794982, AUC: 0.770827\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013007211685180664, AUC: 0.811997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012499591708183288, AUC: 0.815539\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006927756369113922, AUC: 0.518756\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011860894560813903, AUC: 0.7321660000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001197644829750061, AUC: 0.792614\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012735504508018494, AUC: 0.824929\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000690514475107193, AUC: 0.6386790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001202582597732544, AUC: 0.767078\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001092694878578186, AUC: 0.817475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011756229400634766, AUC: 0.840884\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006964456140995025, AUC: 0.36736450000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001126945436000824, AUC: 0.7768120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001062393069267273, AUC: 0.8200779999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012183101773262023, AUC: 0.8248995\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006926549971103668, AUC: 0.532581\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011874104738235473, AUC: 0.76892\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012728224992752076, AUC: 0.8023020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012980905175209046, AUC: 0.826338\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0007008664906024933, AUC: 0.3223985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012494014501571655, AUC: 0.74944\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012630877494812013, AUC: 0.7997829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013315749168395995, AUC: 0.8174549999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006991316080093384, AUC: 0.33853449999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011583274602890014, AUC: 0.7944675000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012185652256011964, AUC: 0.824557\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001334098756313324, AUC: 0.8564489999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006847937405109406, AUC: 0.705627\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011758599281311035, AUC: 0.7457389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012690225839614869, AUC: 0.793452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013103622794151306, AUC: 0.813781\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006925979852676392, AUC: 0.57975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011308208107948304, AUC: 0.750285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012988250255584718, AUC: 0.7969920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011933872103691101, AUC: 0.824703\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000695866584777832, AUC: 0.42814450000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001182684600353241, AUC: 0.750788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001265819251537323, AUC: 0.814612\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001408158242702484, AUC: 0.8340939999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006949114203453064, AUC: 0.45558\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014354689121246337, AUC: 0.5758650000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013175629377365113, AUC: 0.675001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001132315695285797, AUC: 0.706466\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000690006971359253, AUC: 0.6346470000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013463624715805054, AUC: 0.507289\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012996394634246825, AUC: 0.6847089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001164216935634613, AUC: 0.7226600000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000693820834159851, AUC: 0.471494\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013598218560218812, AUC: 0.5849859999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014249856472015382, AUC: 0.669624\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001213870644569397, AUC: 0.704044\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000694388210773468, AUC: 0.4512545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015957908034324646, AUC: 0.6283139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015082186460494995, AUC: 0.693443\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012417213916778565, AUC: 0.7187049999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006957093477249146, AUC: 0.38944199999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000748483657836914, AUC: 0.582685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010371949672698974, AUC: 0.700732\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010211527645587921, AUC: 0.7242780000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006927612721920013, AUC: 0.5246435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021042466163636, AUC: 0.674104\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010914167761802674, AUC: 0.7199840000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001138194501399994, AUC: 0.738764\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006960340440273284, AUC: 0.6067405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001342705488204956, AUC: 0.590044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013636059165000915, AUC: 0.6818770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012205939888954164, AUC: 0.710601\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006931335330009461, AUC: 0.5170950000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013164325952529908, AUC: 0.626445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001208702564239502, AUC: 0.710852\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011207187175750733, AUC: 0.734961\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006902595460414886, AUC: 0.606362\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001275633454322815, AUC: 0.6548795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012172811031341553, AUC: 0.7008749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011490079164505004, AUC: 0.7142949999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006903997659683228, AUC: 0.6133195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011215248703956603, AUC: 0.6702289999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011986317038536073, AUC: 0.7079549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011585350036621093, AUC: 0.7373209999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006897677779197693, AUC: 0.6272169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013875076174736023, AUC: 0.811742\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012797714471817017, AUC: 0.850464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015719639658927918, AUC: 0.8589289999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006917991638183594, AUC: 0.5485805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013168479800224305, AUC: 0.816217\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014584704637527466, AUC: 0.8348270000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014950295686721802, AUC: 0.848802\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006945172548294068, AUC: 0.4559815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012278074026107787, AUC: 0.828437\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010732338428497315, AUC: 0.845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011716055870056152, AUC: 0.846575\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006956190168857574, AUC: 0.4005905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011252890825271607, AUC: 0.8047855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012148600220680237, AUC: 0.841369\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001485383629798889, AUC: 0.8348699999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006944859623908996, AUC: 0.46946450000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014134332537651061, AUC: 0.806386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011645344495773316, AUC: 0.8577410000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015893993973731995, AUC: 0.8401670000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000698531299829483, AUC: 0.3484005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012929802536964417, AUC: 0.8213739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014258021712303163, AUC: 0.832266\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017997503876686096, AUC: 0.828209\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006885204911231995, AUC: 0.683658\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001426667034626007, AUC: 0.8046515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001412131667137146, AUC: 0.8346359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011856539249420167, AUC: 0.837064\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000691462904214859, AUC: 0.6056925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013597025275230408, AUC: 0.824577\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014291183352470398, AUC: 0.856176\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015407857894897462, AUC: 0.843618\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0007023155391216278, AUC: 0.270157\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012542378306388854, AUC: 0.8123429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018739871978759766, AUC: 0.817189\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015053725242614746, AUC: 0.834164\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006912327408790588, AUC: 0.64205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011190905570983886, AUC: 0.82108\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013176281452178956, AUC: 0.862773\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014106485247612, AUC: 0.85267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# capped smote using triplet loss w/ positive dist squared \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-3, 1e-3, 1e-2]\n",
    "\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "loss_caps = [1, 5, 10]\n",
    "cap_calc = loss_fns.TripletLoss\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    learning_rate_aucs = []\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10): \n",
    "            print(loss_cap)\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(2)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                _, _ = train.train_triplet_capped_loss(epoch, train_loader_tripletloss_smote, network, optimizer, verbose=False, cap_calc=cap_calc, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "\n",
    "\n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"triplet_loss_capped_smote_pos_squared\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8700dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0f560c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
