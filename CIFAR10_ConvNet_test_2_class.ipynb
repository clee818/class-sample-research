{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c3c2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378c3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91dda12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "NUM_CLASSES_REDUCED = 2\n",
    "nums = (0, 1)\n",
    "ratio = (100, 1)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b57e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "          #   \"mean_40\", \"variance_40\",\n",
    "          #   \"mean_50\", \"variance_50\",\n",
    "             \"cap\", \"normalization\", \"other\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c2a1ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "norm=False\n",
    "\n",
    "if norm:\n",
    "    transform=torchvision.transforms.Compose([torchvision.transforms.Normalize(mean=[143.8888, 127.1705, 117.5357], std=[69.8313, 64.5137, 66.9933])])\n",
    "else:\n",
    "   # transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "    transform=None\n",
    "\n",
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "    \n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums, transform=transform)\n",
    "\n",
    "triplet_train_CIFAR10 = class_sampling.ForTripletLoss(reduced_train_CIFAR10, smote=False, transform=transform, num_classes=2)\n",
    "triplet_ratio_train_CIFAR10 = class_sampling.ForTripletLoss(ratio_train_CIFAR10, smote=False, transform=transform, num_classes=2)\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED, transform=transform)\n",
    "triplet_smote_train_CIFAR10 = class_sampling.ForTripletLoss(smote_train_CIFAR10, smote=True, transform=transform, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13159c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000   50]\n"
     ]
    }
   ],
   "source": [
    "targets = ratio_train_CIFAR10.labels \n",
    "\n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "print(class_count)\n",
    "\n",
    "weight = 1. / class_count\n",
    "\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "undersampler_smote = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * 50 * NUM_CLASSES_REDUCED), replacement=False)\n",
    "weight *= class_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af8cd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_smote_undersampled = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler_smote)\n",
    "\n",
    "train_loader_tripletloss = DataLoader(triplet_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_ratio = DataLoader(triplet_ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_smote = DataLoader(triplet_smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a9137b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be used in distance capped smote - get average tensor for the entire class \n",
    "dataset = train_loader_ratio.dataset\n",
    "class0 = dataset.images[dataset.labels==0]\n",
    "class1 = dataset.images[dataset.labels==1]\n",
    "class0_avg = torch.mean(class0.float(), 0)\n",
    "class1_avg = torch.mean(class1.float(), 0)\n",
    "avg_tensors_list = [class0_avg, class1_avg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 CLASS normal\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8f1d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d211694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0076671247482299806, AUC: 0.6110735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020734254121780394, AUC: 0.628587\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008975799964501125, AUC: 0.6439400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019357805252075196, AUC: 0.643949\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009144600006834705, AUC: 0.675112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020213021636009214, AUC: 0.67089\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008470717963236983, AUC: 0.7536120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003886705994606018, AUC: 0.514122\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015828227996826172, AUC: 0.595936\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012174508654245056, AUC: 0.567244\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015572131872177125, AUC: 0.6836819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011150310660647874, AUC: 0.643232\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014421252608299256, AUC: 0.712404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012044163608905113, AUC: 0.668856\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01665996742248535, AUC: 0.6862579999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015813767313957213, AUC: 0.6905600000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010870761579216117, AUC: 0.705184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017244288921356202, AUC: 0.734825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009154729095130863, AUC: 0.7596320000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018377559781074524, AUC: 0.756024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008260445851869513, AUC: 0.80658\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002282716631889343, AUC: 0.587105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015921525359153747, AUC: 0.615949\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011881166224432464, AUC: 0.561692\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016021928191184997, AUC: 0.6867319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010604898484036474, AUC: 0.658272\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013688442707061768, AUC: 0.7098280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001267247179357132, AUC: 0.728272\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013568562865257264, AUC: 0.532369\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020910239219665526, AUC: 0.550422\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009813344559752115, AUC: 0.502484\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016323612332344055, AUC: 0.619579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011425297581913448, AUC: 0.5709839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001749391257762909, AUC: 0.6334299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001038080965085785, AUC: 0.591348\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019294458627700805, AUC: 0.441978\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015018232464790345, AUC: 0.532296\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001446212606854958, AUC: 0.590052\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016287894248962402, AUC: 0.637854\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010995772665384972, AUC: 0.673188\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001581701934337616, AUC: 0.659426\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001093492754320107, AUC: 0.7412399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007909612655639649, AUC: 0.5652269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021779963970184326, AUC: 0.6679660000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008836146163763387, AUC: 0.63694\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002288204073905945, AUC: 0.702678\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008463866534725865, AUC: 0.690752\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002026849448680878, AUC: 0.719262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008203459484963724, AUC: 0.747512\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010193572998046876, AUC: 0.748682\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010386010408401489, AUC: 0.743842\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019852848542798863, AUC: 0.7785920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014001491665840149, AUC: 0.7602965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101876115887472, AUC: 0.796392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012921421527862548, AUC: 0.7561570000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012562816885143223, AUC: 0.820144\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009717195510864259, AUC: 0.687356\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019463917016983032, AUC: 0.690081\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009062620215486772, AUC: 0.6662239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017195783257484437, AUC: 0.709087\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009551594546525785, AUC: 0.70116\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016595609188079835, AUC: 0.7358560000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009313118195917346, AUC: 0.7405039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009570589363574982, AUC: 0.5011805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016350972056388855, AUC: 0.5762929999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012110045397340662, AUC: 0.553572\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018443724513053895, AUC: 0.645478\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009665573517432307, AUC: 0.63528\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001650454878807068, AUC: 0.654357\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010650237553780622, AUC: 0.67218\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010591432452201842, AUC: 0.494801\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019391133189201354, AUC: 0.53718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010679098203925804, AUC: 0.46274000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018079440593719482, AUC: 0.56423\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086755657550132, AUC: 0.5107520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018325347900390624, AUC: 0.605268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001027805893359208, AUC: 0.549512\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002467226266860962, AUC: 0.49211449999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018154305219650268, AUC: 0.6301359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001004764328867492, AUC: 0.5766800000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019403483271598816, AUC: 0.653424\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009370906521925832, AUC: 0.595504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00173450368642807, AUC: 0.6438625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001024536982842601, AUC: 0.605344\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021772370338439943, AUC: 0.6291599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019137043356895448, AUC: 0.558418\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086977752718595, AUC: 0.459476\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017537219524383545, AUC: 0.66186\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010254885189899124, AUC: 0.571672\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001654850125312805, AUC: 0.693519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010333627373865335, AUC: 0.6243879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012681103229522706, AUC: 0.5257050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017512226104736328, AUC: 0.396621\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013878306112076976, AUC: 0.43118799999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016085657477378846, AUC: 0.41888000000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014239362044499652, AUC: 0.44889999999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016571618914604186, AUC: 0.48058\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012518959057213057, AUC: 0.491518\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008851375579833985, AUC: 0.394701\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022655904293060304, AUC: 0.6417440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009069401588254046, AUC: 0.60942\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018319138884544373, AUC: 0.650572\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009779671255019631, AUC: 0.635076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017738329172134399, AUC: 0.6864399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009667404914403906, AUC: 0.667876\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001978057026863098, AUC: 0.42523900000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001972887694835663, AUC: 0.5121659999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011145488486284077, AUC: 0.40484000000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020207125544548033, AUC: 0.6013890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009857453996002085, AUC: 0.49250799999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019874536991119386, AUC: 0.635966\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009604879815389614, AUC: 0.53166\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017852833867073059, AUC: 0.3784155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023233342170715333, AUC: 0.5972189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000935311899105511, AUC: 0.521724\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00229307758808136, AUC: 0.665186\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008898888480397734, AUC: 0.6355280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001983289897441864, AUC: 0.6787810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008811807359504227, AUC: 0.6924520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024164196252822877, AUC: 0.49547800000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016742894053459168, AUC: 0.571703\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012209183014560453, AUC: 0.5585760000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018676289916038514, AUC: 0.633934\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009904458447553143, AUC: 0.608856\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019339900016784668, AUC: 0.6533000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009348994578317841, AUC: 0.6239119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025133802890777586, AUC: 0.45490200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002249223232269287, AUC: 0.627273\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009032244074831504, AUC: 0.615916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002208526015281677, AUC: 0.6674635\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0008650489547981484, AUC: 0.670572\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021552451848983766, AUC: 0.7147754999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008405291248518642, AUC: 0.711684\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025969254970550537, AUC: 0.4249035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020958303213119505, AUC: 0.5708455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009637036760992343, AUC: 0.552364\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018786900043487548, AUC: 0.6407069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009473569576840589, AUC: 0.6412439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020249083042144776, AUC: 0.679206\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008764915754741961, AUC: 0.679316\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001352885603904724, AUC: 0.4485485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024831844568252563, AUC: 0.42361899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010917736091973758, AUC: 0.361972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002164663791656494, AUC: 0.455156\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011078405144191025, AUC: 0.398448\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021346499919891355, AUC: 0.501624\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010488035323301165, AUC: 0.43668399999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016203722953796386, AUC: 0.530114\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003072163462638855, AUC: 0.588485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010479836682742922, AUC: 0.5180480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023032039403915404, AUC: 0.587286\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009547403945338608, AUC: 0.526236\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001921819806098938, AUC: 0.605945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010153998250123299, AUC: 0.552732\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008129218101501464, AUC: 0.333634\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030005383491516113, AUC: 0.435077\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010937118794916586, AUC: 0.394652\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021180523633956908, AUC: 0.491343\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001148930329571266, AUC: 0.4620079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022330018281936643, AUC: 0.565871\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010301658605216164, AUC: 0.524832\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003371488690376282, AUC: 0.589884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002845911741256714, AUC: 0.36794899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001133701276071001, AUC: 0.340316\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021184813976287842, AUC: 0.41797999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011628526869681803, AUC: 0.370284\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023320701122283936, AUC: 0.45413400000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010569927607211147, AUC: 0.397404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003613470792770386, AUC: 0.6405080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002160325050354004, AUC: 0.468642\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001171023680432008, AUC: 0.34930800000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001940733015537262, AUC: 0.521421\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011482540477472957, AUC: 0.42302800000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018642403483390808, AUC: 0.565607\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105369452645283, AUC: 0.47382399999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015669259428977966, AUC: 0.70762\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027528159618377688, AUC: 0.6968620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009593207823887172, AUC: 0.643348\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022390233278274537, AUC: 0.675067\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008987060488008037, AUC: 0.635968\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021265636682510376, AUC: 0.640035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009188651688175626, AUC: 0.603808\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005645490407943725, AUC: 0.68851\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033736543655395507, AUC: 0.671146\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010854890158095646, AUC: 0.6384360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022894320487976075, AUC: 0.6748799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008709144213301415, AUC: 0.6768319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002009647250175476, AUC: 0.6820379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008711621181873402, AUC: 0.7085199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013125876188278198, AUC: 0.63015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029850161075592042, AUC: 0.460643\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011202180633634918, AUC: 0.44290799999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023843228816986086, AUC: 0.5409745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010281204304999055, AUC: 0.48174799999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00209027099609375, AUC: 0.5642759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010499044079886805, AUC: 0.5048440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037168740034103395, AUC: 0.4056835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028641793727874755, AUC: 0.375166\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992970473437322, AUC: 0.293972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002653481364250183, AUC: 0.40545999999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010511783987173055, AUC: 0.323976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002539039492607117, AUC: 0.43797700000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010308365265626718, AUC: 0.34588399999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013203486800193787, AUC: 0.378262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002571802616119385, AUC: 0.599729\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009274383259955609, AUC: 0.57036\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019491751790046692, AUC: 0.554403\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001017076036717632, AUC: 0.530888\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017297410368919373, AUC: 0.552915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011625397810251406, AUC: 0.527944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS ratio\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "learning_rate_train_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                _, train_auc = metric_utils.auc_sigmoid(train_loader_ratio, network) \n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f2d0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f1ccb0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0038651072978973387, AUC: 0.40709100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005063888132572174, AUC: 0.8398785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005734625160694122, AUC: 0.8425819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011212489008903504, AUC: 0.841191\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008080781102180481, AUC: 0.55231\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006464185416698455, AUC: 0.7963709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010616356730461121, AUC: 0.7623549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011121231317520143, AUC: 0.753925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025173540115356447, AUC: 0.5626525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006488811075687408, AUC: 0.6799000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000709791898727417, AUC: 0.6942869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009997697472572326, AUC: 0.726058\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008281596899032593, AUC: 0.572139\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004913568198680878, AUC: 0.8518579999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006246580779552459, AUC: 0.8336830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015956854224205017, AUC: 0.814265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007366308271884918, AUC: 0.513698\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005183021128177642, AUC: 0.840308\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008029429018497467, AUC: 0.818953\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010578397512435913, AUC: 0.804311\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035281330347061157, AUC: 0.38053750000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007264424860477448, AUC: 0.8519850000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008754095733165741, AUC: 0.8259589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012884948253631592, AUC: 0.80179\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006137662887573242, AUC: 0.5142995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005887452065944671, AUC: 0.8550169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000608889102935791, AUC: 0.735435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006282347142696381, AUC: 0.747253\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008330761790275574, AUC: 0.6295200000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004755498617887497, AUC: 0.863896\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001037799596786499, AUC: 0.802286\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001165145993232727, AUC: 0.7922740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012109436988830566, AUC: 0.49136199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005991648733615875, AUC: 0.8487684999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005843090116977691, AUC: 0.795875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005498118996620178, AUC: 0.816298\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008850356578826905, AUC: 0.647549\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940476298332214, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006594555079936981, AUC: 0.705973\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006307438910007477, AUC: 0.7548905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004416076898574829, AUC: 0.323978\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014050147533416749, AUC: 0.766865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026331219673156737, AUC: 0.779182\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004152154922485352, AUC: 0.7604204999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014910327792167664, AUC: 0.49231600000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007511105239391327, AUC: 0.6525365000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001356121063232422, AUC: 0.5043154999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024438369274139404, AUC: 0.580111\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001424083411693573, AUC: 0.5397235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017812739014625549, AUC: 0.684012\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014088171124458312, AUC: 0.6441939999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004744303703308105, AUC: 0.673342\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022509771585464476, AUC: 0.390438\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014318109154701233, AUC: 0.776913\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037591698169708253, AUC: 0.6860169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003917380690574646, AUC: 0.704576\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004887598752975464, AUC: 0.45462800000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008536508679389954, AUC: 0.656873\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021468346118927004, AUC: 0.637953\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034337985515594483, AUC: 0.591375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017650567293167115, AUC: 0.66196\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010653643608093263, AUC: 0.667168\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021494070291519164, AUC: 0.631052\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036370185613632203, AUC: 0.603567\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013272791504859924, AUC: 0.392621\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016511693596839906, AUC: 0.594365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004804892063140869, AUC: 0.554214\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0076607315540313725, AUC: 0.586339\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007192042171955109, AUC: 0.705309\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008659768998622895, AUC: 0.6609050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00251075804233551, AUC: 0.678975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003699234962463379, AUC: 0.612506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00479100751876831, AUC: 0.544395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007851931750774383, AUC: 0.620472\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018445215821266175, AUC: 0.500974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002619206666946411, AUC: 0.47592399999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025211087465286256, AUC: 0.6451229999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019069305658340455, AUC: 0.745953\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015072822570800781, AUC: 0.639524\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002941378951072693, AUC: 0.689565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982449561357498, AUC: 0.581275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006168580055236816, AUC: 0.777878\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000589244395494461, AUC: 0.816397\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000577460139989853, AUC: 0.83312\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004177759647369385, AUC: 0.553401\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006863325834274292, AUC: 0.612715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006757993996143341, AUC: 0.6529855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005748008191585541, AUC: 0.7996274999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028271037340164185, AUC: 0.6727749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006341786980628968, AUC: 0.8080189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006235616207122803, AUC: 0.8241250000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006174919605255127, AUC: 0.8227709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000940509021282196, AUC: 0.5437019999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006323151588439941, AUC: 0.712728\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005493966341018676, AUC: 0.818747\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005441974699497223, AUC: 0.832964\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006498530805110932, AUC: 0.7259580000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006102970540523529, AUC: 0.7819095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005698021352291107, AUC: 0.8150240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005621075034141541, AUC: 0.819928\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006551118850708007, AUC: 0.4868045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000684427797794342, AUC: 0.623251\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006762815713882446, AUC: 0.6828545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006693606078624725, AUC: 0.7149480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014205492973327637, AUC: 0.4715385000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693734735250473, AUC: 0.5195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006764842867851257, AUC: 0.7068864999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006710585653781891, AUC: 0.7434974999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027725353240966796, AUC: 0.398981\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936939358711243, AUC: 0.5009939999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936058700084686, AUC: 0.500993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693530559539795, AUC: 0.5014959999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000877816766500473, AUC: 0.5271775000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006711920201778412, AUC: 0.6687230000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006447704434394836, AUC: 0.766165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006403065919876099, AUC: 0.7804540000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002469860792160034, AUC: 0.593591\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005706853270530701, AUC: 0.843007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005331571698188782, AUC: 0.8573040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005283410847187043, AUC: 0.855439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS oversampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-3, 1e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52d8c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b1541b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006928627490997315, AUC: 0.5610565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006705484390258789, AUC: 0.731087\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006308182775974274, AUC: 0.7689\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006022096574306488, AUC: 0.792463\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006950775682926178, AUC: 0.4702115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006825314462184907, AUC: 0.7292394999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006524708569049835, AUC: 0.754748\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006467648446559906, AUC: 0.7728849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006955726444721221, AUC: 0.3766905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007012077569961548, AUC: 0.601467\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006722729504108429, AUC: 0.7478565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006584925651550293, AUC: 0.764774\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918634176254272, AUC: 0.548633\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006640854477882385, AUC: 0.7368910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006435896456241608, AUC: 0.772193\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006132080554962159, AUC: 0.805193\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006965889036655426, AUC: 0.3598795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006798864006996155, AUC: 0.722772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006425350904464722, AUC: 0.760243\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006240421831607819, AUC: 0.7786550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006957835257053375, AUC: 0.4375025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006834719181060791, AUC: 0.711951\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006513985991477967, AUC: 0.7628889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006210027635097504, AUC: 0.7961360000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007116021811962127, AUC: 0.2701275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006652469336986542, AUC: 0.749688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006440185308456421, AUC: 0.780153\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006193165481090545, AUC: 0.7989299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007008937299251557, AUC: 0.3809365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006463136076927186, AUC: 0.7635879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005850833356380462, AUC: 0.7921960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005856163203716278, AUC: 0.8098865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006968954503536225, AUC: 0.411403\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943140625953674, AUC: 0.7122989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006632572412490845, AUC: 0.767749\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006245843768119812, AUC: 0.795972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006954509615898132, AUC: 0.42410250000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006669250130653382, AUC: 0.768473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006450056135654449, AUC: 0.786346\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006585321128368377, AUC: 0.7965205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006984304785728454, AUC: 0.3510465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006993257403373718, AUC: 0.445579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007002671062946319, AUC: 0.557559\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006986504197120666, AUC: 0.624322\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006875935792922974, AUC: 0.6208015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006796917617321014, AUC: 0.7305775000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006728697121143341, AUC: 0.732001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000661188006401062, AUC: 0.737393\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006826050281524658, AUC: 0.708147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006837069392204285, AUC: 0.688508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006812298893928528, AUC: 0.6901515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006734901666641236, AUC: 0.704798\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932272017002106, AUC: 0.5695859999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006879149675369263, AUC: 0.6111989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006847475469112396, AUC: 0.640729\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006875899136066436, AUC: 0.6689890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902289688587189, AUC: 0.6300385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006905774176120758, AUC: 0.638779\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006883034110069275, AUC: 0.677482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006861826181411743, AUC: 0.7036560000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006971223056316376, AUC: 0.4343925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006849060654640197, AUC: 0.6515434999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006698879599571228, AUC: 0.6940439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006588464975357056, AUC: 0.713968\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927119195461273, AUC: 0.519376\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000688279390335083, AUC: 0.6079015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000678226500749588, AUC: 0.666508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006683720946311951, AUC: 0.6944840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006899041533470154, AUC: 0.6205875000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006839993894100189, AUC: 0.6695689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006716804206371307, AUC: 0.7078499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006677893996238709, AUC: 0.720974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930917203426361, AUC: 0.534002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947171986103058, AUC: 0.5545225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963235139846802, AUC: 0.5801274999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006964735686779023, AUC: 0.6224324999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932185888290405, AUC: 0.5061089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940591931343078, AUC: 0.546071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943828165531158, AUC: 0.6247775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941763460636139, AUC: 0.6806765000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000687966525554657, AUC: 0.7050644999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006887478828430176, AUC: 0.6989965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894939541816712, AUC: 0.690177\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006904767751693726, AUC: 0.6740535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929472386837006, AUC: 0.520876\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930617392063141, AUC: 0.510615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929692327976226, AUC: 0.5129855000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930001080036163, AUC: 0.509326\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000698872059583664, AUC: 0.252758\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988219320774078, AUC: 0.25494649999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988555192947388, AUC: 0.258632\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988353133201599, AUC: 0.26737\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007016754448413849, AUC: 0.3065925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007013347446918487, AUC: 0.3021595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007006541788578033, AUC: 0.3044395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006997955143451691, AUC: 0.3110185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947731077671051, AUC: 0.600099\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941960752010345, AUC: 0.5978035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693750262260437, AUC: 0.5939255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934162974357605, AUC: 0.5842645000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006975911855697632, AUC: 0.3350575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006968346834182739, AUC: 0.35933800000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006962250173091889, AUC: 0.392072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006956771612167358, AUC: 0.43028\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948958039283752, AUC: 0.42235900000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947318315505981, AUC: 0.42986450000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946011483669281, AUC: 0.4361265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945200264453888, AUC: 0.44007999999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006920962631702423, AUC: 0.5704765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006916036903858184, AUC: 0.600501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006912074089050293, AUC: 0.621478\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006909322440624237, AUC: 0.6336729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006977652311325074, AUC: 0.3840535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973227560520172, AUC: 0.3848955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006969808340072631, AUC: 0.38405150000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000696704238653183, AUC: 0.383508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006994488835334778, AUC: 0.386849\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006997025310993194, AUC: 0.392687\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006996612548828125, AUC: 0.40394399999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006992768943309784, AUC: 0.42428699999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988563239574432, AUC: 0.30963050000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006964642107486725, AUC: 0.4378635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936416327953339, AUC: 0.530629\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006915313899517059, AUC: 0.585804\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936706900596618, AUC: 0.5052385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943995654582978, AUC: 0.5241214999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006942475736141204, AUC: 0.5752839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930316984653473, AUC: 0.618519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006896259486675263, AUC: 0.604004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006859594881534576, AUC: 0.636279\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.000681488573551178, AUC: 0.661619\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006757974326610565, AUC: 0.6790075000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006896138191223144, AUC: 0.610887\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893069446086884, AUC: 0.613855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006885486543178558, AUC: 0.633282\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006888088881969452, AUC: 0.6441379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000698607712984085, AUC: 0.47644650000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006970592141151428, AUC: 0.5706005000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006956733465194702, AUC: 0.6301249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951669156551361, AUC: 0.6572\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006925795376300812, AUC: 0.575475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951481699943542, AUC: 0.5105139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006970000267028808, AUC: 0.4651495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000699210911989212, AUC: 0.424679\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938398480415344, AUC: 0.49060099999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006900518834590912, AUC: 0.6081639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006852288544178009, AUC: 0.6609965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006791809201240539, AUC: 0.6885735000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006768987178802491, AUC: 0.7593004999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000679383397102356, AUC: 0.7485544999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006822780966758728, AUC: 0.7235449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006835326850414276, AUC: 0.703198\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000697054535150528, AUC: 0.412941\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006958455443382264, AUC: 0.4202635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951872408390046, AUC: 0.4582435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693785697221756, AUC: 0.5277975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936173141002655, AUC: 0.49884150000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000692090779542923, AUC: 0.5442275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006922757029533387, AUC: 0.548847\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006904305517673493, AUC: 0.5985255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006974986791610717, AUC: 0.47294749999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006974535882472992, AUC: 0.47196799999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006974373161792755, AUC: 0.470164\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006974062919616699, AUC: 0.4685745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945805549621582, AUC: 0.511066\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945959925651551, AUC: 0.512351\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946030259132385, AUC: 0.513831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946027278900146, AUC: 0.5153995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007035338878631592, AUC: 0.3031575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000703522503376007, AUC: 0.3047515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000703513115644455, AUC: 0.306492\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000703542560338974, AUC: 0.307971\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006903063356876373, AUC: 0.6405325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902830302715302, AUC: 0.6414055000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902560889720917, AUC: 0.642237\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902223825454712, AUC: 0.6432424999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973913013935089, AUC: 0.3781635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973600089550018, AUC: 0.37925449999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973490417003632, AUC: 0.37986600000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973181366920472, AUC: 0.3809785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006964112520217896, AUC: 0.37526950000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963879466056823, AUC: 0.3755745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000696378231048584, AUC: 0.3756815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963653862476349, AUC: 0.375803\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952761113643647, AUC: 0.4577645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952200829982758, AUC: 0.45907450000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695160061120987, AUC: 0.46040699999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006950892806053162, AUC: 0.462167\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000690339595079422, AUC: 0.6339035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006903114020824433, AUC: 0.634817\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902790367603302, AUC: 0.6358309999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902419328689575, AUC: 0.6368320000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006928563117980957, AUC: 0.5400750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927987933158875, AUC: 0.5429109999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927626430988312, AUC: 0.544478\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927131414413453, AUC: 0.5468745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936696171760559, AUC: 0.48374\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936245560646057, AUC: 0.48604149999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935936212539673, AUC: 0.4878675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693551242351532, AUC: 0.48999699999999996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS undersampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2, 1e-2, 1e-3, 5e-3, 1e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fda3918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cdf3dafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006929865777492523, AUC: 0.6035210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941011548042297, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01152815311261923, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933517456054688, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010536302033037242, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934399306774139, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01047680665950964, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006829564273357391, AUC: 0.7606660000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007110981047153472, AUC: 0.513034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008160775073684089, AUC: 0.4939\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006991162598133087, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009259643141586001, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932158470153809, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011024211881184342, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006900024712085724, AUC: 0.6275345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006032556593418122, AUC: 0.6915875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009716101306499822, AUC: 0.67254\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006006793975830078, AUC: 0.703085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010080904476713426, AUC: 0.68551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693379133939743, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010516728127356803, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006867653727531433, AUC: 0.6620455000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941483020782471, AUC: 0.5020020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0101606553025765, AUC: 0.5024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947364509105683, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011732829313467044, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932136416435242, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011021087063421118, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946891844272614, AUC: 0.45950349999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940916180610656, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010191532998982043, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006905839145183564, AUC: 0.5271815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010548212835104159, AUC: 0.585972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006954963207244873, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009828539650038918, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695802628993988, AUC: 0.5831175000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006958652138710022, AUC: 0.594743\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008937520402492863, AUC: 0.6741199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006913960874080657, AUC: 0.532716\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009897223864451493, AUC: 0.5484240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006926067471504212, AUC: 0.5197149999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009974045635450004, AUC: 0.543572\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007032238245010376, AUC: 0.26280099999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963876187801361, AUC: 0.5065189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009458458411811602, AUC: 0.5388360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007120804488658906, AUC: 0.567307\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008191924195478458, AUC: 0.6288479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007221527099609375, AUC: 0.5354529999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007242459070564497, AUC: 0.6090860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007022532224655151, AUC: 0.309856\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940309107303619, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01150185514204573, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932080686092377, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011013430510417069, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936182677745819, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011321447296897963, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007022981643676758, AUC: 0.279428\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933487057685852, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010538602512661773, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937902569770813, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011403333208348491, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006939776837825775, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011481297110566998, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007030169069766998, AUC: 0.26083599999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941924095153809, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010158426478357598, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933222115039825, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011132937882206227, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947585344314575, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011739103463616701, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947209239006043, AUC: 0.503085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004985811412334442, AUC: 0.8705590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008741397662918166, AUC: 0.910452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00048722681403160097, AUC: 0.8629110000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007839573692567277, AUC: 0.911974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005121163129806519, AUC: 0.831754\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006779720865853942, AUC: 0.9525359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931937634944916, AUC: 0.6319214999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005152012705802917, AUC: 0.8624559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010805733227493739, AUC: 0.8948520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005664720535278321, AUC: 0.842025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006664018843433645, AUC: 0.9242239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005163897573947907, AUC: 0.8882985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004746445078070801, AUC: 0.95862\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006993449330329895, AUC: 0.308698\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005618657171726227, AUC: 0.837395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007188617935275087, AUC: 0.8682\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005276458859443665, AUC: 0.85389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0053951541858144325, AUC: 0.9311919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005647776126861573, AUC: 0.8183910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0056284768156485985, AUC: 0.942612\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006987028419971466, AUC: 0.419359\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000473314568400383, AUC: 0.8638839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007454072691426419, AUC: 0.884772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005346084535121918, AUC: 0.8605094999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007051103150490487, AUC: 0.9267599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005636484026908874, AUC: 0.8834069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004816935280762096, AUC: 0.9464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006926956474781036, AUC: 0.62321\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005920693576335907, AUC: 0.872232\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00950404413855902, AUC: 0.898072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000488565668463707, AUC: 0.86691\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005895164248966934, AUC: 0.8858999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005560655891895294, AUC: 0.870534\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004082345602535966, AUC: 0.9216799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00069858717918396, AUC: 0.3564225000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005433701276779174, AUC: 0.8413849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007599443150038766, AUC: 0.8711\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005662823319435119, AUC: 0.8394060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0061812017282637036, AUC: 0.9136279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006344362497329712, AUC: 0.863313\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004362780357351398, AUC: 0.933392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006920470595359802, AUC: 0.626553\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005774954557418823, AUC: 0.8625749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005773622346396493, AUC: 0.8950400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005901984572410584, AUC: 0.8413899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005424396850094937, AUC: 0.9351479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005415751934051514, AUC: 0.844387\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005204110599980496, AUC: 0.9476119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006909263730049133, AUC: 0.608957\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005072842240333557, AUC: 0.848906\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006716722314900691, AUC: 0.88254\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000501204401254654, AUC: 0.8605349999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005464349566119732, AUC: 0.916816\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005297437012195587, AUC: 0.852255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006041818249343646, AUC: 0.951712\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929771304130555, AUC: 0.6099144999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005701894760131835, AUC: 0.810224\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007652453461495957, AUC: 0.9052879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005346051752567291, AUC: 0.865758\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004242133191316434, AUC: 0.94032\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005565256476402282, AUC: 0.8667680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006119210065001308, AUC: 0.932944\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973126232624054, AUC: 0.389163\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0005637988150119782, AUC: 0.828475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00714115304521995, AUC: 0.9039240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005115714669227601, AUC: 0.872054\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0058704357690150194, AUC: 0.9506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005990622639656067, AUC: 0.854916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004999826568188054, AUC: 0.9478759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893006265163422, AUC: 0.592972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005413131713867187, AUC: 0.838742\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009401522638774154, AUC: 0.873108\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005082505941390992, AUC: 0.8643450000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007597315765843533, AUC: 0.93034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005349463224411011, AUC: 0.819852\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005960197472336269, AUC: 0.952184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006917398273944855, AUC: 0.5357335000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005320677161216736, AUC: 0.872421\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007882021895729669, AUC: 0.885336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047688519954681394, AUC: 0.8764339999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006409961439595364, AUC: 0.9162440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004914150685071945, AUC: 0.8588819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005879358218447997, AUC: 0.94408\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951043009757996, AUC: 0.4402385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005084710121154785, AUC: 0.8479289999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007602337244713661, AUC: 0.887208\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004919102936983108, AUC: 0.8564430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006338312826534309, AUC: 0.9233879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005057655572891235, AUC: 0.839117\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006557884688424592, AUC: 0.94724\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006957819759845733, AUC: 0.554511\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005380403399467468, AUC: 0.8291689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009521948453223352, AUC: 0.906452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004899816066026687, AUC: 0.8586830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00712057093582531, AUC: 0.9333940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005030784308910369, AUC: 0.850487\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0057055700031837615, AUC: 0.94844\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007004860043525696, AUC: 0.3607595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00053057861328125, AUC: 0.8529720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0070074458405523015, AUC: 0.886212\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004892897009849548, AUC: 0.868004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005604852668129571, AUC: 0.920404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000458210751414299, AUC: 0.875929\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006420788363655015, AUC: 0.946916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940570473670959, AUC: 0.4731449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005776671469211579, AUC: 0.8586539999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012373634175498888, AUC: 0.8687280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00044246050715446473, AUC: 0.889369\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0070867665805438955, AUC: 0.9181079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005300732553005219, AUC: 0.8762265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005396853927338478, AUC: 0.960264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929945051670075, AUC: 0.564638\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005563854575157166, AUC: 0.8648770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009520969921999639, AUC: 0.923976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005157967507839203, AUC: 0.8525540000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006059927674803403, AUC: 0.939404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000465511754155159, AUC: 0.877534\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006717685177774713, AUC: 0.948132\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006980034112930298, AUC: 0.392889\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005355749130249024, AUC: 0.8342959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007846096653749447, AUC: 0.8555839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004474954456090927, AUC: 0.879305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00643831486159032, AUC: 0.898184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046650290489196777, AUC: 0.887073\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00500908629138871, AUC: 0.929724\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006906140744686127, AUC: 0.6270205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005572724044322967, AUC: 0.85573\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008027610229973746, AUC: 0.898772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005564309954643249, AUC: 0.8470209999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004956791843518172, AUC: 0.911748\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004649658203125, AUC: 0.866714\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006907176440305049, AUC: 0.956392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938843429088593, AUC: 0.47562899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000519424855709076, AUC: 0.8551110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007493531125606877, AUC: 0.8831279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000504085659980774, AUC: 0.8609840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006028514882125476, AUC: 0.9416640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004914458394050599, AUC: 0.867466\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006038065062891139, AUC: 0.96608\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963664293289185, AUC: 0.38524\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006432399749755859, AUC: 0.7625959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008959613684380408, AUC: 0.7848919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005628331005573272, AUC: 0.826678\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00864995011598757, AUC: 0.84258\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005056944489479065, AUC: 0.8590329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00824941654016476, AUC: 0.871856\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006910701990127564, AUC: 0.599355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000616879791021347, AUC: 0.7707120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01059920307433251, AUC: 0.781044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005687816441059113, AUC: 0.799205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009738183611690408, AUC: 0.8164119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005393176674842834, AUC: 0.8172925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008101473804747704, AUC: 0.842912\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006939460039138794, AUC: 0.54028\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006021830439567566, AUC: 0.7970139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010496961374093991, AUC: 0.803984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005550967454910278, AUC: 0.828073\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009545001192848282, AUC: 0.839732\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005240070223808289, AUC: 0.844389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0091742208452508, AUC: 0.8625879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006914302408695221, AUC: 0.6094385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005888742804527283, AUC: 0.824057\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01029107907030842, AUC: 0.83066\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005443206429481506, AUC: 0.8330359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009114989762259001, AUC: 0.849788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005262280106544494, AUC: 0.853632\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008392372603463654, AUC: 0.878884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006837622821331024, AUC: 0.691576\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005895999670028686, AUC: 0.782758\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009808489296695973, AUC: 0.80072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005589506924152375, AUC: 0.806367\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009164169448437077, AUC: 0.8281080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005357769429683685, AUC: 0.8292949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00911985157149853, AUC: 0.8574439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945233047008514, AUC: 0.48311099999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006134748458862304, AUC: 0.806477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01052536406139336, AUC: 0.808596\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005562895834445953, AUC: 0.822084\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009412364829884898, AUC: 0.833688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005309949219226837, AUC: 0.8396819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007839170440588848, AUC: 0.857212\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937495470046997, AUC: 0.48171200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006372389495372772, AUC: 0.819923\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0120068094163838, AUC: 0.8215159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005695891678333283, AUC: 0.822279\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009782834808425149, AUC: 0.840788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005112173855304718, AUC: 0.8443940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008199451105429394, AUC: 0.868456\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006921443343162536, AUC: 0.5599354999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005984731018543243, AUC: 0.8115479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010888017746481566, AUC: 0.82364\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005443423092365265, AUC: 0.8276000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009130165565131914, AUC: 0.8468919999999999\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0005055390298366547, AUC: 0.849865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008350928868397627, AUC: 0.8736079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963136792182922, AUC: 0.39968349999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005984225273132324, AUC: 0.771389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009001542176350508, AUC: 0.78562\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000558992862701416, AUC: 0.8067799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008595412568290635, AUC: 0.829488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000538530021905899, AUC: 0.82298\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007287021987509019, AUC: 0.8586119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006968409717082977, AUC: 0.38355749999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006436823904514313, AUC: 0.803137\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01135210377154964, AUC: 0.7909520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000583594560623169, AUC: 0.8077380000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01014432730060993, AUC: 0.820932\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005280269980430603, AUC: 0.833491\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008627873015875864, AUC: 0.8455999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870892643928528, AUC: 0.656811\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006438588798046112, AUC: 0.785184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011279767680876325, AUC: 0.77802\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005855887830257415, AUC: 0.790768\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009658517200167816, AUC: 0.799412\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005556738078594208, AUC: 0.8051299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008975181662210143, AUC: 0.823952\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007047882378101349, AUC: 0.3414305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006809703409671784, AUC: 0.7605970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010495741154887888, AUC: 0.7505620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006465067863464356, AUC: 0.7901370000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010764568288727561, AUC: 0.7835139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006005278527736664, AUC: 0.7971060000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010074336410749077, AUC: 0.8006639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952670216560364, AUC: 0.435129\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006432079970836639, AUC: 0.7717779999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010464417521316226, AUC: 0.7932920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005900460183620453, AUC: 0.7872790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009503405046935129, AUC: 0.809028\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005679526627063752, AUC: 0.8025730000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008954270877460441, AUC: 0.825492\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006970075964927673, AUC: 0.3704055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006620379984378815, AUC: 0.8133829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010983516284734896, AUC: 0.824464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005913313031196594, AUC: 0.8132949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009973079898569843, AUC: 0.826076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005637726485729218, AUC: 0.8200820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009417588061625414, AUC: 0.837888\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006897422671318054, AUC: 0.598884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006093161702156067, AUC: 0.7742269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00976294757115959, AUC: 0.785188\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005706937611103058, AUC: 0.7944450000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008533435092113986, AUC: 0.8078\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005440973341464996, AUC: 0.820147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008820098668041796, AUC: 0.837528\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000686633974313736, AUC: 0.66552\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006343769133090973, AUC: 0.7708459999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00998953613904443, AUC: 0.77996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000589023768901825, AUC: 0.781485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009906717373593018, AUC: 0.791872\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005734374821186065, AUC: 0.7928010000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009384710352019508, AUC: 0.8104199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006928159296512603, AUC: 0.5240104999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006608406603336335, AUC: 0.7565659999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010737750010915321, AUC: 0.761488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006106067299842835, AUC: 0.7730439999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010074301509573908, AUC: 0.781744\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005812602043151855, AUC: 0.7897865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009442508126249407, AUC: 0.805176\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943551301956176, AUC: 0.5027984999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006626200675964356, AUC: 0.8112789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010643097292078603, AUC: 0.82374\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005914508402347564, AUC: 0.8046445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010367770584503022, AUC: 0.8259\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005529929101467133, AUC: 0.8113150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009098475008907885, AUC: 0.837008\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006970823407173157, AUC: 0.3750015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006655546128749848, AUC: 0.788638\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011142519086894422, AUC: 0.803384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005962426960468292, AUC: 0.7917019999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010091654510781317, AUC: 0.800416\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005699568390846253, AUC: 0.8046564999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009837563828666612, AUC: 0.8201919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006922467052936554, AUC: 0.5355885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006529574692249298, AUC: 0.762027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011207672263136004, AUC: 0.763824\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006132130324840545, AUC: 0.778235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010650047831016012, AUC: 0.7884599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005724292099475861, AUC: 0.790807\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00923902967188618, AUC: 0.8058000000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 Class Weighted Loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2, 1e-2, 5e-3, 1e-3, 5e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['pos_weight'] = torch.tensor([weight[1]])\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                _, train_auc = metric_utils.auc_sigmoid(train_loader_ratio, network) \n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9eed2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7c5a840",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0009200392961502075, AUC: 0.678816\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006507715582847595, AUC: 0.8034624999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006657144129276276, AUC: 0.8041109999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006028525233268738, AUC: 0.836938\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011849600076675416, AUC: 0.401832\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00063573157787323, AUC: 0.8188884999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006494357883930206, AUC: 0.8221820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006822991371154785, AUC: 0.823094\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00185699599981308, AUC: 0.656679\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006367193758487702, AUC: 0.8178314999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006523110866546631, AUC: 0.8215329999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006271962523460388, AUC: 0.846208\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003958873510360718, AUC: 0.591901\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006755812168121338, AUC: 0.683454\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005986700654029846, AUC: 0.836574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006900097727775573, AUC: 0.7680600000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005183618307113647, AUC: 0.43149550000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005428347587585449, AUC: 0.8790509999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005170415639877319, AUC: 0.888849\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000678589254617691, AUC: 0.886983\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001686727821826935, AUC: 0.5514745000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006351503431797028, AUC: 0.824693\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006846866309642792, AUC: 0.75287\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007116355299949646, AUC: 0.7508920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011202555298805237, AUC: 0.4642265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006064935028553009, AUC: 0.864204\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005545421242713929, AUC: 0.8484050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006670324802398682, AUC: 0.8753179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015502147674560547, AUC: 0.424438\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006380311846733094, AUC: 0.7967835000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005722104907035828, AUC: 0.86531\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005263795554637908, AUC: 0.885898\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006291325569152832, AUC: 0.3599755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006726720035076141, AUC: 0.7612955000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006954313814640045, AUC: 0.7701930000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007297416627407074, AUC: 0.7539054999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013372409343719483, AUC: 0.627812\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006781833469867707, AUC: 0.707896\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006622196435928344, AUC: 0.789044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006352911293506622, AUC: 0.850006\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS SMOTE\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66655269",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfecae8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "Test set: Avg. loss: 0.0010205329358577728, AUC: 0.7048180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005711885392665863, AUC: 0.8074610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008731485605239868, AUC: 0.7810469999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000770821899175644, AUC: 0.8042990000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0053196785449981685, AUC: 0.47757400000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005356418192386627, AUC: 0.853467\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001122176170349121, AUC: 0.833386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011779612302780151, AUC: 0.8550099999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001117554724216461, AUC: 0.5081295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006274791061878205, AUC: 0.8039589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006286765336990356, AUC: 0.827536\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000628991961479187, AUC: 0.836644\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016907118558883666, AUC: 0.5814725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007117418646812439, AUC: 0.8346469999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007231528460979462, AUC: 0.8660589999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 Class Capped SMOTE \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [0]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    print(cap)\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['print_loss'] = False\n",
    "    loss_fn_args['print_capped'] = False \n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                loss_fn_args['loss_cap'] = 10 / ((epoch + 1) / 2)\n",
    "                _, _ = train.train_sigmoid_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm, None]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "606da0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = (col_names))\n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "563f22d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0012810109853744506, AUC: 0.4544585\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'Tensor' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 21\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSigmoidFocalLoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     23\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:23\u001b[0m, in \u001b[0;36mtrain_sigmoid\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m output \u001b[38;5;241m=\u001b[39m network(data)\n\u001b[0;32m---> 23\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/loss_fns.py:20\u001b[0m, in \u001b[0;36mSigmoidFocalLoss.forward\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, targets):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torchvision\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39msigmoid_focal_loss(\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m, targets, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'Tensor' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# 2 CLASS Focal Loss\n",
    "# this code might not work \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SigmoidFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d438d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5f6eb22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf2\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7924e1f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006921598315238952, AUC: 0.5632135\n",
      "\n",
      "Loss before cap\n",
      "tensor([0.6831, 0.7459, 0.5364, 0.7497, 0.6382, 0.6398, 0.6786, 0.6400, 0.6401,\n",
      "        0.7816, 0.5922, 0.6889, 0.8030, 0.6779, 0.7631, 0.6872, 0.5974, 0.6980,\n",
      "        0.7272, 0.7339, 0.7323, 0.6067, 0.6251, 0.5210, 0.7205, 0.8327, 0.6379,\n",
      "        0.6990, 0.6212, 0.4908, 0.7118, 0.6685, 0.5900, 0.6023, 0.5205, 0.7086,\n",
      "        0.6193, 0.6757, 0.7100, 0.6086, 0.5149, 0.6846, 0.7233, 0.5548, 0.5982,\n",
      "        0.5765, 0.6585, 0.7136, 0.7176, 0.6636, 0.6148, 0.5052, 0.7105, 0.6931,\n",
      "        0.6034, 0.7542, 0.6064, 0.6969, 0.6078, 0.6080, 0.6383, 0.7433, 0.6470,\n",
      "        0.5520], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        1., 0., 1., 0., 1., 1., 1., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1.\n",
      " 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 2.1579,    nan, 1.2739,    nan,    nan,    nan,    nan,    nan,\n",
      "        1.0214,    nan, 1.5438, 1.0787,    nan, 1.7746,    nan,    nan, 1.2351,\n",
      "           nan, 1.8036, 2.3029,    nan,    nan,    nan,    nan, 1.6781,    nan,\n",
      "        1.7715,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.1300,\n",
      "           nan,    nan, 1.0166,    nan,    nan,    nan, 1.2885,    nan,    nan,\n",
      "           nan,    nan, 1.3378, 2.9578,    nan, 2.1528,    nan, 1.3932,    nan,\n",
      "           nan, 1.3675,    nan, 1.1432,    nan,    nan,    nan, 1.0213,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6284, 0.6326, 0.7166, 0.8558, 0.6018, 0.8419, 0.7810, 0.7182, 0.6671,\n",
      "        0.7296, 0.5275, 0.6001, 0.9214, 0.6683, 0.7445, 0.7128, 0.7619, 0.6269,\n",
      "        0.6085, 0.6900, 0.6682, 0.7153, 0.6448, 0.7235, 0.7118, 0.8081, 0.7448,\n",
      "        0.6017, 0.6866, 0.4971, 0.6344, 0.6922, 0.7478, 0.6766, 0.5336, 0.6953,\n",
      "        0.7440, 0.7062, 0.6859, 0.7706, 0.6719, 0.5437, 0.6233, 0.6613, 0.6371,\n",
      "        0.6355, 0.5208, 0.5996, 0.7773, 0.6754, 0.7489, 0.7360, 0.5466, 0.6566,\n",
      "        0.6697, 0.7545, 0.6887, 0.6568, 0.6468, 0.6412, 0.7067, 0.6816, 0.6428,\n",
      "        0.7305], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 0.9693, 2.1278,    nan, 2.1410, 2.4992,    nan,    nan,\n",
      "        0.9181,    nan,    nan, 2.7047,    nan, 1.3065, 1.0615, 1.2417,    nan,\n",
      "           nan,    nan,    nan, 0.9531,    nan, 0.8647, 1.4704, 1.9191, 1.0183,\n",
      "           nan,    nan,    nan,    nan,    nan, 1.6488,    nan,    nan,    nan,\n",
      "        3.0393, 1.0866, 1.7479, 2.0201,    nan,    nan,    nan, 1.2628,    nan,\n",
      "           nan,    nan,    nan, 1.4437,    nan, 2.7338, 0.9063,    nan,    nan,\n",
      "           nan, 1.2688, 1.2747,    nan,    nan,    nan, 1.5132,    nan,    nan,\n",
      "        1.3508], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6343, 0.7311, 0.6376, 0.6615, 0.6250, 0.6768, 0.6595, 0.7754, 0.8138,\n",
      "        0.6732, 0.7151, 0.5773, 0.6690, 0.5779, 0.7135, 0.6996, 0.7251, 0.7093,\n",
      "        0.7659, 0.7094, 0.5918, 0.8854, 0.6445, 0.6989, 0.6842, 0.8208, 0.7184,\n",
      "        0.7486, 0.7101, 0.7312, 0.6434, 0.8590, 0.5794, 0.5201, 0.8326, 0.5809,\n",
      "        0.7507, 0.7134, 0.7164, 0.7612, 0.6610, 0.6182, 0.6704, 0.6376, 0.7128,\n",
      "        0.7419, 0.7962, 0.6574, 0.7133, 0.7158, 0.6037, 0.6952, 0.7719, 0.6914,\n",
      "        0.6367, 0.7253, 0.6657, 0.6201, 0.7191, 0.5634, 0.7114, 0.7598, 0.7495,\n",
      "        0.6572], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 0., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.1580,    nan,    nan,    nan,    nan,    nan, 1.5501, 3.1286,\n",
      "           nan, 2.0660,    nan, 1.7870,    nan, 1.6301, 1.1375, 1.2407, 1.6259,\n",
      "        1.4352, 0.9657,    nan, 2.2998,    nan,    nan, 1.4819, 1.2521, 1.3888,\n",
      "        1.6097, 2.0324, 2.7085, 2.7188, 2.7626,    nan,    nan, 2.0445,    nan,\n",
      "        1.1093, 1.3019, 1.0204, 2.5057,    nan,    nan,    nan,    nan, 0.9500,\n",
      "        1.8736, 1.3249, 1.2668, 1.2341, 1.8731,    nan, 2.3377, 1.3838, 1.4135,\n",
      "           nan, 1.2449,    nan,    nan, 1.7886,    nan, 1.0733, 1.7101, 1.1011,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7833, 0.5282, 0.5316, 0.6680, 0.6571, 0.6862, 0.7362, 0.7583, 0.5898,\n",
      "        0.8151, 0.7067, 0.5099, 0.8979, 0.6343, 0.7498, 0.6842, 0.6525, 0.8035,\n",
      "        0.7615, 0.6847, 0.6504, 0.8426, 0.7240, 0.5781, 0.7308, 0.5840, 0.7145,\n",
      "        0.7305, 0.7129, 0.7466, 0.6612, 0.7419, 0.7976, 0.6312, 0.7200, 0.5808,\n",
      "        0.5884, 0.7670, 0.6382, 0.7894, 0.7658, 0.6272, 0.7234, 0.7096, 0.6961,\n",
      "        0.8182, 0.7754, 0.7021, 0.6083, 0.7268, 0.7391, 0.7391, 0.7302, 0.7752,\n",
      "        0.7429, 0.6040, 0.7569, 0.7168, 0.5774, 0.7562, 0.6545, 0.5696, 0.7616,\n",
      "        0.6389], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 0., 1., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0.\n",
      " 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.2984, 2.5525, 2.9199, 4.0865, 0.0000, 2.9676, 1.3985, 3.4296, 3.3712,\n",
      "        1.6399, 1.7867, 3.7598, 4.1661, 1.6141, 1.3759, 1.2884, 1.4531, 1.6762,\n",
      "        2.0665, 0.9382, 3.3416, 1.7585, 1.9609, 3.3166, 1.1825, 2.4039, 1.4907,\n",
      "        1.0096, 1.9025, 1.5767, 2.2295, 2.3715, 1.7700, 2.6462, 0.9815, 1.9285,\n",
      "        2.7496, 1.5725, 1.5702, 2.3733, 1.5671, 2.7240, 1.2895, 0.9639, 1.5140,\n",
      "        1.5124, 3.1174, 2.8396, 2.5467, 1.0118, 2.1895, 1.3566, 1.0053, 1.0231,\n",
      "        1.2427, 2.5387, 1.3266, 1.2624, 3.1939, 1.4725, 1.4662, 2.0737, 2.0423,\n",
      "        2.8315], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6903, 0.6799, 0.7210, 0.6980, 0.6437, 0.8123, 0.7029, 0.6140, 0.7433,\n",
      "        0.5757, 0.6750, 0.7402, 0.6342, 0.6535, 0.7392, 0.7465, 0.7933, 0.7587,\n",
      "        0.6262, 0.6413, 0.6627, 0.5516, 0.6423, 0.6214, 0.5430, 0.6638, 0.6264,\n",
      "        0.7683, 0.7524, 0.6945, 0.6074, 0.7512, 0.5728, 0.6644, 0.6429, 0.5752,\n",
      "        0.7371, 0.6271, 0.7361, 0.6437, 0.6798, 0.7362, 0.8179, 0.7430, 0.5748,\n",
      "        0.5531, 0.6259, 0.6749, 0.7164, 0.6640, 0.5925, 0.6673, 0.7097, 0.6028,\n",
      "        0.7024, 0.6519, 0.6492, 0.7378, 0.6473, 0.7169, 0.6285, 0.7345, 0.5664,\n",
      "        0.5999], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
      "        0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 0., 1., 0., 1., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.9438, 0.8807,    nan,    nan, 2.8609, 1.1071,    nan, 0.9342,\n",
      "           nan,    nan, 0.8944,    nan,    nan, 1.0858, 1.4259, 1.8712, 1.2875,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "        1.3286, 0.9496,    nan,    nan, 0.7127,    nan, 3.8867,    nan,    nan,\n",
      "        1.0683,    nan, 0.9914,    nan,    nan, 1.0978, 1.3738, 0.6471,    nan,\n",
      "           nan,    nan,    nan, 0.9454,    nan,    nan,    nan,    nan,    nan,\n",
      "        1.8026, 1.6167,    nan, 0.8649,    nan, 1.2705,    nan, 1.0720,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6401, 0.7080, 0.6124, 0.6722, 0.7169, 0.6338, 0.7213, 0.8479, 0.7107,\n",
      "        0.5792, 0.6343, 0.6769, 0.7181, 0.6499, 0.7218, 0.6978, 0.7961, 0.8412,\n",
      "        0.6053, 0.7098, 0.7375, 0.7903, 0.7479, 0.8070, 0.7226, 0.6277, 0.6975,\n",
      "        0.7938, 0.7039, 0.7714, 0.6350, 0.7752, 0.7214, 0.7520, 0.7148, 0.5909,\n",
      "        0.5461, 0.6440, 0.6060, 0.5523, 0.6850, 0.6297, 0.7078, 0.6569, 0.5881,\n",
      "        0.7878, 0.6574, 0.7269, 0.7168, 0.6304, 0.5501, 0.7117, 0.7449, 0.7224,\n",
      "        0.6758, 0.7017, 0.6682, 0.7491, 0.6614, 0.7293, 0.5576, 0.6335, 0.6632,\n",
      "        0.6039], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.0819,    nan,    nan, 1.4889,    nan, 2.0125, 3.9010,    nan,\n",
      "           nan,    nan,    nan, 1.2863,    nan, 1.1336, 1.0819, 1.3927, 1.5065,\n",
      "           nan, 1.1191, 1.1759, 1.8558, 1.1838, 1.5995,    nan,    nan, 1.2603,\n",
      "        2.3063, 1.4626, 2.0289,    nan, 1.7254, 1.2852, 1.2514,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan, 2.2760,    nan,    nan,\n",
      "        2.3632, 1.6570, 1.9608, 1.3655,    nan,    nan, 1.1177, 2.1182, 0.9105,\n",
      "           nan, 1.5148, 2.0249, 2.0739,    nan, 1.2694,    nan,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6309, 0.6555, 0.6986, 0.6892, 0.7397, 0.7505, 0.7308, 0.7991, 0.6277,\n",
      "        0.5693, 0.6334, 0.7251, 0.6714, 0.8030, 0.5440, 0.7853, 0.7093, 0.6191,\n",
      "        0.6812, 0.6781, 0.8316, 0.7828, 0.5562, 0.6079, 0.6813, 0.7128, 0.7274,\n",
      "        0.6358, 0.7251, 0.7068, 0.6122, 0.7195, 0.5217, 0.7123, 0.6978, 0.7392,\n",
      "        0.6782, 0.6773, 0.4774, 0.7218, 0.5941, 0.6179, 0.8107, 0.7727, 0.6035,\n",
      "        0.7722, 0.7668, 0.7200, 0.7331, 0.5983, 0.6187, 0.7459, 0.6992, 0.6120,\n",
      "        0.5806, 0.6079, 0.7191, 0.6411, 0.6118, 0.6342, 0.6755, 0.7191, 0.7296,\n",
      "        0.7221], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.8685, 3.0334, 1.5953, 1.3514, 1.2914, 1.4896, 1.3647, 1.4351, 2.2023,\n",
      "        3.3195, 3.0573, 1.1677, 2.5663, 1.4662, 3.6810, 2.3874, 0.9721, 2.1686,\n",
      "        1.7936, 0.0000, 1.9000, 1.6000, 3.1316, 2.1356, 2.6424, 1.1890, 1.0417,\n",
      "        2.3795, 1.1984, 1.2649, 2.1710, 1.6529, 2.4886, 2.6868, 1.6396, 2.1305,\n",
      "        2.3003, 1.7984, 2.6004, 0.9452, 3.2557, 2.5009, 1.2829, 1.5665, 2.2780,\n",
      "        1.0589, 1.6801, 1.4154, 1.1624, 1.8391, 2.0968, 1.5606, 1.9800, 1.7197,\n",
      "        3.8758, 2.0191, 1.1072, 1.9422, 2.9917, 1.4188, 1.7057, 2.2211, 3.1288,\n",
      "        1.2844], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7498, 0.8218, 0.6781, 0.6413, 0.7590, 0.7319, 0.7242, 0.6876, 0.5775,\n",
      "        0.6113, 0.7598, 0.7943, 0.7378, 0.6929, 0.7043, 0.6357, 0.7991, 0.7432,\n",
      "        0.6346, 0.6055, 0.7436, 0.7022, 0.7083, 0.7228, 0.6062, 0.6448, 0.6031,\n",
      "        0.6850, 0.7072, 0.5958, 0.7247, 0.7027, 0.7254, 0.7894, 0.7480, 0.7095,\n",
      "        0.7117, 0.7895, 0.6832, 0.7287, 0.6323, 0.7454, 0.7336, 0.7289, 0.7000,\n",
      "        0.7499, 0.5874, 0.6578, 0.7350, 0.6030, 0.7048, 0.6206, 0.7041, 0.7090,\n",
      "        0.7484, 0.6991, 0.6455, 0.6532, 0.6110, 0.7372, 0.6806, 0.7731, 0.6602,\n",
      "        0.7242], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 1., 1., 1., 0., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.5593, 1.6384, 2.4840, 1.8793, 1.5511, 0.9378, 1.1242, 2.1623, 3.6102,\n",
      "        3.4105, 1.7284, 2.2454, 1.0398, 2.6073, 0.8655, 3.3112, 0.8629, 1.5031,\n",
      "        2.7279, 2.4052, 1.4487, 1.3464, 1.2603, 1.1359, 0.0000, 3.2797, 2.8027,\n",
      "        3.0588, 0.9502, 3.5580, 2.6219, 1.7012, 2.2424, 1.3358, 1.1268, 3.0723,\n",
      "        0.9611, 1.3725, 1.1679, 1.0232, 2.8417, 2.3091, 1.2092, 0.8878, 3.2734,\n",
      "        3.0868, 3.1355, 3.6668, 0.6762, 2.6009, 1.4559, 2.6741, 3.0501, 1.1921,\n",
      "        1.0489, 1.0225, 2.0831, 2.2262, 3.6542, 1.8517, 1.3676, 1.0913, 2.7663,\n",
      "        1.1100], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6646, 0.7216, 0.7038, 0.7404, 0.7057, 0.6167, 0.8136, 0.7865, 0.7121,\n",
      "        0.6362, 0.7792, 0.6658, 0.7411, 0.7872, 0.6128, 0.6136, 0.7216, 0.7309,\n",
      "        0.8734, 0.6258, 0.5447, 0.6479, 0.7391, 0.5802, 0.6492, 0.5697, 0.5335,\n",
      "        0.6083, 0.5742, 0.5543, 0.7023, 0.7760, 0.7751, 0.5247, 0.7793, 0.6868,\n",
      "        0.7275, 0.6229, 0.7283, 0.6707, 0.6055, 0.6852, 0.6719, 0.6033, 0.7330,\n",
      "        0.6453, 0.6787, 0.6461, 0.5736, 0.7323, 0.7309, 0.7057, 0.6168, 0.5916,\n",
      "        0.7607, 0.7903, 0.8047, 0.6523, 0.7672, 0.5968, 0.7061, 0.6723, 0.6358,\n",
      "        0.6970], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 1., 1., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.2406, 1.1753, 1.4304, 1.2293, 0.9581,    nan, 1.6736, 1.0688, 1.3384,\n",
      "           nan, 1.5681,    nan, 1.1073, 1.3144,    nan,    nan, 1.5927, 1.3367,\n",
      "        2.8070,    nan,    nan,    nan, 1.7431,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan, 1.8799, 2.8469,    nan, 2.8929, 1.2668,\n",
      "        0.9114,    nan, 1.3394, 2.6776,    nan,    nan, 1.5707,    nan, 1.1357,\n",
      "           nan,    nan,    nan,    nan, 0.9752, 2.3292, 1.1052,    nan,    nan,\n",
      "        2.1223, 1.3619, 2.3338,    nan, 1.4613,    nan,    nan, 1.2142,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.7145, 0.6344, 0.5506, 0.7495, 0.7407, 0.6616, 0.6919, 0.7536, 0.6877,\n",
      "        0.5829, 0.6430, 0.7394, 0.6798, 0.7321, 0.6201, 0.6801, 0.5896, 0.6265,\n",
      "        0.6878, 0.6842, 0.6186, 0.8466, 0.7698, 0.7747, 0.8332, 0.6185, 0.7476,\n",
      "        0.6019, 0.8067, 0.5933, 0.6862, 0.6779, 0.6503, 0.7643, 0.7107, 0.7879,\n",
      "        0.7026, 0.6814, 0.6751, 0.7027, 0.5822, 0.7281, 0.8299, 0.7485, 0.7020,\n",
      "        0.7527, 0.6714, 0.5699, 0.5841, 0.8051, 0.6592, 0.8382, 0.6943, 0.6130,\n",
      "        0.6624, 0.6264, 0.7296, 0.6505, 0.6685, 0.7520, 0.6091, 0.6592, 0.5673,\n",
      "        0.7971], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 0., 1., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.1853, 2.3070,    nan, 1.1451, 1.1173,    nan,    nan, 1.4066,    nan,\n",
      "           nan,    nan, 1.7932, 2.3864, 1.1510,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan, 1.8398, 1.4984, 1.5767, 1.8636,    nan, 2.3755,\n",
      "           nan, 1.8335,    nan,    nan,    nan,    nan, 1.3255,    nan, 1.3279,\n",
      "        2.0168,    nan,    nan,    nan,    nan, 1.1774, 1.7585, 1.7023, 0.9081,\n",
      "        1.1680,    nan,    nan,    nan, 1.9395,    nan, 1.8251,    nan,    nan,\n",
      "           nan,    nan, 1.3496,    nan,    nan, 1.6744,    nan,    nan,    nan,\n",
      "        1.6841], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6792, 0.7073, 0.6902, 0.6717, 0.7652, 0.7495, 0.7267, 0.6509, 0.7291,\n",
      "        0.6138, 0.7150, 0.5857, 0.7535, 0.7625, 0.6478, 0.6709, 0.6231, 0.7511,\n",
      "        0.6386, 0.5501, 0.6235, 0.7475, 0.7196, 0.6139, 0.6906, 0.7730, 0.7614,\n",
      "        0.6262, 0.6533, 0.7520, 0.7312, 0.5594, 0.6350, 0.7147, 0.6715, 0.7683,\n",
      "        0.6154, 0.6470, 0.7344, 0.6649, 0.7205, 0.7135, 0.6385, 0.7687, 0.7885,\n",
      "        0.7562, 0.5510, 0.6010, 0.7084, 0.5597, 0.6136, 0.7052, 0.7869, 0.7530,\n",
      "        0.7572, 0.7632, 0.7159, 0.7008, 0.6071, 0.6019, 0.6480, 0.7677, 0.6391,\n",
      "        0.6440], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 1., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.7062,    nan, 1.0025,    nan, 1.2798, 1.6208, 1.5968,    nan, 1.4786,\n",
      "           nan, 1.2154,    nan, 1.6043, 1.6897,    nan,    nan,    nan, 1.0188,\n",
      "           nan,    nan,    nan, 1.1073, 1.4784,    nan, 1.7947, 1.8299, 1.4535,\n",
      "           nan,    nan, 1.2128, 1.2222,    nan, 2.2596, 1.0467,    nan, 1.5012,\n",
      "           nan, 2.0595, 1.2727,    nan, 1.2893,    nan,    nan, 2.2081, 2.9516,\n",
      "        1.5252,    nan,    nan, 1.0255,    nan,    nan,    nan, 1.7788, 1.2271,\n",
      "        0.9205, 1.1973, 1.3205,    nan,    nan,    nan,    nan, 1.5575,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7559, 0.7602, 0.7451, 0.7731, 0.6783, 0.6801, 0.7625, 0.6928, 0.6994,\n",
      "        0.7492, 0.7904, 0.6431, 0.7237, 0.7046, 0.5247, 0.7122, 0.5490, 0.6597,\n",
      "        0.7751, 0.7656, 0.7296, 0.7325, 0.7573, 0.5259, 0.5814, 0.6014, 0.5842,\n",
      "        0.7912, 0.6694, 0.6431, 0.8391, 0.7345, 0.6065, 0.6905, 0.6111, 0.8057,\n",
      "        0.6987, 0.5703, 0.7576, 0.5817, 0.6972, 0.6153, 0.8173, 0.6961, 0.7374,\n",
      "        0.7008, 0.5624, 0.5669, 0.6182, 0.7272, 0.5791, 0.6376, 0.6185, 0.6201,\n",
      "        0.7072, 0.7429, 0.5280, 0.7743, 0.7293, 0.6562, 0.7285, 0.7690, 0.5796,\n",
      "        0.5824], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0.\n",
      " 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.8148, 1.2651, 2.1549, 1.5350,    nan, 1.2009, 1.4832, 1.0793, 1.3700,\n",
      "        0.9848, 1.2869,    nan, 1.0446, 1.4806,    nan, 1.3288,    nan,    nan,\n",
      "        1.9070, 2.3173, 2.0182, 3.2920, 1.3118,    nan,    nan,    nan,    nan,\n",
      "        1.3514,    nan,    nan, 2.3413, 0.8665,    nan,    nan,    nan, 1.5679,\n",
      "           nan,    nan, 1.1941,    nan, 1.5825,    nan, 2.8870, 1.2098, 1.9116,\n",
      "        1.0448,    nan,    nan,    nan, 1.2431,    nan,    nan,    nan,    nan,\n",
      "           nan, 1.8367,    nan, 1.8183, 1.3779,    nan, 1.6478, 2.0316,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7607, 0.6527, 0.7201, 0.5839, 0.8073, 0.7004, 0.7535, 0.7373, 0.6360,\n",
      "        0.6427, 0.6716, 0.7821, 0.5553, 0.6224, 0.7080, 0.6873, 0.7507, 0.7354,\n",
      "        0.5684, 0.8164, 0.6116, 0.6222, 0.6278, 0.7956, 0.7465, 0.6275, 0.5947,\n",
      "        0.6037, 0.7450, 0.5744, 0.7278, 0.7405, 0.6114, 0.7209, 0.7040, 0.6433,\n",
      "        0.5177, 0.7199, 0.6396, 0.5174, 0.6394, 0.7808, 0.7158, 0.7620, 0.6708,\n",
      "        0.7259, 0.6265, 0.5279, 0.7265, 0.6092, 0.6618, 0.6705, 0.7531, 0.6243,\n",
      "        0.7646, 0.7547, 0.5925, 0.7801, 0.7571, 0.6398, 0.7237, 0.6033, 0.6742,\n",
      "        0.6928], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.4677, 2.5010, 1.4295, 2.0172, 1.1376, 1.0410, 1.2153, 1.8968, 1.4880,\n",
      "        3.5639, 2.0364, 1.5218, 3.7291, 3.1471, 1.0089, 1.9947, 0.9360, 1.2217,\n",
      "        1.9832, 1.8779, 2.4064, 2.2959, 1.8743, 2.2541, 1.4370, 3.5211, 2.8278,\n",
      "        2.5382, 1.1365, 3.2829, 1.5051, 1.3610, 1.9125, 1.8271, 1.9185, 1.8073,\n",
      "        2.2191, 0.8981, 2.8453, 2.8798, 2.1968, 2.9190, 0.9424, 1.6243, 1.5508,\n",
      "        1.7289, 2.2183, 3.1192, 1.0541, 3.3239, 0.0000, 3.1811, 1.1048, 2.6299,\n",
      "        1.7338, 0.9852, 3.1536, 1.7352, 1.0196, 1.0027, 1.6266, 2.1395, 1.0590,\n",
      "        1.2746], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6644, 0.6979, 0.6045, 0.6159, 0.6385, 0.5114, 0.6482, 0.8083, 0.5389,\n",
      "        0.6893, 0.5870, 0.7585, 0.6598, 0.7179, 0.6077, 0.7034, 0.7995, 0.6538,\n",
      "        0.7208, 0.7420, 0.6996, 0.6586, 0.7302, 0.7320, 0.6973, 0.7259, 0.6845,\n",
      "        0.6425, 0.7477, 0.7604, 0.6382, 0.5085, 0.7084, 0.6944, 0.7305, 0.7549,\n",
      "        0.7085, 0.6387, 0.8124, 0.7429, 0.7143, 0.7349, 0.5728, 0.6309, 0.7099,\n",
      "        0.7239, 0.5789, 0.7628, 0.6374, 0.6805, 0.7375, 0.7460, 0.6626, 0.7236,\n",
      "        0.7324, 0.6801, 0.7295, 0.7063, 0.7435, 0.6058, 0.6674, 0.6896, 0.6353,\n",
      "        0.8345], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1.\n",
      " 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.5955, 1.2969,    nan,    nan,    nan,    nan,    nan, 1.0064,    nan,\n",
      "        0.7462,    nan,    nan,    nan, 0.9320,    nan, 2.0256, 1.9194,    nan,\n",
      "        1.4872, 2.0543,    nan,    nan, 0.9965, 1.1048, 1.0866, 1.0995, 1.6301,\n",
      "           nan, 1.3789, 1.5019,    nan,    nan, 0.9799, 1.0817, 1.5295, 2.8884,\n",
      "        0.9062,    nan, 1.9350, 0.9042, 1.1568, 2.4045,    nan,    nan, 0.9895,\n",
      "        1.2165,    nan, 1.8904,    nan,    nan, 1.0604, 1.1203,    nan, 1.0451,\n",
      "        0.9077,    nan, 0.9674, 1.8494, 2.5613,    nan, 1.2646, 2.0214,    nan,\n",
      "        3.1216], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6138, 0.5268, 0.6880, 0.7315, 0.6230, 0.6028, 0.6631, 0.6331, 0.7102,\n",
      "        0.7182, 0.7450, 0.6643, 0.5758, 0.7928, 0.8095, 0.6030, 0.6774, 0.7261,\n",
      "        0.5734, 0.7480, 0.7428, 0.8370, 0.7451, 0.7051, 0.5835, 0.6409, 0.6342,\n",
      "        0.7216, 0.6432, 0.7527, 0.7340, 0.7425, 0.5896, 0.5959, 0.6258, 0.7121,\n",
      "        0.6730, 0.8217, 0.7003, 0.6609, 0.8178, 0.7247, 0.6292, 0.5715, 0.6993,\n",
      "        0.6872, 0.7551, 0.6861, 0.7585, 0.6297, 0.5403, 0.6101, 0.6491, 0.7340,\n",
      "        0.6689, 0.5647, 0.7515, 0.7404, 0.6770, 0.6711, 0.7303, 0.5951, 0.6502,\n",
      "        0.6926], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 1., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 2.2810, 1.3653, 2.3932,    nan,    nan,    nan, 0.9554,\n",
      "        1.0469, 1.2800, 2.9432,    nan, 1.5188, 1.6367,    nan,    nan, 1.0294,\n",
      "           nan, 0.9169, 1.4546, 1.7208, 0.9902, 1.5489,    nan,    nan,    nan,\n",
      "        1.6016,    nan, 1.7766, 1.4151, 1.2334,    nan,    nan, 3.4043, 0.9907,\n",
      "           nan, 1.6066, 1.0861,    nan, 1.8909, 1.5829,    nan,    nan, 1.0421,\n",
      "           nan, 1.9849, 1.2863, 1.3548,    nan,    nan,    nan, 1.5130,    nan,\n",
      "           nan,    nan, 1.6817, 1.1986, 1.4686,    nan, 1.2289,    nan,    nan,\n",
      "        2.0834], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7379, 0.5891, 0.6497, 0.7713, 0.5822, 0.7372, 0.8172, 0.7410, 0.7073,\n",
      "        0.7780, 0.6631, 0.6812, 0.6827, 0.7214, 0.6374, 0.6085, 0.5984, 0.7434,\n",
      "        0.6469, 0.5674, 0.7018, 0.7466, 0.6633, 0.6404, 0.6012, 0.7395, 0.7413,\n",
      "        0.6750, 0.8764, 0.7077, 0.5677, 0.7319, 0.7191, 0.5795, 0.7665, 0.6866,\n",
      "        0.7142, 0.6085, 0.7775, 0.7271, 0.8073, 0.7457, 0.5271, 0.5337, 0.6205,\n",
      "        0.7482, 0.6557, 0.7919, 0.7712, 0.6873, 0.6466, 0.7500, 0.6431, 0.6479,\n",
      "        0.6208, 0.7695, 0.6160, 0.7602, 0.7131, 0.6885, 0.7432, 0.5969, 0.7043,\n",
      "        0.6200], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 1., 0., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1.\n",
      " 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.1794, 2.6613, 3.2654, 1.8958, 3.0814, 1.7131, 1.8009, 1.4257, 1.2842,\n",
      "        1.7496, 1.7973, 1.1511, 2.9771, 1.7058, 2.7008, 2.9896, 2.8298, 1.2715,\n",
      "        2.0022, 3.1040, 1.0884, 1.5627, 2.7471, 2.9681, 3.2903, 1.5544, 1.3479,\n",
      "        3.1284, 2.6265, 2.3318, 2.9461, 1.0378, 1.2692, 3.2492, 1.2794, 2.5838,\n",
      "        1.8972, 3.3123, 1.2732, 1.7334, 1.5215, 1.0263, 3.5765, 3.0864, 2.5715,\n",
      "        1.8905, 0.0000, 1.6402, 1.9335, 2.1351, 3.1672, 1.3084, 2.6623, 1.7464,\n",
      "        3.1273, 1.6880, 2.8517, 1.6710, 2.2591, 2.5815, 0.9669, 2.4450, 0.9639,\n",
      "        2.6742], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6053, 0.6970, 0.5652, 0.5861, 0.7189, 0.5702, 0.6456, 0.6321, 0.6861,\n",
      "        0.6866, 0.7127, 0.6743, 0.5935, 0.6285, 0.7176, 0.7256, 0.6050, 0.7211,\n",
      "        0.6005, 0.6813, 0.7042, 0.8219, 0.5659, 0.6096, 0.6234, 0.6238, 0.8451,\n",
      "        0.7205, 0.7316, 0.5786, 0.6906, 0.7309, 0.6967, 0.6491, 0.7032, 0.7086,\n",
      "        0.5948, 0.7023, 0.7450, 0.7147, 0.5542, 0.7195, 0.6768, 0.7376, 0.6581,\n",
      "        0.6325, 0.7558, 0.5965, 0.7421, 0.5854, 0.5960, 0.6991, 0.6161, 0.7051,\n",
      "        0.7227, 0.8454, 0.8032, 0.6259, 0.6574, 0.8012, 0.7546, 0.7007, 0.7516,\n",
      "        0.7233], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 1., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan,    nan, 1.3541,    nan,    nan,    nan, 1.1479,\n",
      "        1.4126, 1.8470, 1.3376,    nan,    nan, 1.2818, 2.1900,    nan, 1.1295,\n",
      "           nan,    nan, 2.0322, 2.3706,    nan,    nan,    nan,    nan, 1.6759,\n",
      "           nan, 1.1911,    nan, 0.9727, 1.6930, 1.4981,    nan, 1.2144,    nan,\n",
      "           nan, 1.2491, 1.9026, 1.5097,    nan, 1.6985, 1.6742, 1.5415,    nan,\n",
      "           nan, 3.9797,    nan, 1.1257,    nan,    nan,    nan,    nan,    nan,\n",
      "        0.9833, 2.1167, 2.1308,    nan, 2.3350, 0.9538, 2.7815,    nan, 1.2388,\n",
      "        1.0537], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6806, 0.5686, 0.5665, 0.7227, 0.6260, 0.6983, 0.7559, 0.6937, 0.7007,\n",
      "        0.6876, 0.6300, 0.7342, 0.6941, 0.6978, 0.6505, 0.5793, 0.6980, 0.6897,\n",
      "        0.7165, 0.7450, 0.7252, 0.6938, 0.7608, 0.6670, 0.6893, 0.7120, 0.7077,\n",
      "        0.7382, 0.5941, 0.7146, 0.7196, 0.7550, 0.6398, 0.7242, 0.5616, 0.7093,\n",
      "        0.7369, 0.6550, 0.6151, 0.6944, 0.7558, 0.7678, 0.7146, 0.6323, 0.7571,\n",
      "        0.8189, 0.7419, 0.8262, 0.7477, 0.7185, 0.5328, 0.6697, 0.7455, 0.7860,\n",
      "        0.7691, 0.5966, 0.6802, 0.7472, 0.6293, 0.5090, 0.7646, 0.6997, 0.6919,\n",
      "        0.7613], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.1745,    nan,    nan, 1.1362,    nan, 0.9991, 1.0338, 1.8668, 1.0534,\n",
      "           nan,    nan, 1.5131, 0.8650, 0.8763,    nan,    nan,    nan,    nan,\n",
      "        2.4334, 1.1569, 1.0130,    nan, 1.3324,    nan, 1.2104, 1.3657, 0.9338,\n",
      "        1.2128,    nan, 1.1675, 1.4477, 1.6314,    nan, 1.4208,    nan,    nan,\n",
      "        0.7527,    nan,    nan, 1.5744, 0.9718, 2.2071,    nan,    nan, 1.2422,\n",
      "        1.8424, 2.0460, 1.9604, 1.5072,    nan,    nan,    nan, 1.1736, 1.9271,\n",
      "        0.9220,    nan,    nan, 0.8369,    nan,    nan, 1.1942, 0.8962,    nan,\n",
      "        1.7196], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7008, 0.6284, 0.7139, 0.4985, 0.6645, 0.7602, 0.5741, 0.7236, 0.7732,\n",
      "        0.6017, 0.7714, 0.6192, 0.8456, 0.7002, 0.6960, 0.8023, 0.4925, 0.6105,\n",
      "        0.6464, 0.6608, 0.5792, 0.6520, 0.5635, 0.5976, 0.7101, 0.6050, 0.6080,\n",
      "        0.7915, 0.7022, 0.7011, 0.6859, 0.6766, 0.6880, 0.5830, 0.8347, 0.7744,\n",
      "        0.8070, 0.6055, 0.5139, 0.7738, 0.5953, 0.6177, 0.7205, 0.7079, 0.6557,\n",
      "        0.6863, 0.6549, 0.5810, 0.8302, 0.6351, 0.6737, 0.6344, 0.6599, 0.5686,\n",
      "        0.7480, 0.7527, 0.6187, 0.6650, 0.8080, 0.7530, 0.7159, 0.7446, 0.7286,\n",
      "        0.6640], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 1., 1., 0., 0., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 1.0223,    nan, 2.1173, 0.8912,    nan, 1.4641, 1.1314,\n",
      "           nan, 1.0966,    nan, 2.1426, 1.3798,    nan, 1.7764,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan, 1.3203,    nan,    nan,\n",
      "        1.4145,    nan, 1.6303, 1.3979,    nan, 3.2490,    nan, 1.4928, 1.2414,\n",
      "        1.1658,    nan,    nan, 1.4526,    nan,    nan, 1.3058, 1.4565,    nan,\n",
      "        3.7488,    nan,    nan, 2.2360,    nan,    nan,    nan,    nan,    nan,\n",
      "        1.7794, 1.8682,    nan,    nan,    nan, 1.2575, 1.2428, 1.1479, 1.4163,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6547, 0.6826, 0.5298, 0.6430, 0.5477, 0.6769, 0.7792, 0.6990, 0.6361,\n",
      "        0.5893, 0.5245, 0.6931, 0.7236, 0.7550, 0.7054, 0.6321, 0.6991, 0.7688,\n",
      "        0.6232, 0.7773, 0.7176, 0.7330, 0.7456, 0.7015, 0.7224, 0.6469, 0.6459,\n",
      "        0.6448, 0.5558, 0.8279, 0.8029, 0.6464, 0.7887, 0.6738, 0.5844, 0.6588,\n",
      "        0.6274, 0.6631, 0.6877, 0.6031, 0.7052, 0.5826, 0.6412, 0.7870, 0.6335,\n",
      "        0.6554, 0.6809, 0.7911, 0.6817, 0.7529, 0.7266, 0.5933, 0.6204, 0.7444,\n",
      "        0.7123, 0.6600, 0.6454, 0.6770, 0.8086, 0.8002, 0.8117, 0.6493, 0.6787,\n",
      "        0.7717], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.8611, 0.0000, 3.2353, 1.9896, 1.5919, 1.8561, 2.4954, 1.6064, 2.2978,\n",
      "        2.4154, 3.1981, 1.5537, 1.8724, 1.3523, 1.6182, 2.0514, 1.2030, 1.3311,\n",
      "        3.2557, 2.6264, 1.0256, 1.8841, 1.6026, 1.1370, 1.0057, 1.7941, 1.3447,\n",
      "        2.9265, 2.9697, 2.2391, 2.4144, 2.2964, 2.7836, 1.5824, 2.2322, 2.6374,\n",
      "        1.3507, 2.5408, 0.8493, 2.0430, 1.8165, 3.1278, 1.5625, 1.2402, 3.3023,\n",
      "        2.2958, 1.5280, 1.8569, 2.2893, 1.0127, 1.4837, 2.9719, 3.3673, 1.0999,\n",
      "        0.9585, 1.5824, 1.9845, 3.5844, 2.7955, 0.9627, 1.9162, 3.2033, 1.5074,\n",
      "        1.1676], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6673, 0.6470, 0.6001, 0.8397, 0.5801, 0.8090, 0.6873, 0.5793, 0.6435,\n",
      "        0.6181, 0.6835, 0.7403, 0.7358, 0.6476, 0.6484, 0.6128, 0.5937, 0.6042,\n",
      "        0.7622, 0.5760, 0.7985, 0.7105, 0.7707, 0.6315, 0.5740, 0.6426, 0.7436,\n",
      "        0.7091, 0.6336, 0.6299, 0.6308, 0.8459, 0.6014, 0.6985, 0.7037, 0.7340,\n",
      "        0.7180, 0.8093, 0.7567, 0.6076, 0.6911, 0.7633, 0.8267, 0.6935, 0.7235,\n",
      "        0.6073, 0.6333, 0.6482, 0.7248, 0.6778, 0.7710, 0.6830, 0.6470, 0.7898,\n",
      "        0.6178, 0.4593, 0.6010, 0.7409, 0.7422, 0.7084, 0.7579, 0.7370, 0.5625,\n",
      "        0.7513], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan, 1.4701,    nan, 2.4479, 1.8440,    nan,    nan,\n",
      "           nan,    nan, 1.2286, 1.9088,    nan,    nan,    nan,    nan,    nan,\n",
      "        2.1177,    nan, 1.5222, 1.9217, 1.7724,    nan,    nan,    nan, 1.3465,\n",
      "        1.1542,    nan,    nan,    nan, 2.7626,    nan,    nan,    nan, 1.9048,\n",
      "        1.3107, 2.0510, 1.2736,    nan, 2.2171, 1.4738, 3.5389,    nan, 1.9730,\n",
      "           nan,    nan,    nan, 1.4730,    nan, 1.8996, 1.2475,    nan, 2.1518,\n",
      "           nan,    nan,    nan, 1.1892, 1.1302, 1.1555, 1.3590, 1.7459,    nan,\n",
      "        1.3440], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.6458, 0.6410, 0.7406, 0.7063, 0.6537, 0.5887, 0.6702, 0.5825, 0.5149,\n",
      "        0.7292, 0.5692, 0.6561, 0.5797, 0.6868, 0.6796, 0.7380, 0.7566, 0.6767,\n",
      "        0.6471, 0.4971, 0.6168, 0.6472, 0.5740, 0.7441, 0.7318, 0.5590, 0.7292,\n",
      "        0.6963, 0.6422, 0.5792, 0.6838, 0.6461, 0.7455, 0.7506, 0.6693, 0.5473,\n",
      "        0.6429, 0.6441, 0.7288, 0.7795, 0.6565, 0.6977, 0.5896, 0.7862, 0.6485,\n",
      "        0.6075, 0.7351, 0.6552, 0.7069, 0.6095, 0.6823, 0.7597, 0.8440, 0.7639,\n",
      "        0.7211, 0.5573, 0.7657, 0.6122, 0.7548, 0.6744, 0.8279, 0.7616, 0.7890,\n",
      "        0.6711], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
      "        1., 1., 0., 1., 0., 1., 0., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "        1.3206,    nan,    nan,    nan, 2.2324,    nan, 1.2642, 0.9304,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan, 1.5058, 0.9291,    nan,    nan,\n",
      "           nan,    nan,    nan, 1.6456,    nan, 1.3410, 1.0051, 1.7955,    nan,\n",
      "           nan,    nan, 0.9864, 1.6353,    nan,    nan,    nan, 1.6480,    nan,\n",
      "           nan, 1.2671,    nan, 1.9368,    nan,    nan, 1.3198, 2.0426, 1.4309,\n",
      "           nan,    nan, 1.7179,    nan, 1.6261,    nan, 2.0894, 1.2110, 1.5077,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6396, 0.6709, 0.5185, 0.6402, 0.5852, 0.6642, 0.7409, 0.6836, 0.6754,\n",
      "        0.7724, 0.6520, 0.6705, 0.5844, 0.7051, 0.7776, 0.7204, 0.6024, 0.8190,\n",
      "        0.6392, 0.6592, 0.5843, 0.7409, 0.6274, 0.8473, 0.6527, 0.7269, 0.7513,\n",
      "        0.6395, 0.7619, 0.7782, 0.6530, 0.5999, 0.6992, 0.7307, 0.6979, 0.6647,\n",
      "        0.7189, 0.6444, 0.7605, 0.7311, 0.7812, 0.7145, 0.5384, 0.5693, 0.6213,\n",
      "        0.7948, 0.5017, 0.7102, 0.7836, 0.6509, 0.6048, 0.6315, 0.7557, 0.7063,\n",
      "        0.6529, 0.7469, 0.6219, 0.6161, 0.8867, 0.5991, 0.6394, 0.5805, 0.7206,\n",
      "        0.7309], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 1., 1., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan,    nan,    nan,    nan, 1.2787, 1.6222,    nan,\n",
      "        1.1662,    nan, 1.4426,    nan, 1.2077, 1.7390, 1.7978,    nan, 1.8172,\n",
      "           nan,    nan,    nan, 0.8373,    nan, 1.5995,    nan, 1.0264, 1.2924,\n",
      "           nan, 1.1249, 1.2800,    nan,    nan, 1.3399,    nan,    nan,    nan,\n",
      "        1.3371,    nan, 0.9069, 0.9410, 1.7111, 2.4533,    nan,    nan,    nan,\n",
      "        1.4594,    nan,    nan, 1.9365,    nan,    nan,    nan, 1.0540, 2.7801,\n",
      "           nan, 1.6093,    nan,    nan, 3.1781,    nan,    nan,    nan, 1.0895,\n",
      "        1.4755], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7138, 0.7203, 0.5837, 0.7404, 0.6434, 0.7225, 0.7653, 0.7257, 0.6353,\n",
      "        0.6017, 0.6614, 0.8902, 0.7252, 0.7424, 0.5983, 0.6433, 0.5496, 0.5588,\n",
      "        0.7714, 0.8104, 0.6875, 0.6328, 0.7278, 0.6113, 0.6575, 0.7093, 0.5920,\n",
      "        0.7364, 0.7402, 0.6776, 0.6652, 0.7949, 0.5574, 0.7569, 0.7204, 0.7728,\n",
      "        0.7011, 0.7543, 0.5728, 0.6039, 0.8929, 0.7220, 0.6708, 0.6595, 0.5816,\n",
      "        0.7892, 0.7991, 0.6221, 0.7564, 0.6875, 0.7348, 0.7200, 0.6235, 0.6576,\n",
      "        0.6907, 0.5577, 0.6474, 0.7624, 0.6787, 0.7529, 0.7621, 0.5093, 0.7092,\n",
      "        0.6788], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0.\n",
      " 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0.\n",
      " 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([0.8296, 1.1507,    nan, 1.2566,    nan, 0.8703,    nan,    nan,    nan,\n",
      "           nan,    nan, 3.2854, 0.8332, 1.1548,    nan,    nan,    nan,    nan,\n",
      "        1.1349, 1.0474, 1.0592,    nan, 1.3474,    nan,    nan, 1.0581,    nan,\n",
      "        0.9222, 1.0014,    nan,    nan, 1.5777,    nan, 1.4135, 1.0116, 1.4641,\n",
      "        0.8571, 1.4022,    nan,    nan, 2.7390, 1.6707,    nan,    nan,    nan,\n",
      "        1.4990, 1.8661,    nan, 2.0917,    nan, 1.0591, 2.3356,    nan,    nan,\n",
      "           nan,    nan,    nan, 1.1091,    nan, 0.9653, 1.4165,    nan, 1.2651,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6383, 0.7171, 0.7444, 0.6915, 0.6770, 0.6607, 0.7763, 0.5517, 0.7100,\n",
      "        0.7346, 0.7143, 0.7945, 0.6260, 0.7890, 0.5274, 0.7337, 0.5982, 0.7212,\n",
      "        0.8210, 0.5070, 0.6499, 0.7641, 0.7583, 0.7898, 0.6148, 0.6662, 0.8056,\n",
      "        0.7132, 0.7259, 0.5787, 0.8211, 0.6889, 0.7560, 0.7837, 0.7055, 0.6269,\n",
      "        0.7025, 0.7597, 0.7276, 0.7133, 0.7254, 0.5252, 0.7089, 0.5955, 0.7446,\n",
      "        0.7213, 0.7342, 0.5519, 0.5966, 0.5941, 0.7256, 0.7011, 0.5690, 0.7254,\n",
      "        0.6282, 0.6346, 0.7791, 0.7628, 0.8335, 0.7060, 0.7248, 0.5790, 0.7258,\n",
      "        0.7132], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.6901, 1.4136, 2.4681, 0.0000, 4.0592, 2.5795, 1.8425, 2.9555, 0.9757,\n",
      "        1.7490, 1.0396, 2.1604, 2.1261, 2.6313, 2.8059, 1.1693, 3.5766, 1.9329,\n",
      "        1.6246, 3.1056, 2.3165, 2.0211, 1.8134, 1.9685, 2.7309, 1.6774, 1.4892,\n",
      "        1.0525, 0.9784, 2.4841, 2.3024, 1.1358, 1.4284, 1.4840, 0.9755, 3.3215,\n",
      "        1.1076, 1.8957, 0.9071, 1.1584, 1.7291, 3.4947, 1.1299, 2.4541, 1.4688,\n",
      "        3.6876, 1.2810, 2.5585, 4.1361, 2.2063, 1.2586, 1.2907, 3.1797, 3.5577,\n",
      "        2.2568, 2.7780, 1.3533, 1.0842, 1.2385, 1.4086, 2.5103, 2.3558, 2.9378,\n",
      "        1.6403], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6386, 0.6142, 0.7396, 0.6121, 0.6486, 0.6924, 0.6266, 0.6186, 0.7507,\n",
      "        0.7741, 0.7014, 0.6376, 0.6300, 0.6216, 0.5983, 0.7123, 0.8034, 0.8069,\n",
      "        0.6696, 0.5965, 0.7667, 0.6985, 0.6624, 0.6796, 0.6186, 0.7715, 0.7553,\n",
      "        0.6401, 0.6526, 0.7216, 0.6700, 0.7929, 0.7112, 0.7437, 0.7660, 0.6880,\n",
      "        0.7554, 0.6517, 0.7332, 0.7685, 0.7442, 0.5971, 0.6320, 0.9130, 0.7782,\n",
      "        0.6926, 0.5519, 0.5896, 0.7879, 0.6804, 0.6704, 0.6570, 0.5439, 0.5375,\n",
      "        0.4960, 0.6031, 0.6428, 0.6407, 0.6569, 0.7188, 0.6104, 0.8188, 0.6804,\n",
      "        0.5972], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.1034, 1.9695, 1.9158, 2.9237, 3.0278, 2.7468, 3.1054, 2.8894, 1.2382,\n",
      "        1.3460, 2.1454, 2.9721, 2.7896, 1.9488, 2.9960, 1.3885, 1.4053, 1.6081,\n",
      "        1.3149, 2.9058, 1.2027, 3.0446, 3.5416, 2.8269, 2.5182, 1.1657, 1.4363,\n",
      "        2.3336, 3.4964, 1.1860, 1.4162, 1.9429, 1.6629, 0.7656, 1.4587, 1.3337,\n",
      "        0.9014, 2.6168, 1.1345, 1.7397, 1.7710, 3.3460, 3.1678, 2.5581, 1.4285,\n",
      "        1.0654, 3.3829, 2.7202, 3.0206, 2.8258, 1.7033, 2.8299, 3.6136, 3.5015,\n",
      "        3.0488, 2.9522, 2.7709, 4.1317, 2.6860, 1.0741, 0.0000, 1.8930, 2.4812,\n",
      "        3.6901], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7258, 0.7681, 0.6087, 0.7548, 0.6605, 0.7264, 0.5976, 0.7514, 0.6678,\n",
      "        0.6729, 0.6095, 0.6108, 0.7156, 0.5723, 0.7275, 0.6974, 0.7271, 0.6168,\n",
      "        0.5878, 0.6279, 0.6212, 0.8069, 0.7366, 0.5694, 0.6151, 0.7555, 0.7429,\n",
      "        0.6916, 0.7971, 0.7750, 0.7108, 0.6569, 0.7690, 0.7091, 0.7173, 0.5950,\n",
      "        0.7523, 0.6786, 0.6153, 0.6929, 0.6204, 0.6108, 0.7501, 0.7690, 0.7414,\n",
      "        0.6641, 0.6943, 0.8335, 0.6195, 0.7529, 0.7114, 0.6234, 0.6850, 0.7438,\n",
      "        0.7510, 0.6280, 0.7054, 0.7526, 0.7610, 0.8305, 0.7289, 0.6799, 0.5321,\n",
      "        0.8056], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.\n",
      " 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.4528,    nan, 0.8744,    nan, 0.8713,    nan, 1.7952,    nan,\n",
      "        1.0274,    nan,    nan, 1.3732,    nan, 1.3251, 0.9483, 0.8685,    nan,\n",
      "           nan,    nan,    nan, 1.5069, 1.0570,    nan,    nan, 1.7249, 1.0014,\n",
      "        0.9378, 3.0013,    nan, 1.0325,    nan, 1.6591, 1.2215, 0.9196,    nan,\n",
      "        1.6219,    nan,    nan, 1.0115,    nan,    nan, 1.3918, 2.7340, 1.4385,\n",
      "           nan, 0.9318, 2.3477,    nan, 1.8037, 1.7327,    nan, 2.2906, 1.0354,\n",
      "        1.4926,    nan, 1.5301, 1.0965, 1.1390, 2.0475, 1.1027, 1.3096,    nan,\n",
      "        2.4838], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6279, 0.7708, 0.7454, 0.5738, 0.6938, 0.5725, 0.7006, 0.8513, 0.7522,\n",
      "        0.6808, 0.7529, 0.6613, 0.6493, 0.7926, 0.6215, 0.7394, 0.6751, 0.8149,\n",
      "        0.6356, 0.7454, 0.7144, 0.7348, 0.9135, 0.6693, 0.7641, 0.6993, 0.6569,\n",
      "        0.5630, 0.5781, 0.6251, 0.7283, 0.5951, 0.6174, 0.6821, 0.5379, 0.6079,\n",
      "        0.8102, 0.6887, 0.7571, 0.6324, 0.6876, 0.7662, 0.5790, 0.7935, 0.6330,\n",
      "        0.6202, 0.7737, 0.6349, 0.7537, 0.5851, 0.6901, 0.7179, 0.6451, 0.7005,\n",
      "        0.6792, 0.7475, 0.6564, 0.7617, 0.7594, 0.5371, 0.7444, 0.6326, 0.6582,\n",
      "        0.6720], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Distance\n",
      "tensor([2.2695, 1.8076, 1.6731, 2.8876, 2.2041, 1.7040, 1.6246, 2.2201, 1.1105,\n",
      "        0.8366, 1.6278, 1.7117, 3.5674, 1.9450, 2.5822, 1.3902, 1.1990, 1.4050,\n",
      "        2.3217, 1.0849, 1.9276, 1.2853, 2.9911, 2.1755, 1.1824, 1.5054, 1.7366,\n",
      "        2.5692, 1.6844, 1.8588, 1.6405, 1.4267, 2.8694, 1.8759, 2.0112, 1.9127,\n",
      "        1.2185, 1.9580, 1.5937, 2.5462, 2.1940, 1.4033, 2.0665, 1.2850, 1.9635,\n",
      "        2.0185, 1.1124, 2.9488, 2.5211, 2.8693, 2.2231, 1.0368, 2.0095, 1.3712,\n",
      "        2.1094, 1.2337, 2.4917, 0.9233, 1.4401, 2.7859, 2.4632, 1.7040, 1.7020,\n",
      "        1.0562], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7280, 0.7862, 0.5992, 0.7383, 0.6572, 0.6452, 0.7184, 0.6748, 0.7178,\n",
      "        0.6683, 0.7259, 0.6572, 0.7179, 0.6702, 0.7938, 0.5410, 0.7856, 0.7172,\n",
      "        0.7167, 0.8307, 0.6174, 0.7430, 0.6275, 0.5906, 0.6609, 0.6611, 0.7509,\n",
      "        0.7382, 0.6783, 0.7051, 0.7546, 0.5992, 0.7668, 0.5724, 0.7222, 0.6548,\n",
      "        0.4607, 0.7460, 0.7431, 0.8643, 0.4726, 0.7782, 0.6951, 0.5894, 0.6611,\n",
      "        0.6569, 0.8043, 0.6005, 0.6125, 0.6392, 0.7764, 0.6029, 0.5914, 0.6802,\n",
      "        0.5555, 0.7162, 0.6205, 0.6809, 0.6836, 0.7182, 0.6393, 0.5745, 0.5444,\n",
      "        0.7596], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.7707, 1.3777, 1.7494, 1.7386, 2.3538, 1.3227, 1.6266, 2.0090, 0.8571,\n",
      "        3.0989, 2.7637, 1.8662, 1.2606, 2.3316, 2.2047, 2.4480, 1.3143, 1.7700,\n",
      "        1.4470, 2.0491, 2.6288, 1.4058, 1.7333, 2.6493, 1.7212, 1.3195, 1.3866,\n",
      "        1.5527, 3.1728, 1.0446, 2.1624, 2.9170, 1.5333, 2.2763, 1.1083, 1.7508,\n",
      "        3.1899, 1.0423, 1.1953, 2.1848, 2.9358, 1.4611, 1.8228, 1.6888, 2.2779,\n",
      "        1.9570, 1.1837, 2.9624, 1.9273, 1.3227, 1.8828, 1.9224, 2.6077, 3.1453,\n",
      "        2.3414, 1.0132, 2.0656, 1.1136, 2.2799, 1.1383, 2.3081, 2.1574, 3.6872,\n",
      "        2.1155], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7601, 0.6764, 0.7141, 0.6341, 0.7372, 0.6045, 0.6697, 0.7376, 0.6009,\n",
      "        0.7073, 0.5475, 0.7418, 0.6036, 0.6605, 0.7459, 0.7441, 0.7863, 0.6599,\n",
      "        0.5404, 0.6735, 0.6619, 0.7348, 0.7194, 0.7105, 0.8211, 0.7198, 0.6312,\n",
      "        0.7196, 0.6616, 0.7141, 0.7203, 0.6447, 0.7044, 0.7447, 0.7472, 0.6877,\n",
      "        0.7614, 0.6328, 0.5640, 0.6596, 0.6739, 0.7583, 0.7300, 0.6745, 0.7542,\n",
      "        0.6792, 0.6730, 0.5647, 0.6005, 0.6172, 0.6741, 0.7231, 0.7232, 0.6509,\n",
      "        0.6289, 0.7012, 0.6252, 0.5291, 0.5568, 0.7758, 0.7299, 0.6084, 0.7174,\n",
      "        0.7433], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 0., 1., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0.\n",
      " 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.8105, 1.6078, 1.2295, 2.4259, 2.0791, 2.2600, 1.8902, 1.5550, 1.8343,\n",
      "        1.5950, 2.6983, 1.1359, 2.7218, 0.0000, 1.4294, 1.3673, 1.3787, 3.0640,\n",
      "        3.0234, 2.5785, 2.8197, 1.0618, 1.1733, 1.4434, 1.7143, 1.0134, 2.5809,\n",
      "        1.7335, 1.5730, 1.0045, 2.0253, 2.3508, 1.3275, 1.2931, 1.1190, 2.2711,\n",
      "        0.9270, 2.1300, 3.0507, 2.3708, 1.4422, 1.9983, 1.1676, 3.3570, 1.2872,\n",
      "        3.5770, 2.0448, 3.8593, 3.0373, 3.3752, 2.5148, 1.0618, 1.0297, 1.8939,\n",
      "        2.2565, 1.1905, 1.3957, 2.8030, 2.2537, 1.2564, 0.9817, 1.6477, 1.3083,\n",
      "        1.4135], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.7469, 0.5831, 0.5732, 0.6479, 0.5905, 0.6276, 0.5340, 0.7392, 0.7687,\n",
      "        0.6833, 0.7602, 0.6503, 0.7080, 0.5271, 0.7387, 0.6432, 0.6419, 0.7276,\n",
      "        0.6422, 0.7341, 0.5969, 0.5588, 0.7364, 0.6953, 0.6954, 0.7255, 0.6641,\n",
      "        0.6255, 0.7075, 0.7037, 0.6259, 0.5614, 0.5571, 0.6800, 0.7395, 0.6252,\n",
      "        0.7295, 0.7647, 0.6470, 0.6822, 0.6471, 0.7291, 0.7688, 0.6844, 0.6045,\n",
      "        0.7625, 0.4275, 0.7184, 0.7232, 0.7473, 0.6859, 0.7605, 0.7399, 0.7421,\n",
      "        0.6608, 0.7069, 0.6620, 0.5786, 0.5767, 0.6101, 0.4848, 0.7421, 0.6519,\n",
      "        0.6221], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0.\n",
      " 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.3584,    nan,    nan,    nan,    nan,    nan,    nan, 0.9994, 0.9873,\n",
      "           nan, 1.2743,    nan, 1.4518,    nan, 1.1919,    nan,    nan, 1.4595,\n",
      "           nan, 0.8254,    nan,    nan, 1.8975,    nan, 1.1588, 2.7583,    nan,\n",
      "           nan, 1.8524, 2.4834,    nan,    nan,    nan,    nan, 1.2235,    nan,\n",
      "        1.3732, 0.8605,    nan, 1.5172,    nan, 1.3693, 1.1106, 2.4161,    nan,\n",
      "        1.5671,    nan, 1.5026, 1.0497, 2.5230,    nan, 0.9973, 1.2311, 1.2365,\n",
      "           nan, 2.5115,    nan,    nan,    nan,    nan,    nan, 1.5084,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6278, 0.7364, 0.6661, 0.6060, 0.7052, 0.5939, 0.7219, 0.8612, 0.5690,\n",
      "        0.6045, 0.7080, 0.7705, 0.6747, 0.6285, 0.6910, 0.7770, 0.9120, 0.6424,\n",
      "        0.6266, 0.5594, 0.6643, 0.5784, 0.6610, 0.7062, 0.6800, 0.7347, 0.6806,\n",
      "        0.7526, 0.6464, 0.6385, 0.6593, 0.5932, 0.6695, 0.6507, 0.7261, 0.7080,\n",
      "        0.7527, 0.5350, 0.7508, 0.7395, 0.5703, 0.6141, 0.6400, 0.6857, 0.7808,\n",
      "        0.7552, 0.7241, 0.8510, 0.6438, 0.7000, 0.6630, 0.7556, 0.6902, 0.7429,\n",
      "        0.7413, 0.7054, 0.7612, 0.5630, 0.6330, 0.6031, 0.6692, 0.7538, 0.7615,\n",
      "        0.6105], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 1., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 1.4506,    nan, 1.0350,    nan,    nan, 3.2907,    nan,\n",
      "           nan, 1.2433, 1.1436,    nan,    nan,    nan, 1.1997, 2.4457,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan, 0.9937,    nan, 1.2145,    nan,\n",
      "        0.8825,    nan,    nan,    nan,    nan, 2.5414,    nan, 1.0234, 1.0679,\n",
      "        1.2838,    nan, 1.3975, 1.3064,    nan,    nan,    nan, 1.1782, 1.5498,\n",
      "        1.2152, 1.0650, 1.5533,    nan,    nan,    nan, 1.8372,    nan, 1.1808,\n",
      "        1.3122, 0.9918, 0.7596,    nan,    nan,    nan,    nan, 1.3733, 1.2971,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7301, 0.7910, 0.8286, 0.6233, 0.7342, 0.7586, 0.6967, 0.6255, 0.7176,\n",
      "        0.7040, 0.7562, 0.6229, 0.7092, 0.6725, 0.6370, 0.7179, 0.6611, 0.7599,\n",
      "        0.7308, 0.6149, 0.5852, 0.7018, 0.6137, 0.6733, 0.7558, 0.6424, 0.8048,\n",
      "        0.6889, 0.6989, 0.7147, 0.5589, 0.8411, 0.6594, 0.7672, 0.7485, 0.6329,\n",
      "        0.6240, 0.7429, 0.5962, 0.8461, 0.7360, 0.6291, 0.6760, 0.6359, 0.8215,\n",
      "        0.7472, 0.6388, 0.6826, 0.6578, 0.6044, 0.7563, 0.7473, 0.6807, 0.6054,\n",
      "        0.5433, 0.5748, 0.6943, 0.5777, 0.6673, 0.7524, 0.6873, 0.5806, 0.9361,\n",
      "        0.5978], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.2798, 1.9367, 1.9647,    nan, 2.7855, 2.1569, 1.0555,    nan, 1.2783,\n",
      "        1.6591, 1.8064,    nan, 1.2097, 2.3889,    nan, 1.1565,    nan,    nan,\n",
      "        1.0301,    nan,    nan, 1.0455,    nan,    nan, 1.5795,    nan, 1.5999,\n",
      "        1.0261, 1.2244, 2.0060,    nan, 1.8658,    nan, 1.9639, 1.7108,    nan,\n",
      "           nan, 0.9381,    nan,    nan, 0.9813,    nan, 1.9301,    nan, 1.5639,\n",
      "        1.5183,    nan,    nan,    nan,    nan, 1.0925, 1.6272,    nan,    nan,\n",
      "           nan,    nan, 1.9707,    nan,    nan,    nan, 1.0696,    nan, 2.8511,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6939, 0.7054, 0.5695, 0.7226, 0.7333, 0.7300, 0.5480, 0.6830, 0.6421,\n",
      "        0.8297, 0.5874, 0.7396, 0.7564, 0.6689, 0.6165, 0.7890, 0.7232, 0.6489,\n",
      "        0.6557, 0.6327, 0.7235, 0.7408, 0.6757, 0.5433, 0.6084, 0.7541, 0.7867,\n",
      "        0.6818, 0.7132, 0.8066, 0.6304, 0.7829, 0.6935, 0.5430, 0.7512, 0.6344,\n",
      "        0.6900, 0.7135, 0.5760, 0.7192, 0.6787, 0.6173, 0.7140, 0.6853, 0.7133,\n",
      "        0.6356, 0.5699, 0.6514, 0.5297, 0.8444, 0.7336, 0.6450, 0.6536, 0.7242,\n",
      "        0.7501, 0.6965, 0.6386, 0.8273, 0.7064, 0.7339, 0.7384, 0.7255, 0.6867,\n",
      "        0.6843], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([0.9862, 1.0044, 4.4975, 0.8340, 0.8305, 1.5223, 2.8928, 2.5539, 1.9528,\n",
      "        1.6215, 2.3511, 1.0066, 1.7875, 1.7767, 2.5552, 1.3528, 1.3329, 1.6863,\n",
      "        2.3746, 1.6206, 0.8937, 1.8927, 1.4441, 2.6529, 3.0849, 1.1977, 1.0523,\n",
      "        1.4333, 3.4732, 1.7245, 1.9678, 4.1964, 3.2941, 2.4157, 1.0814, 2.7803,\n",
      "        1.9519, 1.5351, 2.9667, 1.4881, 2.2695, 2.4995, 1.3763, 1.2202, 2.0481,\n",
      "        1.1978, 3.4363, 3.0488, 2.1727, 2.3203, 0.9578, 0.0000, 1.9108, 1.7270,\n",
      "        0.9831, 2.2653, 2.7419, 2.1973, 0.8319, 0.9412, 1.1978, 0.9126, 1.2504,\n",
      "        1.2888], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7194, 0.7792, 0.6927, 0.6525, 0.7761, 0.5462, 0.7776, 0.5186, 0.7892,\n",
      "        0.6713, 0.6421, 0.7094, 0.5347, 0.7707, 0.6522, 0.6993, 0.7297, 0.6893,\n",
      "        0.6956, 0.7525, 0.6316, 0.6039, 0.6258, 0.7162, 0.5854, 0.7261, 0.7929,\n",
      "        0.8284, 0.7433, 0.6173, 0.8002, 0.6188, 0.5926, 0.6134, 0.6085, 0.7079,\n",
      "        0.7340, 0.6166, 0.6252, 0.6740, 0.6918, 0.7069, 0.5725, 0.8077, 0.6894,\n",
      "        0.6527, 0.7240, 0.7307, 0.7160, 0.6242, 0.6779, 0.6717, 0.5699, 0.6848,\n",
      "        0.5678, 0.7215, 0.6857, 0.6627, 0.6883, 0.7995, 0.6810, 0.6128, 0.6544,\n",
      "        0.6195], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.2585, 1.4638,    nan,    nan, 1.3874,    nan, 1.4372,    nan, 1.9578,\n",
      "           nan,    nan, 1.2219,    nan, 1.4230,    nan,    nan,    nan,    nan,\n",
      "           nan, 1.3094,    nan,    nan,    nan, 0.9464,    nan, 0.9195, 1.9688,\n",
      "        2.1209, 2.3289,    nan, 2.1433,    nan,    nan,    nan,    nan, 0.9608,\n",
      "           nan,    nan, 2.2022,    nan, 1.0254, 1.0776,    nan, 1.2311, 1.2556,\n",
      "           nan, 1.1469, 1.1166, 1.3060,    nan, 1.2961, 2.1649,    nan,    nan,\n",
      "           nan, 1.1741,    nan,    nan,    nan, 2.0674,    nan,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6569, 0.6942, 0.7727, 0.5931, 0.6784, 0.7506, 0.5880, 0.7176, 0.7093,\n",
      "        0.7017, 0.7847, 0.7405, 0.7033, 0.5711, 0.6224, 0.7779, 0.8560, 0.7253,\n",
      "        0.7979, 0.6588, 0.6880, 0.7317, 0.8350, 0.6897, 0.7356, 0.5953, 0.7115,\n",
      "        0.7292, 0.6390, 0.7667, 0.5950, 0.5626, 0.8167, 0.6742, 0.7345, 0.6576,\n",
      "        0.6490, 0.7451, 0.8105, 0.7132, 0.7025, 0.7253, 0.6782, 0.7795, 0.6594,\n",
      "        0.6662, 0.7414, 0.7269, 0.6701, 0.6606, 0.6727, 0.6892, 0.6173, 0.7308,\n",
      "        0.5810, 0.7258, 0.4728, 0.7037, 0.6241, 0.6983, 0.5378, 0.7720, 0.7505,\n",
      "        0.7759], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 0., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 0.9522, 2.1075,    nan,    nan, 1.0303,    nan, 1.1198, 1.5599,\n",
      "        1.0618, 1.3931, 1.2730, 1.1547,    nan,    nan, 1.6643, 1.9867, 1.4462,\n",
      "        1.8932,    nan, 1.4665, 1.2237, 2.5575,    nan, 1.1563,    nan, 1.3270,\n",
      "        1.3712,    nan, 1.4491,    nan,    nan, 1.9319, 1.5875, 1.0236,    nan,\n",
      "           nan, 1.3915, 1.8363, 1.4153, 1.9721,    nan,    nan, 1.3550,    nan,\n",
      "           nan, 1.3640, 1.5784,    nan,    nan,    nan, 1.3774,    nan, 2.0117,\n",
      "           nan, 0.8758,    nan,    nan,    nan,    nan,    nan, 1.9269, 2.1865,\n",
      "        1.3490], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6122, 0.6235, 0.7466, 0.6983, 0.7570, 0.6294, 0.7629, 0.6685, 0.5734,\n",
      "        0.7136, 0.6858, 0.7656, 0.5978, 0.5686, 0.5475, 0.6875, 0.7704, 0.8817,\n",
      "        0.6831, 0.8423, 0.7004, 0.6906, 0.6815, 0.7446, 0.7233, 0.6744, 0.7580,\n",
      "        0.7478, 0.6023, 0.7036, 0.7010, 0.7604, 0.6436, 0.7088, 0.5924, 0.6948,\n",
      "        0.7500, 0.7811, 0.5225, 0.6434, 0.7202, 0.7245, 0.6666, 0.7890, 0.7026,\n",
      "        0.7690, 0.6751, 0.6147, 0.7438, 0.7127, 0.6549, 0.6773, 0.5519, 0.6760,\n",
      "        0.6564, 0.7402, 0.6011, 0.7748, 0.6062, 0.5567, 0.8881, 0.5220, 0.6686,\n",
      "        0.5821], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 1., 0., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0.\n",
      " 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 0.9618,    nan, 1.3623,    nan, 1.1458,    nan,    nan,\n",
      "        1.0747, 1.2342, 2.0679,    nan,    nan,    nan, 1.2682, 1.5114, 2.8659,\n",
      "           nan, 1.4697,    nan, 2.0288, 0.9931, 2.4471, 1.0496,    nan, 1.5357,\n",
      "        0.9107,    nan, 0.9308, 1.1529, 1.0544,    nan, 0.9025,    nan, 1.5127,\n",
      "        1.3156, 1.1328,    nan,    nan, 0.9283, 1.4213,    nan, 1.3609, 2.5751,\n",
      "           nan, 1.0786,    nan, 2.2212, 1.7210,    nan,    nan,    nan, 1.4038,\n",
      "           nan, 1.3793,    nan, 1.5014,    nan,    nan, 2.8702,    nan, 1.2748,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7071, 0.5242, 0.5726, 0.6877, 0.5161, 0.7546, 0.7542, 0.6951, 0.7156,\n",
      "        0.7102, 0.6851, 0.6286, 0.7593, 0.6672, 0.8273, 0.5524, 0.6901, 0.5737,\n",
      "        0.7750, 0.6983, 0.7400, 0.6600, 0.6865, 0.7418, 0.8873, 0.6959, 0.7937,\n",
      "        0.8289, 0.7171, 0.7774, 0.7079, 0.7521, 0.6169, 0.7042, 0.6765, 0.6755,\n",
      "        0.7230, 0.6720, 0.8627, 0.6997, 0.7710, 0.6749, 0.7015, 0.7124, 0.5871,\n",
      "        0.5951, 0.7488, 0.6403, 0.6267, 0.7956, 0.6094, 0.7811, 0.6714, 0.7068,\n",
      "        0.7369, 0.7162, 0.7079, 0.7295, 0.6217, 0.5714, 0.6743, 0.6745, 0.6471,\n",
      "        0.9220], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan, 1.4691,    nan, 0.9216, 2.1732, 2.0632, 1.0836,\n",
      "        1.3111,    nan,    nan, 1.1142, 1.4358, 2.2890,    nan,    nan,    nan,\n",
      "        1.2100,    nan, 2.5337,    nan,    nan, 0.8658, 3.4100,    nan, 1.7103,\n",
      "        1.2635, 1.2721, 2.0235, 1.9665, 1.5356,    nan, 1.2195, 1.6612,    nan,\n",
      "        1.2800, 1.8234, 1.9982,    nan, 1.8217, 2.3210, 0.8360, 1.2120,    nan,\n",
      "           nan, 0.9683,    nan,    nan, 1.3754,    nan, 1.0341, 2.3489, 1.2343,\n",
      "        0.9291, 1.1489, 0.9277, 1.1436,    nan,    nan, 0.9771,    nan,    nan,\n",
      "        2.7887], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.7240, 0.7115, 0.5243, 0.6873, 0.6555, 0.5974, 0.7727, 0.7707, 0.5108,\n",
      "        0.7134, 0.5425, 0.4908, 0.5942, 0.7196, 0.7379, 0.7367, 0.6640, 0.6140,\n",
      "        0.6814, 0.8362, 0.5846, 0.7958, 0.7313, 0.7703, 0.6834, 0.7733, 0.7042,\n",
      "        0.6327, 0.7403, 0.6490, 0.6563, 0.7932, 0.6931, 0.6267, 0.6792, 0.7445,\n",
      "        0.6718, 0.7445, 0.6310, 0.6413, 0.7195, 0.8783, 0.6282, 0.8038, 0.5508,\n",
      "        0.6266, 0.7716, 0.5647, 0.7023, 0.7336, 0.5810, 0.7171, 0.5917, 0.7332,\n",
      "        0.6546, 0.6018, 0.7009, 0.6234, 0.7420, 0.7687, 0.5505, 0.6197, 0.7164,\n",
      "        0.6121], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 1., 0., 0., 1., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.\n",
      " 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 2.0619,    nan,    nan,    nan,    nan, 1.8417, 1.8687,    nan,\n",
      "        1.0346,    nan,    nan,    nan, 0.9047, 2.4032, 1.1272,    nan,    nan,\n",
      "        1.3013, 1.7929,    nan, 1.2343, 1.3966, 0.9944, 1.9008, 1.7391, 0.9746,\n",
      "           nan, 1.6391,    nan,    nan, 1.1984,    nan,    nan, 1.3555, 1.6815,\n",
      "           nan, 1.4960,    nan,    nan, 0.8898, 1.7917,    nan, 2.1663,    nan,\n",
      "           nan, 1.7811,    nan, 1.0751, 1.0968,    nan, 1.0657,    nan, 1.4341,\n",
      "           nan,    nan,    nan,    nan, 1.1669, 0.9587,    nan,    nan, 0.9896,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6122, 0.7273, 0.7419, 0.8095, 0.6779, 0.7176, 0.6601, 0.6225, 0.7000,\n",
      "        0.5694, 0.7181, 0.6200, 0.7041, 0.6724, 0.5374, 0.7729, 0.6628, 0.6002,\n",
      "        0.7219, 0.7700, 0.6954, 0.6994, 0.6414, 0.6847, 0.7364, 0.7434, 0.7095,\n",
      "        0.7990, 0.6592, 0.6581, 0.7179, 0.6851, 0.7345, 0.7187, 0.7428, 0.7124,\n",
      "        0.7350, 0.7104, 0.7973, 0.6041, 0.6293, 0.6412, 0.6902, 0.6896, 0.6519,\n",
      "        0.7772, 0.6585, 0.6873, 0.7161, 0.7253, 0.7137, 0.6523, 0.7301, 0.6750,\n",
      "        0.7009, 0.8191, 0.6500, 0.6262, 0.5743, 0.6917, 0.5888, 0.7757, 0.6962,\n",
      "        0.7337], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 1., 0., 1., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.0288, 0.9672, 1.3597, 0.9224, 2.5154,    nan,    nan, 1.3975,\n",
      "           nan, 2.0612,    nan, 1.1720,    nan,    nan, 1.8506,    nan,    nan,\n",
      "        2.5304, 1.4431, 1.0492,    nan, 1.4896, 1.4708, 0.9667, 1.2725, 1.7693,\n",
      "        2.3431, 1.2254,    nan, 1.1227,    nan, 0.9921,    nan, 1.8773, 1.2504,\n",
      "        1.1155, 1.0033, 1.9614,    nan,    nan,    nan,    nan, 1.7306,    nan,\n",
      "        2.1914,    nan,    nan, 2.1430,    nan, 1.3242,    nan, 1.1747,    nan,\n",
      "        0.9602, 3.2697,    nan,    nan,    nan, 1.5807,    nan, 1.7277, 1.0425,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7146, 0.7114, 0.6230, 0.5741, 0.6097, 0.5405, 0.7828, 0.6514, 0.6853,\n",
      "        0.7861, 0.6464, 0.7200, 0.6985, 0.7132, 0.8263, 0.6586, 0.6970, 0.5969,\n",
      "        0.7599, 0.6544, 0.6000, 0.7459, 0.7218, 0.7820, 0.7155, 0.7808, 0.6241,\n",
      "        0.7268, 0.7237, 0.8019, 0.6590, 0.6332, 0.7884, 0.6841, 0.5951, 0.8137,\n",
      "        0.4837, 0.7441, 0.7035, 0.6383, 0.7358, 0.7199, 0.5563, 0.7011, 0.5514,\n",
      "        0.7113, 0.5737, 0.7176, 0.7100, 0.7338, 0.7594, 0.7098, 0.6533, 0.4874,\n",
      "        0.7277, 0.6377, 0.6404, 0.7646, 0.5847, 0.7080, 0.6122, 0.5988, 0.7097,\n",
      "        0.6755], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 1., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.3813, 1.5248,    nan,    nan,    nan,    nan, 1.4622,    nan,    nan,\n",
      "        1.4273,    nan, 1.4140,    nan, 1.2020, 1.6309,    nan, 0.7950,    nan,\n",
      "        1.7802,    nan,    nan, 1.7610, 0.9478, 1.5410, 0.8580, 1.0397,    nan,\n",
      "           nan, 1.0695, 1.7579,    nan,    nan, 1.6013,    nan,    nan, 1.5268,\n",
      "           nan, 1.3967, 1.2160,    nan, 1.3345, 0.9571,    nan, 2.2616,    nan,\n",
      "        1.1298,    nan, 1.5283, 0.9241, 1.1320, 1.5102, 1.2604,    nan,    nan,\n",
      "        0.9979,    nan,    nan, 1.1690,    nan,    nan,    nan,    nan, 1.0332,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7208, 0.5915, 0.7007, 0.6617, 0.7669, 0.7237, 0.7493, 0.7197, 0.6671,\n",
      "        0.7572, 0.6436, 0.7621, 0.6614, 0.7475, 0.6967, 0.7265, 0.7225, 0.7107,\n",
      "        0.6082, 0.6050, 0.5142, 0.7506, 0.6301, 0.6120, 0.7571, 0.6643, 0.5501,\n",
      "        0.6494, 0.7552, 0.6953, 0.6266, 0.6980, 0.7465, 0.6089, 0.7920, 0.6076,\n",
      "        0.7198, 0.7118, 0.7209, 0.7071, 0.6601, 0.8837, 0.8052, 0.6898, 0.6162,\n",
      "        0.7372, 0.7583, 0.6429, 0.7141, 0.7503, 0.6988, 0.6041, 0.6498, 0.7251,\n",
      "        0.6217, 0.5379, 0.6418, 0.6017, 0.6631, 0.5743, 0.7801, 0.6728, 0.6175,\n",
      "        0.7561], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.7604,    nan, 1.6603,    nan, 1.6970, 1.3641, 1.1259, 1.6877,    nan,\n",
      "        1.2766,    nan, 1.1768,    nan, 1.4063, 1.2654, 1.2145, 2.0811, 1.7917,\n",
      "           nan,    nan,    nan, 2.3849,    nan,    nan, 1.3441,    nan,    nan,\n",
      "           nan, 1.4739, 1.1836, 2.4103, 1.2477, 1.6242,    nan, 1.7148,    nan,\n",
      "        1.4311, 1.2586, 1.8826, 1.2217,    nan, 1.7902, 1.7354, 1.7264,    nan,\n",
      "        1.2193, 1.4169,    nan, 1.1838, 1.8481, 1.0346,    nan,    nan, 1.2794,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan, 2.1628, 1.3037,    nan,\n",
      "        1.1669], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6341, 0.7695, 0.6860, 0.7171, 0.7729, 0.6821, 0.6076, 0.5814, 0.7353,\n",
      "        0.7780, 0.6954, 0.7411, 0.6274, 0.6013, 0.5890, 0.6669, 0.6133, 0.5906,\n",
      "        0.7205, 0.7235, 0.6679, 0.7064, 0.6444, 0.7351, 0.8460, 0.8476, 0.6670,\n",
      "        0.7445, 0.7715, 0.5591, 0.5263, 0.7133, 0.7243, 0.6579, 0.7660, 0.6635,\n",
      "        0.7083, 0.6453, 0.6734, 0.7946, 0.8021, 0.6858, 0.7550, 0.7823, 0.5791,\n",
      "        0.6879, 0.7360, 0.6808, 0.5989, 0.6165, 0.6479, 0.7460, 0.7481, 0.7255,\n",
      "        0.7403, 0.7290, 0.5526, 0.6950, 0.7505, 0.7304, 0.7216, 0.6266, 0.7830,\n",
      "        0.7389], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 1., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1.\n",
      " 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0.\n",
      " 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 0.9571,    nan, 1.2482, 1.0913,    nan,    nan,    nan, 1.2846,\n",
      "        1.9629,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "        1.4737, 1.2938,    nan, 1.4359,    nan, 1.8470, 2.0634, 2.2252,    nan,\n",
      "        2.5480, 1.4407,    nan,    nan, 0.8797, 0.9461, 1.3342, 1.7088,    nan,\n",
      "        1.3511,    nan,    nan, 1.8670, 1.0793,    nan, 1.4524, 1.9552,    nan,\n",
      "           nan, 1.0634,    nan, 2.0688,    nan,    nan, 1.3680, 0.9108, 1.3920,\n",
      "        1.9174, 1.1510,    nan, 1.1301, 1.6291, 0.8351, 1.0340,    nan, 1.4622,\n",
      "        1.1740], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7620, 0.6389, 0.6009, 0.7639, 0.6614, 0.6183, 0.7422, 0.6812, 0.7405,\n",
      "        0.6505, 0.6579, 0.6955, 0.7994, 0.6007, 0.6155, 0.6980, 0.6876, 0.5523,\n",
      "        0.7153, 0.6079, 0.7066, 0.6658, 0.8173, 0.7368, 0.6960, 0.7042, 0.6939,\n",
      "        0.7118, 0.5210, 0.7449, 0.4961, 0.7954, 0.6675, 0.6509, 0.6161, 0.6506,\n",
      "        0.5845, 0.6181, 0.5713, 0.6824, 0.7211, 0.7354, 0.6056, 0.5499, 0.6995,\n",
      "        0.5038, 0.6220, 0.7643, 0.5703, 0.5608, 0.6984, 0.6481, 0.6925, 0.7488,\n",
      "        0.6569, 0.7728, 0.7378, 0.7609, 0.6266, 0.7383, 0.7092, 0.5787, 0.6339,\n",
      "        0.8587], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 0., 1., 0., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.4805,    nan,    nan, 1.1481,    nan,    nan, 1.9093, 1.1481, 1.3642,\n",
      "           nan,    nan, 1.0789, 1.5337,    nan,    nan, 1.2741, 1.3804,    nan,\n",
      "        1.9720,    nan, 0.9626, 2.0613, 1.5325, 0.8681, 0.8264, 1.1358, 1.6226,\n",
      "        1.2091,    nan,    nan,    nan, 1.4587,    nan,    nan,    nan, 1.8200,\n",
      "           nan,    nan,    nan, 2.0880, 1.3149, 1.0852,    nan,    nan,    nan,\n",
      "           nan,    nan, 1.6420,    nan,    nan, 1.1584,    nan,    nan, 1.2879,\n",
      "           nan, 1.6537, 1.9416, 0.9316,    nan, 1.0160, 1.2453,    nan,    nan,\n",
      "        2.5355], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.5393, 0.6625, 0.5712, 0.7425, 0.7489, 0.6582, 0.6366, 0.6380, 0.7130,\n",
      "        0.7153, 0.7167, 0.7650, 0.6490, 0.7441, 0.7161, 0.6211, 0.7286, 0.6949,\n",
      "        0.6266, 0.7803, 0.7070, 0.5745, 0.6288, 0.6644, 0.6962, 0.6907, 0.7824,\n",
      "        0.7253, 0.7836, 0.5840, 0.7611, 0.7428, 0.6641, 0.6894, 0.6082, 0.5422,\n",
      "        0.7587, 0.6427, 0.7331, 0.7446, 0.7152, 0.7602, 0.6442, 0.7032, 0.6732,\n",
      "        0.6007, 0.6835, 0.7616, 0.6647, 0.7370, 0.5691, 0.6424, 0.5855, 0.6404,\n",
      "        0.5757, 0.6712, 0.7047, 0.7180, 0.7510, 0.7026, 0.5259, 0.6409, 0.6210,\n",
      "        0.7832], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.6249, 2.7516, 2.1933, 1.2047, 1.8954, 1.9942, 2.3097, 2.1836, 1.0669,\n",
      "        1.3966, 0.9793, 0.9195, 3.6909, 1.5019, 0.9803, 2.3680, 1.2655, 1.0388,\n",
      "        2.0074, 1.6510, 2.1096, 2.8602, 2.5718, 1.7828, 2.8138, 1.6575, 2.5598,\n",
      "        0.9908, 1.9662, 2.5387, 1.2217, 1.3216, 2.2033, 1.8209, 2.8258, 3.2755,\n",
      "        0.8392, 3.4491, 1.3516, 1.7048, 1.1767, 1.2487, 3.0657, 1.6265, 1.6213,\n",
      "        2.2681, 1.7440, 1.1409, 1.4552, 1.3106, 1.7182, 2.6247, 3.3696, 0.0000,\n",
      "        2.9040, 1.2612, 0.8596, 2.2737, 1.1159, 0.9952, 2.6759, 3.0828, 1.9084,\n",
      "        1.5249], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6574, 0.5947, 0.5629, 0.7587, 0.6259, 0.5372, 0.6962, 0.6169, 0.5973,\n",
      "        0.6034, 0.6756, 0.6638, 0.6922, 0.7598, 0.5663, 0.7275, 0.7039, 0.7807,\n",
      "        0.7032, 0.8172, 0.7372, 0.6280, 0.5773, 0.8408, 0.7308, 0.6278, 0.7665,\n",
      "        0.6878, 0.6583, 0.6419, 0.5243, 0.7073, 0.6443, 0.6282, 0.8612, 0.6224,\n",
      "        0.6618, 0.7661, 0.6658, 0.7497, 0.5859, 0.6119, 0.7078, 0.6446, 0.6471,\n",
      "        0.5821, 0.7724, 0.7517, 0.7000, 0.7865, 0.6974, 0.6860, 0.7627, 0.6356,\n",
      "        0.8304, 0.7612, 0.7142, 0.7602, 0.7206, 0.6729, 0.7788, 0.6686, 0.6830,\n",
      "        0.7155], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1.\n",
      " 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      " 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.4994, 2.7485, 2.7732, 1.5087, 2.0959, 3.3043, 1.8925, 4.1329, 2.7158,\n",
      "        2.3909, 2.3452, 2.7973, 1.3551, 1.4762, 3.2796, 1.6686, 1.3387, 2.5032,\n",
      "        0.8791, 1.6229, 0.9775, 2.6478, 3.2483, 1.5073, 0.9535, 3.3588, 1.4060,\n",
      "        2.0042, 1.6961, 2.5527, 2.0307, 1.3755, 0.0000, 3.1581, 3.2087, 2.9923,\n",
      "        2.3407, 1.6437, 4.3951, 1.9263, 2.9776, 1.8488, 1.2957, 2.7147, 2.3747,\n",
      "        3.0575, 2.8595, 1.0428, 1.9940, 1.7613, 2.5487, 1.4928, 3.4549, 2.2611,\n",
      "        1.6557, 2.2792, 1.2296, 1.5907, 1.0621, 1.1233, 1.4515, 2.4397, 1.6709,\n",
      "        0.9003], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6424, 0.5851, 0.6687, 0.6582, 0.7623, 0.7194, 0.6911, 0.7827, 0.6281,\n",
      "        0.7798, 0.6275, 0.5907, 0.7121, 0.6130, 0.8029, 0.6233, 0.7752, 0.5655,\n",
      "        0.7404, 0.7186, 0.5601, 0.5542, 0.5531, 0.6726, 0.5949, 0.7239, 0.7645,\n",
      "        0.7119, 0.5956, 0.7307, 0.6246, 0.7069, 0.6724, 0.7425, 0.7629, 0.5049,\n",
      "        0.6981, 0.6716, 0.6068, 0.6937, 0.6872, 0.6090, 0.7795, 0.7957, 0.6244,\n",
      "        0.5800, 0.5556, 0.6778, 0.6105, 0.6909, 0.7755, 0.7330, 0.6283, 0.5617,\n",
      "        0.6090, 0.7820, 0.5834, 0.7004, 0.7633, 0.6985, 0.7448, 0.6620, 0.7196,\n",
      "        0.7493], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.5231,    nan,    nan,    nan, 1.0335, 1.2102,    nan, 1.1632,    nan,\n",
      "        0.9978,    nan,    nan,    nan,    nan, 1.9779,    nan, 1.3749,    nan,\n",
      "        1.1403, 1.1908,    nan,    nan,    nan,    nan,    nan, 1.4290, 1.2188,\n",
      "           nan,    nan, 0.9864,    nan, 0.9327,    nan, 1.2316, 1.2838,    nan,\n",
      "        2.2266,    nan,    nan, 1.1592,    nan,    nan, 1.1735, 1.8915,    nan,\n",
      "           nan,    nan,    nan,    nan, 1.7698, 1.3707, 1.1417,    nan,    nan,\n",
      "           nan, 1.7098, 2.5179, 0.9575, 1.3148, 1.2965, 3.1606,    nan, 1.0686,\n",
      "        1.1673], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7962, 0.8115, 0.5538, 0.6240, 0.6060, 0.7515, 0.6565, 0.6360, 0.6438,\n",
      "        0.7324, 0.7742, 0.6699, 0.6629, 0.7168, 0.7213, 0.5776, 0.6791, 0.6410,\n",
      "        0.5872, 0.6759, 0.7600, 0.6920, 0.7442, 0.8198, 0.5786, 0.5987, 0.7147,\n",
      "        0.8533, 0.5847, 0.6395, 0.5883, 0.7397, 0.6817, 0.7147, 0.6981, 0.7762,\n",
      "        0.7093, 0.5830, 0.6807, 0.5662, 0.7280, 0.7189, 0.7579, 0.7126, 0.7209,\n",
      "        0.6901, 0.6255, 0.7486, 0.7010, 0.7585, 0.6148, 0.7873, 0.7274, 0.6502,\n",
      "        0.7542, 0.7290, 0.7055, 0.7322, 0.6863, 0.7879, 0.6179, 0.6842, 0.5342,\n",
      "        0.7001], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.4637, 1.8293,    nan,    nan,    nan, 1.7200, 1.2540,    nan,    nan,\n",
      "        0.9615, 1.4291,    nan,    nan, 1.0024, 1.2265,    nan,    nan,    nan,\n",
      "           nan,    nan, 1.3983, 1.5291, 1.4724, 1.7707,    nan,    nan, 1.8523,\n",
      "        1.4831,    nan,    nan,    nan,    nan,    nan,    nan, 1.9748, 1.3279,\n",
      "        1.4607,    nan,    nan,    nan, 1.0031, 1.0700, 1.4610, 0.8174, 1.1492,\n",
      "        1.2896,    nan, 1.4023, 1.8693, 2.1848,    nan, 1.2487, 1.9078,    nan,\n",
      "        1.4483, 1.4290, 0.9524, 2.1696, 1.5756, 1.5144,    nan, 1.4552,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.5659, 0.5724, 0.6767, 0.5512, 0.6199, 0.7019, 0.7156, 0.7840, 0.6942,\n",
      "        0.8380, 0.7226, 0.7192, 0.6931, 0.6380, 0.7277, 0.7964, 0.8215, 0.5972,\n",
      "        0.7833, 0.7780, 0.7736, 0.6453, 0.6204, 0.7259, 0.6531, 0.6639, 0.7482,\n",
      "        0.5619, 0.7756, 0.5837, 0.6447, 0.6518, 0.6956, 0.6853, 0.7371, 0.7352,\n",
      "        0.6111, 0.6609, 0.7004, 0.5879, 0.7506, 0.7766, 0.6440, 0.6488, 0.5788,\n",
      "        0.6896, 0.6438, 0.7298, 0.5767, 0.7229, 0.7801, 0.7313, 0.7137, 0.6314,\n",
      "        0.6354, 0.7022, 0.5712, 0.6893, 0.7100, 0.7730, 0.6747, 0.7241, 0.7374,\n",
      "        0.6104], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 1., 0., 0., 0., 1., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.5946, 2.0397, 3.4240, 3.9739, 1.9266, 1.6008, 0.7535, 1.1849, 1.0571,\n",
      "        2.2254, 1.9175, 1.9185, 0.9670, 1.9572, 0.8528, 1.7459, 1.5568, 3.1081,\n",
      "        1.1601, 2.7235, 1.9087, 3.1906, 2.0484, 1.1151, 1.4196, 0.0000, 1.1932,\n",
      "        1.6689, 1.6099, 1.7187, 3.1190, 2.0363, 2.8226, 2.3669, 0.9250, 1.2309,\n",
      "        2.7250, 1.9940, 1.0300, 3.2089, 1.2724, 2.8442, 1.8880, 1.6725, 2.9621,\n",
      "        3.7794, 1.6903, 1.2636, 2.3796, 1.1729, 1.6613, 1.0178, 1.8174, 2.7623,\n",
      "        2.5112, 0.8393, 3.6435, 1.7300, 1.1798, 1.1028, 2.7835, 1.0508, 0.9937,\n",
      "        4.3020], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7439, 0.6869, 0.5744, 0.7043, 0.7506, 0.6430, 0.6979, 0.5527, 0.6738,\n",
      "        0.7507, 0.6887, 0.7028, 0.6336, 0.7009, 0.8518, 0.6876, 0.6750, 0.6705,\n",
      "        0.6060, 0.8141, 0.6660, 0.5780, 0.7504, 0.8222, 0.7270, 0.6430, 0.7473,\n",
      "        0.7427, 0.7929, 0.6112, 0.7432, 0.6429, 0.7371, 0.6246, 0.6981, 0.7073,\n",
      "        0.6959, 0.7565, 0.6522, 0.7681, 0.6882, 0.6491, 0.7729, 0.6971, 0.5798,\n",
      "        0.5509, 0.7144, 0.9031, 0.7129, 0.7039, 0.6811, 0.7262, 0.6326, 0.7917,\n",
      "        0.6459, 0.6611, 0.6649, 0.6714, 0.6799, 0.7419, 0.7461, 0.7188, 0.6571,\n",
      "        0.6774], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1.\n",
      " 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([0.8634,    nan,    nan, 0.8300, 1.3915,    nan, 1.0435,    nan, 1.7158,\n",
      "        1.9159, 1.3367, 1.0778,    nan,    nan, 2.3967, 1.0896,    nan, 1.3298,\n",
      "           nan, 1.5999,    nan,    nan, 0.7456, 2.2434, 1.5643,    nan, 1.1455,\n",
      "        1.4059, 1.1700,    nan, 1.1555,    nan, 1.4743,    nan,    nan, 0.9107,\n",
      "        0.8287, 0.9043,    nan, 1.7896, 1.6220,    nan, 1.7393, 1.0239,    nan,\n",
      "           nan, 0.9009, 2.5910, 1.2793, 0.8881, 1.4277, 0.7729,    nan, 1.9759,\n",
      "        1.4768, 0.8920,    nan, 1.1171, 0.7760, 1.7567, 1.5378, 1.0761, 1.2811,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6206, 0.6336, 0.7208, 0.7245, 0.7468, 0.6051, 0.7092, 0.6683, 0.7101,\n",
      "        0.7005, 0.6698, 0.7234, 0.7057, 0.7538, 0.7331, 0.6674, 0.7126, 0.7642,\n",
      "        0.7877, 0.6217, 0.7171, 0.6664, 0.7238, 0.6816, 0.6304, 0.7677, 0.8306,\n",
      "        0.7065, 0.6714, 0.6893, 0.7022, 0.7498, 0.6315, 0.6597, 0.7120, 0.6193,\n",
      "        0.6996, 0.7080, 0.6884, 0.6148, 0.6181, 0.6047, 0.7378, 0.6536, 0.7217,\n",
      "        0.7185, 0.7717, 0.6619, 0.7353, 0.7955, 0.6196, 0.6998, 0.7899, 0.7154,\n",
      "        0.8138, 0.7311, 0.7376, 0.5844, 0.7077, 0.7086, 0.7377, 0.6612, 0.6415,\n",
      "        0.8253], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 0.9652, 1.1038, 1.2178,    nan, 0.9541, 3.0563, 1.3405,\n",
      "        0.7988,    nan, 1.3259,    nan, 1.4373, 0.9004,    nan, 0.9812, 1.2453,\n",
      "        1.1822,    nan, 1.1933,    nan, 1.6504, 2.0468,    nan, 1.0165, 2.2157,\n",
      "        0.8099,    nan, 1.2840, 1.2377, 1.0022,    nan,    nan,    nan,    nan,\n",
      "        0.8541, 0.9087, 0.8976,    nan,    nan,    nan, 1.2804,    nan, 1.5351,\n",
      "        2.6103, 2.0246,    nan, 1.2909, 1.2674,    nan, 1.0195, 1.1358, 1.2796,\n",
      "        2.1078, 0.8600, 1.5222,    nan,    nan,    nan, 1.1518,    nan,    nan,\n",
      "        1.1554], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7529, 0.6197, 0.7758, 0.4761, 0.6372, 0.7491, 0.8120, 0.5989, 0.5988,\n",
      "        0.5501, 0.7283, 0.6237, 0.6404, 0.7197, 0.6995, 0.6587, 0.7384, 0.7295,\n",
      "        0.5297, 0.6719, 0.6330, 0.6044, 0.7690, 0.6386, 0.7125, 0.7800, 0.6461,\n",
      "        0.6389, 0.6358, 0.7477, 0.7774, 0.6202, 0.7156, 0.5486, 0.6285, 0.6406,\n",
      "        0.5981, 0.5959, 0.6834, 0.6701, 0.6689, 0.7143, 0.6135, 0.6874, 0.7599,\n",
      "        0.7503, 0.6613, 0.7028, 0.7746, 0.5514, 0.6571, 0.8087, 0.6146, 0.7439,\n",
      "        0.6946, 0.6451, 0.6943, 0.5368, 0.6554, 0.7452, 0.7517, 0.7630, 0.5960,\n",
      "        0.6501], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0.\n",
      " 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.2896,    nan, 1.8460,    nan,    nan, 1.2591, 1.4472,    nan,    nan,\n",
      "           nan, 1.5532,    nan,    nan, 0.9770, 1.4516,    nan, 0.9737, 0.9984,\n",
      "           nan, 0.8210,    nan,    nan, 0.9537,    nan, 1.0398, 1.9725,    nan,\n",
      "           nan,    nan, 1.5461, 1.1435,    nan, 0.8729,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan, 0.8082,    nan, 1.2150, 1.5639,\n",
      "        1.5643,    nan, 0.7713, 1.2612,    nan,    nan,    nan,    nan, 2.6657,\n",
      "           nan,    nan, 1.2562,    nan,    nan, 1.3090, 1.6784, 2.2526,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.5496, 0.8404, 0.6482, 0.5075, 0.6470, 0.6517, 0.6658, 0.6568, 0.7126,\n",
      "        0.6977, 0.6047, 0.6445, 0.6566, 0.7388, 0.7240, 0.6735, 0.7289, 0.7321,\n",
      "        0.7676, 0.6647, 0.6734, 0.7128, 0.5955, 0.7061, 0.8257, 0.7201, 0.6999,\n",
      "        0.7334, 0.5461, 0.7150, 0.6390, 0.5937, 0.7397, 0.7409, 0.6436, 0.6589,\n",
      "        0.6475, 0.7402, 0.7062, 0.8099, 0.6814, 0.6185, 0.6668, 0.6699, 0.7762,\n",
      "        0.7748, 0.7614, 0.8132, 0.6511, 0.6918, 0.7519, 0.5780, 0.7635, 0.7503,\n",
      "        0.5646, 0.7658, 0.7746, 0.6953, 0.7068, 0.7540, 0.6527, 0.5045, 0.5388,\n",
      "        0.7333], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1.\n",
      " 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.5862,    nan,    nan,    nan,    nan,    nan,    nan, 1.1970,\n",
      "        1.7024,    nan,    nan,    nan, 0.8568, 1.1786,    nan, 1.5570, 1.0935,\n",
      "        1.4711,    nan, 1.3469, 1.0291,    nan, 1.1070, 2.0207, 1.4739, 1.2847,\n",
      "        1.7615,    nan, 1.1152,    nan,    nan, 1.9510, 1.1473,    nan,    nan,\n",
      "           nan, 1.6550, 0.9636, 1.4926, 1.2164,    nan,    nan,    nan, 1.9042,\n",
      "        2.1181, 2.0581, 2.7898,    nan,    nan, 1.6652,    nan, 2.3733, 1.1698,\n",
      "           nan, 1.0831, 1.5486, 1.6475,    nan, 1.0133,    nan,    nan,    nan,\n",
      "        0.8675], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6527, 0.7427, 0.7650, 0.6903, 0.5858, 0.6538, 0.5933, 0.6566, 0.7151,\n",
      "        0.7498, 0.6171, 0.5567, 0.8897, 0.7323, 0.6316, 0.5913, 0.7734, 0.6659,\n",
      "        0.7651, 0.5198, 0.4700, 0.6730, 0.6842, 0.7868, 0.7890, 0.6791, 0.7340,\n",
      "        0.7307, 0.6311, 0.6669, 0.5724, 0.6633, 0.5999, 0.5880, 0.7874, 0.7046,\n",
      "        0.5867, 0.5975, 0.5927, 0.6412, 0.6027, 0.7099, 0.9370, 0.5725, 0.7426,\n",
      "        0.6227, 0.6871, 0.6884, 0.7278, 0.7067, 0.7300, 0.7554, 0.7298, 0.6188,\n",
      "        0.4809, 0.8049, 0.7429, 0.6544, 0.6180, 0.7714, 0.6254, 0.7227, 0.7129,\n",
      "        0.6437], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 1., 1., 0., 1., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.\n",
      " 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.9790, 1.3277, 1.5190, 1.8514, 1.0832, 1.8503, 2.1738, 2.1792, 1.8050,\n",
      "        0.9636, 3.0790, 3.1083, 1.6390, 2.2915, 1.2020, 1.5083, 1.2610, 1.9560,\n",
      "        1.9421, 3.0509, 4.4422, 1.0117, 2.4245, 1.7546, 1.7283, 1.8687, 1.4773,\n",
      "        1.4566, 2.7966, 1.3844, 2.2190, 1.5267, 2.3395, 2.1785, 0.9737, 1.4362,\n",
      "        3.3021, 2.6295, 1.5241, 1.0832, 2.4097, 1.0196, 2.3866, 1.9497, 1.9141,\n",
      "        3.5951, 1.6286, 3.0365, 1.0610, 1.3021, 1.1714, 1.7963, 1.8838, 2.7974,\n",
      "        2.7115, 1.3750, 2.3251, 3.3966, 2.4289, 1.2847, 1.2684, 1.2583, 1.2558,\n",
      "        2.0712], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6686, 0.5732, 0.6652, 0.7580, 0.6331, 0.7687, 0.6094, 0.6423, 0.6603,\n",
      "        0.6665, 0.5860, 0.7318, 0.7505, 0.7224, 0.5693, 0.6947, 0.7579, 0.7265,\n",
      "        0.7334, 0.7525, 0.5986, 0.6270, 0.7223, 0.7013, 0.7623, 0.6325, 0.6260,\n",
      "        0.7505, 0.7301, 0.5564, 0.6485, 0.5288, 0.6495, 0.7220, 0.5816, 0.9272,\n",
      "        0.7259, 0.7278, 0.8265, 0.7695, 0.7545, 0.6384, 0.5738, 0.7236, 0.6862,\n",
      "        0.6878, 0.6588, 0.7138, 0.6263, 0.7341, 0.5973, 0.7921, 0.6165, 0.8088,\n",
      "        0.5342, 0.4985, 0.7095, 0.7009, 0.7876, 0.7236, 0.5819, 0.7218, 0.7152,\n",
      "        0.7275], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1.\n",
      " 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 1.7557, 1.5385,    nan, 1.0794,    nan,    nan,    nan,\n",
      "        1.5568,    nan, 1.6621, 1.1532, 1.7182,    nan,    nan, 0.8025, 1.0034,\n",
      "        1.1259, 1.8562,    nan,    nan,    nan, 1.0504, 1.8975,    nan,    nan,\n",
      "        1.2441, 1.1153,    nan,    nan,    nan,    nan, 0.9026,    nan, 3.6338,\n",
      "        1.2953, 2.6931, 1.4019, 1.8140, 0.9741,    nan,    nan,    nan, 1.3973,\n",
      "           nan, 2.6098, 2.8652,    nan, 0.7343,    nan, 1.5603,    nan, 0.9035,\n",
      "           nan,    nan, 1.0653, 0.9441, 1.5806, 0.9917,    nan, 1.0668, 1.5019,\n",
      "        0.9147], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6153, 0.6566, 0.6399, 0.7552, 0.7476, 0.5364, 0.6904, 0.7229, 0.7754,\n",
      "        0.6457, 0.5731, 0.5341, 0.7741, 0.7078, 0.5765, 0.6822, 0.7323, 0.5852,\n",
      "        0.6065, 0.7392, 0.8589, 0.7676, 0.5759, 0.4909, 0.6875, 0.7538, 0.6592,\n",
      "        0.5754, 0.7345, 0.7388, 0.5978, 0.6029, 0.7023, 0.6330, 0.8063, 0.6224,\n",
      "        0.7405, 0.7507, 0.6894, 0.6416, 0.5499, 0.7072, 0.8001, 0.6404, 0.7035,\n",
      "        0.6639, 0.6915, 0.5664, 0.7623, 0.7096, 0.6536, 0.7454, 0.6617, 0.6101,\n",
      "        0.6042, 0.7282, 0.6955, 0.7346, 0.6349, 0.7425, 0.6083, 0.5401, 0.6547,\n",
      "        0.7169], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan, 1.2231, 1.1870,    nan, 1.3385, 0.9404, 1.1500,\n",
      "           nan,    nan,    nan, 1.2141,    nan,    nan,    nan, 1.0232,    nan,\n",
      "           nan, 2.3887, 2.8782, 1.1363,    nan,    nan, 1.2358, 1.1350,    nan,\n",
      "           nan, 1.7021,    nan,    nan,    nan, 1.0765,    nan, 2.2579,    nan,\n",
      "        1.2501, 1.2452, 1.4784,    nan,    nan, 1.7675, 1.6482,    nan, 2.2741,\n",
      "           nan,    nan,    nan, 1.5368,    nan,    nan, 1.0485,    nan,    nan,\n",
      "           nan, 0.8843, 1.0776, 1.0169,    nan, 0.7794,    nan,    nan,    nan,\n",
      "        1.0064], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6290, 0.6076, 0.5772, 0.7100, 0.6392, 0.7152, 0.8114, 0.6758, 0.7400,\n",
      "        0.6445, 0.6524, 0.6363, 0.7073, 0.6598, 0.6465, 0.5883, 0.5854, 0.8112,\n",
      "        0.7880, 0.6367, 0.6083, 0.7684, 0.7401, 0.6387, 0.7268, 0.6544, 0.7041,\n",
      "        0.6566, 0.6350, 0.5888, 0.5812, 0.8053, 0.6782, 0.8604, 0.6248, 0.6518,\n",
      "        0.6002, 0.6746, 0.7305, 0.6770, 0.7402, 0.7942, 0.6946, 0.7574, 0.6781,\n",
      "        0.7549, 0.6338, 0.6797, 0.7181, 0.7112, 0.5827, 0.7013, 0.5986, 0.7784,\n",
      "        0.8169, 0.7239, 0.7211, 0.7847, 0.6375, 0.7774, 0.5570, 0.7513, 0.7879,\n",
      "        0.6995], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 1., 0., 1., 0., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.6618, 3.5244, 2.0911, 0.8302, 2.0441, 0.9809, 2.5365, 1.9485, 1.2802,\n",
      "        1.6828, 2.1515, 2.6708, 1.8734, 0.0000, 1.1531, 3.1854, 1.3142, 1.7007,\n",
      "        2.6053, 2.4444, 1.9484, 2.8893, 1.1225, 3.0991, 1.1807, 2.9482, 1.0651,\n",
      "        1.6146, 1.3152, 2.5895, 2.9785, 1.5526, 3.0201, 1.9303, 2.2937, 2.2868,\n",
      "        1.4950, 1.6226, 1.0576, 1.2301, 1.1240, 1.3311, 1.2080, 0.8553, 1.1237,\n",
      "        0.8626, 1.5818, 1.2211, 1.1997, 1.2270, 2.3706, 1.2077, 2.5322, 1.9594,\n",
      "        1.5709, 2.1838, 0.8882, 2.6999, 2.2581, 1.8050, 2.9401, 1.4446, 1.6779,\n",
      "        1.6875], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6939, 0.7209, 0.8030, 0.6531, 0.8011, 0.7803, 0.7458, 0.7678, 0.7751,\n",
      "        0.5664, 0.7719, 0.6316, 0.7824, 0.6382, 0.7047, 0.6084, 0.6909, 0.7075,\n",
      "        0.7492, 0.6089, 0.7331, 0.7217, 0.6419, 0.7089, 0.6923, 0.6286, 0.6107,\n",
      "        0.7064, 0.7649, 0.7149, 0.7083, 0.7474, 0.6765, 0.6004, 0.6036, 0.6201,\n",
      "        0.6591, 0.6838, 0.6816, 0.4345, 0.7668, 0.6382, 0.7745, 0.7887, 0.6885,\n",
      "        0.6069, 0.7979, 0.6152, 0.7395, 0.7143, 0.7133, 0.5996, 0.6826, 0.5637,\n",
      "        0.5499, 0.8279, 0.7121, 0.7381, 0.7061, 0.6378, 0.8090, 0.7136, 0.6968,\n",
      "        0.7420], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1.\n",
      " 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0.\n",
      " 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.2558, 1.2232, 1.6674, 1.4749,    nan, 1.0088, 2.0454, 2.1609,\n",
      "           nan, 1.3344,    nan, 1.5195,    nan, 0.9326,    nan, 1.3949, 0.9592,\n",
      "        0.8735,    nan, 1.3847, 0.7486,    nan, 1.9948, 1.3297,    nan,    nan,\n",
      "           nan, 1.0978, 0.9227, 1.0118, 1.2031, 1.0162,    nan,    nan,    nan,\n",
      "           nan,    nan, 1.3391,    nan, 1.9882,    nan, 1.8509, 2.0427, 1.2827,\n",
      "           nan, 2.7108,    nan, 1.2088, 1.3955, 1.8585,    nan, 0.8940,    nan,\n",
      "           nan, 1.7399, 1.0451, 1.1190, 1.1098,    nan, 1.6905, 0.9720,    nan,\n",
      "        1.3411], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.5920, 0.5997, 0.5741, 0.6587, 0.6158, 0.7300, 0.6417, 0.8204, 0.7038,\n",
      "        0.5822, 0.7016, 0.5536, 0.7047, 0.4920, 0.5008, 0.7754, 0.5754, 0.7240,\n",
      "        0.5751, 0.6814, 0.7771, 0.7807, 0.6097, 0.6190, 0.6794, 0.6019, 0.6437,\n",
      "        0.7034, 0.7527, 0.6339, 0.5636, 0.5672, 0.6181, 0.5072, 0.6486, 0.7756,\n",
      "        0.5692, 0.7685, 0.8108, 0.7429, 0.7325, 0.6899, 0.6299, 0.7152, 0.5538,\n",
      "        0.7268, 0.5985, 0.7840, 0.7486, 0.6956, 0.7841, 0.6840, 0.4974, 0.7752,\n",
      "        0.7331, 0.7085, 0.6690, 0.7121, 0.6316, 0.5514, 0.6728, 0.5729, 0.8203,\n",
      "        0.8032], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan,    nan,    nan, 1.4291,    nan, 1.0879, 1.4019,\n",
      "           nan, 1.9825,    nan, 1.5605,    nan,    nan, 1.4806,    nan, 1.3436,\n",
      "           nan,    nan, 1.8174, 2.1887,    nan,    nan, 1.6109,    nan,    nan,\n",
      "           nan, 1.1418,    nan,    nan,    nan,    nan,    nan,    nan, 1.1019,\n",
      "           nan, 1.7001, 1.9025, 0.9489, 0.9621, 1.5219,    nan, 1.1675,    nan,\n",
      "        1.0248,    nan, 1.0570, 1.2386, 1.5300, 1.0721, 1.0111,    nan, 1.3446,\n",
      "        1.1960, 1.5307,    nan,    nan,    nan,    nan,    nan,    nan, 1.4146,\n",
      "        1.5552], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.6256, 0.6520, 0.6551, 0.6943, 0.8201, 0.5644, 0.8646, 0.6809, 0.7479,\n",
      "        0.6818, 0.6685, 0.6161, 0.6755, 0.6755, 0.7494, 0.6811, 0.6278, 0.7095,\n",
      "        0.7089, 0.7241, 0.6463, 0.8001, 0.5918, 0.8573, 0.7356, 0.6650, 0.9409,\n",
      "        0.6293, 0.7328, 0.7465, 0.7210, 0.5874, 0.6237, 0.6706, 0.6995, 0.8008,\n",
      "        0.8010, 0.7190, 0.6558, 0.6688, 0.6991, 0.7792, 0.7017, 0.5886, 0.6498,\n",
      "        0.6096, 0.6055, 0.6653, 0.6573, 0.7507, 0.7404, 0.6732, 0.7690, 0.7436,\n",
      "        0.6388, 0.6167, 0.5735, 0.8358, 0.5701, 0.7427, 0.7281, 0.7534, 0.6666,\n",
      "        0.7914], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1.\n",
      " 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan, 1.4193, 2.4552,    nan, 2.2887, 1.1243, 1.9213,\n",
      "        1.2196,    nan,    nan,    nan, 1.2962,    nan,    nan,    nan, 1.6022,\n",
      "        1.4006, 1.9411,    nan, 1.6438,    nan, 1.8478, 1.7003,    nan, 2.9885,\n",
      "           nan, 1.0583, 1.4242,    nan,    nan,    nan, 1.3167,    nan, 1.8053,\n",
      "        2.0775, 1.3524,    nan,    nan, 1.0537, 1.4067, 1.0393,    nan,    nan,\n",
      "           nan,    nan, 1.3302,    nan, 1.4790, 1.1573, 1.1953, 1.5261, 1.5045,\n",
      "           nan,    nan,    nan, 2.5776,    nan, 1.3399, 1.1499, 1.5728,    nan,\n",
      "        2.1007], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7430, 0.6195, 0.6529, 0.6264, 0.6087, 0.7270, 0.7240, 0.7374, 0.7181,\n",
      "        0.6368, 0.6778, 0.6863, 0.6578, 0.6455, 0.7276, 0.6310, 0.5710, 0.6133,\n",
      "        0.7273, 0.6137, 0.6175, 0.7307, 0.7795, 0.6295, 0.5546, 0.8690, 0.6756,\n",
      "        0.6020, 0.8772, 0.7088, 0.7645, 0.6788, 0.6946, 0.6964, 0.6369, 0.5993,\n",
      "        0.7304, 0.7351, 0.5660, 0.6041, 0.6130, 0.7153, 0.7545, 0.7019, 0.6853,\n",
      "        0.7432, 0.7609, 0.7672, 0.6983, 0.7199, 0.7287, 0.6848, 0.7146, 0.5910,\n",
      "        0.7305, 0.7711, 0.6004, 0.7395, 0.6563, 0.6631, 0.7215, 0.6798, 0.5735,\n",
      "        0.7282], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 0., 1., 1., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0.\n",
      " 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([0.9790,    nan, 1.2896,    nan,    nan, 1.0493, 0.9542, 1.7482, 1.2279,\n",
      "           nan,    nan, 1.3351,    nan,    nan, 0.7518,    nan,    nan,    nan,\n",
      "        1.2743,    nan,    nan, 1.2529, 1.9319,    nan,    nan, 1.8050,    nan,\n",
      "           nan, 1.6142, 1.2728, 1.2999, 2.3648, 2.2352, 1.0726,    nan,    nan,\n",
      "        1.5163, 1.1431,    nan,    nan,    nan, 1.2820, 0.9825, 0.8954,    nan,\n",
      "        2.0161, 1.6883, 1.2704, 0.9043, 0.9868, 1.6581,    nan, 1.0306,    nan,\n",
      "        1.0511, 1.1529,    nan, 1.5984,    nan,    nan, 1.0844,    nan,    nan,\n",
      "        1.0777], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6944, 0.7607, 0.7320, 0.7248, 0.7623, 0.7053, 0.6437, 0.7499, 0.6130,\n",
      "        0.6567, 0.7822, 0.6915, 0.7093, 0.5715, 0.7136, 0.5087, 0.7419, 0.7277,\n",
      "        0.5951, 0.7265, 0.6487, 0.6413, 0.7303, 0.7263, 0.6832, 0.6998, 0.5524,\n",
      "        0.6598, 0.8694, 0.6972, 0.6584, 0.6569, 0.7319, 0.6931, 0.5814, 0.6089,\n",
      "        0.6827, 0.5612, 0.5277, 0.6492, 0.6939, 0.5973, 0.7172, 0.7331, 0.6341,\n",
      "        0.6930, 0.6559, 0.6617, 0.6992, 0.6649, 0.7343, 0.7112, 0.6159, 0.6965,\n",
      "        0.7709, 0.5370, 0.6132, 0.6443, 0.7422, 0.7812, 0.5958, 0.8244, 0.7486,\n",
      "        0.6303], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 1., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.0144, 1.6233, 1.7711, 1.0022, 2.1515, 1.1273, 2.4729, 0.7415, 3.4584,\n",
      "        2.8334, 1.4686, 0.9771, 2.0787, 2.9767, 1.2088, 4.5325, 1.4541, 2.5576,\n",
      "        3.0878, 1.2067, 3.2834, 3.1585, 1.0333, 1.0037, 2.7868, 1.2899, 0.0000,\n",
      "        2.9374, 3.2315, 1.7205, 3.2513, 2.7030, 1.1716, 1.1833, 3.7968, 2.6927,\n",
      "        2.4445, 2.9589, 3.5615, 2.8991, 0.8397, 3.5851, 1.6271, 3.5917, 1.9869,\n",
      "        2.2044, 2.2199, 2.9837, 1.0204, 2.4749, 1.5561, 3.5329, 2.4532, 1.0210,\n",
      "        1.1350, 3.9811, 2.4473, 2.8780, 0.8496, 2.0142, 2.8925, 1.4412, 1.8672,\n",
      "        2.6729], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7347, 0.6504, 0.7890, 0.7462, 0.7240, 0.7438, 0.7415, 0.7923, 0.6618,\n",
      "        0.5982, 0.7296, 0.7179, 0.6643, 0.7336, 0.5646, 0.7206, 0.7354, 0.6973,\n",
      "        0.6539, 0.6128, 0.7613, 0.7416, 0.7030, 0.7518, 0.5219, 0.5441, 0.6815,\n",
      "        0.7359, 0.8012, 0.5744, 0.5800, 0.6756, 0.5546, 0.5427, 0.6525, 0.6759,\n",
      "        0.7284, 0.7194, 0.7368, 0.7877, 0.6323, 0.6406, 0.6675, 0.6655, 0.7507,\n",
      "        0.6950, 0.5969, 0.7069, 0.5896, 0.7147, 0.7040, 0.6081, 0.6129, 0.6895,\n",
      "        0.6348, 0.5504, 0.7029, 0.7550, 0.7186, 0.6902, 0.7520, 0.6940, 0.6675,\n",
      "        0.8216], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 1., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([0.7894,    nan, 1.0472, 1.0473, 0.8828, 1.8553, 1.4856, 1.6773,    nan,\n",
      "           nan, 1.0296, 0.9510, 1.1238, 1.3495,    nan,    nan, 1.2757, 0.9835,\n",
      "           nan,    nan, 1.1073, 0.8177, 1.2845, 1.3128,    nan,    nan,    nan,\n",
      "        1.0242, 1.4407,    nan,    nan, 1.0734,    nan,    nan,    nan, 1.3629,\n",
      "        1.2348, 0.8092,    nan, 2.3604,    nan,    nan, 1.4857, 2.4577, 0.8945,\n",
      "        1.5778,    nan, 0.9254,    nan, 1.3841, 0.8698,    nan,    nan,    nan,\n",
      "           nan,    nan, 1.3244, 0.8553, 1.9019,    nan, 1.0181, 0.9777,    nan,\n",
      "        2.0387], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6086, 0.6142, 0.7835, 0.8800, 0.6497, 0.7156, 0.5544, 0.8018, 0.7959,\n",
      "        0.6785, 0.6337, 0.6302, 0.7356, 0.7294, 0.6914, 0.6204, 0.6909, 0.6553,\n",
      "        0.7644, 0.8204, 0.6522, 0.7536, 0.4873, 0.5770, 0.7291, 0.6307, 0.6472,\n",
      "        0.7327, 0.7692, 0.7671, 0.6040, 0.7333, 0.6427, 0.6167, 0.5390, 0.5600,\n",
      "        0.7137, 0.6731, 0.7209, 0.7637, 0.6616, 0.6847, 0.6921, 0.8362, 0.5858,\n",
      "        0.7652, 0.7673, 0.5990, 0.6237, 0.6810, 0.7283, 0.5910, 0.7346, 0.7565,\n",
      "        0.7277, 0.6787, 0.6940, 0.6600, 0.6061, 0.6402, 0.8059, 0.6993, 0.6919,\n",
      "        0.7210], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0.\n",
      " 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 1.5585, 2.2604,    nan, 1.1464,    nan, 3.1901, 1.3725,\n",
      "        1.0780,    nan,    nan, 1.3769, 1.0755, 2.0334,    nan,    nan,    nan,\n",
      "        1.9788, 2.6029,    nan, 1.2895,    nan,    nan, 0.9138,    nan,    nan,\n",
      "        0.8653, 1.2486, 1.3270,    nan, 0.9619,    nan,    nan,    nan,    nan,\n",
      "        1.0259,    nan, 0.9198, 1.4006,    nan,    nan, 1.3045, 2.7571,    nan,\n",
      "        0.9860, 1.2826,    nan,    nan,    nan, 1.0044,    nan, 0.9957, 1.8367,\n",
      "        1.1283,    nan, 1.1363,    nan,    nan,    nan, 2.2503, 1.1088,    nan,\n",
      "        1.2032], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7629, 0.7807, 0.6827, 0.8040, 0.7492, 0.6247, 0.4846, 0.6697, 0.7222,\n",
      "        0.6192, 0.7403, 0.6172, 0.7229, 0.6405, 0.6897, 0.6989, 0.6554, 0.5796,\n",
      "        0.7743, 0.7548, 0.6369, 0.7064, 0.5190, 0.7298, 0.6995, 0.5595, 0.7623,\n",
      "        0.6111, 0.7327, 0.7631, 0.6843, 0.6067, 0.7214, 0.7003, 0.7129, 0.7933,\n",
      "        0.6796, 0.6867, 0.6905, 0.7341, 0.6680, 0.7053, 0.7361, 0.6445, 0.6986,\n",
      "        0.6051, 0.7093, 0.7242, 0.6224, 0.7925, 0.8149, 0.6896, 0.5885, 0.5348,\n",
      "        0.6716, 0.7296, 0.7357, 0.8007, 0.6121, 0.6066, 0.6786, 0.6408, 0.7574,\n",
      "        0.6681], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1.\n",
      " 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.0751,    nan, 1.6206, 2.5746, 2.2634,    nan,    nan,    nan, 1.1283,\n",
      "           nan, 1.5942,    nan, 0.8358,    nan,    nan, 0.9910,    nan,    nan,\n",
      "        1.4672, 1.0107,    nan, 1.9638,    nan, 1.0088, 1.7605,    nan, 1.1195,\n",
      "           nan, 1.0555, 0.9475, 1.0478,    nan,    nan, 1.2861, 0.8063, 0.9939,\n",
      "           nan,    nan,    nan, 1.2373,    nan, 1.4874, 2.0997,    nan, 1.6018,\n",
      "           nan, 1.1820, 1.3124,    nan, 1.2726, 2.2571,    nan,    nan,    nan,\n",
      "           nan,    nan, 1.5683, 1.7675,    nan,    nan,    nan,    nan, 2.1701,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7148, 0.6033, 0.7272, 0.6781, 0.7360, 0.5289, 0.6651, 0.7082, 0.6501,\n",
      "        0.6107, 0.6848, 0.6312, 0.7817, 0.7339, 0.6348, 0.7213, 0.7519, 0.7100,\n",
      "        0.6860, 0.6956, 0.7883, 0.8361, 0.6558, 0.7408, 0.6297, 0.5195, 0.9486,\n",
      "        0.7095, 0.7035, 0.6235, 0.7123, 0.7405, 0.7122, 0.5970, 0.7572, 0.7323,\n",
      "        0.7622, 0.6905, 0.6472, 0.6470, 0.7541, 0.7160, 0.6206, 0.7456, 0.5940,\n",
      "        0.6378, 0.7509, 0.8875, 0.6999, 0.7489, 0.6379, 0.7376, 0.6036, 0.6431,\n",
      "        0.6529, 0.7328, 0.7528, 0.7162, 0.6775, 0.7595, 0.7132, 0.6116, 0.7612,\n",
      "        0.6144], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1.\n",
      " 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1.\n",
      " 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.4201,    nan, 1.1362,    nan, 2.4817,    nan, 1.2287, 1.1157,    nan,\n",
      "           nan,    nan,    nan, 1.5461, 1.3946,    nan, 1.0188, 1.2822, 1.4111,\n",
      "           nan,    nan, 1.6120, 2.0464,    nan, 1.5555,    nan,    nan, 3.1939,\n",
      "        1.8488, 1.1642,    nan, 0.8890, 1.0389, 1.1050,    nan, 1.8424, 1.5068,\n",
      "        1.2239,    nan,    nan,    nan, 2.2375, 1.1332,    nan, 1.6855,    nan,\n",
      "           nan, 1.3602, 2.2418, 1.2647, 1.1522,    nan, 1.4176,    nan,    nan,\n",
      "        1.6994, 1.0338, 1.1073, 1.2639,    nan, 1.8916, 2.0122,    nan, 1.6659,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.6839, 0.6874, 0.7495, 0.5968, 0.5854, 0.5950, 0.6346, 0.7589, 0.6323,\n",
      "        0.6006, 0.5769, 0.6720, 0.7173, 0.6993, 0.7450, 0.7548, 0.6807, 0.6757,\n",
      "        0.7551, 0.6301, 0.7548, 0.6280, 0.6320, 0.5918, 0.6066, 0.6590, 0.6472,\n",
      "        0.7072, 0.6255, 0.6592, 0.5400, 0.8463, 0.5831, 0.6492, 0.4835, 0.6359,\n",
      "        0.7356, 0.8209, 0.6405, 0.5882, 0.6070, 0.6204, 0.6123, 0.7752, 0.6770,\n",
      "        0.5867, 0.6495, 0.6149, 0.7170, 0.5548, 0.8017, 0.7411, 0.6419, 0.6227,\n",
      "        0.5639, 0.7824, 0.5935, 0.5766, 0.6676, 0.6450, 0.7528, 0.7957, 0.6670,\n",
      "        0.7002], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 1., 0., 0., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.6004, 1.3396, 2.3326, 2.3236, 2.5488, 1.8762, 2.6543, 2.0734, 3.0370,\n",
      "        2.4733, 2.5117, 1.9344, 0.8865, 1.3966, 0.9913, 1.9859, 2.1581, 1.6373,\n",
      "        1.3000, 1.9446, 2.0488, 2.3594, 1.7079, 2.4435, 2.0216, 1.3746, 1.6985,\n",
      "        2.0146, 2.6624, 1.6566, 2.5113, 2.3281, 2.9296, 2.5357, 3.2012, 2.1223,\n",
      "        1.4720, 2.2440, 2.6402, 3.7268, 2.2742, 2.7799, 1.8786, 1.6165, 2.4030,\n",
      "        1.1768, 1.7894, 2.7438, 1.0428, 2.5399, 2.2184, 1.4485, 2.2899, 2.4828,\n",
      "        2.1316, 1.0903, 2.4451, 2.0806, 1.3104, 2.2205, 1.2752, 1.5943, 1.6234,\n",
      "        1.3654], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7856, 0.6212, 0.6920, 0.7605, 0.6326, 0.5446, 0.6507, 0.5758, 0.6612,\n",
      "        0.6131, 0.6889, 0.5961, 0.7016, 0.6938, 0.5865, 0.7237, 0.7604, 0.8602,\n",
      "        0.5239, 0.6362, 0.6814, 0.6874, 0.5873, 0.7975, 0.7833, 0.7173, 0.5856,\n",
      "        0.6661, 0.6044, 0.5505, 0.7022, 0.6612, 0.6715, 0.7399, 0.8208, 0.7277,\n",
      "        0.7217, 0.6414, 0.7302, 0.6110, 0.8361, 0.6675, 0.7308, 0.6910, 0.6325,\n",
      "        0.7285, 0.5972, 0.6405, 0.6029, 0.5941, 0.7433, 0.7115, 0.7872, 0.7099,\n",
      "        0.6723, 0.5396, 0.5941, 0.7231, 0.4834, 0.7341, 0.6230, 0.7247, 0.7501,\n",
      "        0.5512], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 1., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1.\n",
      " 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.7746,    nan, 1.5794, 2.3406,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan, 1.3794,    nan,    nan, 1.5303, 1.8085, 2.3469,\n",
      "           nan,    nan, 1.1407, 0.8990,    nan, 1.9572, 1.7188, 0.8931,    nan,\n",
      "           nan,    nan,    nan, 1.9481,    nan, 1.8153, 1.0638, 2.2484, 1.2716,\n",
      "        0.9686,    nan, 2.0181,    nan, 2.4008,    nan, 0.9316, 1.7904,    nan,\n",
      "        0.8567,    nan,    nan,    nan,    nan,    nan, 1.7232,    nan, 0.9123,\n",
      "           nan,    nan,    nan, 1.2854,    nan, 1.0668,    nan, 1.2285, 1.2317,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6411, 0.7362, 0.6710, 0.6451, 0.7536, 0.7024, 0.7478, 0.7269, 0.7255,\n",
      "        0.6093, 0.6401, 0.7082, 0.7770, 0.8772, 0.5950, 0.7305, 0.6984, 0.8383,\n",
      "        0.6854, 0.6110, 0.7380, 0.7147, 0.8025, 0.7144, 0.6664, 0.6207, 0.6941,\n",
      "        0.6731, 0.6269, 0.7113, 0.7354, 0.7024, 0.7785, 0.6233, 0.6211, 0.4520,\n",
      "        0.6530, 0.6076, 0.5975, 0.7564, 0.7410, 0.7376, 0.5516, 0.6298, 0.5181,\n",
      "        0.7239, 0.5296, 0.5480, 0.5496, 0.6677, 0.6872, 0.5928, 0.6227, 0.8680,\n",
      "        0.6094, 0.7551, 0.6387, 0.7597, 0.4867, 0.8862, 0.6719, 0.6683, 0.7070,\n",
      "        0.6845], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 1., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 0.9241, 1.1951,    nan, 1.0342, 0.8916, 1.4062, 1.3707, 1.4740,\n",
      "           nan,    nan, 1.1689, 1.7310, 2.1387,    nan, 2.1091, 1.6455, 1.3537,\n",
      "           nan,    nan, 0.9943, 1.1848, 1.5071, 0.8172,    nan,    nan,    nan,\n",
      "           nan,    nan, 1.5139, 1.6362, 1.8749, 1.1689,    nan,    nan,    nan,\n",
      "        1.3455,    nan,    nan, 1.4894, 1.1173, 2.9922,    nan,    nan,    nan,\n",
      "        2.3049,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 2.0912,\n",
      "           nan,    nan,    nan, 0.9706,    nan, 2.3924,    nan,    nan, 1.0744,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7029, 0.5891, 0.7398, 0.7220, 0.6493, 0.6641, 0.6967, 0.6902, 0.6741,\n",
      "        0.6320, 0.7763, 0.8375, 0.6256, 0.7889, 0.6662, 0.7454, 0.6074, 0.6284,\n",
      "        0.7140, 0.6122, 0.7549, 0.7369, 0.6048, 0.9090, 0.7053, 0.7225, 0.6724,\n",
      "        0.6845, 0.6747, 0.6851, 0.7207, 0.5667, 0.6588, 0.4903, 0.7518, 0.7380,\n",
      "        0.6814, 0.7304, 0.5502, 0.6532, 0.7152, 0.5990, 0.7573, 0.6669, 0.7975,\n",
      "        0.7709, 0.7716, 0.5214, 0.7162, 0.6563, 0.5796, 0.7681, 0.7084, 0.6571,\n",
      "        0.7242, 0.6692, 0.7431, 0.7019, 0.7736, 0.9168, 0.7151, 0.7911, 0.8154,\n",
      "        0.7017], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1.\n",
      " 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0.\n",
      " 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.0029,    nan, 1.0827, 1.1644,    nan,    nan, 1.9623, 1.3985,    nan,\n",
      "           nan, 2.2891, 1.8862,    nan, 1.3134,    nan, 1.2508,    nan,    nan,\n",
      "        1.0848,    nan, 1.9385, 0.8001,    nan, 2.1706, 1.2210, 1.4262, 2.2899,\n",
      "           nan, 1.5704,    nan, 2.1855,    nan,    nan,    nan, 1.1155, 2.0656,\n",
      "        1.0797, 1.2996,    nan,    nan, 1.8345,    nan, 1.4919,    nan, 1.4021,\n",
      "        1.3770, 1.7284,    nan, 1.0650,    nan,    nan, 1.2428, 1.1536,    nan,\n",
      "        1.1374,    nan, 1.1329, 1.0029, 0.7205, 2.8942, 1.2060, 2.8782, 1.7789,\n",
      "        2.1797], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.5622, 0.5699, 0.8065, 0.7041, 0.5829, 0.7239, 0.8670, 0.7516, 0.8384,\n",
      "        0.6939, 0.7794, 0.6760, 0.7036, 0.8924, 0.6618, 0.6076, 0.7226, 0.7542,\n",
      "        0.7816, 0.5888, 0.6788, 0.7533, 0.7039, 0.5989, 0.6963, 0.6585, 0.8299,\n",
      "        0.7942, 0.6364, 0.5764, 0.7320, 0.7436, 0.6312, 0.6557, 0.6849, 0.6917,\n",
      "        0.7433, 0.6341, 0.6779, 0.6812, 0.7126, 0.5888, 0.7533, 0.7480, 0.6399,\n",
      "        0.6234, 0.5878, 0.5930, 0.6254, 0.6306, 0.7209, 0.5850, 0.6702, 0.6583,\n",
      "        0.7070, 0.8224, 0.7030, 0.7800, 0.7314, 0.6465, 0.5502, 0.6020, 0.6617,\n",
      "        0.8763], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0.\n",
      " 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 1.3260, 0.9814,    nan, 1.3180, 2.3910, 1.9247, 2.3490,\n",
      "        0.9130, 1.1185,    nan, 1.6290, 2.6002,    nan,    nan, 1.1212, 1.0489,\n",
      "        1.5342,    nan,    nan, 1.1217, 0.9645,    nan, 1.0244,    nan, 2.7718,\n",
      "        1.0426,    nan,    nan, 1.3572, 1.0382,    nan,    nan,    nan, 0.8134,\n",
      "        1.0286,    nan, 1.5251, 1.5215, 0.8775,    nan, 2.6667, 1.5827,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan, 2.5675,    nan, 1.3491, 0.8871,    nan,    nan,    nan,    nan,\n",
      "        2.6103], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.5264, 0.6953, 0.5922, 0.7854, 0.5798, 0.5837, 0.6498, 0.6037, 0.6388,\n",
      "        0.6893, 0.5843, 0.7004, 0.8698, 0.7127, 0.5392, 0.7087, 0.6388, 0.6114,\n",
      "        0.8131, 0.5636, 0.6342, 0.7735, 0.6099, 0.6009, 0.7956, 0.6338, 0.6356,\n",
      "        0.6997, 0.5981, 0.7137, 0.5775, 0.5953, 0.6853, 0.6570, 0.6934, 0.5769,\n",
      "        0.7593, 0.7513, 0.6138, 0.6064, 0.5852, 0.6051, 0.6401, 0.6156, 0.7395,\n",
      "        0.6215, 0.6380, 0.7510, 0.6183, 0.6070, 0.6905, 0.7379, 0.6294, 0.7497,\n",
      "        0.6168, 0.5807, 0.6699, 0.7085, 0.7137, 0.6438, 0.7273, 0.7754, 0.5937,\n",
      "        0.5177], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 0., 1., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.3735,    nan, 1.4588,    nan,    nan,    nan,    nan,    nan,\n",
      "        1.3264,    nan, 1.0665, 2.6606, 1.0015,    nan, 1.6375,    nan,    nan,\n",
      "        2.1852,    nan,    nan, 1.8353,    nan,    nan, 2.3844,    nan,    nan,\n",
      "        1.1653,    nan, 1.4676,    nan,    nan,    nan,    nan, 1.0244,    nan,\n",
      "        1.2383, 1.1506,    nan,    nan,    nan,    nan,    nan,    nan, 1.0019,\n",
      "           nan,    nan, 0.9610,    nan,    nan, 1.2588, 2.0421,    nan, 1.4086,\n",
      "           nan,    nan, 2.2212, 1.1349, 1.1662,    nan, 1.6869, 1.0379,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6029, 0.7008, 0.6285, 0.7123, 0.6908, 0.7138, 0.7161, 0.6176, 0.5707,\n",
      "        0.6985, 0.4844, 0.7004, 0.5899, 0.6933, 0.7836, 0.6325, 0.7217, 0.7426,\n",
      "        0.7512, 0.6088, 0.7083, 0.6112, 0.5528, 0.8262, 0.5959, 0.7850, 0.4527,\n",
      "        0.5313, 0.5930, 0.8585, 0.7722, 0.5869, 0.7858, 0.5646, 0.7427, 0.7350,\n",
      "        0.5428, 0.6958, 0.8036, 0.6573, 0.7769, 0.7355, 0.6827, 0.6650, 0.6510,\n",
      "        0.8225, 0.6022, 0.6358, 0.7673, 0.7134, 0.7065, 0.6761, 0.5868, 0.7266,\n",
      "        0.7126, 0.6046, 0.6831, 0.7120, 0.7758, 0.7650, 0.6428, 0.7630, 0.5923,\n",
      "        0.7085], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 1., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1.\n",
      " 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.8007,    nan, 0.8109,    nan, 1.1840, 1.2218,    nan,    nan,\n",
      "        0.8939,    nan, 1.1008,    nan, 1.1445, 1.7100,    nan, 0.9720, 1.6562,\n",
      "        1.0385,    nan, 1.8019,    nan,    nan, 1.9554,    nan, 1.1690,    nan,\n",
      "           nan,    nan, 2.1887, 2.3074,    nan, 1.0760,    nan, 1.6204, 1.8882,\n",
      "           nan,    nan, 1.1968,    nan, 1.4548, 1.0944,    nan,    nan,    nan,\n",
      "        1.9402,    nan,    nan, 1.0464, 0.9906, 1.0127,    nan,    nan, 0.8001,\n",
      "        1.0603,    nan,    nan, 1.1845, 1.7044, 1.5899,    nan, 1.5592,    nan,\n",
      "        1.3756], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6562, 0.6308, 0.6227, 0.7705, 0.5637, 0.7910, 0.5294, 0.5932, 0.7415,\n",
      "        0.7385, 0.6302, 0.5762, 0.6999, 0.5935, 0.7283, 0.7830, 0.6801, 0.6695,\n",
      "        0.8415, 0.6591, 0.6014, 0.6597, 0.8072, 0.7580, 0.7650, 0.7266, 0.7650,\n",
      "        0.7976, 0.7389, 0.7749, 0.7641, 0.7304, 0.5640, 0.6614, 0.5557, 0.6800,\n",
      "        0.7036, 0.7288, 0.5913, 0.7706, 0.7166, 0.7325, 0.7205, 0.7388, 0.7641,\n",
      "        0.5013, 0.6420, 0.6919, 0.6803, 0.7698, 0.7553, 0.6267, 0.6367, 0.5441,\n",
      "        0.6236, 0.8186, 0.6049, 0.6322, 0.6633, 0.7900, 0.7368, 0.7920, 0.6232,\n",
      "        0.5826], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan, 1.4685,    nan, 2.6762,    nan,    nan, 1.0063,\n",
      "        1.1188,    nan,    nan, 1.2519,    nan, 1.3080, 1.7463, 1.0618,    nan,\n",
      "        2.0361,    nan,    nan,    nan, 1.5591, 1.8073, 1.6835, 1.1391, 2.2175,\n",
      "        1.5436, 1.3109, 2.7393,    nan, 1.8298,    nan,    nan,    nan, 2.1018,\n",
      "        1.0236, 1.3187,    nan, 1.7289, 1.9265, 1.4367, 1.0112, 2.0927, 1.4162,\n",
      "           nan,    nan, 1.5875,    nan, 2.2091, 1.3223,    nan,    nan,    nan,\n",
      "           nan, 1.4618,    nan,    nan,    nan, 1.3627, 1.2738, 1.3990,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7083, 0.6300, 0.7076, 0.6912, 0.7240, 0.6708, 0.8431, 0.7499, 0.6685,\n",
      "        0.6949, 0.7504, 0.6203, 0.6982, 0.6598, 0.7211, 0.7407, 0.6261, 0.6716,\n",
      "        0.6820, 0.7399, 0.6716, 0.6385, 0.7807, 0.6300, 0.7056, 0.7373, 0.8217,\n",
      "        0.8069, 0.7086, 0.8181, 0.7446, 0.6269, 0.6517, 0.7110, 0.6312, 0.7242,\n",
      "        0.6624, 0.8263, 0.6153, 0.6917, 0.6225, 0.8387, 0.7603, 0.8051, 0.6140,\n",
      "        0.6310, 0.7235, 0.7358, 0.6798, 0.7722, 0.7365, 0.7546, 0.6070, 0.6248,\n",
      "        0.7080, 0.6383, 0.7246, 0.4826, 0.6870, 0.7473, 0.7138, 0.7259, 0.6365,\n",
      "        0.7147], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 0., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.6008,    nan, 0.7854, 0.9005, 1.2081,    nan, 2.3496,    nan,    nan,\n",
      "        1.0844, 1.0408,    nan, 0.9278,    nan, 0.9687, 1.2571,    nan,    nan,\n",
      "           nan, 0.9942, 1.0018,    nan, 1.7339,    nan, 1.1069, 1.4764, 1.8543,\n",
      "        1.5785, 1.0379, 1.7791,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan, 1.7641,    nan, 1.7782,    nan, 1.5916, 2.4559, 1.3397,    nan,\n",
      "           nan, 1.6736, 2.0352,    nan, 1.4415, 1.1538, 0.8618,    nan,    nan,\n",
      "        1.3900,    nan,    nan,    nan,    nan, 2.1102, 2.2404, 0.9698,    nan,\n",
      "        0.8955], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6292, 0.7312, 0.5153, 0.6337, 0.6388, 0.8267, 0.9207, 0.6034, 0.5297,\n",
      "        0.6928, 0.7843, 0.7548, 0.6710, 0.5829, 0.7316, 0.6635, 0.6898, 0.6256,\n",
      "        0.6907, 0.5445, 0.6959, 0.5707, 0.7693, 0.5670, 0.7114, 0.5435, 0.7380,\n",
      "        0.6476, 0.6749, 0.6840, 0.5825, 0.6743, 0.7115, 0.6222, 0.7785, 0.7156,\n",
      "        0.6255, 0.5861, 0.7506, 0.6129, 0.6775, 0.5988, 0.6829, 0.6408, 0.6210,\n",
      "        0.5126, 0.6159, 0.6858, 0.7911, 0.7165, 0.7190, 0.5915, 0.7307, 0.6343,\n",
      "        0.7662, 0.7424, 0.5958, 0.5753, 0.8491, 0.7030, 0.7632, 0.7271, 0.6808,\n",
      "        0.8168], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.5362,    nan,    nan,    nan, 2.1175, 2.6015,    nan,    nan,\n",
      "        1.2933, 2.5550, 2.1903,    nan,    nan, 3.2014,    nan, 1.0628,    nan,\n",
      "        1.2429,    nan,    nan,    nan, 1.5302,    nan, 0.9186,    nan, 0.9187,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.3611, 1.0772,\n",
      "           nan,    nan, 0.8964,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan, 2.2364, 2.2866, 1.5227,    nan, 0.9932,    nan,\n",
      "        1.8486, 1.3052,    nan,    nan, 2.0026, 1.1194, 0.9900, 1.0429,    nan,\n",
      "        1.3728], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.7119, 0.7780, 0.5780, 0.5498, 0.9141, 0.6790, 0.7296, 0.6370, 0.6121,\n",
      "        0.6541, 0.5590, 0.5650, 0.6901, 0.6202, 0.6306, 0.7774, 0.7242, 0.5623,\n",
      "        0.6011, 0.5615, 0.7529, 0.7555, 0.7911, 0.5658, 0.7359, 0.8008, 0.7342,\n",
      "        0.7128, 0.8304, 0.7000, 0.7359, 0.6357, 0.7364, 0.7498, 0.5840, 0.5429,\n",
      "        0.6293, 0.7001, 0.7521, 0.7480, 0.6904, 0.8151, 0.6010, 0.7732, 0.5622,\n",
      "        0.6659, 0.6805, 0.5733, 0.6874, 0.7394, 0.6102, 0.7019, 0.6634, 0.7312,\n",
      "        0.6763, 0.6575, 0.7574, 0.6755, 0.7847, 0.7372, 0.7589, 0.5637, 0.6453,\n",
      "        0.7679], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0.\n",
      " 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.0209, 1.3320,    nan,    nan, 2.3217,    nan, 1.1652,    nan,    nan,\n",
      "           nan,    nan,    nan, 3.4261,    nan,    nan, 1.0769, 0.8629,    nan,\n",
      "           nan,    nan, 1.7807, 1.1619, 1.0510,    nan, 1.0717, 1.4268, 1.2169,\n",
      "           nan, 1.1372, 0.8798, 1.0663,    nan, 1.8462, 2.7338,    nan,    nan,\n",
      "           nan, 0.9261, 1.0492, 1.1398,    nan, 1.1529,    nan, 1.1905,    nan,\n",
      "           nan,    nan,    nan, 0.9891, 0.9388,    nan, 1.1359,    nan, 0.8492,\n",
      "        1.4758,    nan, 2.1701, 2.5732, 1.3692, 1.3907, 0.9498,    nan,    nan,\n",
      "        1.3771], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.8155, 0.6002, 0.8149, 0.6968, 0.8213, 0.6428, 0.7516, 0.7160, 0.6902,\n",
      "        0.6559, 0.7302, 0.7677, 0.7750, 0.6369, 0.6698, 0.7323, 0.6666, 0.6170,\n",
      "        0.6019, 0.6624, 0.7266, 0.7186, 0.6883, 0.7110, 0.6942, 0.7114, 0.7750,\n",
      "        0.7614, 0.8991, 0.8034, 0.7836, 0.6907, 0.7627, 0.4928, 0.6594, 0.7641,\n",
      "        0.6964, 0.7504, 0.7450, 0.6150, 0.6200, 0.7445, 0.7193, 0.7039, 0.7623,\n",
      "        0.8703, 0.6549, 0.6899, 0.7231, 0.5665, 0.5936, 0.6312, 0.6168, 0.7183,\n",
      "        0.6529, 0.5569, 0.6805, 0.7150, 0.5255, 0.7604, 0.7224, 0.7326, 0.6446,\n",
      "        0.5832], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.5012, 2.6364, 1.2790, 1.1067, 1.2559, 1.3346, 1.1972, 1.1354, 0.5705,\n",
      "        3.5133, 0.7882, 1.4031, 1.7898, 1.8046, 0.0000, 0.8588, 1.2605, 2.2822,\n",
      "        3.7132, 2.1274, 1.2529, 1.6934, 1.0691, 1.0074, 1.1214, 1.0089, 1.1247,\n",
      "        1.7393, 3.5751, 0.9018, 1.4856, 2.8363, 2.9026, 3.6129, 3.0184, 1.7335,\n",
      "        1.6011, 1.0456, 1.5374, 2.1095, 2.0479, 2.1100, 0.9999, 1.0409, 2.3384,\n",
      "        1.4906, 1.9123, 1.8752, 1.2708, 4.0902, 1.8866, 1.6503, 1.3609, 1.2584,\n",
      "        3.4283, 2.8134, 1.3461, 1.3621, 2.6732, 1.1548, 0.9407, 1.3161, 2.0226,\n",
      "        2.2607], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7202, 0.5695, 0.7245, 0.5387, 0.7433, 0.6207, 0.6287, 0.7065, 0.7189,\n",
      "        0.5251, 0.7213, 0.7972, 0.7195, 0.6337, 0.7618, 0.5098, 0.7346, 0.6229,\n",
      "        0.6774, 0.5579, 0.7211, 0.5914, 0.6620, 0.7593, 0.6901, 0.5950, 0.5811,\n",
      "        0.7159, 0.7021, 0.5761, 0.6888, 0.7159, 0.5601, 0.6838, 0.5613, 0.7362,\n",
      "        0.7125, 0.6943, 0.6856, 0.6018, 0.7216, 0.4842, 0.6559, 0.8197, 0.6652,\n",
      "        0.6414, 0.6615, 0.7085, 0.6954, 0.7161, 0.7277, 0.8770, 0.6396, 0.7452,\n",
      "        0.7438, 0.5915, 0.6672, 0.7128, 0.5907, 0.5942, 0.7241, 0.6374, 0.6071,\n",
      "        0.5664], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 1., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1.\n",
      " 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.8738,    nan, 0.8547,    nan, 1.5379,    nan,    nan, 1.1718, 1.2159,\n",
      "           nan, 1.4143, 1.2486, 2.4055,    nan, 1.1376,    nan, 0.9288,    nan,\n",
      "           nan,    nan, 1.5878,    nan,    nan, 1.3702, 1.2122,    nan,    nan,\n",
      "        1.0384, 1.0194,    nan,    nan, 1.0880,    nan,    nan,    nan, 1.2218,\n",
      "        2.0148, 1.1809, 1.6272,    nan, 1.1262,    nan, 1.6595, 2.7326, 2.4127,\n",
      "           nan, 2.0386, 1.4076,    nan, 0.9140, 1.5016, 2.9258,    nan, 0.8571,\n",
      "        1.3567,    nan, 1.8029, 0.7667,    nan,    nan, 1.1464,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6137, 0.6491, 0.7703, 0.5992, 0.7113, 0.7119, 0.4899, 0.7155, 0.7539,\n",
      "        0.7229, 0.7313, 0.7559, 0.5453, 0.7377, 0.6748, 0.7485, 0.7357, 0.5378,\n",
      "        0.6256, 0.7342, 0.7065, 0.5986, 0.5909, 0.4930, 0.7730, 0.7034, 0.6068,\n",
      "        0.6247, 0.7212, 0.5976, 0.6202, 0.6276, 0.5279, 0.6354, 0.6459, 0.6320,\n",
      "        0.7049, 0.6769, 0.6901, 0.7713, 0.7079, 0.6233, 0.6387, 0.7248, 0.7643,\n",
      "        0.7389, 0.6387, 0.6146, 0.6381, 0.7247, 0.5752, 0.5566, 0.5766, 0.6621,\n",
      "        0.7632, 0.8354, 0.7317, 0.7674, 0.8510, 0.5968, 0.7330, 0.7878, 0.6725,\n",
      "        0.6662], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 1.4807,    nan, 1.3086, 1.5699,    nan, 1.7650, 1.5999,\n",
      "        1.0782, 0.8787,    nan,    nan, 1.1800,    nan, 2.4692, 1.0799,    nan,\n",
      "           nan, 1.2784, 1.8230,    nan,    nan,    nan, 1.2999, 1.0339,    nan,\n",
      "           nan, 1.8704,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan, 1.2322, 0.9971, 0.9952,    nan,    nan, 1.4112, 2.6465,\n",
      "        1.3098,    nan,    nan,    nan, 1.5479,    nan,    nan,    nan,    nan,\n",
      "        1.7901, 2.2154, 1.1233, 1.2345, 2.0022,    nan, 1.0389, 2.2210,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7569, 0.6179, 0.6692, 0.6345, 0.6814, 0.5578, 0.7106, 0.6575, 0.6364,\n",
      "        0.5847, 0.6841, 0.6969, 0.5876, 0.5638, 0.5801, 0.5506, 0.7694, 0.7152,\n",
      "        0.6281, 0.6459, 0.6767, 0.6977, 0.6442, 0.8873, 0.7601, 0.7094, 0.5371,\n",
      "        0.7466, 0.5662, 0.6803, 0.7659, 0.7518, 0.6031, 0.6984, 0.6043, 0.6867,\n",
      "        0.7499, 0.6785, 0.6610, 0.6880, 0.5749, 0.6124, 0.7687, 0.5963, 0.7236,\n",
      "        0.7183, 0.7425, 0.6762, 0.6473, 0.7254, 0.6809, 0.6889, 0.6384, 0.5375,\n",
      "        0.6203, 0.7006, 0.6994, 0.8099, 0.7774, 0.7274, 0.6069, 0.7333, 0.5178,\n",
      "        0.7267], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.\n",
      " 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0.\n",
      " 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.8480,    nan,    nan,    nan, 2.2461,    nan, 0.9566,    nan,    nan,\n",
      "           nan,    nan, 1.0667,    nan,    nan,    nan,    nan, 2.0531, 0.8670,\n",
      "           nan,    nan,    nan, 1.0092,    nan, 2.8417, 1.0534, 0.8907,    nan,\n",
      "        1.6001,    nan, 1.4665, 1.3225, 2.0798,    nan, 1.2597,    nan,    nan,\n",
      "        1.8976,    nan,    nan, 1.5888,    nan,    nan, 2.0474,    nan, 1.5126,\n",
      "        1.5733, 1.0491,    nan,    nan, 0.8271, 1.0908, 1.5728,    nan,    nan,\n",
      "           nan, 1.7494, 0.9311, 1.6070, 1.8793, 2.3753,    nan, 0.7870,    nan,\n",
      "        1.1088], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6303, 0.6947, 0.7147, 0.6369, 0.6220, 0.7522, 0.7267, 0.6167, 0.5926,\n",
      "        0.5912, 0.5950, 0.5528, 0.5790, 0.7284, 0.7548, 0.7289, 0.7373, 0.7180,\n",
      "        0.7162, 0.7656, 0.7519, 0.4599, 0.5328, 0.7390, 0.6136, 0.8070, 0.7555,\n",
      "        0.6199, 0.7934, 0.8351, 0.7981, 0.6355, 0.6218, 0.7255, 0.7348, 0.6179,\n",
      "        0.6812, 0.5430, 0.7081, 0.7067, 0.6438, 0.5561, 0.7707, 0.6035, 0.6939,\n",
      "        0.7775, 0.5970, 0.7644, 0.6283, 0.7461, 0.6321, 0.6647, 0.6371, 0.6249,\n",
      "        0.7434, 0.5905, 0.6209, 0.6154, 0.6353, 0.6395, 0.6763, 0.6944, 0.5542,\n",
      "        0.5892], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 1., 1., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.\n",
      " 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.9824, 1.1800,    nan,    nan, 1.3423, 1.2849,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan, 1.7478, 1.0415, 0.9016, 1.9176, 1.5498,\n",
      "        1.1674, 1.7289, 1.1969,    nan,    nan, 1.1840,    nan, 2.1937, 1.6487,\n",
      "           nan, 1.5795, 1.8344, 1.2690,    nan,    nan, 1.4416, 1.1607,    nan,\n",
      "           nan,    nan,    nan, 1.2868,    nan,    nan, 2.0671,    nan, 1.0718,\n",
      "        1.6766,    nan, 1.0989,    nan, 1.2042,    nan,    nan,    nan,    nan,\n",
      "        1.4175,    nan,    nan,    nan,    nan,    nan,    nan, 1.8500,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6319, 0.7454, 0.5241, 0.7887, 0.6024, 0.6828, 0.7266, 0.6044, 0.6220,\n",
      "        0.7871, 0.6331, 0.7415, 0.7843, 0.7393, 0.8277, 0.7847, 0.6397, 0.6306,\n",
      "        0.6864, 0.5582, 0.7016, 0.8678, 0.7901, 0.6324, 0.6466, 0.7246, 0.5453,\n",
      "        0.5513, 0.7098, 0.7372, 0.6549, 0.5090, 0.7569, 0.6449, 0.6911, 0.7103,\n",
      "        0.7442, 0.7355, 0.7278, 0.5395, 0.7207, 0.6073, 0.7469, 0.5773, 0.7002,\n",
      "        0.5812, 0.6143, 0.6154, 0.6828, 0.7514, 0.7327, 0.6743, 0.6892, 0.7370,\n",
      "        0.6243, 0.6688, 0.7654, 0.6050, 0.6071, 0.7063, 0.6982, 0.6368, 0.6748,\n",
      "        0.6500], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 0.8588,    nan, 1.0874,    nan, 1.8512, 0.8030,    nan,    nan,\n",
      "        1.1597,    nan, 0.8052, 1.7683,    nan, 1.6075, 2.0841, 1.4065,    nan,\n",
      "        1.1765,    nan, 1.3016, 2.6117, 1.7660,    nan,    nan, 1.2611,    nan,\n",
      "           nan, 1.0729, 1.0689,    nan,    nan, 2.1108,    nan,    nan, 1.9311,\n",
      "        1.3872, 1.0051, 1.7312,    nan, 1.1254,    nan, 1.5309,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan, 0.8731, 1.0043,    nan, 0.9250, 0.9025,\n",
      "           nan,    nan, 1.7445,    nan,    nan, 1.9257, 1.9184,    nan, 1.0150,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6730, 0.7611, 0.5310, 0.7227, 0.7859, 0.7174, 0.6846, 0.7827, 0.6981,\n",
      "        0.6536, 0.6079, 0.7243, 0.7432, 0.8179, 0.6695, 0.7234, 0.7573, 0.7889,\n",
      "        0.7209, 0.6914, 0.7415, 0.6363, 0.6500, 0.5938, 0.7462, 0.6428, 0.6766,\n",
      "        0.5596, 0.8352, 0.7418, 0.7196, 0.8463, 0.8085, 0.6717, 0.6160, 0.6867,\n",
      "        0.7326, 0.7120, 0.6717, 0.6813, 0.6928, 0.7065, 0.6793, 0.5977, 0.6885,\n",
      "        0.4789, 0.5226, 0.6076, 0.7178, 0.7112, 0.7718, 0.5927, 0.4771, 0.5998,\n",
      "        0.6512, 0.7029, 0.7479, 0.7982, 0.6154, 0.7246, 0.6537, 0.7334, 0.5797,\n",
      "        0.6489], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.6995,    nan, 1.0941, 1.7812, 1.6392,    nan, 1.7585,    nan,\n",
      "           nan,    nan, 1.1514, 1.8855, 2.0737,    nan, 1.2799, 1.1348, 1.5815,\n",
      "           nan, 1.5164, 2.1478,    nan,    nan,    nan, 1.1136,    nan, 2.2145,\n",
      "           nan, 3.8733, 1.0778, 0.9030, 1.2764, 1.5903, 1.7737,    nan, 1.3692,\n",
      "        1.0549, 1.1300, 1.3397,    nan,    nan, 1.1001, 1.5177,    nan, 1.5611,\n",
      "           nan,    nan,    nan, 1.0995, 0.9501, 2.1613,    nan,    nan,    nan,\n",
      "           nan, 2.4115, 0.9653, 2.4205,    nan, 1.1354,    nan, 1.2927,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.7452, 0.7668, 0.6320, 0.7334, 0.7026, 0.6085, 0.6841, 0.7132, 0.7459,\n",
      "        0.5934, 0.8191, 0.8322, 0.6325, 0.6801, 0.5397, 0.7497, 0.5969, 0.6297,\n",
      "        0.7530, 0.6538, 0.6796, 0.5769, 0.6140, 0.5638, 0.7459, 0.5066, 0.6381,\n",
      "        0.6558, 0.6952, 0.6134, 0.6952, 0.6209, 0.7424, 0.6761, 0.6856, 0.6062,\n",
      "        0.7738, 0.7131, 0.6981, 0.7106, 0.7847, 0.7695, 0.6186, 0.7211, 0.5529,\n",
      "        0.6929, 0.7347, 0.6405, 0.8081, 0.7004, 0.7631, 0.6371, 0.6938, 0.7255,\n",
      "        0.7667, 0.6255, 0.7078, 0.6446, 0.6877, 0.6495, 0.7082, 0.7372, 0.6534,\n",
      "        0.7396], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.2815, 1.3973,    nan, 1.1170, 1.4607,    nan, 1.0251, 1.1030, 1.6748,\n",
      "           nan, 1.7719, 2.0306,    nan,    nan,    nan, 1.2271,    nan,    nan,\n",
      "        1.3930,    nan,    nan,    nan,    nan,    nan, 1.0378,    nan,    nan,\n",
      "           nan, 1.0308,    nan, 2.4965,    nan, 1.4118,    nan,    nan,    nan,\n",
      "        2.0075, 0.8714, 1.3365, 1.2367, 1.8700, 1.3812,    nan, 0.9136,    nan,\n",
      "        0.8834, 1.5882,    nan, 1.3159, 1.3018, 1.1988,    nan, 0.9993, 1.0737,\n",
      "        1.0459,    nan, 0.8656,    nan,    nan,    nan, 1.4007, 1.1424,    nan,\n",
      "        1.7107], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6188, 0.6515, 0.5827, 0.7210, 0.7470, 0.7639, 0.5460, 0.7010, 0.7184,\n",
      "        0.7457, 0.7183, 0.7416, 0.5337, 0.6607, 0.7276, 0.4884, 0.7453, 0.6128,\n",
      "        0.6027, 0.8271, 0.7258, 0.6052, 0.5657, 0.6858, 0.7414, 0.8172, 0.7162,\n",
      "        0.6535, 0.7629, 0.6259, 0.6569, 0.7350, 0.5835, 0.6434, 0.7227, 0.7199,\n",
      "        0.6072, 0.7815, 0.6263, 0.6568, 0.6800, 0.6438, 0.6305, 0.7598, 0.4488,\n",
      "        0.6126, 0.5642, 0.7503, 0.7661, 0.6567, 0.7180, 0.5775, 0.7259, 0.6108,\n",
      "        0.7455, 0.6073, 0.7724, 0.6870, 0.7124, 0.6727, 0.4933, 0.7561, 0.6610,\n",
      "        0.5988], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan, 0.8792, 1.7275, 1.5126,    nan, 1.2305, 1.4277,\n",
      "        0.9862, 1.2938, 2.2276,    nan,    nan, 1.2958,    nan, 0.9232,    nan,\n",
      "           nan, 1.9729, 1.3841,    nan,    nan,    nan, 1.9794, 2.8475, 1.1907,\n",
      "           nan, 1.9726,    nan,    nan, 1.4664,    nan,    nan, 1.1015, 1.9880,\n",
      "           nan, 1.4304,    nan,    nan, 1.8748,    nan,    nan, 1.5691,    nan,\n",
      "           nan,    nan, 1.2684, 1.4205,    nan, 1.2555,    nan,    nan,    nan,\n",
      "        1.4438,    nan, 2.8077, 1.2078, 1.9243,    nan,    nan, 2.2344, 1.7127,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6662, 0.7159, 0.6998, 0.6471, 0.7319, 0.5656, 0.7967, 0.8629, 0.6116,\n",
      "        0.7605, 0.8191, 0.6925, 0.6691, 0.7410, 0.7766, 0.8030, 0.7073, 0.6694,\n",
      "        0.6425, 0.7432, 0.6920, 0.6844, 0.7779, 0.6809, 0.7086, 0.7900, 0.7337,\n",
      "        0.6568, 0.5953, 0.6063, 0.7224, 0.7961, 0.7512, 0.8181, 0.5661, 0.7148,\n",
      "        0.5442, 0.8175, 0.6733, 0.8895, 0.7731, 0.7055, 0.6909, 0.5433, 0.7639,\n",
      "        0.8283, 0.6450, 0.6513, 0.7208, 0.7343, 0.6600, 0.5921, 0.7056, 0.5470,\n",
      "        0.7032, 0.6316, 0.5048, 0.7028, 0.7278, 0.5978, 0.8752, 0.6552, 0.5732,\n",
      "        0.7497], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.5956, 1.2508, 1.3494, 1.7051, 1.1084, 3.7906, 1.9006, 1.5541, 1.7896,\n",
      "        1.2286, 2.0596, 1.1657, 2.0931, 2.2911, 1.5884, 1.5714, 1.0498, 1.7475,\n",
      "        3.8055, 1.1483, 0.9570, 0.9443, 1.1498, 0.0000, 0.9492, 1.7861, 1.2615,\n",
      "        2.2353, 2.8152, 3.9327, 1.0190, 2.1485, 1.6287, 1.4890, 3.0758, 1.8225,\n",
      "        3.2338, 1.6483, 1.3727, 2.5076, 1.1680, 2.1558, 1.0470, 3.0901, 1.2582,\n",
      "        1.5222, 2.5804, 1.8958, 1.5611, 1.2917, 1.3138, 2.9574, 0.9411, 3.8188,\n",
      "        1.0154, 2.9499, 3.4677, 1.1400, 1.9877, 3.1839, 2.6446, 0.7819, 2.6312,\n",
      "        1.0287], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.5719, 0.7592, 0.5938, 0.6207, 0.6268, 0.6442, 0.6039, 0.6690, 0.6063,\n",
      "        0.6029, 0.6769, 0.6250, 0.8591, 0.8523, 0.7175, 0.5853, 0.7797, 0.6649,\n",
      "        0.7004, 0.6667, 0.6651, 0.7343, 0.7055, 0.6374, 0.7166, 0.6838, 0.6384,\n",
      "        0.7254, 0.4215, 0.7267, 0.7845, 0.8024, 0.7624, 0.6952, 0.6090, 0.7025,\n",
      "        0.6174, 0.6625, 0.6146, 0.7192, 0.6826, 0.6679, 0.6391, 0.5887, 0.5695,\n",
      "        0.6238, 0.6918, 0.6129, 0.6520, 0.6688, 0.8142, 0.6652, 0.6747, 0.5863,\n",
      "        0.7478, 0.7904, 0.6035, 0.7521, 0.8399, 0.7015, 0.8450, 0.5604, 0.7396,\n",
      "        0.6746], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 1., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0.\n",
      " 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0.]\n",
      "Mask\n",
      "[1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "Distance\n",
      "tensor([1.1533, 2.0105, 2.6722, 3.0839, 1.5529, 2.2276, 2.4800, 2.3993, 2.1162,\n",
      "        1.3334, 2.7115, 2.3761, 1.7001, 2.0396, 1.2790, 2.3793, 1.5981, 2.2673,\n",
      "        1.0301, 2.6018, 2.1188, 1.2961, 2.3644, 1.7791, 2.2028, 2.5726, 2.0061,\n",
      "        1.6209, 4.1505, 1.1740, 2.7955, 2.0321, 1.1751, 1.1432, 2.3605, 1.2792,\n",
      "        1.9712, 2.1836, 1.9861, 1.2064, 1.5186, 2.5405, 2.3603, 2.7660, 3.5493,\n",
      "        2.2041, 2.2663, 1.8163, 2.6071, 2.0882, 1.8733, 1.7737, 2.1742, 2.8393,\n",
      "        1.5587, 1.5599, 1.5987, 0.9958, 1.6697, 2.2076, 1.9319, 2.4536, 1.9696,\n",
      "        1.0192], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.8403, 0.8364, 0.6325, 0.6132, 0.6993, 0.8649, 0.7655, 0.6918, 0.7812,\n",
      "        0.8193, 0.6550, 0.6850, 0.6893, 0.7265, 0.7580, 0.6449, 0.7788, 0.7927,\n",
      "        0.5804, 0.7807, 0.5411, 0.7431, 0.6372, 0.7411, 0.7113, 0.8252, 0.8667,\n",
      "        0.6695, 0.6502, 0.6015, 0.7259, 0.6413, 0.5631, 0.6336, 0.7872, 0.7424,\n",
      "        0.6297, 0.6519, 0.7230, 0.7214, 0.6004, 0.7546, 0.6168, 0.7385, 0.7331,\n",
      "        0.7006, 0.7048, 0.7054, 0.6130, 0.5866, 0.5709, 0.8096, 0.7012, 0.7616,\n",
      "        0.7410, 0.6700, 0.6071, 0.7436, 0.5920, 0.6184, 0.7494, 0.7947, 0.7470,\n",
      "        0.6836], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 0., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.2193, 2.3699,    nan,    nan, 1.2780, 1.7192, 1.3118, 2.1034, 2.7108,\n",
      "        1.1388, 1.9411,    nan, 1.5029, 1.3497, 0.8356,    nan, 1.8265, 1.8872,\n",
      "           nan, 1.0847,    nan, 1.2337,    nan, 1.0740, 1.5570, 2.5545, 1.6244,\n",
      "           nan,    nan,    nan, 1.2419,    nan,    nan,    nan, 1.3977, 1.2930,\n",
      "           nan,    nan, 1.7428, 1.2555,    nan, 0.7746,    nan, 1.9814, 1.4275,\n",
      "        1.3759, 1.6683, 1.0556,    nan,    nan,    nan, 2.0353,    nan, 1.1605,\n",
      "        1.0987,    nan,    nan, 2.1166,    nan,    nan, 1.0014, 1.7374, 1.6198,\n",
      "        1.4771], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6955, 0.6827, 0.6634, 0.5525, 0.8117, 0.7286, 0.6729, 0.7567, 0.7613,\n",
      "        0.7499, 0.6245, 0.7208, 0.5903, 0.6415, 0.5291, 0.5901, 0.6166, 0.6647,\n",
      "        0.7400, 0.6487, 0.7999, 0.6204, 0.6856, 0.5390, 0.5407, 0.6704, 0.6166,\n",
      "        0.7287, 0.7529, 0.7524, 0.6201, 0.6530, 0.7757, 0.7024, 0.6668, 0.7556,\n",
      "        0.7887, 0.7087, 0.6155, 0.6862, 0.7250, 0.7476, 0.6049, 0.8437, 0.6829,\n",
      "        0.5758, 0.7800, 0.7560, 0.7023, 0.6970, 0.8605, 0.5498, 0.8289, 0.7112,\n",
      "        0.7255, 0.8557, 0.6295, 0.7770, 0.5972, 0.6112, 0.5661, 0.7364, 0.6837,\n",
      "        0.5484], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 1., 1., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.2876,    nan,    nan, 1.9436,    nan, 1.7025, 1.3477, 1.5108,\n",
      "        1.6567,    nan, 1.2198,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "        2.6322,    nan, 1.6139,    nan, 1.3879,    nan,    nan,    nan,    nan,\n",
      "        0.9889, 1.8458, 1.2712,    nan,    nan, 1.5944,    nan,    nan, 1.4837,\n",
      "        1.5877, 1.0262,    nan, 0.9391, 1.1556, 1.9411,    nan, 1.6460, 1.3833,\n",
      "           nan, 1.5455, 2.1638, 1.8037, 1.0977, 1.8289,    nan, 2.2000, 1.3078,\n",
      "        1.0959, 2.1150,    nan, 1.0564,    nan,    nan,    nan, 1.6494,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6893, 0.6104, 0.6688, 0.8401, 0.4314, 0.7113, 0.6306, 0.6288, 0.6770,\n",
      "        0.6916, 0.6157, 0.7897, 0.5842, 0.6512, 0.7238, 0.6014, 0.7229, 0.7177,\n",
      "        0.6766, 0.6407, 0.7596, 0.6680, 0.6006, 0.5901, 0.6942, 0.5334, 0.7449,\n",
      "        0.7319, 0.6933, 0.7478, 0.7129, 0.6920, 0.6488, 0.5321, 0.5870, 0.7375,\n",
      "        0.6716, 0.5967, 0.6168, 0.6946, 0.7225, 0.6596, 0.6120, 0.7547, 0.7561,\n",
      "        0.8211, 0.7481, 0.7350, 0.5966, 0.6065, 0.5777, 0.9410, 0.6860, 0.5292,\n",
      "        0.5766, 0.6128, 0.5133, 0.7789, 0.6386, 0.6034, 0.7216, 0.8468, 0.6408,\n",
      "        0.6025], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 0., 0., 1., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.8229,    nan,    nan, 4.0752,    nan, 1.9899,    nan,    nan,    nan,\n",
      "        2.2870,    nan, 1.9174, 2.4364, 2.4973, 2.1273,    nan, 1.0583, 1.5220,\n",
      "           nan,    nan, 1.2744,    nan,    nan,    nan, 1.2354,    nan,    nan,\n",
      "        0.8329,    nan, 1.3210, 1.0940,    nan,    nan,    nan,    nan, 1.4966,\n",
      "        1.8125,    nan,    nan, 1.1232, 1.5416,    nan,    nan, 1.1069, 1.5438,\n",
      "        2.0826, 1.8368, 2.8558,    nan,    nan,    nan, 2.5402,    nan,    nan,\n",
      "           nan,    nan,    nan, 1.9714, 1.9083,    nan, 1.1924, 2.4014,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7156, 0.6817, 0.7427, 0.7184, 0.6595, 0.5830, 0.5780, 0.7350, 0.7465,\n",
      "        0.6266, 0.6535, 0.6988, 0.7513, 0.6573, 0.6587, 0.7252, 0.6805, 0.6377,\n",
      "        0.9008, 0.5515, 0.7005, 0.7168, 0.6304, 0.5821, 0.6497, 0.7650, 0.7724,\n",
      "        0.7109, 0.8071, 0.5991, 0.5312, 0.5851, 0.6220, 0.6981, 0.8049, 0.7673,\n",
      "        0.5650, 0.7177, 0.6776, 0.6864, 0.6372, 0.7071, 0.8012, 0.6747, 0.7160,\n",
      "        0.6201, 0.7139, 0.7714, 0.7982, 0.6747, 0.8572, 0.5379, 0.9811, 0.5905,\n",
      "        0.5990, 0.6652, 0.6045, 0.7554, 0.7257, 0.7140, 0.7225, 0.6370, 0.8115,\n",
      "        0.7202], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 1., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n",
      " 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.1264,    nan, 1.8355, 0.9652,    nan,    nan,    nan, 0.9353,    nan,\n",
      "           nan,    nan, 1.1200, 0.9007,    nan,    nan, 1.0720,    nan,    nan,\n",
      "        3.2095,    nan, 1.4898, 0.8559,    nan,    nan,    nan, 1.1673, 1.8078,\n",
      "        1.1974, 1.8677,    nan,    nan,    nan,    nan,    nan, 1.6964, 1.0764,\n",
      "           nan, 1.1178, 1.6255, 2.1786,    nan, 1.1260, 1.6697,    nan,    nan,\n",
      "           nan, 1.1040, 1.6158, 1.7055, 1.1547, 2.2996,    nan, 2.9339,    nan,\n",
      "           nan,    nan,    nan, 1.2365, 0.9970, 1.0420, 1.6486,    nan, 1.5720,\n",
      "        1.1240], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.7709, 0.6721, 0.7480, 0.5557, 0.7172, 0.8102, 0.5077, 0.5995, 0.6562,\n",
      "        0.7406, 0.7264, 0.5174, 0.7890, 0.7636, 0.5579, 0.7150, 0.6640, 0.6387,\n",
      "        0.7328, 0.8401, 0.7294, 0.7157, 0.5634, 0.6481, 0.5322, 0.7172, 0.5146,\n",
      "        0.6718, 0.4895, 0.6621, 0.6719, 0.6042, 0.8009, 0.7526, 0.6535, 0.6816,\n",
      "        0.7598, 0.7673, 0.7440, 0.6534, 0.7452, 0.7290, 0.6888, 0.6454, 0.6952,\n",
      "        0.6034, 0.7177, 0.5928, 0.6423, 0.7548, 0.6276, 0.7005, 0.5778, 0.7126,\n",
      "        0.7189, 0.6796, 0.7445, 0.6992, 0.7870, 0.6349, 0.5714, 0.7122, 0.6540,\n",
      "        0.7933], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 0., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0.\n",
      " 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.9111, 2.3712, 1.4384, 3.3588, 0.9912, 1.5103, 3.3993, 3.0555, 2.2892,\n",
      "        0.9553, 0.7361, 2.6076, 1.8437, 2.0654, 2.3485, 2.6800, 3.1119, 1.9519,\n",
      "        1.5903, 1.5120, 1.0785, 1.0603, 2.6221, 2.5131, 3.0516, 1.7427, 3.2520,\n",
      "        2.2218, 3.4082, 0.0000, 0.6739, 2.2789, 1.3682, 1.4525, 2.3159, 1.5956,\n",
      "        1.3623, 1.7449, 1.9140, 1.6732, 0.9287, 0.9664, 2.0887, 3.2875, 0.9901,\n",
      "        2.3971, 1.2765, 2.8235, 2.8062, 0.9178, 2.0987, 1.0809, 2.1268, 1.3415,\n",
      "        0.9372, 2.0726, 0.9275, 0.7544, 2.1495, 2.0088, 4.1654, 0.7935, 1.7619,\n",
      "        0.9366], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6885, 0.6032, 0.7081, 0.5520, 0.7257, 0.7117, 0.6389, 0.7406, 0.6277,\n",
      "        0.7280, 0.6295, 0.7204, 0.6304, 0.7035, 0.5969, 0.6259, 0.6340, 0.6647,\n",
      "        0.7526, 0.7947, 0.6697, 0.7172, 0.7301, 0.7550, 0.7151, 0.7000, 0.6351,\n",
      "        0.6737, 0.7007, 0.6798, 0.6603, 0.5717, 0.5903, 0.7898, 0.7284, 0.7307,\n",
      "        0.7632, 0.6074, 0.7544, 0.7378, 0.7547, 0.7198, 0.7394, 0.5576, 0.5358,\n",
      "        0.7045, 0.6855, 0.7387, 0.7468, 0.8134, 0.5993, 0.6196, 0.7200, 0.6156,\n",
      "        0.7329, 0.6410, 0.7471, 0.6610, 0.5526, 0.6739, 0.6314, 0.6820, 0.6180,\n",
      "        0.4964], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 0., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 1.9206,    nan, 1.4218, 1.0015,    nan, 1.8957,    nan,\n",
      "        1.6238,    nan, 0.7912,    nan, 1.0695,    nan,    nan,    nan, 1.5285,\n",
      "        1.4211, 1.6587,    nan, 1.1181, 2.1133, 1.7068, 0.7475, 1.5222,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan, 2.0308, 1.3800, 1.2789,\n",
      "        0.8991,    nan, 1.7646, 1.2659, 1.0317, 1.0520, 2.0117,    nan,    nan,\n",
      "        1.3403, 1.4306, 1.0718, 1.2702, 1.8957,    nan,    nan, 1.3043,    nan,\n",
      "           nan,    nan, 1.5142,    nan,    nan, 1.0847,    nan,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7215, 0.5991, 0.6464, 0.6923, 0.6202, 0.7312, 0.7686, 0.6472, 0.7261,\n",
      "        0.7458, 0.6194, 0.6862, 0.7303, 0.6784, 0.6122, 0.7256, 0.5852, 0.6871,\n",
      "        0.7702, 0.7204, 0.4742, 0.5825, 0.7434, 0.7474, 0.7021, 0.7665, 0.6628,\n",
      "        0.7831, 0.7271, 0.7122, 0.5714, 0.6324, 0.7591, 0.5900, 0.7620, 0.7145,\n",
      "        0.7384, 0.5781, 0.7345, 0.5373, 0.7152, 0.6139, 0.7831, 0.5824, 0.6852,\n",
      "        0.8522, 0.5636, 0.7804, 0.8769, 0.7078, 0.8001, 0.8424, 0.6067, 0.5972,\n",
      "        0.6984, 0.5911, 0.6424, 0.7089, 0.6254, 0.7190, 0.6870, 0.8677, 0.6358,\n",
      "        0.6652], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 1., 0., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan, 1.7711,    nan, 1.4233, 1.4207,    nan, 1.2878,\n",
      "        1.7972,    nan, 1.7310, 1.2527, 1.5979,    nan, 2.5988,    nan,    nan,\n",
      "        1.5664, 1.2388,    nan,    nan, 1.1834, 1.5193, 1.6708, 1.0969,    nan,\n",
      "        1.7025, 1.1461, 1.7293,    nan,    nan, 1.2890,    nan, 1.7938, 1.9030,\n",
      "        1.0236,    nan, 1.2303,    nan, 2.3089,    nan, 1.3085,    nan,    nan,\n",
      "        1.9781,    nan, 1.4925, 2.1644, 1.1311, 1.6501, 2.3563,    nan,    nan,\n",
      "        1.4766,    nan,    nan, 1.0683,    nan, 1.3120, 1.7257, 1.7376,    nan,\n",
      "        1.7057], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7456, 0.5299, 0.6812, 0.5541, 0.5085, 0.7730, 0.7969, 0.7130, 0.6079,\n",
      "        0.7660, 0.6650, 0.5996, 0.6550, 0.6832, 0.7451, 0.7349, 0.6810, 0.7016,\n",
      "        0.6199, 0.5910, 0.7314, 0.7330, 0.8016, 0.6925, 0.7076, 0.7358, 0.7076,\n",
      "        0.7514, 0.7237, 0.6034, 0.6379, 0.7437, 0.6099, 0.6858, 0.7557, 0.7496,\n",
      "        0.7059, 0.5343, 0.6268, 0.5837, 0.7881, 0.4908, 0.6913, 0.7324, 0.7133,\n",
      "        0.6317, 0.6570, 0.6384, 0.7592, 0.7153, 0.6639, 0.7368, 0.6271, 0.7340,\n",
      "        0.5789, 0.6894, 0.5676, 0.7500, 0.7271, 0.7497, 0.6631, 0.7554, 0.7519,\n",
      "        0.6733], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 0., 0., 1., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0.\n",
      " 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([0.9674, 3.3831, 0.9928, 2.5860, 4.3056, 1.4505, 2.8066, 1.4861, 2.5876,\n",
      "        1.2851, 4.2034, 3.0854, 0.0000, 1.1178, 1.7658, 1.4011, 2.9936, 3.6230,\n",
      "        4.1969, 3.8245, 1.2691, 2.0002, 1.1479, 1.0135, 1.1140, 1.2222, 0.9608,\n",
      "        1.2891, 1.2391, 2.6484, 3.2459, 1.1122, 4.5678, 1.0742, 2.2064, 1.2946,\n",
      "        1.0353, 3.1357, 3.0642, 3.2625, 1.4374, 3.4435, 0.9774, 1.4880, 1.0225,\n",
      "        3.6721, 3.5965, 3.9437, 1.5310, 0.8681, 3.7151, 0.8592, 3.5407, 1.3358,\n",
      "        3.0977, 4.0205, 3.0528, 1.9344, 0.6321, 1.7330, 4.0620, 1.7652, 1.4385,\n",
      "        2.4717], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6455, 0.6534, 0.7421, 0.5596, 0.7086, 0.6753, 0.6646, 0.6633, 0.7675,\n",
      "        0.6540, 0.5381, 0.7433, 0.6200, 0.7674, 0.5939, 0.7090, 0.7728, 0.6371,\n",
      "        0.7146, 0.5273, 0.7246, 0.5935, 0.6185, 0.6269, 0.6288, 0.7007, 0.7460,\n",
      "        0.6716, 0.7153, 0.6735, 0.6603, 0.7454, 0.7184, 0.6717, 0.7092, 0.7220,\n",
      "        0.5634, 0.6460, 0.7066, 0.7952, 0.7156, 0.7262, 0.6034, 0.6617, 0.6699,\n",
      "        0.8434, 0.7675, 0.5867, 0.7408, 0.7141, 0.7764, 0.7826, 0.6576, 0.8969,\n",
      "        0.7912, 0.6503, 0.6771, 0.5897, 0.7132, 0.8245, 0.6447, 0.6379, 0.5516,\n",
      "        0.6910], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.2871, 1.5481, 0.8777, 2.0981, 2.2996, 2.1814, 3.2119, 3.9022, 2.0197,\n",
      "        2.1563, 2.5487, 1.0430, 2.0215, 1.7160, 2.3641, 1.1965, 1.3101, 1.4815,\n",
      "        1.0689, 2.9524, 1.1147, 1.7593, 1.8315, 1.6167, 2.2269, 1.3146, 0.9169,\n",
      "        1.5884, 1.6542, 2.4813, 2.4294, 1.4285, 1.0179, 1.9534, 1.4845, 1.7479,\n",
      "        2.2674, 2.1602, 1.5600, 1.5973, 1.5527, 1.2434, 2.5109, 1.8089, 2.4519,\n",
      "        2.6683, 2.0176, 3.2578, 1.3397, 1.0404, 2.5934, 1.3309, 1.4626, 1.5260,\n",
      "        1.4324, 3.1008, 1.9458, 3.8149, 1.5485, 1.9580, 0.0000, 2.3517, 2.8056,\n",
      "        1.9614], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m loss_fn_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_cap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m loss_cap\n\u001b[1;32m     28\u001b[0m loss_fn_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprint_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid_with_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCappedBCELoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     31\u001b[0m     _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network, embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:94\u001b[0m, in \u001b[0;36mtrain_sigmoid_with_embeddings\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     92\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(output\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mfloat(), smote_target, embeds\u001b[38;5;241m=\u001b[39membeds)\n\u001b[1;32m     93\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m---> 94\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# distance + capped loss\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_cap = 5.0\n",
    "loss_fn_args['distance'] = 'euclidean'\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(start_epoch):\n",
    "            loss_fn_args['loss_cap'] = None\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "        for epoch in range(start_epoch, n_epochs + 1):\n",
    "            loss_fn_args['loss_cap'] = loss_cap\n",
    "            loss_fn_args['print_loss']=True\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"distance_capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], loss_fn_args['loss_cap'], norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd4a6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2e6fbeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011167092323303223, AUC: 0.386697\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006031518578529358, AUC: 0.858937\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007109511196613312, AUC: 0.7610769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870960295200347, AUC: 0.8417955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001104489803314209, AUC: 0.600618\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000642341673374176, AUC: 0.8279484999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006148490905761719, AUC: 0.850252\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000648755669593811, AUC: 0.8505670000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003014646053314209, AUC: 0.291365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004923479855060577, AUC: 0.879308\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006954239904880524, AUC: 0.875877\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011900174617767334, AUC: 0.828035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005821780920028686, AUC: 0.2825655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006962125897407532, AUC: 0.5029950000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007013527750968933, AUC: 0.500999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006328832805156708, AUC: 0.8417349999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010350900292396545, AUC: 0.5116319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005985706448554993, AUC: 0.869651\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006641439795494079, AUC: 0.8528939999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006386174559593201, AUC: 0.869876\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004070553302764892, AUC: 0.28285150000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006690648198127747, AUC: 0.7827440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006640035510063171, AUC: 0.836218\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006909519731998444, AUC: 0.8481285000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010408833980560302, AUC: 0.42287100000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005983325839042663, AUC: 0.8296979999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005095854103565216, AUC: 0.8688549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008358694911003113, AUC: 0.869614\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019695369005203246, AUC: 0.529735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006076576709747315, AUC: 0.817355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006246574521064759, AUC: 0.8117829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006132007539272308, AUC: 0.8284549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031859323978424073, AUC: 0.5519069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00064655601978302, AUC: 0.7904860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006860113739967347, AUC: 0.729322\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940434575080872, AUC: 0.7799879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017462496161460876, AUC: 0.5587095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006375198364257813, AUC: 0.803598\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951749324798584, AUC: 0.7356915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006903870105743409, AUC: 0.8188140000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003682754635810852, AUC: 0.72714\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931703984737396, AUC: 0.4980015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931946575641632, AUC: 0.4960035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932390928268432, AUC: 0.497485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008150892972946166, AUC: 0.615618\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006959446072578431, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00069865682721138, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006991885304450989, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005849985837936401, AUC: 0.530389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007183641493320465, AUC: 0.7454580000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006590591669082642, AUC: 0.8046010000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000667313128709793, AUC: 0.831111\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036837735176086426, AUC: 0.6623190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005900403559207917, AUC: 0.854571\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006531637310981751, AUC: 0.8704399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008710229694843292, AUC: 0.831809\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006570617437362671, AUC: 0.307783\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007063640952110291, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007778040170669556, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008163844347000122, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007283446311950684, AUC: 0.294702\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006219432353973389, AUC: 0.767158\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007401881217956543, AUC: 0.8477879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007037338614463806, AUC: 0.809848\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003341115713119507, AUC: 0.645134\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006473135650157929, AUC: 0.763382\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008510353863239288, AUC: 0.8070969999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001051732063293457, AUC: 0.75856\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008944459557533264, AUC: 0.5253450000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006474015414714813, AUC: 0.8412895000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005467826128005981, AUC: 0.852883\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009898068308830262, AUC: 0.8218835000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002038348078727722, AUC: 0.4345785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000632057934999466, AUC: 0.8526094999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007608428597450257, AUC: 0.8438945000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008893090188503265, AUC: 0.8455439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014304816722869873, AUC: 0.5787530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005523658692836762, AUC: 0.8657459999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008717012703418731, AUC: 0.8040055000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012707716226577758, AUC: 0.7470045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008962847232818604, AUC: 0.3239725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006552395522594452, AUC: 0.787344\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006126144826412201, AUC: 0.8505949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006459107398986817, AUC: 0.8330690000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001517598569393158, AUC: 0.37078999999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006873375773429871, AUC: 0.6304895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006447897851467133, AUC: 0.8148749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005917177200317383, AUC: 0.8342799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012216086983680725, AUC: 0.410544\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006230823695659637, AUC: 0.8173069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006082694232463836, AUC: 0.8354685000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005996787548065186, AUC: 0.8433635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008937579691410065, AUC: 0.5942160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000516150712966919, AUC: 0.86044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006218810081481934, AUC: 0.879383\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008213289082050323, AUC: 0.867273\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013579566478729248, AUC: 0.420485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006716706454753875, AUC: 0.7562589999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006357133984565735, AUC: 0.8217745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000607578158378601, AUC: 0.8418739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009739689528942108, AUC: 0.507497\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006433453261852264, AUC: 0.7997609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006650987565517425, AUC: 0.754156\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007035081684589386, AUC: 0.612663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006212436676025391, AUC: 0.338569\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006442999541759491, AUC: 0.830352\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006167554557323456, AUC: 0.855208\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006213405728340149, AUC: 0.8599019999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030257081985473632, AUC: 0.4369275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006047576069831848, AUC: 0.8590180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006385755836963654, AUC: 0.8453775000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006208844482898712, AUC: 0.8663074999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000844988614320755, AUC: 0.48914199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006432944536209107, AUC: 0.8134499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006319419741630555, AUC: 0.8238475000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006463786959648132, AUC: 0.8245465000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012476526498794555, AUC: 0.49352500000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006240181028842926, AUC: 0.821736\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006657649576663971, AUC: 0.7625749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006580678224563599, AUC: 0.8098390000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005461705923080444, AUC: 0.6724304999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006305331885814667, AUC: 0.7906440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006276651918888092, AUC: 0.8245330000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006302262842655181, AUC: 0.837702\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005111518144607544, AUC: 0.704111\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006351277232170105, AUC: 0.7958849999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006340876817703247, AUC: 0.8140925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000620926558971405, AUC: 0.8270389999999999\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004506844282150269, AUC: 0.4027195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006527964770793915, AUC: 0.7828725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006425573527812958, AUC: 0.8165255000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006454679667949677, AUC: 0.8199185000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018708327412605287, AUC: 0.7087910000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006200909316539764, AUC: 0.8200980000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00060707026720047, AUC: 0.8420890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006036153733730317, AUC: 0.8501780000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018799042701721192, AUC: 0.435471\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006488969027996063, AUC: 0.7791954999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006456703245639801, AUC: 0.7969269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006377524137496948, AUC: 0.8093465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030028048753738402, AUC: 0.6811449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006707220077514648, AUC: 0.7317315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006651672124862671, AUC: 0.7647250000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000675165593624115, AUC: 0.7217529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015551764965057373, AUC: 0.431527\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006551293730735778, AUC: 0.755158\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000630591481924057, AUC: 0.820212\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006192907989025116, AUC: 0.8332430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003594107508659363, AUC: 0.45831600000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006502471566200256, AUC: 0.7843225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006359868347644806, AUC: 0.823748\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006457171142101287, AUC: 0.8039014999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023303091526031493, AUC: 0.49453499999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006688508987426758, AUC: 0.6637615000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006485538482666016, AUC: 0.7582665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006592766940593719, AUC: 0.759756\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006414365291595459, AUC: 0.47114600000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006454251706600189, AUC: 0.7980594999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000623058170080185, AUC: 0.8309124999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006380527019500732, AUC: 0.8154155000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001017431139945984, AUC: 0.506379\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006403837502002716, AUC: 0.7945329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006420062482357025, AUC: 0.8130939999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007151409089565277, AUC: 0.6614549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012609771490097047, AUC: 0.5167619999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006104160845279694, AUC: 0.8304\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005727035701274871, AUC: 0.8629475000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006096052825450897, AUC: 0.8482730000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003424067497253418, AUC: 0.624035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006540656387805939, AUC: 0.799191\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006554121971130371, AUC: 0.8200145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006740454733371735, AUC: 0.8278089999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002012110710144043, AUC: 0.58151\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006342897713184357, AUC: 0.8511870000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000651513010263443, AUC: 0.8589120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006493346095085144, AUC: 0.832655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007157927751541138, AUC: 0.653856\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006741209328174591, AUC: 0.7315010000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007046773433685302, AUC: 0.7206349999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007337704598903656, AUC: 0.727763\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001929965913295746, AUC: 0.723617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006455275416374207, AUC: 0.8169630000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006436339020729065, AUC: 0.8202835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005817281603813172, AUC: 0.8600785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003946157217025757, AUC: 0.35955200000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006627475619316101, AUC: 0.8113495000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006152502596378326, AUC: 0.8706630000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006735461354255676, AUC: 0.840601\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015395767092704774, AUC: 0.38436200000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000650970458984375, AUC: 0.8354475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006349138617515564, AUC: 0.8514140000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006325644850730896, AUC: 0.8483330000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026759549379348755, AUC: 0.699327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006425965130329132, AUC: 0.837047\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006524446904659271, AUC: 0.8588120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006378405690193176, AUC: 0.8879790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033821326494216917, AUC: 0.39384549999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006031894087791443, AUC: 0.8463044999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000638938844203949, AUC: 0.8333230000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006279009878635407, AUC: 0.8521135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032605456113815306, AUC: 0.5082249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006222620904445649, AUC: 0.8491980000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007118767499923706, AUC: 0.8699465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008287992477416992, AUC: 0.8676384999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007854342460632324, AUC: 0.616108\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006404253542423248, AUC: 0.851687\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006514720618724823, AUC: 0.743521\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006090995967388154, AUC: 0.792372\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002396261692047119, AUC: 0.586344\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006002520322799683, AUC: 0.8520080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007673485577106476, AUC: 0.8214509999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000831710934638977, AUC: 0.8107489999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008516221046447754, AUC: 0.540615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006150689721107483, AUC: 0.8242820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006093184947967529, AUC: 0.8167479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007971402704715729, AUC: 0.850568\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014432351589202881, AUC: 0.6370979999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006632784307003021, AUC: 0.784486\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005500666797161102, AUC: 0.869852\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007410721480846405, AUC: 0.8736290000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029079641103744508, AUC: 0.44233000000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00064616858959198, AUC: 0.798807\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007127874791622162, AUC: 0.8362280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007203975319862365, AUC: 0.87533\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01647649669647217, AUC: 0.48802349999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00074449223279953, AUC: 0.7880375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000776654839515686, AUC: 0.8620509999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007252789735794068, AUC: 0.8569089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002321468472480774, AUC: 0.5451005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006163032352924347, AUC: 0.8529359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008131569921970368, AUC: 0.793421\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008159118294715881, AUC: 0.8478385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017147484421730043, AUC: 0.48178\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006332261860370635, AUC: 0.84343\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008276103436946869, AUC: 0.8601350000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008118048608303071, AUC: 0.8697895000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001215238869190216, AUC: 0.713287\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007109126746654511, AUC: 0.8230355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006799267530441284, AUC: 0.871839\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007423769235610962, AUC: 0.8923549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000865539014339447, AUC: 0.3507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006276043653488159, AUC: 0.8354545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006278457641601563, AUC: 0.8520575000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006317637860774993, AUC: 0.8612360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0042450916767120365, AUC: 0.5442055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006913377642631531, AUC: 0.5867089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006540327668190003, AUC: 0.8006775000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006782406568527222, AUC: 0.755042\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002738734483718872, AUC: 0.5124245000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006218081712722778, AUC: 0.8459670000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006169845163822174, AUC: 0.854941\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006157867014408111, AUC: 0.882121\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007815569639205933, AUC: 0.496174\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006372871696949005, AUC: 0.7805260000000002\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0005985681712627411, AUC: 0.821609\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006865729093551635, AUC: 0.7165375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008287043273448944, AUC: 0.524346\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000615205317735672, AUC: 0.8489694999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005668532848358154, AUC: 0.880726\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006448164582252503, AUC: 0.834132\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002872795581817627, AUC: 0.682523\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006151713728904724, AUC: 0.848335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000639518678188324, AUC: 0.881957\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005816068947315216, AUC: 0.893872\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002558753967285156, AUC: 0.509605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006600413918495178, AUC: 0.7879510000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006359151005744934, AUC: 0.834866\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006491739749908447, AUC: 0.8294060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002579985499382019, AUC: 0.547603\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006696692407131195, AUC: 0.752434\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006088915169239044, AUC: 0.8270765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006226509511470795, AUC: 0.8307360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002399453639984131, AUC: 0.5354135000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006554636061191559, AUC: 0.78122\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006352069079875946, AUC: 0.8218769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006357714831829071, AUC: 0.8162389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009859429001808167, AUC: 0.5281129999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006691616475582123, AUC: 0.771687\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006289474666118621, AUC: 0.8404530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006705114841461182, AUC: 0.7969920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012093530893325806, AUC: 0.5511170000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006568885743618012, AUC: 0.8129464999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006497064530849457, AUC: 0.8348690000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006378562450408935, AUC: 0.8477945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013916534185409546, AUC: 0.4037875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006520987749099731, AUC: 0.783021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006473710238933564, AUC: 0.8026170000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006493825912475586, AUC: 0.80835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013141767978668212, AUC: 0.569262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006382672190666199, AUC: 0.811207\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006522636711597442, AUC: 0.7667535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006412925124168395, AUC: 0.7478275000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012522388100624083, AUC: 0.588642\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006463400423526764, AUC: 0.7445424999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006443194448947907, AUC: 0.7641754999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000644333004951477, AUC: 0.7757139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003155824542045593, AUC: 0.5250214999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006385092139244079, AUC: 0.8081239999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006316430568695068, AUC: 0.827729\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006353287696838379, AUC: 0.8233750000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016243504285812379, AUC: 0.453808\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006402815878391266, AUC: 0.8378585000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006302759349346161, AUC: 0.8570169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006398556232452392, AUC: 0.8463375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009972366988658906, AUC: 0.46974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006549290418624878, AUC: 0.786769\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006504984200000763, AUC: 0.8053285000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006464186310768127, AUC: 0.8181805000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007296150445938111, AUC: 0.399261\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006667903363704682, AUC: 0.7363664999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006600469350814819, AUC: 0.7587809999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006548345983028412, AUC: 0.7722765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008953097343444825, AUC: 0.6664410000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006459901332855224, AUC: 0.774139\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006434657871723175, AUC: 0.7999825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006329295933246613, AUC: 0.8185020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003010343670845032, AUC: 0.650929\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006342986524105072, AUC: 0.795698\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006366912126541138, AUC: 0.8012079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006145212054252624, AUC: 0.8317430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019065686464309693, AUC: 0.43029599999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000633844792842865, AUC: 0.8369224999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006281218826770783, AUC: 0.857288\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006540034711360931, AUC: 0.8527155000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0070686333179473875, AUC: 0.47005549999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006764470636844635, AUC: 0.75729\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006838809549808502, AUC: 0.7938815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007219825088977814, AUC: 0.7704719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001585438847541809, AUC: 0.512639\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005969812273979188, AUC: 0.8646445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006483840644359589, AUC: 0.8427075000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007412040829658509, AUC: 0.7266725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030903979539871214, AUC: 0.43740199999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006641749739646911, AUC: 0.786327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006803033053874969, AUC: 0.7834180000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007344175279140472, AUC: 0.7337064999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002261680245399475, AUC: 0.5998585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000666724145412445, AUC: 0.7850969999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006744700372219085, AUC: 0.8244719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007174608409404755, AUC: 0.8001904999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009108697772026062, AUC: 0.376419\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006453503966331482, AUC: 0.8545330000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006565827131271362, AUC: 0.8637030000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006542005836963654, AUC: 0.8730235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020546566247940062, AUC: 0.7167220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006834564208984375, AUC: 0.6659039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935597062110901, AUC: 0.706643\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007390382885932922, AUC: 0.661534\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016851609945297242, AUC: 0.5149705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006183342933654785, AUC: 0.8266560000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006165139973163604, AUC: 0.8534605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006577858030796051, AUC: 0.8511185000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007644746601581573, AUC: 0.681346\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006233455836772919, AUC: 0.820471\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006681894361972809, AUC: 0.790943\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006021735668182373, AUC: 0.8552850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0056903259754180905, AUC: 0.6397999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006546853184700012, AUC: 0.7830140000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005921323895454407, AUC: 0.86582\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006127922534942627, AUC: 0.8666529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003387275218963623, AUC: 0.573636\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005819030404090881, AUC: 0.842483\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006873447000980377, AUC: 0.8771874999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010440179109573364, AUC: 0.8564180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012365414500236512, AUC: 0.671203\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007051685750484466, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007244949340820312, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006816542446613312, AUC: 0.8330895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012388140559196471, AUC: 0.393594\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005945240557193756, AUC: 0.8491125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006822313964366912, AUC: 0.8610519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007117080688476562, AUC: 0.875274\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005893571615219116, AUC: 0.540947\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005766810774803161, AUC: 0.8653474999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009652488827705383, AUC: 0.8247009999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008848970532417298, AUC: 0.8603719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018766866326332093, AUC: 0.4677885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006262609660625457, AUC: 0.85872\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008047955930233001, AUC: 0.8575335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006820729672908783, AUC: 0.854156\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001056917428970337, AUC: 0.6985669999999999\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.000771642655134201, AUC: 0.7791220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007035957872867585, AUC: 0.8487339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009450371563434601, AUC: 0.8339224999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001141647696495056, AUC: 0.38259699999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000636038988828659, AUC: 0.8309105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000625913679599762, AUC: 0.8521540000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008344018459320069, AUC: 0.861994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002030069828033447, AUC: 0.4423475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006735901832580567, AUC: 0.8416589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008218474090099335, AUC: 0.8355880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010065958201885222, AUC: 0.8438029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007690939903259277, AUC: 0.342897\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006489973664283753, AUC: 0.8481580000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008452142179012299, AUC: 0.770876\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007234020829200745, AUC: 0.8641479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008416123390197754, AUC: 0.491224\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000644277811050415, AUC: 0.829857\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008760230541229248, AUC: 0.8156110000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007920498549938202, AUC: 0.8760770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000791007936000824, AUC: 0.627872\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006153611242771149, AUC: 0.8181685000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006129129528999329, AUC: 0.8237099999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006172406673431396, AUC: 0.814045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012789042592048646, AUC: 0.732713\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006778664588928222, AUC: 0.699446\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006803655326366424, AUC: 0.723309\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006658150255680084, AUC: 0.8097459999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003712557792663574, AUC: 0.33022050000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006263379454612732, AUC: 0.824365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006382162272930145, AUC: 0.814394\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006707744002342225, AUC: 0.7654439999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013318101167678832, AUC: 0.525192\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011761444807052612, AUC: 0.749506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006208254098892212, AUC: 0.8276614999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006688858866691589, AUC: 0.7336665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00940386962890625, AUC: 0.3917355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006446035504341126, AUC: 0.8214160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006826947629451752, AUC: 0.7134920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006707464754581451, AUC: 0.7770210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008242230713367462, AUC: 0.650641\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006280728578567505, AUC: 0.8001154999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00060952028632164, AUC: 0.8156819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006019294261932373, AUC: 0.8269770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010691524744033812, AUC: 0.46391000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006150645315647125, AUC: 0.8670479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006207771897315979, AUC: 0.8001560000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006198365688323974, AUC: 0.8649965000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007920382738113404, AUC: 0.33797299999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006597490310668946, AUC: 0.7675829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005848396122455597, AUC: 0.8510110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006920455694198608, AUC: 0.6889755000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013054142594337464, AUC: 0.5124150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006663603782653809, AUC: 0.7958445000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006695971190929413, AUC: 0.7887140000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006468302011489868, AUC: 0.8437880000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009068244457244874, AUC: 0.6474485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006775861978530884, AUC: 0.7417125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000610583633184433, AUC: 0.8675039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006571337580680847, AUC: 0.832741\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004912603855133057, AUC: 0.640137\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006636110544204712, AUC: 0.727049\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006821190416812896, AUC: 0.6637230000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006608140468597413, AUC: 0.778229\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008090060949325562, AUC: 0.55802\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006426056027412414, AUC: 0.767259\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006305809319019318, AUC: 0.7874680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006359743773937225, AUC: 0.7888845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007502280712127686, AUC: 0.40528949999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006698185205459595, AUC: 0.7510300000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006640048325061798, AUC: 0.7896745000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006384897530078888, AUC: 0.829197\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012867011427879333, AUC: 0.671285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005477673709392548, AUC: 0.846054\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005128110647201538, AUC: 0.8618239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005000462830066681, AUC: 0.8711260000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017187106609344483, AUC: 0.41594200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005536031126976013, AUC: 0.8371629999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005635906755924225, AUC: 0.851013\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004902825802564621, AUC: 0.867587\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001913213312625885, AUC: 0.657652\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006231704950332642, AUC: 0.8302404999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006171026825904846, AUC: 0.8412639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006111167669296264, AUC: 0.8513710000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007896609008312225, AUC: 0.597113\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006539371311664582, AUC: 0.7882359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006436131894588471, AUC: 0.8228885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006328016519546509, AUC: 0.8299570000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002287644147872925, AUC: 0.35553500000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006486883163452148, AUC: 0.807277\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006395595073699951, AUC: 0.8269859999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006109453439712524, AUC: 0.85014\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008864049315452575, AUC: 0.506929\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005768367350101471, AUC: 0.802956\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005419397354125976, AUC: 0.8285950000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005577489137649536, AUC: 0.835186\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001472331702709198, AUC: 0.549365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006355356276035309, AUC: 0.7932810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006475826799869537, AUC: 0.7811100000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006428416967391967, AUC: 0.7901005000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# distance + capped loss using whole class average tensor\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs=[]\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_caps = [1, 5, 10]\n",
    "loss_fn_args['distance'] = 'cosine'\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(2)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                loss_fn_args['avg_tensors'] = None\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELossAvgDistance, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                loss_fn_args['print_loss']=False\n",
    "                loss_fn_args['avg_tensors'] = []\n",
    "                for k in range(2):\n",
    "                    _, avg_tensor = network(avg_tensors_list[k])\n",
    "                    loss_fn_args['avg_tensors'].append(avg_tensor)\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELossAvgDistance, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "        \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"cosine_distance_capped_smote_avg\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], loss_fn_args['loss_cap'], norm, None]\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6f79075",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "639f921d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0008583230078220368, AUC: 0.43444499999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005388941168785095, AUC: 0.8616320000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005308660864830017, AUC: 0.866123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004951866567134857, AUC: 0.877475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013088712692260742, AUC: 0.5226230000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006043859422206879, AUC: 0.855531\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014131583571434022, AUC: 0.8066570000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006903581321239472, AUC: 0.889435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00092867112159729, AUC: 0.46022599999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006813707053661347, AUC: 0.6800725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006745235919952393, AUC: 0.7929700000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007089492678642273, AUC: 0.7771460000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002867063760757446, AUC: 0.368242\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006618314683437348, AUC: 0.8063965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007057085931301117, AUC: 0.8448909999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010201417803764343, AUC: 0.8662270000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029970524311065676, AUC: 0.317261\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004949656873941421, AUC: 0.8749319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010935214161872863, AUC: 0.843783\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009253529608249664, AUC: 0.86827\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012364006638526917, AUC: 0.48199\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000760403573513031, AUC: 0.824966\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004907668381929398, AUC: 0.8816269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011189890503883362, AUC: 0.8614910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007932246923446656, AUC: 0.567204\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000649562269449234, AUC: 0.8105994999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007014006376266479, AUC: 0.8558819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001000213921070099, AUC: 0.8580869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010540963411331177, AUC: 0.5096350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006098486483097076, AUC: 0.84569\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005522469580173492, AUC: 0.8683169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005703777372837067, AUC: 0.871268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005450478792190552, AUC: 0.588353\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005153369903564453, AUC: 0.858522\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005610305666923523, AUC: 0.8637419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005184028148651123, AUC: 0.869973\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019556466341018675, AUC: 0.4348595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006650846004486084, AUC: 0.7495465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007182583808898926, AUC: 0.5491865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006906697750091553, AUC: 0.773535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00671780776977539, AUC: 0.692122\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006369730532169342, AUC: 0.8202339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006341103315353393, AUC: 0.830431\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005929659307003021, AUC: 0.8391055000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024325718879699707, AUC: 0.664256\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006660756766796112, AUC: 0.8110430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005021544843912125, AUC: 0.867516\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004976052790880203, AUC: 0.869704\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030899120569229128, AUC: 0.520138\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005926059782505036, AUC: 0.7987825000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005598410665988922, AUC: 0.826069\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007510675489902496, AUC: 0.793634\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013597240447998046, AUC: 0.595097\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005361493229866028, AUC: 0.854938\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00048749743402004244, AUC: 0.867055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005675345063209533, AUC: 0.880757\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010069740116596222, AUC: 0.361574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006106729507446289, AUC: 0.8329089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006113706529140472, AUC: 0.8394314999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005993430316448212, AUC: 0.848223\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011201880574226379, AUC: 0.6012419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006539611220359803, AUC: 0.8402539999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000691236674785614, AUC: 0.85748\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006659743189811706, AUC: 0.873186\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029034204483032225, AUC: 0.29356750000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005795241296291351, AUC: 0.833502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005456612706184387, AUC: 0.8811850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008156781196594239, AUC: 0.8638500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001962056756019592, AUC: 0.37737600000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005941676497459411, AUC: 0.8744419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000457743838429451, AUC: 0.8838770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000511616438627243, AUC: 0.8758179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007660882949829102, AUC: 0.3370745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006723164021968841, AUC: 0.7292084999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006742360889911652, AUC: 0.7381550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005849372744560242, AUC: 0.836365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001568294584751129, AUC: 0.470024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006495228409767151, AUC: 0.8163999999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000625961035490036, AUC: 0.8432499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005910392105579376, AUC: 0.8445910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018975435495376587, AUC: 0.5455155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006656229496002197, AUC: 0.7037720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006548465490341187, AUC: 0.7459655000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006489648222923279, AUC: 0.757134\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003970930218696594, AUC: 0.5287735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006147268414497376, AUC: 0.771498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005792110860347747, AUC: 0.8094460000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006093349158763885, AUC: 0.810805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027595105171203615, AUC: 0.613934\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005699459314346313, AUC: 0.839959\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005330704152584076, AUC: 0.857678\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000535402774810791, AUC: 0.867747\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004458563804626465, AUC: 0.315581\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005683842599391938, AUC: 0.822407\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005635464489459991, AUC: 0.83663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000532811850309372, AUC: 0.8498769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006098070859909058, AUC: 0.35796700000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006614099144935608, AUC: 0.765555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006404350101947785, AUC: 0.7935420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006434889733791351, AUC: 0.7965784999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002963059067726135, AUC: 0.43284300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006690295934677124, AUC: 0.7449290000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006609757840633393, AUC: 0.7984860000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006544631421566009, AUC: 0.815279\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022956138849258424, AUC: 0.643438\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005496495962142944, AUC: 0.8514855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005343407988548279, AUC: 0.8637560000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005310011804103851, AUC: 0.861016\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033374519348144532, AUC: 0.3407195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005263854265213012, AUC: 0.851622\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004978960454463958, AUC: 0.864592\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005477036535739898, AUC: 0.8694919999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002559414744377136, AUC: 0.569814\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006580994725227356, AUC: 0.800422\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006516083776950836, AUC: 0.8231649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006451568603515625, AUC: 0.830537\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007428362369537354, AUC: 0.488115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005567219853401185, AUC: 0.814405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006410351097583771, AUC: 0.827464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006132548451423645, AUC: 0.8438680000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cosine distance + capped loss\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "cap_aucs = []\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_caps = [0.5]\n",
    "loss_fn_args['distance'] = 'cosine'\n",
    "\n",
    "\n",
    "start_epoch = 1\n",
    "\n",
    "\n",
    "for loss_cap in loss_caps: \n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(2)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['print_capped'] = False\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['print_capped'] = False\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"cosine_distance_capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], loss_fn_args['loss_cap'], norm, None]\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab098399",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17af0e21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004998538017272949, AUC: 0.5778245\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m15\u001b[39m):\n\u001b[0;32m---> 24\u001b[0m     _, train_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_triplet_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_tripletloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(train_losses))))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:128\u001b[0m, in \u001b[0;36mtrain_triplet_loss\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn_args)\u001b[0m\n\u001b[1;32m    126\u001b[0m neg_embeds \u001b[38;5;241m=\u001b[39m network(neg_data\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m    127\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(anchor_embeds, pos_embeds, neg_embeds)\n\u001b[0;32m--> 128\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "# 2 class triplet loss no ratio \n",
    "# no smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(5e-6, 1e-3), (1e-6, 5e-4), (1e-6, 1e-4), (5e-6, 5e-4)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "    \n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10): \n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(complete_network.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(15):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss, embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_reduced, complete_network, linear_optimizer, verbose=False)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "    \n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"triplet_loss\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], 0.0, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8afa13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02571d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006959781944751739, AUC: 0.44473399999999996\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m15\u001b[39m):\n\u001b[0;32m---> 27\u001b[0m     _, train_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_triplet_loss_smote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_tripletloss_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(train_losses))))\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:164\u001b[0m, in \u001b[0;36mtrain_triplet_loss_smote\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn_args)\u001b[0m\n\u001b[1;32m    162\u001b[0m neg_embeds \u001b[38;5;241m=\u001b[39m network(neg_data)\n\u001b[1;32m    163\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(anchor_embeds, pos_embeds, neg_embeds)\n\u001b[0;32m--> 164\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "# 2 class triplet loss with capped SMOTE \n",
    "\n",
    "# note: sometimes can get a very high accuracy but may diverge\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(5e-6, 1e-3), (1e-6, 5e-4), (1e-6, 1e-4), (5e-6, 5e-4)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = 5.0\n",
    "loss_fn_args['distance'] = 'euclidean'\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10): \n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(complete_network.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(15):\n",
    "            _, train_losses = train.train_triplet_loss_smote(epoch, train_loader_tripletloss_smote, embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_reduced, complete_network, linear_optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"triplet_loss\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], 0.0, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f34cc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0007769797742366791, AUC: 0.581019\n",
      "\n",
      "Train loss: 16.096790712090986\n",
      "Train loss: 7.95101783849016\n",
      "Train loss: 1.6742434924161886\n",
      "Train loss: 0.0\n",
      "Train loss: 6.369154097158698\n",
      "Train loss: 1.0685905746266813\n",
      "Train loss: 0.9919730077815961\n",
      "Train loss: 4.3510085480122624\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 1.729099128819719\n",
      "Train loss: 0.0\n",
      "Train loss: 2.9074361294130737\n",
      "Train loss: 0.0\n",
      "\n",
      "Test set: Avg. loss: 0.0007081505060195923, AUC: 0.595766\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918745040893554, AUC: 0.599685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006822091042995453, AUC: 0.606827\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006758022308349609, AUC: 0.614831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006717154681682586, AUC: 0.620797\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006670982539653778, AUC: 0.6299465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006633270978927612, AUC: 0.641555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006615970730781555, AUC: 0.647514\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006594435572624207, AUC: 0.6563729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006575994789600372, AUC: 0.666123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006559013724327088, AUC: 0.6735810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006542154252529145, AUC: 0.6845415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006518358290195465, AUC: 0.698683\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006498202979564666, AUC: 0.7103269999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006485094726085662, AUC: 0.717825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006470210254192352, AUC: 0.7272419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000646136075258255, AUC: 0.7334495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000645109623670578, AUC: 0.7394789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006443145573139191, AUC: 0.7454485000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000643904983997345, AUC: 0.746941\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006429647505283355, AUC: 0.7538564999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000642709881067276, AUC: 0.7577389999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006426226794719696, AUC: 0.760882\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006425316333770752, AUC: 0.764992\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006421145796775818, AUC: 0.768756\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006414653062820435, AUC: 0.773288\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006413009464740754, AUC: 0.774419\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006411257684230805, AUC: 0.7756689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000640982300043106, AUC: 0.7791325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006405025720596314, AUC: 0.780958\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006400455534458161, AUC: 0.7839640000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006398386657238006, AUC: 0.7868439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006397152841091156, AUC: 0.7885960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006394894123077393, AUC: 0.791209\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006395555436611175, AUC: 0.7915060000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006393527686595916, AUC: 0.7925110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006391496956348419, AUC: 0.794049\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006389279961585999, AUC: 0.795107\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006387557983398437, AUC: 0.7956540000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000638495922088623, AUC: 0.79725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006383010745048523, AUC: 0.797925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006380480825901032, AUC: 0.7989930000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006377894580364227, AUC: 0.8008280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006376721858978271, AUC: 0.8018989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006373582482337952, AUC: 0.8024609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006369325518608093, AUC: 0.803453\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006367611885070801, AUC: 0.8036590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006366349756717682, AUC: 0.8035285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006367577910423278, AUC: 0.8037650000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006367309987545014, AUC: 0.8036895000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023003580570220947, AUC: 0.5429729999999999\n",
      "\n",
      "Train loss: 136.23961547658413\n",
      "Train loss: 17.986870584608635\n",
      "Train loss: 15.341035212142557\n",
      "Train loss: 4.752364243133159\n",
      "Train loss: 4.356625907028778\n",
      "Train loss: 0.7004882715925386\n",
      "Train loss: 1.1259002685546875\n",
      "Train loss: 6.454442579534989\n",
      "Train loss: 4.747905163825313\n",
      "Train loss: 0.3334539147871959\n",
      "Train loss: 3.8129760645612887\n",
      "Train loss: 0.0\n",
      "Train loss: 3.7372857588755934\n",
      "Train loss: 3.130930067617682\n",
      "Train loss: 3.3125161279605915\n",
      "\n",
      "Test set: Avg. loss: 0.0007151560485363006, AUC: 0.45533100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007060944736003875, AUC: 0.4688575000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007020347714424133, AUC: 0.4808285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007004459500312805, AUC: 0.4868415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006991749107837677, AUC: 0.49186949999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006983735859394073, AUC: 0.49412800000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006977417469024659, AUC: 0.5011685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006969517469406128, AUC: 0.511384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006968002021312714, AUC: 0.5048075000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006965127289295197, AUC: 0.5116275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006962951421737671, AUC: 0.514332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006961811780929565, AUC: 0.5129704999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006960669755935669, AUC: 0.505204\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695921242237091, AUC: 0.5012044999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006958386301994324, AUC: 0.4976050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006957364678382874, AUC: 0.49709449999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006956190168857574, AUC: 0.500152\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006955111622810364, AUC: 0.493364\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006954317688941955, AUC: 0.48901300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006953255832195282, AUC: 0.48931600000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952438652515411, AUC: 0.4857020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951936483383179, AUC: 0.48956999999999984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951353847980499, AUC: 0.495209\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695084810256958, AUC: 0.49530450000000015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006950275301933289, AUC: 0.49494200000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006949717700481415, AUC: 0.49411399999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006949259042739868, AUC: 0.49331900000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948856711387634, AUC: 0.493639\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948323547840119, AUC: 0.49492200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947891116142273, AUC: 0.498717\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00069475057721138, AUC: 0.503362\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947134435176849, AUC: 0.505495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946909725666046, AUC: 0.508237\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946579813957214, AUC: 0.504954\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946228444576263, AUC: 0.5067439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945843696594238, AUC: 0.5100250000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945613324642182, AUC: 0.506382\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000694526344537735, AUC: 0.5097929999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944917440414429, AUC: 0.507986\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944626867771149, AUC: 0.506527\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944490969181061, AUC: 0.506454\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000694442331790924, AUC: 0.5044559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944300532341003, AUC: 0.502201\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944151818752288, AUC: 0.5003179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943970620632171, AUC: 0.49821999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943894028663635, AUC: 0.4986275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943775415420532, AUC: 0.49697849999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943643987178803, AUC: 0.500112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943606734275818, AUC: 0.50076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943549513816834, AUC: 0.49980600000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001080885648727417, AUC: 0.41414299999999993\n",
      "\n",
      "Train loss: 24.389437904840783\n",
      "Train loss: 0.8292029658450356\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 1.7853070995475673\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 3.6756908561609967\n",
      "Train loss: 0.0\n",
      "\n",
      "Test set: Avg. loss: 0.0007218130528926849, AUC: 0.518904\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006989753544330597, AUC: 0.540902\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946681439876556, AUC: 0.5388165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931599974632263, AUC: 0.5372144999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918653249740601, AUC: 0.5308125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006907552778720856, AUC: 0.5248035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006905039250850678, AUC: 0.5236075\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006901566684246063, AUC: 0.5230110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006899235546588897, AUC: 0.5261325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006896623373031616, AUC: 0.5253125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894396543502808, AUC: 0.5309490000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006897644698619842, AUC: 0.52842\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006899001598358154, AUC: 0.525083\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006898137331008911, AUC: 0.5242764999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006899007260799408, AUC: 0.5234755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006897071897983551, AUC: 0.5284414999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894522905349732, AUC: 0.5316019999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894703507423401, AUC: 0.5321385000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006892255246639252, AUC: 0.5359525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006892721951007843, AUC: 0.5328990000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006891983449459076, AUC: 0.537638\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006890941560268402, AUC: 0.5369265000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006891008913516998, AUC: 0.53901\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893084347248077, AUC: 0.5359465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893291175365448, AUC: 0.5363020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893750429153442, AUC: 0.535502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000689245194196701, AUC: 0.536058\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006888734996318817, AUC: 0.5422319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006889899671077729, AUC: 0.538168\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006888663470745087, AUC: 0.5418495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006884230673313141, AUC: 0.5464029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006882088780403137, AUC: 0.550612\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006883909702301025, AUC: 0.546473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006878928244113922, AUC: 0.5528475000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006881430745124817, AUC: 0.5479885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000688222199678421, AUC: 0.546259\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006877389848232269, AUC: 0.5549315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006872588694095611, AUC: 0.5582575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870459020137787, AUC: 0.5611119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006872363984584809, AUC: 0.5597035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870978772640228, AUC: 0.5620409999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006871461272239685, AUC: 0.561431\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870918273925781, AUC: 0.561923\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006871341466903687, AUC: 0.5619075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000687317818403244, AUC: 0.551667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870152950286865, AUC: 0.5539374999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006868258118629455, AUC: 0.5586725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006866075992584228, AUC: 0.560983\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006869733929634094, AUC: 0.5588695000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006868163049221039, AUC: 0.5607644999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 class triplet loss with ratio \n",
    "# no smote \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(1e-7, 1e-5)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(3): \n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(complete_network.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(15):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss_ratio, embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "        for epoch in range(50):\n",
    "            _, _ = train.train_linear_probe(epoch, train_loader_reduced, complete_network, linear_optimizer, verbose=False)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"triplet_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], \n",
    "           auc_mean[i][4], auc_variance[i][4],\n",
    "           auc_mean[i][5], auc_variance[i][5],\n",
    "           None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10048fce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "14 columns passed, passed data had 18 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:934\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 934\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_or_indexify_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:981\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    982\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    983\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    984\u001b[0m     )\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 14 columns passed, passed data had 18 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/convnet_aucs.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m df2 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcol_names\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df1, df2])\n\u001b[1;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/convnet_aucs.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:781\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    780\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 781\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    783\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    789\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    790\u001b[0m         arrays,\n\u001b[1;32m    791\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    794\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    795\u001b[0m     )\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:498\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 498\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:840\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    837\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    838\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 840\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:937\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    934\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[0;32m--> 937\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[1;32m    940\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: 14 columns passed, passed data had 18 columns"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de718f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "041af005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006921598315238952, AUC: 0.5632135\n",
      "\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.9810e-01, 1.5458e+00, 1.6848e+01, 1.5036e+01, 1.0000e-05,\n",
      "        2.2834e-01, 6.2688e+00, 1.0000e-05, 3.2163e+00, 1.0000e-05, 7.8707e+00,\n",
      "        7.7019e+00, 6.4259e+00, 6.5882e+00, 2.3989e+01, 2.4655e-01, 5.3974e+01,\n",
      "        1.5999e+01, 1.1811e+00, 2.1506e+01, 2.7749e+00, 1.0000e-05, 3.0049e+00,\n",
      "        1.0000e-05, 3.4720e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 5.6800e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.4092e+02, 1.0000e-05, 1.6968e+01, 2.8015e+00, 7.9793e-01, 2.9657e+00,\n",
      "        6.2538e-01, 1.0000e-05, 2.3499e+01, 2.4282e+01, 6.7770e+01, 1.0000e-05,\n",
      "        9.8537e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.4370e+01, 5.5514e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0707e+01, 1.0000e-05, 6.6199e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.9218e+01, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 6.9518e+00, 3.0094e+01, 1.0000e-05, 5.7642e+00, 9.5099e+00,\n",
      "        1.0000e-05, 1.1625e+01, 1.0000e-05, 4.3892e+00, 1.0000e-05, 1.0000e-05,\n",
      "        4.0294e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.2101e+01, 1.0000e-05,\n",
      "        2.2890e+02, 1.3084e+01, 5.0670e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.3719e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.3049e+00, 1.0000e-05, 1.6318e+00, 1.0000e-05,\n",
      "        1.0000e-05, 2.8902e+01, 2.0815e+01, 1.0000e-05, 1.2433e+01, 2.4487e+01,\n",
      "        1.0000e-05, 5.8444e+00, 1.0000e-05, 1.0000e-05, 3.8971e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.4348e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.2456e+02, 1.0000e-05, 1.0000e-05, 3.3153e+00,\n",
      "        1.0000e-05, 1.0052e+00, 1.0000e-05, 6.7949e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 6.9926e+00, 1.1623e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 5.2753e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0262e+02,\n",
      "        1.0000e-05, 2.2293e+00, 1.0146e+01, 1.0000e-05, 1.0422e+01, 7.3951e+00,\n",
      "        5.5069e+01, 9.1311e-01, 1.0000e-05, 4.7311e+00, 3.4377e+01, 1.0000e-05,\n",
      "        4.7039e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.8967e+00, 1.0000e-05,\n",
      "        7.1934e+00, 2.1237e+01, 4.1615e+00, 5.0780e+00, 1.8998e+02, 1.6187e+01,\n",
      "        1.4891e+00, 2.3622e+00, 1.0000e-05, 2.5665e+01, 5.3024e+00, 1.0000e-05,\n",
      "        1.0000e-05, 4.7473e+00, 1.7368e+01, 1.0000e-05, 1.0000e-05, 7.8439e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.8155e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0756e+01, 1.0000e-05, 7.6361e+01, 1.5848e+01, 1.0000e-05, 1.0000e-05,\n",
      "        2.6837e+01, 5.0740e+00, 3.6043e+01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 2.4349e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 4.4107e+01, 7.0109e-01, 3.7495e+00, 1.0000e-05,\n",
      "        4.9313e+00, 1.0000e-05, 1.0000e-05, 1.9820e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.5366e+02, 1.0000e-05, 7.2653e+00,\n",
      "        1.0000e-05, 2.2752e+01, 1.0000e-05, 9.8527e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0255e+01, 7.1129e+01, 1.0000e-05, 9.6010e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.4549e-01, 1.0876e+01, 1.0000e-05, 6.5752e+00,\n",
      "        5.9330e+00, 1.0000e-05, 1.0000e-05, 6.8576e+00, 5.2095e+00, 1.0000e-05,\n",
      "        5.8625e+00, 1.0000e-05, 1.0000e-05, 2.8267e+00, 1.0000e-05, 1.0000e-05,\n",
      "        7.1087e+01, 1.3382e+01, 1.0000e-05, 9.3467e-01, 6.1170e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0623e+01, 1.6181e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([4.1087e+00, 1.0000e-05, 5.4458e+00, 1.0000e-05, 1.0000e-05, 3.1749e+00,\n",
      "        2.6246e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.7857e+01, 1.1586e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.5744e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0465e+01, 4.8086e+00, 1.5864e+01, 1.0000e-05, 1.0993e+01, 4.2261e+00,\n",
      "        4.7900e+01, 6.3077e+00, 3.4670e+01, 1.0000e-05, 6.5799e+00, 8.2433e+01,\n",
      "        2.5006e+01, 1.0000e-05, 4.0842e+00, 9.5198e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0276e+02, 6.4110e+00, 1.0085e+01, 4.9007e+01, 2.1078e+00,\n",
      "        1.0000e-05, 1.0000e-05, 5.4304e+00, 5.3343e+00, 1.0000e-05, 1.5682e+02,\n",
      "        1.0000e-05, 8.9187e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.3255e-01,\n",
      "        1.0000e-05, 1.0000e-05, 4.9186e+00, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0443e+01, 1.2624e+01, 4.6419e+00, 2.7176e+01, 1.0000e-05, 3.7460e-02,\n",
      "        1.0000e-05, 1.9946e+01, 1.0000e-05, 8.8073e+00, 1.0000e-05, 1.3992e+01,\n",
      "        2.5920e+00, 1.0000e-05, 1.5083e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.3322e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 5.0626e+01, 1.5855e+01, 1.0000e-05, 7.4757e+00, 9.5035e-01,\n",
      "        1.0000e-05, 1.0000e-05, 1.6872e+01, 5.8571e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 3.3471e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.9030e+01, 6.8566e+00, 1.0000e-05, 1.0000e-05, 1.3019e+01, 5.7508e+00,\n",
      "        1.0000e-05, 5.8177e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        5.6868e+01, 1.0000e-05, 4.7451e+01, 1.0000e-05, 5.2172e+00, 1.0000e-05,\n",
      "        1.0000e-05, 5.3640e+00, 1.4985e+01, 7.5196e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 8.8561e+00, 1.0000e-05, 1.0000e-05, 6.2071e+00, 1.0000e-05,\n",
      "        3.4340e+00, 7.4413e+01, 7.1693e+00, 1.0000e-05, 1.0636e+01, 2.8862e+00,\n",
      "        1.0000e-05, 6.3711e+01, 1.2243e+00, 4.9497e+00, 4.3174e+01, 3.3592e+01,\n",
      "        1.0000e-05, 1.0000e-05, 7.0770e+00, 2.0247e-01, 1.0000e-05, 1.0000e-05,\n",
      "        4.0341e+00, 6.8840e-01, 1.2026e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.1483e+01, 3.2623e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 4.9097e-01, 1.0000e-05, 8.2275e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.9753e+01, 1.5029e+01, 4.9901e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.8805e+01, 1.0000e-05, 1.9548e+02, 1.0000e-05,\n",
      "        1.0000e-05, 7.7305e+00, 1.1009e+01, 1.0000e-05, 3.6598e+01, 1.2795e+00,\n",
      "        2.6355e+01, 1.0000e-05, 8.3502e+01, 2.2991e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 2.3878e+00, 3.8752e+01, 2.7550e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.1330e+02, 1.0000e-05, 5.5368e+01, 2.5665e+01,\n",
      "        1.0000e-05, 2.5782e+01, 1.8247e+01, 1.0000e-05, 2.6834e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 8.3359e+01, 1.0000e-05,\n",
      "        1.0000e-05, 9.1637e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 9.1942e+00,\n",
      "        1.0000e-05, 1.0000e-05, 9.4043e+00, 3.0023e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 2.8078e+01, 8.9835e+00, 3.9245e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.9568e+01, 1.5163e+00, 1.8816e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.3260e+00, 2.8724e+01, 1.0000e-05, 1.0000e-05, 1.2952e+00, 1.0000e-05,\n",
      "        1.4218e+01, 1.0000e-05, 1.9576e-01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 4.2283e+01, 8.7142e+01, 1.0000e-05, 7.8430e+01, 4.3752e+01,\n",
      "        5.3373e+01, 1.2935e+00, 1.0000e-05, 5.6911e+00, 1.0000e-05, 4.7531e+00,\n",
      "        1.0417e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.2537e+01, 1.4679e+02, 1.0000e-05, 3.6407e+01, 1.0000e-05, 1.6136e+00,\n",
      "        2.4300e+00, 3.4018e+01, 1.0000e-05, 1.5768e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.3531e+01, 3.0294e+01, 1.6073e+01, 7.7718e+01,\n",
      "        1.2242e+00, 2.0890e+01, 1.0000e-05, 1.0000e-05, 5.1065e-01, 1.0000e-05,\n",
      "        1.8007e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.1920e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.1231e+00, 5.5213e-01, 1.0000e-05, 1.0302e+00, 2.9548e+01,\n",
      "        1.0000e-05, 1.6197e+01, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([7.7364e+00, 1.0000e-05, 1.2418e+01, 1.0000e-05, 1.0000e-05, 1.8112e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.7145e+00,\n",
      "        1.0970e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.1792e+01, 1.0000e-05,\n",
      "        3.8414e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.7982e+01, 1.0000e-05,\n",
      "        6.6513e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.4453e+01, 2.2629e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0892e+02, 3.0923e+01, 1.0000e-05, 1.7724e+01, 2.2308e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.3566e+01, 1.0000e-05, 1.0000e-05, 6.9074e+00,\n",
      "        1.0000e-05, 1.0000e-05, 8.2342e+00, 1.0080e+00, 6.5051e+01, 1.0000e-05,\n",
      "        4.9991e-01, 1.3792e+01, 1.6586e+01, 4.6195e+01, 1.0000e-05, 2.0631e+01,\n",
      "        1.6207e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss Calculation\n",
      "tensor([8.1309e+01, 1.0000e-05, 3.2372e+01, 3.4662e+01, 1.0000e-05, 1.4970e+01,\n",
      "        9.5361e+01, 6.7757e+01, 8.4828e+01, 1.0000e-05, 4.7372e+01, 1.0000e-05,\n",
      "        6.9025e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.0591e+01, 4.6644e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 5.9640e+01, 1.1249e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.6429e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 7.3334e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 8.5215e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.8436e+01,\n",
      "        3.9171e+00, 1.1749e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.1211e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 8.2269e+00, 1.8661e+00,\n",
      "        1.0000e-05, 1.8646e+02, 2.8457e+00, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 3.7766e-02, 1.0000e-05, 1.1705e+01, 1.0000e-05, 1.0187e+01,\n",
      "        5.5376e-01, 5.9577e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.7675e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 3.4404e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5160e+02, 3.1658e+01,\n",
      "        1.0000e-05, 1.0000e-05, 4.1032e+01, 1.0000e-05, 2.8029e+00, 6.8477e+00,\n",
      "        1.0000e-05, 1.0000e-05, 4.3370e+00, 1.0000e-05, 1.0000e-05, 3.6807e+01,\n",
      "        1.0000e-05, 1.0000e-05, 3.0270e+00, 1.9909e+01, 1.0000e-05, 1.0000e-05,\n",
      "        2.7248e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 4.6774e+00, 1.0000e-05, 1.0000e-05, 1.9412e+00,\n",
      "        2.5522e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([8.9173e+01, 6.6594e+00, 9.4131e+01, 1.0000e-05, 1.4803e+00, 7.5380e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.3699e+01, 1.0000e-05,\n",
      "        2.9392e+01, 1.0000e-05, 1.0105e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 8.4523e+00, 1.0000e-05, 1.0000e-05, 1.0951e+01,\n",
      "        5.2958e-01, 1.0000e-05, 5.5537e+01, 1.0000e-05, 1.2025e+01, 1.0000e-05,\n",
      "        1.8957e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.1951e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 4.3023e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 4.3821e+01, 1.0000e-05, 1.4241e+02, 1.0000e-05, 4.9865e+01,\n",
      "        1.0000e-05, 1.0000e-05, 2.0578e-01, 1.9553e+01, 9.4585e+00, 3.1198e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0079e+00, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([2.6851e+01, 1.0000e-05, 5.6169e+00, 1.5107e+01, 1.0000e-05, 6.9918e+01,\n",
      "        1.0000e-05, 1.0000e-05, 9.7441e+00, 1.0000e-05, 1.0000e-05, 4.4223e+01,\n",
      "        9.2288e+00, 1.0000e-05, 2.3024e+00, 1.0000e-05, 4.1206e-01, 6.4005e+00,\n",
      "        1.0000e-05, 3.1871e+00, 1.0000e-05, 1.4783e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0147e+02, 1.2729e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.9493e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 6.8282e+01, 1.0000e-05, 2.7940e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.1241e+01, 2.8382e+00, 1.0000e-05, 1.0000e-05,\n",
      "        2.3514e+02, 4.5616e+01, 1.0000e-05, 4.8528e+01, 8.7755e+00, 1.0000e-05,\n",
      "        1.0000e-05, 3.2391e+01, 1.3230e+01, 1.0000e-05, 1.8183e+01, 1.0000e-05,\n",
      "        9.5003e+01, 1.0000e-05, 1.0000e-05, 2.4446e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.8008e+00, 2.5404e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.8465e-01, 3.0662e+00, 2.2096e+01, 1.0000e-05, 1.0000e-05, 1.2404e+01,\n",
      "        1.8444e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.2265e+02, 1.0000e-05, 1.0000e-05, 2.5461e+00, 1.0000e-05,\n",
      "        1.0000e-05, 8.5055e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5874e+01,\n",
      "        3.3964e+00, 1.3022e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 3.0104e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.2812e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.3550e+00, 1.1149e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.3821e+02, 1.0000e-05, 1.1848e+01, 9.6875e-01, 2.3702e+02, 1.0000e-05,\n",
      "        1.0000e-05, 8.4552e+01, 6.1088e+01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.3476e+01, 2.3130e-01, 1.0000e-05,\n",
      "        1.0000e-05, 2.4396e+00, 1.0000e-05, 1.0000e-05, 3.5712e+00, 1.0000e-05,\n",
      "        9.8791e+00, 1.8217e+01, 2.3053e+00, 5.9570e+00, 1.0000e-05, 1.6211e+01,\n",
      "        1.0000e-05, 4.9341e+01, 2.3625e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 8.3838e+00, 6.9605e+00, 4.6373e+00,\n",
      "        1.5172e+01, 1.1170e+00, 1.0000e-05, 2.1594e+00, 1.0000e-05, 9.0309e+01,\n",
      "        9.0680e+00, 1.0000e-05, 1.1454e+02, 4.5772e+01, 1.0000e-05, 2.9116e+00,\n",
      "        1.0000e-05, 1.3469e+01, 7.7918e+00, 1.0000e-05, 3.9228e+00, 1.5343e+01,\n",
      "        1.0000e-05, 7.0728e+01, 1.0000e-05, 1.0000e-05, 1.1305e+02, 1.0000e-05,\n",
      "        7.5581e+01, 1.0000e-05, 1.6492e+00, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 9.8855e+00, 5.2022e+01, 1.0000e-05,\n",
      "        1.1279e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.8573e+00, 5.2798e+00,\n",
      "        2.0296e+01, 2.1372e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.7639e+01, 1.0000e-05, 1.0000e-05, 3.1774e+00, 1.0000e-05, 7.0213e-01,\n",
      "        9.4730e+01, 3.8892e+01, 1.5355e+02, 1.2502e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 3.3866e+01, 7.2368e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 5.6473e+01, 8.3044e+00, 1.0000e-05, 5.4066e-01, 1.0000e-05,\n",
      "        1.5924e+01, 1.5677e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 8.6051e+00, 1.1302e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.2326e-01, 1.4380e+01, 6.5676e+01, 4.2903e+01, 1.0000e-05,\n",
      "        5.7951e+01, 1.1224e+02, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.2218e+01, 1.0000e-05, 2.2713e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        4.8015e+00, 7.6721e+01, 1.0000e-05, 4.7474e-01, 2.5471e+01, 1.0000e-05,\n",
      "        1.0000e-05, 5.1727e+01, 4.7674e+01, 5.9556e+00, 1.0000e-05, 1.0000e-05,\n",
      "        4.2339e+01, 1.5450e+00, 1.0000e-05, 1.2556e+00, 1.0000e-05, 1.0000e-05,\n",
      "        3.0957e+01, 1.0000e-05, 7.5707e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 4.4469e+01, 1.0000e-05, 3.6208e+01,\n",
      "        1.0000e-05, 1.9357e+00, 1.0776e+02, 1.5979e+01, 1.0000e-05, 8.4559e-02,\n",
      "        1.6120e+00, 1.0000e-05, 1.0000e-05, 1.1450e+01, 1.8154e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 5.1623e+00, 7.2115e-01, 1.0000e-05, 2.8012e+00,\n",
      "        1.0000e-05, 1.0000e-05, 6.6464e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.1771e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.5479e+02, 3.8645e+00, 1.0000e-05, 1.8808e+00, 3.8827e+01, 9.1075e+00,\n",
      "        6.5015e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        7.1507e-01, 1.0000e-05, 1.0000e-05, 6.2372e+00, 1.5635e+02, 6.3498e+00,\n",
      "        1.0000e-05, 1.0000e-05, 7.4888e+00, 1.0000e-05, 1.0000e-05, 1.5275e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.4317e+01, 1.0000e-05,\n",
      "        4.1032e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.4138e+00, 1.4055e+02, 1.0000e-05, 1.0000e-05, 3.8641e+00, 1.0000e-05,\n",
      "        5.8806e+00, 2.1614e+01, 1.0000e-05, 2.6149e+01, 1.0000e-05, 5.1136e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        5.0416e+01, 5.3407e+00, 1.0000e-05, 1.0000e-05, 2.4083e+00, 9.4902e+01,\n",
      "        1.0000e-05, 8.3517e+00, 7.7587e+01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 3.0978e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        4.6673e-01, 2.4118e+01, 1.2338e+00, 1.0000e-05, 4.0576e+00, 2.2935e+00,\n",
      "        1.0000e-05, 3.7732e+01, 1.0000e-05, 3.3884e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 6.5920e+01, 1.7724e+02, 7.8903e+00, 1.0000e-05,\n",
      "        1.0000e-05, 5.2995e+00, 1.0000e-05, 7.5137e+00, 1.0000e-05, 9.2116e+00,\n",
      "        7.2271e+01, 1.0000e-05, 4.2864e+00, 3.4042e+01, 1.0000e-05, 1.0000e-05,\n",
      "        9.2707e+00, 1.6012e+00, 7.3752e+00, 1.0000e-05, 1.0000e-05, 1.0698e+02,\n",
      "        2.6291e+00, 1.2479e+01, 1.0000e-05, 1.0000e-05, 8.6917e+01, 1.5292e+01,\n",
      "        3.7635e+00, 1.0000e-05, 3.3983e+01, 2.9514e+00, 3.2471e-01, 1.0000e-05,\n",
      "        2.0967e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 6.1894e+00, 1.4377e+01,\n",
      "        1.0000e-05, 1.0000e-05, 6.3176e+01, 2.7025e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 9.8061e+01, 1.6961e+01, 1.0000e-05, 6.5753e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.9812e+01,\n",
      "        1.0000e-05, 5.0008e+00, 1.0000e-05, 1.0000e-05, 2.6709e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0051e+01,\n",
      "        6.6557e+00, 4.1239e-01, 1.0000e-05, 1.0000e-05, 2.8866e+00, 1.0000e-05,\n",
      "        1.3786e+01, 1.0000e-05, 1.0000e-05, 9.0496e+01, 7.8513e-01, 5.3633e+00,\n",
      "        5.1730e+01, 1.0000e-05, 1.0000e-05, 3.3571e+00, 1.0000e-05, 3.7679e+00,\n",
      "        3.3632e+01, 6.6759e+00, 1.0000e-05, 7.9594e+00, 3.8840e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.4275e+01, 1.0000e-05, 2.2651e+00, 1.0000e-05,\n",
      "        1.0000e-05, 4.1026e+00, 3.8503e+00, 2.3494e+01, 3.4742e+00, 1.3511e+01,\n",
      "        1.2760e+02, 1.0000e-05, 4.4258e+01, 2.8187e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss Calculation\n",
      "tensor([6.2957e+01, 1.0000e-05, 1.0000e-05, 5.2106e-01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5816e+01, 8.4301e+00,\n",
      "        7.3015e+01, 1.0000e-05, 1.6642e+01, 1.0000e-05, 1.0000e-05, 4.3282e+01,\n",
      "        1.4071e+01, 1.1287e+01, 2.9678e+01, 1.0000e-05, 3.5300e+01, 1.0000e-05,\n",
      "        4.2568e+00, 4.8654e+00, 1.0309e+02, 1.4843e+01, 1.0000e-05, 1.0000e-05,\n",
      "        3.7864e+00, 1.0000e-05, 1.0000e-05, 8.6994e+00, 4.6624e-01, 5.2929e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1371e+01,\n",
      "        1.1258e+02, 6.6859e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.1986e+01, 2.1362e+00, 1.3331e+02, 1.1181e+00, 3.7818e+00, 1.0000e-05,\n",
      "        1.0000e-05, 6.0909e+00, 1.9902e+00, 1.5457e+02],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 4.1377e+01, 1.2247e+01, 1.0390e+02,\n",
      "        1.0589e+01, 1.0000e-05, 5.9098e+01, 1.3090e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.9019e+00, 1.9122e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.9471e+00,\n",
      "        1.0000e-05, 1.3061e+01, 4.7019e+01, 1.0000e-05, 1.1921e+01, 1.0307e+01,\n",
      "        4.7416e+01, 1.0000e-05, 1.3248e+02, 1.4259e+01, 1.0000e-05, 7.3320e+01,\n",
      "        1.0000e-05, 8.4036e+00, 1.0000e-05, 1.0000e-05, 2.1566e+00, 1.0000e-05,\n",
      "        1.0000e-05, 6.7373e+00, 1.0000e-05, 8.1012e+01, 2.5835e+02, 6.6883e+00,\n",
      "        1.0000e-05, 8.8150e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.1682e+01,\n",
      "        1.0000e-05, 2.7209e+00, 1.0000e-05, 2.6681e+00, 1.0000e-05, 3.3152e+00,\n",
      "        6.6523e+00, 1.0000e-05, 1.8901e+00, 4.0442e-01, 1.0000e-05, 1.6376e+01,\n",
      "        7.9589e-01, 1.0000e-05, 1.0000e-05, 2.4439e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([2.6711e+01, 9.2124e-01, 7.1799e+00, 1.0000e-05, 1.0000e-05, 5.7506e-01,\n",
      "        3.6132e+01, 1.0000e-05, 1.9619e+00, 7.2008e-02, 1.0000e-05, 1.6031e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 7.2576e+00, 6.0934e-01, 1.0000e-05, 5.8312e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.8318e+00, 3.2435e+01, 1.0000e-05,\n",
      "        3.4102e+00, 1.0000e-05, 8.6397e+01, 1.0000e-05, 1.0000e-05, 2.6979e+00,\n",
      "        3.4519e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        6.2244e-01, 1.0000e-05, 1.7861e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.6668e+00, 1.4407e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.1372e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.5129e+01, 6.1079e+00, 1.4629e+01, 4.1239e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.1474e+02, 5.1657e+00, 9.2442e-01, 1.0000e-05,\n",
      "        4.3340e+01, 1.0000e-05, 4.1276e+01, 2.0845e+01, 2.9568e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1250e+02, 1.0000e-05, 1.0000e-05,\n",
      "        1.1784e+01, 1.0000e-05, 2.8658e-01, 6.9585e-01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.2070e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.7051e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 5.7088e+01, 1.0000e-05, 3.1572e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 5.8718e+00, 1.6378e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.8020e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.3563e+01, 2.8267e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 7.2605e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 3.4865e+00, 9.3616e+00, 5.7839e+01, 9.2410e+01, 1.2310e+01,\n",
      "        1.0000e-05, 1.2250e+01, 1.0000e-05, 1.0000e-05, 6.0484e+01, 6.0068e-01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 4.0430e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 9.4233e-01, 1.0000e-05, 3.5242e+01, 9.1333e+00, 2.7092e+01,\n",
      "        1.0000e-05, 1.6819e+01, 6.9925e+00, 1.0000e-05, 1.8063e+00, 1.0000e-05,\n",
      "        9.9874e+00, 1.1899e+01, 1.0000e-05, 4.9687e+00, 1.0000e-05, 1.1468e+00,\n",
      "        1.1449e+00, 1.6319e+00, 1.0000e-05, 1.0000e-05, 3.5147e+01, 2.0487e+00,\n",
      "        1.0000e-05, 1.0029e+01, 1.0000e-05, 4.0977e+01, 1.0000e-05, 1.1249e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.2965e+01, 1.0000e-05, 1.7884e+01, 1.0000e-05,\n",
      "        1.8026e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 3.9197e+01, 4.3838e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([7.7416e+00, 1.0000e-05, 5.9556e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5338e+00, 1.0000e-05, 1.0000e-05,\n",
      "        4.6998e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.6956e+00, 1.0000e-05,\n",
      "        1.7216e+01, 5.3031e+00, 1.0000e-05, 4.1188e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 7.8362e+00, 2.0065e+00, 1.0000e-05, 1.9809e+01,\n",
      "        1.6520e+01, 2.1289e+02, 1.5454e+00, 3.8262e+01, 1.0000e-05, 1.0000e-05,\n",
      "        2.4224e+01, 1.0000e-05, 1.7947e+00, 1.1602e+01, 2.5882e+01, 1.4065e+00,\n",
      "        1.0000e-05, 1.0000e-05, 9.2434e+01, 1.0000e-05, 1.0000e-05, 1.0899e+02,\n",
      "        4.7456e+01, 1.0000e-05, 2.5003e+01, 1.7118e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.9133e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.0758e+01, 1.2539e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 3.3929e+00, 8.3463e+01, 2.5982e+00, 1.0000e-05, 2.0364e+00,\n",
      "        9.5552e+01, 1.7755e+01, 1.0000e-05, 1.0000e-05, 9.9351e+00, 1.0000e-05,\n",
      "        1.0000e-05, 7.3963e+00, 4.7977e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 7.2498e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 9.2228e+00, 1.0000e-05, 7.1313e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.5731e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.6930e+01,\n",
      "        1.7789e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.2690e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 5.1741e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.6144e+00, 1.0000e-05, 1.0631e+02, 1.1992e+00,\n",
      "        1.2650e+01, 1.0000e-05, 1.0000e-05, 1.7991e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.3162e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([4.6819e+01, 1.0000e-05, 5.4481e+00, 3.2104e+01, 2.0138e+01, 1.4644e+01,\n",
      "        1.0000e-05, 5.9650e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0044e+00, 1.0000e-05, 1.0000e-05, 1.0610e+01, 1.0000e-05,\n",
      "        1.0000e-05, 8.1622e+01, 5.5959e+00, 1.0000e-05, 1.0000e-05, 1.0100e+01,\n",
      "        1.0000e-05, 9.8310e-01, 8.4198e-02, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.7972e+01,\n",
      "        2.2080e+01, 4.7899e+00, 1.5793e+01, 1.7027e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1216e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 4.9486e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.9742e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 2.0153e+00, 1.0000e-05, 6.5344e+01, 1.0000e-05, 2.3507e-01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 6.1289e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.4195e+01, 1.0000e-05, 2.2639e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0993e+01, 4.1477e+01, 9.1295e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 8.4333e+01, 8.3025e+01, 9.5481e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.4881e+02, 8.0813e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5475e+01,\n",
      "        3.6234e-02, 6.9079e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        6.4298e+00, 1.0000e-05, 6.1136e+01, 3.6319e+00, 1.0000e-05, 1.2325e+02,\n",
      "        1.5765e-01, 1.0000e-05, 1.4899e+01, 2.4512e+01, 9.6558e+00, 1.0000e-05,\n",
      "        1.0000e-05, 2.5554e+01, 1.0000e-05, 1.0000e-05, 1.9592e+01, 1.0000e-05,\n",
      "        1.0000e-05, 3.9175e+00, 1.0000e-05, 3.4386e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([4.7050e-01, 1.0000e-05, 2.9681e+01, 8.0593e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 3.5063e+00, 1.8007e+01, 1.0000e-05, 1.0000e-05, 2.0884e+02,\n",
      "        1.0000e-05, 1.0000e-05, 5.8296e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.4523e+00, 1.0000e-05, 4.1591e+01, 2.0596e+01, 1.7595e+01,\n",
      "        6.4438e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.7658e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.7786e+01, 1.6669e+01,\n",
      "        9.7227e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 7.2186e+00, 5.7458e-02,\n",
      "        3.1998e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.6198e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 5.4356e-01, 2.5985e+00, 4.5198e+01,\n",
      "        1.0000e-05, 4.9317e+00, 1.0000e-05, 1.0000e-05, 1.3083e+02, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.8512e+01, 1.3571e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 9.2988e-01, 1.3647e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 8.8742e+00, 1.7362e+01, 4.5864e+00,\n",
      "        1.7704e+02, 1.0000e-05, 1.0000e-05, 2.8170e+00, 9.7766e+01, 1.0000e-05,\n",
      "        3.0052e+01, 1.3797e+02, 1.0000e-05, 4.5184e+00, 4.1387e+00, 6.5075e+00,\n",
      "        1.0000e-05, 4.6276e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        4.5462e+00, 1.0000e-05, 1.0000e-05, 7.9652e+01, 1.4598e+00, 1.6164e+01,\n",
      "        1.2825e+00, 3.2113e+01, 4.6428e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0005e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.2298e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 7.6961e-01,\n",
      "        1.1025e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.3476e+01, 2.1310e+01, 1.0000e-05, 1.2216e+01, 1.0000e-05,\n",
      "        2.1944e+01, 6.1457e+00, 7.9575e+00, 1.0000e-05, 1.3725e+01, 1.0000e-05,\n",
      "        8.1142e+00, 1.0000e-05, 1.1451e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 6.4693e+00, 1.0000e-05, 1.0000e-05, 4.0372e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.5488e+01, 1.0000e-05, 1.0000e-05, 4.1650e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.9016e+01, 7.1184e-01,\n",
      "        1.0000e-05, 1.0000e-05, 1.7127e+00, 9.4128e+00, 2.6249e+01, 1.0000e-05,\n",
      "        7.8826e+01, 3.0626e+01, 7.8392e+00, 1.0000e-05, 1.2451e+01, 1.0000e-05,\n",
      "        4.0367e+00, 1.0616e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 8.8239e-01,\n",
      "        3.7133e+00, 8.3462e+00, 1.9987e+01, 1.0000e-05, 1.5906e+02, 1.0000e-05,\n",
      "        1.0000e-05, 6.7304e+00, 4.7341e+01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 2.1339e+02, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.8162e+00, 1.0000e-05, 6.2975e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.7507e+01, 1.0000e-05, 1.0000e-05, 4.8549e+01, 9.1903e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 9.1431e+00, 2.5405e+00, 2.1313e+01, 5.8958e+01,\n",
      "        1.9566e+01, 1.0000e-05, 5.6774e+00, 1.0000e-05, 2.4215e+01, 1.0000e-05,\n",
      "        1.0000e-05, 5.6047e+00, 9.1312e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        5.0009e+00, 1.4035e+01, 3.0474e+01, 1.1352e+01, 1.0000e-05, 2.1285e+01,\n",
      "        1.0000e-05, 1.0000e-05, 7.7523e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 3.8502e+01, 1.5592e+01, 1.4719e+00, 1.0000e-05, 1.1800e+00,\n",
      "        1.0000e-05, 4.5556e+01, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 6.6234e+00, 1.0000e-05, 4.4631e+00, 1.0304e+01,\n",
      "        1.1019e+01, 1.0000e-05, 3.4382e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.5482e+00, 1.0000e-05, 1.4744e+01, 1.0000e-05,\n",
      "        2.5474e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.4344e+01,\n",
      "        1.6016e+01, 1.0000e-05, 3.1913e+00, 2.7638e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 4.4616e+01, 1.6684e+00, 1.5468e+00, 1.0000e-05, 1.0000e-05,\n",
      "        4.4911e+01, 1.0000e-05, 1.0000e-05, 2.1244e+00, 1.0000e-05, 1.2154e+01,\n",
      "        1.0000e-05, 2.7372e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.7702e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        7.5434e+00, 1.0000e-05, 1.0000e-05, 1.3046e+02],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([6.0267e+00, 1.1789e+00, 2.2735e+01, 1.0000e-05, 8.7317e+00, 1.5065e+02,\n",
      "        8.1045e+00, 3.8746e-02, 1.0000e-05, 3.8730e+01, 1.0000e-05, 5.0305e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5687e+01, 1.5017e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.7186e+01, 1.0000e-05, 5.2257e+01,\n",
      "        7.2447e+01, 1.0000e-05, 1.3181e+01, 1.0000e-05, 1.3845e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.0243e+00, 2.1775e+01,\n",
      "        5.7295e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 7.9737e-01,\n",
      "        1.0000e-05, 6.3432e+01, 1.0000e-05, 1.1075e+01, 1.0000e-05, 1.7213e-01,\n",
      "        1.0000e-05, 1.4987e+00, 1.0000e-05, 1.0000e-05, 3.6945e+00, 8.7737e+00,\n",
      "        1.0000e-05, 1.0000e-05, 4.8017e+00, 1.0000e-05, 1.0000e-05, 1.1023e+01,\n",
      "        1.0000e-05, 1.0000e-05, 2.3776e+00, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 2.9676e+00, 1.0000e-05, 1.0000e-05,\n",
      "        2.0110e+01, 1.7277e+02, 5.5907e+01, 1.7788e+01, 1.8898e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.4147e+00, 5.2925e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.1363e+01, 1.0000e-05, 2.7299e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 2.3019e+00, 1.0000e-05, 9.2793e+00, 1.0000e-05, 1.1772e+01,\n",
      "        2.9008e+00, 1.7023e+00, 1.0000e-05, 1.0000e-05, 5.6021e+01, 9.7712e-01,\n",
      "        1.0000e-05, 3.1916e+00, 1.0000e-05, 9.2332e-01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 7.2128e+00, 2.4623e-01, 1.0000e-05,\n",
      "        4.9818e+01, 1.0000e-05, 1.7868e+00, 1.6882e+02, 9.7719e+00, 4.3037e+01,\n",
      "        1.0000e-05, 7.6797e+00, 1.0000e-05, 1.0000e-05, 7.1992e-01, 1.0000e-05,\n",
      "        1.0000e-05, 5.2364e+00, 1.8888e+01, 2.3354e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([3.3579e+00, 1.0000e-05, 2.3958e+01, 1.0000e-05, 2.5659e+01, 7.0254e+00,\n",
      "        4.2309e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.8073e+01,\n",
      "        1.0000e-05, 2.7099e+00, 1.0000e-05, 1.3238e+01, 3.6712e+00, 2.5259e+02,\n",
      "        5.6854e+01, 9.3048e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 4.3182e+01, 4.1312e-01, 1.0000e-05, 7.1764e+00, 1.0000e-05,\n",
      "        1.5744e+00, 2.1404e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.4075e+01, 4.1680e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.1736e+01, 1.0000e-05, 8.0820e+00, 1.0000e-05, 2.5125e+00, 2.7067e+00,\n",
      "        4.9321e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.8703e+01, 1.0000e-05, 1.0000e-05, 4.8704e+01, 1.0000e-05, 1.0000e-05,\n",
      "        9.2567e+01, 2.5327e+01, 2.1501e+00, 2.5285e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([9.0386e+01, 1.0000e-05, 1.5130e+00, 6.9383e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 9.3827e+00,\n",
      "        7.4605e+00, 4.7105e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0772e+02, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 6.4214e+00, 1.0000e-05, 8.3280e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 7.4157e+00, 3.9830e+01, 9.7460e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.3503e+01, 4.9285e+01, 1.0000e-05, 1.5351e+02, 1.8645e+01,\n",
      "        5.9003e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.9180e+00,\n",
      "        1.0000e-05, 1.2576e+01, 1.0000e-05, 1.0000e-05, 1.6129e+00, 1.6143e+01,\n",
      "        1.0000e-05, 7.9025e+00, 1.0000e-05, 1.5623e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 3.8987e+01, 8.6386e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 7.4142e-01, 1.0000e-05, 5.6598e+00, 2.9065e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 7.3248e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.3775e+00, 1.3164e+01, 1.0000e-05, 5.9093e+00,\n",
      "        1.6599e+00, 1.0000e-05, 1.0000e-05, 9.6359e+00, 1.0000e-05, 1.6666e+01,\n",
      "        1.0000e-05, 2.8026e+01, 2.2721e+01, 1.0086e+01, 1.3186e+00, 1.0000e-05,\n",
      "        4.3261e+00, 1.0000e-05, 1.0000e-05, 4.2361e+01, 1.0000e-05, 1.8982e+00,\n",
      "        1.0000e-05, 2.1021e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.1062e-01,\n",
      "        1.0000e-05, 1.0000e-05, 3.7359e+00, 5.4188e+01, 6.7253e+00, 3.9305e+01,\n",
      "        1.0000e-05, 6.1822e+00, 1.0000e-05, 1.0000e-05, 4.4523e+01, 2.0500e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.4315e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.6083e+00, 1.0000e-05, 4.0296e+00, 1.0000e-05, 7.6211e+01, 1.0000e-05,\n",
      "        1.0000e-05, 8.2336e+00, 1.0000e-05, 1.4066e+01, 5.3542e+00, 5.3407e-01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.7159e+01, 1.9568e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0770e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.0313e+00, 1.0000e-05, 1.0000e-05, 1.2377e+01, 1.0000e-05, 4.8485e+00,\n",
      "        8.5456e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 9.7505e-01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.6959e+01, 4.7528e+01, 1.4445e+00, 5.0990e-01, 7.5084e+00, 1.0000e-05,\n",
      "        2.4207e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.1234e+00,\n",
      "        1.0000e-05, 2.6379e+01, 1.0000e-05, 1.0000e-05, 1.8905e+01, 3.5663e+00,\n",
      "        1.0000e-05, 4.3475e+00, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss Calculation\n",
      "tensor([1.3114e+02, 1.0000e-05, 2.8383e+00, 3.8215e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1780e+01, 1.6594e+02, 2.8124e+01,\n",
      "        3.7852e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.4877e+01, 2.8342e+01, 6.0890e-01, 7.6770e-01, 1.0000e-05, 1.0000e-05,\n",
      "        3.2925e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.6102e+02, 1.0000e-05, 1.0000e-05, 4.4224e+01, 3.4084e+00,\n",
      "        3.1044e+01, 1.0000e-05, 1.0000e-05, 2.4473e+00, 5.1023e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.5014e+01, 1.0000e-05, 5.3993e+00, 2.7245e+01,\n",
      "        1.0000e-05, 2.9755e+00, 1.0000e-05, 5.3099e+00, 1.0000e-05, 4.9329e+00,\n",
      "        1.0000e-05, 9.2819e+01, 1.0000e-05, 1.8710e+00, 1.0000e-05, 1.0000e-05,\n",
      "        2.5021e+01, 7.5605e+01, 3.9480e-01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 2.2226e+00, 2.9219e-02, 1.2783e+02, 1.0472e+02, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.1797e+00,\n",
      "        1.0000e-05, 8.5177e+01, 1.0000e-05, 1.6345e+01, 1.0656e+02, 1.0000e-05,\n",
      "        1.0000e-05, 6.7117e+00, 6.8794e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.1430e+00, 9.3998e-01, 1.0000e-05, 1.3975e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 3.2550e+00, 3.4714e+00, 1.0000e-05,\n",
      "        6.9991e+00, 1.4948e+01, 1.0000e-05, 9.5533e+00, 1.0000e-05, 1.0169e+02,\n",
      "        4.3664e+01, 2.4584e+00, 1.3232e+00, 1.0000e-05, 2.7841e+01, 1.0000e-05,\n",
      "        1.0000e-05, 3.9032e+00, 1.0000e-05, 3.0364e+00, 1.0000e-05, 3.7155e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.2162e+01,\n",
      "        1.0000e-05, 1.0000e-05, 2.2587e+01, 1.4105e+02, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.8378e+01, 1.2821e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.2946e+01, 1.0000e-05, 2.6453e+01, 1.0000e-05, 1.5563e+01, 1.1103e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.9589e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.8981e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.1778e+02, 4.4033e+00, 1.0000e-05, 1.0000e-05,\n",
      "        3.3564e+01, 1.9116e+00, 7.5857e+01, 1.0000e-05, 1.4228e+00, 1.9110e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 6.8731e+01, 1.3050e+02,\n",
      "        4.6516e+01, 5.2503e+00, 6.7943e+00, 1.0000e-05, 1.0000e-05, 6.6513e+00,\n",
      "        1.0000e-05, 1.4107e+01, 3.5710e+00, 1.9661e+02],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.5073e+01, 1.0000e-05, 3.3916e-01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.3641e+01, 1.0850e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 4.6719e+01, 5.3608e+00, 1.3257e+01, 7.4363e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.4477e+01, 2.5321e+01,\n",
      "        7.0028e-01, 7.9295e+00, 1.0000e-05, 3.8796e+00, 3.2711e+01, 1.4531e+01,\n",
      "        1.0000e-05, 9.8511e+01, 1.5630e-01, 3.1432e+00, 4.7159e+00, 1.0000e-05,\n",
      "        1.0000e-05, 3.4085e+01, 1.0000e-05, 7.5477e+00, 3.3285e+01, 1.7530e+01,\n",
      "        4.3910e+00, 7.5338e+00, 1.2241e+02, 1.0601e+01, 1.0000e-05, 1.0000e-05,\n",
      "        2.5209e+00, 3.0056e-02, 5.6763e+01, 7.2267e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0166e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.2330e+01, 1.2646e+01, 6.8422e+00, 1.0000e-05, 1.7486e+01,\n",
      "        3.3900e+01, 1.5550e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5345e+00,\n",
      "        9.9045e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.3128e+01,\n",
      "        1.0000e-05, 3.2738e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        8.8906e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 3.9175e+01, 2.8968e+00, 5.8128e+01, 5.7767e+00,\n",
      "        5.8412e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 9.2206e+01, 1.0000e-05,\n",
      "        1.8650e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.9379e+02, 1.3549e+01, 9.2267e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.0219e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 8.5352e+01, 7.9661e+01, 1.0000e-05, 1.0000e-05,\n",
      "        6.0078e+00, 1.4204e+01, 2.0767e+00, 1.4120e+00, 1.0000e-05, 1.0000e-05,\n",
      "        2.3411e+01, 1.6071e+02, 3.0937e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        8.0302e+00, 5.8082e+01, 1.0000e-05, 4.1214e+00, 1.0000e-05, 8.1504e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.2604e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.7184e+01, 3.7450e+00, 1.0000e-05, 5.8914e+01,\n",
      "        4.5540e+00, 2.1987e+01, 1.0000e-05, 6.7674e+00, 6.2380e+00, 4.1593e+01,\n",
      "        7.6898e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 9.1226e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 4.4223e+01, 4.6159e+01, 3.2555e+00, 1.0000e-05,\n",
      "        9.0214e+01, 1.0000e-05, 1.0000e-05, 4.8491e+00, 1.0000e-05, 7.9923e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 8.7594e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 6.6511e+00, 3.2895e+01, 1.0000e-05, 1.0000e-05, 1.4876e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.6811e+00, 2.6650e+00, 1.0000e-05, 1.8825e+00,\n",
      "        1.4049e+02, 1.0000e-05, 1.0576e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 3.5577e+00, 1.1363e+00, 6.9649e+01, 9.4972e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.3810e+01, 1.0000e-05, 1.0000e-05, 7.1215e+00,\n",
      "        1.0000e-05, 1.0000e-05, 2.9350e+02, 6.9949e+01, 2.2532e+01, 4.4796e+00,\n",
      "        1.1647e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 7.6835e+00, 1.0000e-05,\n",
      "        2.0845e+01, 1.0000e-05, 1.0000e-05, 2.9068e+00, 1.0000e-05, 4.3012e+00,\n",
      "        1.9453e+01, 1.0000e-05, 1.4483e+01, 1.0000e-05, 3.1848e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.6388e+01, 1.0000e-05, 1.1866e+00, 1.4800e-01,\n",
      "        1.7916e+01, 5.0631e+00, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([4.3836e+00, 1.6311e+02, 1.0000e-05, 8.1778e+00, 1.0000e-05, 1.3971e+02,\n",
      "        6.3183e+00, 1.4453e+02, 2.0720e+01, 3.2711e+01, 8.0036e-01, 1.0000e-05,\n",
      "        1.0000e-05, 6.6618e+00, 1.0000e-05, 1.0000e-05, 2.5360e+02, 2.8917e+00,\n",
      "        1.1369e+01, 2.8285e+01, 1.4271e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 5.9001e+00, 2.4565e+01, 1.0000e-05, 1.4596e+01,\n",
      "        1.0000e-05, 2.7683e+02, 1.0000e-05, 5.9877e+01, 1.0000e-05, 1.0000e-05,\n",
      "        2.9892e+01, 1.0000e-05, 1.1471e+01, 2.7204e+00, 1.0000e-05, 7.2305e+01,\n",
      "        1.0000e-05, 1.5338e+00, 1.0000e-05, 1.3466e+01, 1.0000e-05, 1.0000e-05,\n",
      "        4.1640e+00, 1.0000e-05, 2.3969e+01, 1.0000e-05, 1.0000e-05, 1.2749e+01,\n",
      "        1.0000e-05, 9.0776e+00, 1.0513e+01, 1.0000e-05, 1.0000e-05, 4.4860e+01,\n",
      "        5.6057e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 6.9793e+00, 1.0000e-05,\n",
      "        1.0000e-05, 4.6530e+00, 1.6677e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.5060e+00, 7.6107e+01, 1.0000e-05, 1.0000e-05, 2.0119e+00,\n",
      "        9.7526e-01, 1.0000e-05, 1.0000e-05, 2.5130e+00, 1.0000e-05, 8.5509e+00,\n",
      "        2.5300e+00, 2.7916e+01, 1.0000e-05, 4.5823e+00, 5.9502e+00, 1.0000e-05,\n",
      "        7.1597e+00, 1.7752e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05, 5.1454e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0687e+01, 1.7203e+01, 1.0000e-05, 1.8623e+00,\n",
      "        1.0000e-05, 8.8451e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.3069e+00,\n",
      "        1.9394e+01, 4.9886e+00, 4.7026e+00, 1.0000e-05, 1.2711e+00, 1.0000e-05,\n",
      "        1.2589e+01, 1.0000e-05, 1.0000e-05, 1.0814e+01, 4.3101e+00, 3.1781e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([4.2172e+00, 4.0610e+00, 1.0000e-05, 1.0000e-05, 1.1400e+01, 7.4952e+01,\n",
      "        1.0000e-05, 9.9506e+00, 5.6030e+00, 1.0000e-05, 1.9481e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.2619e+01, 1.6356e+01, 3.5979e+00, 1.0000e-05, 1.0000e-05,\n",
      "        3.9500e+00, 2.7476e+01, 1.0000e-05, 6.0160e+00, 3.3287e+00, 1.0000e-05,\n",
      "        2.8665e+01, 2.7879e+01, 3.9262e+02, 1.0000e-05, 7.8957e+01, 4.6822e+00,\n",
      "        1.0000e-05, 6.9107e+00, 1.6514e+01, 4.1361e+00, 1.0000e-05, 1.0000e-05,\n",
      "        5.4653e+00, 3.3398e+00, 1.0000e-05, 1.0000e-05, 6.8833e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.8108e+02, 1.0000e-05, 1.0148e+01, 1.9362e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.2534e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.6663e+01, 1.0000e-05, 1.0000e-05,\n",
      "        8.9282e+00, 8.8863e+00, 5.3544e+00, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss Calculation\n",
      "tensor([3.5559e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.8519e-01, 2.9362e+02,\n",
      "        2.4697e+00, 1.0000e-05, 1.6041e+00, 3.9376e+00, 1.0000e-05, 1.0000e-05,\n",
      "        5.0284e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 5.4763e+01, 1.0431e+01,\n",
      "        1.0000e-05, 1.0000e-05, 4.9373e+00, 1.0000e-05, 7.8920e+00, 3.7608e+01,\n",
      "        5.7601e+00, 6.4839e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 7.9369e-01,\n",
      "        8.4661e+00, 4.4620e+01, 1.0000e-05, 2.4463e+00, 1.8936e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 9.7531e+00, 1.0000e-05,\n",
      "        1.1793e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 3.7862e+01, 1.0000e-05, 1.1028e+01, 9.3143e-02, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.2960e+01,\n",
      "        2.7434e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.3054e+01, 5.4040e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.4553e+02, 1.0000e-05, 2.4596e+00, 2.7429e+00,\n",
      "        9.2592e+00, 3.1208e+00, 1.0000e-05, 1.3068e+01, 5.1871e-01, 1.0000e-05,\n",
      "        1.0000e-05, 2.1348e+01, 1.0000e-05, 3.7446e+01, 1.0000e-05, 1.7089e+00,\n",
      "        1.3036e+01, 2.4117e+01, 2.1170e+00, 1.1759e+01, 2.1280e+01, 1.2290e+01,\n",
      "        3.6479e+00, 1.0000e-05, 2.4854e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 2.6452e+01, 2.1273e+01, 2.8351e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.1533e+01, 1.7843e+01, 3.9970e+01, 1.0000e-05, 1.0000e-05, 2.3313e+02,\n",
      "        1.0000e-05, 2.3256e+01, 1.0000e-05, 1.0000e-05, 3.7014e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.7519e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.1078e+00, 1.0000e-05, 1.5720e+01, 1.0000e-05, 2.9162e+00, 1.0000e-05,\n",
      "        2.2319e+00, 2.7692e+01, 2.4353e+01, 3.0617e+01, 8.8836e+00, 3.3843e+01,\n",
      "        1.0000e-05, 7.5416e+00, 1.9099e+00, 8.1982e+00, 1.0000e-05, 2.0842e+01,\n",
      "        1.0000e-05, 1.5534e+02, 1.0000e-05, 1.0000e-05, 2.6724e+00, 1.0000e-05,\n",
      "        8.3569e+00, 1.0000e-05, 1.0000e-05, 5.1100e+00, 1.0000e-05, 8.5719e+00,\n",
      "        1.0000e-05, 1.0000e-05, 2.6358e+00, 4.3888e+01, 3.6814e+00, 1.0000e-05,\n",
      "        1.0000e-05, 5.8046e+00, 5.7452e+00, 2.5071e+01, 3.9149e+00, 1.0000e-05,\n",
      "        5.7864e+00, 1.1119e+02, 2.1051e+01, 1.0000e-05, 1.0000e-05, 1.0328e+01,\n",
      "        1.0000e-05, 1.0000e-05, 3.1816e+01, 9.1881e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.2380e+00, 1.0335e+01, 1.0000e-05, 7.4496e-01, 1.0000e-05,\n",
      "        2.9540e+01, 9.2352e+00, 8.4176e+00, 1.3662e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([6.8311e+01, 3.7659e+00, 1.0000e-05, 4.5459e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.9091e+00, 1.0000e-05, 1.0000e-05, 1.7257e+02,\n",
      "        1.5190e+00, 1.6238e+01, 2.1406e+01, 7.6400e+01, 8.5456e+00, 1.1389e+00,\n",
      "        2.6898e+01, 4.7631e+00, 3.3683e+00, 1.0000e-05, 5.6935e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.6412e+02, 1.0000e-05,\n",
      "        1.0000e-05, 5.3079e+00, 1.5184e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.2030e+01, 1.0000e-05, 1.0000e-05, 8.9263e+00, 1.0175e+00,\n",
      "        1.0000e-05, 4.0115e+01, 1.0000e-05, 1.3226e+01, 5.1922e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.1857e+01, 4.0748e+00, 4.9272e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0975e+01, 1.0000e-05,\n",
      "        1.0000e-05, 5.9895e+00, 1.0000e-05, 8.9344e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 3.9141e+01, 5.5633e+00, 2.6476e+01, 2.3202e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 7.6547e+01, 1.0000e-05, 3.1785e+00,\n",
      "        4.2583e+01, 7.1737e+00, 4.2971e+00, 1.0000e-05, 1.5482e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 7.7393e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.3162e+01, 1.6492e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.5147e+00, 2.5342e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        9.4835e+01, 7.8902e+00, 1.0000e-05, 3.4662e-01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.8047e+00, 1.0000e-05, 1.2401e+01, 1.6929e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 6.5452e+00, 6.2149e+01, 4.4920e+01,\n",
      "        1.0000e-05, 1.9896e+00, 1.0000e-05, 8.7468e+00, 6.2770e-01, 1.8288e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.2212e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 2.4606e+01, 2.1881e+01, 1.0000e-05, 5.6549e+01, 3.5652e+00,\n",
      "        1.0000e-05, 1.0000e-05, 8.7593e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.1421e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 5.0528e+00, 1.7607e+01, 2.3724e+01, 1.0000e-05, 4.7553e+01,\n",
      "        1.0000e-05, 1.2383e+01, 1.0000e-05, 3.0161e+01, 6.2823e+01, 1.0191e+01,\n",
      "        2.1136e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.8229e+01, 6.5744e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.1292e-03, 8.2092e-01, 1.0000e-05, 1.9952e+01,\n",
      "        1.0000e-05, 1.0000e-05, 9.4965e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 9.0089e-02, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([7.2957e+01, 1.0000e-05, 1.6105e+02, 2.3796e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 5.3985e+00, 1.0000e-05, 1.0000e-05, 3.6993e+00,\n",
      "        3.9146e+00, 1.0000e-05, 1.0000e-05, 1.3439e+02, 1.0000e-05, 2.4601e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0886e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        5.2905e+01, 1.0989e+02, 1.0000e-05, 1.6282e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 8.4447e+00, 8.6620e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.3044e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.8791e+01, 1.8345e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.7742e+02, 1.0000e-05, 9.1899e+01, 1.0000e-05, 1.0000e-05, 1.7598e+00,\n",
      "        1.0000e-05, 5.2887e+01, 4.5957e+01, 1.0000e-05, 1.5781e+00, 9.2886e+00,\n",
      "        1.0000e-05, 1.0000e-05, 3.3104e+00, 1.0725e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 7.5746e+00, 1.0000e-05, 3.5647e+01, 1.7847e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.5223e+01, 4.5654e+01, 1.3114e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.3345e+01,\n",
      "        5.3737e+00, 1.0000e-05, 1.1919e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        6.2129e+01, 7.0229e+00, 5.7389e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.6052e+00, 2.4170e+01, 1.0000e-05, 1.0000e-05, 8.3836e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 8.1437e-01, 7.2422e+00, 7.4553e+00,\n",
      "        6.4043e+01, 1.0000e-05, 1.6605e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 4.6756e+01, 1.0000e-05, 1.6343e+02, 1.0000e-05, 1.0000e-05,\n",
      "        1.1975e+00, 1.0000e-05, 1.4247e+02, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 7.6914e+01, 5.9512e+00, 1.0000e-05,\n",
      "        1.2560e+02, 4.0822e+00, 1.0000e-05, 9.9743e+01, 1.0000e-05, 2.7301e+02,\n",
      "        7.9193e+00, 4.8434e+00, 1.0000e-05, 3.6387e+02, 1.0000e-05, 1.0000e-05,\n",
      "        8.2332e+00, 1.1744e+02, 1.0000e-05, 1.0000e-05, 3.0188e+01, 1.0000e-05,\n",
      "        1.0000e-05, 2.9064e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.3194e+01,\n",
      "        4.8654e-01, 1.0000e-05, 1.0687e+01, 1.0000e-05, 8.0914e+00, 1.0000e-05,\n",
      "        3.0578e+00, 6.4847e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.2785e+01,\n",
      "        5.5767e+00, 1.0000e-05, 5.9243e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.2136e+01, 1.0000e-05, 1.0000e-05, 8.2624e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.9734e+01, 2.2385e+01, 1.0000e-05, 3.3324e+02, 1.2301e+01, 1.5163e+00,\n",
      "        1.0000e-05, 1.2445e+00, 2.6751e+01, 1.0000e-05, 1.0000e-05, 7.0577e+00,\n",
      "        1.0000e-05, 1.0768e+01, 1.0000e-05, 1.0000e-05, 2.0106e+00, 9.9020e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.2908e+01, 2.8194e+00, 1.0000e-05, 4.8470e+01,\n",
      "        1.0000e-05, 1.0000e-05, 2.9140e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 9.1901e+01, 7.3892e+01, 1.0000e-05, 1.2507e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.6455e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0280e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.8106e+00, 2.5501e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.4692e+00,\n",
      "        1.0000e-05, 7.2888e-01, 3.2819e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        5.2394e+01, 1.0000e-05, 1.0000e-05, 7.9297e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss Calculation\n",
      "tensor([4.7399e+01, 3.7551e+00, 4.0964e+02, 5.4642e+00, 1.6723e+00, 1.8553e+02,\n",
      "        1.0000e-05, 1.3158e+02, 3.0866e+01, 1.0000e-05, 3.3094e+01, 1.0000e-05,\n",
      "        4.1827e+01, 9.3571e+01, 1.5994e+00, 1.0000e-05, 1.2590e+02, 1.0000e-05,\n",
      "        6.6315e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        4.5046e+01, 5.1913e+01, 1.0000e-05, 1.0000e-05, 4.3779e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.0535e+00, 1.0000e-05, 5.1981e+01, 6.1463e+00,\n",
      "        1.0000e-05, 4.7935e+00, 1.5304e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.7038e+01, 2.3217e+02, 1.0000e-05,\n",
      "        1.0000e-05, 5.1166e+00, 1.0853e+01, 7.9116e+00, 1.0000e-05, 1.6826e+00,\n",
      "        2.0102e+00, 2.3248e+00, 5.7142e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.5377e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([3.9816e+00, 1.0000e-05, 1.3854e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.6098e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.9447e+00, 5.1301e+00,\n",
      "        1.0000e-05, 1.0269e+00, 1.0000e-05, 1.0000e-05, 4.9202e+01, 3.3073e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.5971e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.1791e+02, 4.7442e+00, 1.0000e-05, 1.0000e-05, 1.2664e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.1981e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.9076e+02,\n",
      "        1.0000e-05, 8.5852e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        6.3239e+00, 3.6271e+00, 2.6411e+00, 1.0612e+00, 1.0000e-05, 6.3830e+01,\n",
      "        1.0000e-05, 4.0361e+01, 1.0000e-05, 6.2665e+01, 1.0932e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 3.9855e+01, 8.0019e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([5.7005e+00, 2.5122e+01, 1.6432e+00, 6.7357e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.4656e+02, 1.0000e-05, 6.0520e+00, 6.3436e+01, 8.0699e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.5628e+02, 1.9216e+01, 1.1905e+00, 8.7955e+01, 1.0000e-05,\n",
      "        1.0000e-05, 3.9807e+00, 1.1745e+01, 7.9786e-01, 1.0000e-05, 1.2567e+02,\n",
      "        1.0000e-05, 3.4976e+00, 1.5098e+01, 1.0000e-05, 3.1739e+00, 1.5685e+00,\n",
      "        2.9245e-01, 2.3057e+01, 1.0000e-05, 8.7074e+00, 1.1198e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 6.2391e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 3.3573e+00, 1.0000e-05, 2.2921e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.3284e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.8714e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 9.7051e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.2634e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        5.5872e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.7330e+00,\n",
      "        1.0000e-05, 7.1858e+01, 1.0000e-05, 1.5708e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 3.2954e+01, 1.8883e+00, 1.0000e-05, 1.0000e-05, 1.4866e+02,\n",
      "        9.1206e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 9.7620e+00, 1.0000e-05,\n",
      "        1.2641e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.4638e+02, 7.7074e-01,\n",
      "        1.3759e+02, 5.5121e+00, 3.8343e+01, 1.0000e-05, 1.0000e-05, 6.1323e+00,\n",
      "        6.0745e-02, 2.4691e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.2876e+01, 1.0000e-05, 5.2016e+00, 5.6045e+01, 1.0000e-05,\n",
      "        1.0000e-05, 6.0455e+00, 1.0000e-05, 5.2322e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.4122e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        5.5844e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.7095e+01, 1.0000e-05, 3.7814e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 3.4164e+01, 1.0000e-05, 1.3299e+02,\n",
      "        1.0000e-05, 6.1871e+00, 1.0000e-05, 2.0932e+01, 1.0000e-05, 1.0000e-05,\n",
      "        7.5474e+00, 2.1888e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 6.1255e+01,\n",
      "        5.3159e+00, 2.1888e+01, 1.9402e+00, 8.4240e+00, 4.2128e+00, 7.7989e+01,\n",
      "        1.0000e-05, 1.0000e-05, 4.5344e+01, 6.9254e+00, 1.0000e-05, 7.9458e+00,\n",
      "        2.3298e+02, 1.0000e-05, 6.7549e-01, 6.7597e-01, 2.5388e+01, 1.0000e-05,\n",
      "        2.3042e+00, 1.0000e-05, 2.5728e+01, 1.3071e+01, 1.0000e-05, 1.3243e+02,\n",
      "        7.9511e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([8.5110e+01, 1.0000e-05, 5.5285e+01, 7.9008e-01, 1.0000e-05, 1.0000e-05,\n",
      "        5.4937e+01, 1.0000e-05, 1.0000e-05, 2.5878e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.1261e+00, 1.0000e-05, 1.0000e-05, 1.2964e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 2.1396e+02, 1.0000e-05, 1.0000e-05, 9.6761e+00, 1.3693e-01,\n",
      "        1.0000e-05, 1.0000e-05, 3.2328e-01, 1.0000e-05, 1.0000e-05, 1.1829e+01,\n",
      "        1.7142e+01, 7.3493e+00, 3.5983e+01, 6.0144e+00, 1.7808e+01, 1.0000e-05,\n",
      "        3.6508e+01, 9.1752e+00, 8.1014e+00, 1.0000e-05, 4.0477e+00, 1.0000e-05,\n",
      "        1.0000e-05, 4.4740e+00, 1.4697e+01, 3.0613e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 2.4530e+02, 1.0000e-05, 1.2802e+01, 1.0000e-05, 1.0505e+00,\n",
      "        2.1299e+00, 1.0000e-05, 1.0718e+01, 1.0000e-05, 3.3835e-01, 1.0000e-05,\n",
      "        5.5199e+01, 1.0000e-05, 2.7050e+00, 1.6656e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([9.3646e+01, 2.7068e+01, 7.5748e+01, 1.0000e-05, 1.0000e-05, 4.6771e-01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.7486e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.6116e-01, 1.0000e-05, 6.6705e+00, 7.7544e+01,\n",
      "        1.0000e-05, 2.2950e+01, 3.3652e+00, 1.0789e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 4.4591e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.5138e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.5357e+01, 2.1685e+01, 2.4202e+00, 4.9237e+00,\n",
      "        1.0000e-05, 1.0000e-05, 5.5587e+01, 1.0000e-05, 7.3537e+01, 2.8631e+01,\n",
      "        1.0000e-05, 1.0000e-05, 6.4268e+01, 1.0000e-05, 2.5960e+01, 1.0000e-05,\n",
      "        9.9358e+00, 1.0000e-05, 4.3344e+00, 1.0000e-05, 1.0000e-05, 6.6905e+01,\n",
      "        1.9806e+01, 1.0000e-05, 1.6153e+00, 6.4640e+01, 5.9607e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.0983e+01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 7.6592e-01, 3.3246e+00, 1.0000e-05, 2.1045e+00,\n",
      "        6.4313e+00, 1.5650e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1681e+01,\n",
      "        1.0000e-05, 2.7923e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.0662e+01, 2.8125e+01, 1.0000e-05, 9.6723e+00, 2.4478e+02,\n",
      "        8.3333e+01, 1.0335e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.9766e-01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.4741e+00, 1.0000e-05,\n",
      "        2.7628e+02, 3.9833e+01, 4.6823e+00, 1.0453e+01, 1.0000e-05, 1.0000e-05,\n",
      "        2.9539e+00, 1.7278e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 6.6660e+00, 1.0000e-05, 3.0883e+00, 2.0739e-01,\n",
      "        1.0000e-05, 6.6272e+00, 1.0000e-05, 1.0000e-05, 6.1396e+00, 1.0000e-05,\n",
      "        1.2567e+01, 8.1325e+00, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 4.0404e+01, 1.4072e+01, 2.3715e+00, 1.9712e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.0999e+00, 1.2400e+01,\n",
      "        1.1283e+00, 5.6778e+01, 3.0514e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 8.4201e+00, 1.0000e-05, 5.2625e+00, 1.0000e-05, 7.6094e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.3421e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.9966e+00, 1.0000e-05, 1.0000e-05, 9.8555e+00, 2.9067e+01,\n",
      "        1.0000e-05, 2.1544e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1156e+00,\n",
      "        7.2272e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.4678e+00, 1.0000e-05,\n",
      "        1.0000e-05, 3.2993e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.2894e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.4992e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 2.4422e+01, 2.8720e+01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([3.7988e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.2451e+00,\n",
      "        1.0000e-05, 5.2611e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.2008e-01, 1.0000e-05, 8.8082e+00, 1.1552e+01, 1.0737e+00, 1.5428e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.0921e+00, 6.3478e+01, 4.9823e-01, 5.1105e+00,\n",
      "        2.4034e+01, 5.2192e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.9302e+01,\n",
      "        1.0000e-05, 1.0000e-05, 4.9584e+01, 1.0000e-05, 6.4124e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.1286e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.1743e+01,\n",
      "        1.0000e-05, 2.1335e+00, 2.2917e+01, 2.7752e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 4.8483e+00, 2.1880e-01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss Calculation\n",
      "tensor([1.0898e+01, 1.0000e-05, 3.1477e+01, 1.0000e-05, 1.0000e-05, 1.6543e+01,\n",
      "        5.1291e+00, 8.4552e-01, 1.0000e-05, 9.0497e+01, 1.0000e-05, 1.2182e+02,\n",
      "        1.0000e-05, 8.3125e+00, 2.6412e+01, 3.0136e+01, 1.0000e-05, 7.6907e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1242e+00, 1.0000e-05, 1.0000e-05,\n",
      "        6.3705e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.9548e+02, 7.4929e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.3026e+02, 1.0000e-05, 1.6138e-01, 2.0665e+00, 8.5308e+01, 1.0000e-05,\n",
      "        2.6639e+01, 1.0000e-05, 1.0000e-05, 5.2334e+01, 1.0000e-05, 1.0000e-05,\n",
      "        3.9900e+00, 1.0000e-05, 9.3299e+00, 1.0000e-05, 1.3057e+00, 4.5736e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0112e+01, 1.0658e+00,\n",
      "        1.2689e+01, 1.0000e-05, 1.0000e-05, 4.4302e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([8.9483e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.2626e+01, 1.0000e-05,\n",
      "        2.0777e+00, 1.0000e-05, 1.0195e+02, 9.3278e+01, 1.0000e-05, 8.4376e+00,\n",
      "        1.0000e-05, 4.8974e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.1870e+01,\n",
      "        5.7198e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 5.1755e+01, 7.3693e+00, 1.2558e+01, 3.0502e+00,\n",
      "        1.5115e+01, 1.1026e+00, 1.0000e-05, 1.9323e+02, 9.6114e-01, 1.0000e-05,\n",
      "        1.0000e-05, 8.8049e+00, 1.0955e+01, 1.0590e+02, 2.2885e+00, 8.3926e+00,\n",
      "        1.0000e-05, 1.1671e+01, 1.7773e+01, 1.0000e-05, 3.0456e+01, 1.0000e-05,\n",
      "        4.0883e-01, 1.0000e-05, 3.6889e+00, 1.0000e-05, 2.3114e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 5.2037e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 3.9224e+00, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([3.3967e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.1073e+01, 5.7444e+00,\n",
      "        1.0000e-05, 1.7472e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.9899e+01, 1.0000e-05, 1.0000e-05, 7.5031e+00, 2.8707e+00, 1.0000e-05,\n",
      "        2.4579e+00, 1.0000e-05, 1.8121e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.2805e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.4052e+01,\n",
      "        6.2580e+00, 3.3124e+00, 1.0000e-05, 1.0000e-05, 4.5965e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 6.0956e+00, 1.7773e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0536e+02, 8.4668e+00, 1.0000e-05, 3.0557e+00, 6.7684e+00, 2.8950e+00,\n",
      "        1.0000e-05, 1.0000e-05, 3.9079e+01, 3.4657e+00, 1.0000e-05, 8.0649e+01,\n",
      "        1.0000e-05, 1.6891e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.1470e+02, 1.0000e-05, 6.5920e+00, 1.3494e+02],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.6593e+00, 3.5507e+00,\n",
      "        1.0939e+02, 1.1187e+02, 1.6321e+01, 3.4934e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.3015e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 3.9973e+00, 5.5553e+00, 6.5885e+00,\n",
      "        3.0763e+00, 8.5782e+00, 1.0000e-05, 2.2367e+00, 9.5936e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 6.0579e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 6.7710e+00, 1.0000e-05, 1.0000e-05, 1.1904e+02,\n",
      "        3.3125e+01, 1.0000e-05, 1.0000e-05, 1.7716e+01, 1.2947e+01, 4.6589e+01,\n",
      "        3.4059e+00, 1.0000e-05, 1.0000e-05, 9.6798e+00, 1.0000e-05, 1.0386e+01,\n",
      "        1.9397e+02, 9.4796e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 6.1727e+00,\n",
      "        4.4675e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.4481e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.7842e+01, 2.8214e+00, 1.5095e+01, 1.0000e-05, 6.2618e+00,\n",
      "        1.0000e-05, 1.0000e-05, 5.8089e+00, 1.0000e-05, 1.1232e+02, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 5.5891e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 3.3313e+01, 7.8757e+00, 1.7096e+02, 4.6970e+00,\n",
      "        5.9010e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.8847e+00, 2.1932e+00, 6.0681e+00, 1.0000e-05,\n",
      "        3.4805e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1525e+02,\n",
      "        1.0000e-05, 6.8143e+00, 9.4751e-01, 1.0000e-05, 9.3252e+00, 1.3432e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5951e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 4.1148e+02],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([2.2411e+01, 3.4706e+00, 5.3422e+00, 1.0000e-05, 3.0537e-01, 1.0000e-05,\n",
      "        3.3402e+01, 2.6204e+02, 1.0746e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        6.5585e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.0235e+00,\n",
      "        7.1021e+00, 1.2809e+00, 1.4234e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.1636e+01, 1.0000e-05, 1.7689e+01, 1.0000e-05, 4.7234e+00, 5.4067e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 4.3557e+00, 5.2947e+01, 1.5490e+00,\n",
      "        1.0000e-05, 5.6742e+00, 3.8161e+01, 1.8635e+00, 1.0000e-05, 9.6011e+00,\n",
      "        1.8460e+00, 1.0490e+01, 1.0000e-05, 1.4106e+01, 1.0000e-05, 2.2616e+01,\n",
      "        2.3014e+01, 1.0000e-05, 1.4833e+01, 1.8494e+01, 3.1219e+00, 1.8690e+01,\n",
      "        1.0000e-05, 1.0000e-05, 9.0331e-02, 1.0000e-05, 2.1334e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 5.3859e-01, 8.8666e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([8.4763e+00, 1.0000e-05, 1.3233e+01, 1.0760e+00, 1.0000e-05, 1.0000e-05,\n",
      "        4.0361e+01, 6.0845e+00, 1.0000e-05, 6.2768e+00, 3.6628e+01, 1.0000e-05,\n",
      "        1.3619e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.9281e+01, 3.4310e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.8713e+01, 1.2209e+02,\n",
      "        1.4213e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 4.1218e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 3.7571e+01, 1.0000e-05, 8.6977e+00,\n",
      "        1.0000e-05, 4.4581e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 5.6046e+00,\n",
      "        1.5645e+02, 3.1048e+02, 1.0000e-05, 1.0589e+00, 9.9235e+00, 1.0000e-05,\n",
      "        1.4402e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        5.1722e+01, 1.0000e-05, 1.0000e-05, 1.0936e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 2.1752e+02, 1.0000e-05, 1.2031e+01, 5.7010e-01,\n",
      "        1.7713e+00, 1.0000e-05, 1.0000e-05, 4.6383e+01, 1.4829e+01, 1.5860e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1491e+02, 1.0000e-05,\n",
      "        6.2638e+00, 1.0000e-05, 1.9655e+00, 1.6977e+01, 1.0000e-05, 3.7064e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.3194e+01, 1.2005e+01,\n",
      "        1.0000e-05, 1.0000e-05, 2.1208e+02, 1.8876e+00, 1.0000e-05, 4.6634e+01,\n",
      "        1.0000e-05, 8.2160e+00, 1.4061e+01, 1.0000e-05, 8.3431e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.1066e+00, 1.6004e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.3748e+00, 9.4655e+00, 1.0000e-05, 1.0000e-05, 2.8430e+02, 1.0000e-05,\n",
      "        3.7420e+00, 1.2135e+00, 7.7586e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.8356e+01, 1.0000e-05, 1.0000e-05, 3.2915e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 1.6668e+00, 1.9542e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.2363e+02, 1.0000e-05, 1.0000e-05, 1.0079e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        8.6461e+00, 8.1373e+01, 2.3378e+01, 2.2079e+00, 1.0000e-05, 5.5881e+00,\n",
      "        1.0000e-05, 9.2772e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 8.2802e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.5058e+02, 4.1554e+01, 1.0000e-05,\n",
      "        3.2798e+01, 8.5664e+00, 1.0000e-05, 7.3879e+00, 1.0000e-05, 3.0105e+00,\n",
      "        1.0000e-05, 1.0000e-05, 9.7048e+00, 7.6761e-01, 1.0000e-05, 1.0000e-05,\n",
      "        2.4361e+00, 1.0000e-05, 1.9498e+01, 2.2920e+00, 1.1363e-01, 7.3056e+01,\n",
      "        5.9980e+00, 1.0000e-05, 1.3573e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 8.3344e+01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 5.0354e+00, 2.0579e+01, 1.0000e-05, 2.6947e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0721e+01, 3.9290e+00, 1.5559e+00, 1.0000e-05, 1.0000e-05,\n",
      "        5.8150e+00, 1.5780e+00, 1.0000e-05, 1.0000e-05, 2.0806e+01, 1.5118e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.3448e-01, 1.0000e-05, 1.0000e-05,\n",
      "        4.4431e+00, 1.0000e-05, 2.3481e+01, 1.0000e-05, 1.4207e+02, 1.0000e-05,\n",
      "        5.7146e+00, 1.0000e-05, 1.6779e+00, 4.1960e+01, 1.5169e+02, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 4.3307e+01, 1.4561e+02, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.6392e+01, 6.9723e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.6499e+01, 4.8713e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.2896e+01, 1.0000e-05, 6.4859e+00, 8.6127e+01, 4.0199e+01, 4.6713e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.6658e+02, 1.0000e-05,\n",
      "        1.0000e-05, 5.0047e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        5.5518e+00, 1.0000e-05, 1.0000e-05, 1.4867e+00, 1.0000e-05, 1.2597e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.2010e+00, 1.0000e-05, 1.0000e-05,\n",
      "        4.3041e-01, 8.1967e+00, 1.0000e-05, 5.4478e+00, 1.6148e-01, 4.4258e+00,\n",
      "        8.8405e+01, 1.1068e+00, 1.4040e+02, 1.0000e-05, 1.0000e-05, 9.0585e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.8970e+01, 7.1505e-01, 1.0000e-05,\n",
      "        1.5222e+02, 1.0000e-05, 1.7321e+00, 1.0000e-05, 2.7426e+00, 9.6648e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 6.2731e+00, 1.0000e-05, 1.4654e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 8.0311e+01, 1.0000e-05,\n",
      "        1.2670e+01, 1.0000e-05, 1.0000e-05, 4.6350e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss Calculation\n",
      "tensor([5.8289e+00, 1.0000e-05, 1.0000e-05, 2.8580e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.0079e+00, 2.1275e+00, 1.0000e-05, 1.0000e-05,\n",
      "        4.3033e+01, 7.4919e-01, 1.0000e-05, 2.7836e+01, 2.4359e+01, 1.0000e-05,\n",
      "        1.2296e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 6.9946e+00,\n",
      "        3.6318e+01, 1.0000e-05, 1.0000e-05, 4.1951e+00, 2.8090e+00, 1.0000e-05,\n",
      "        1.9243e+00, 1.1971e+00, 1.0000e-05, 5.3540e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.2537e+01, 9.2674e+01, 1.0000e-05, 8.8826e+00, 2.0657e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.5927e-01, 1.3172e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.5735e+01, 1.0000e-05, 2.6279e+01, 1.5404e+01,\n",
      "        1.7860e+01, 7.4034e+00, 7.2934e+00, 1.3190e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.1162e+00, 8.2692e+00, 3.4450e+00, 1.0000e-05, 1.2332e+01, 1.5290e+00,\n",
      "        2.4308e+02, 1.0000e-05, 1.0000e-05, 2.4340e+02, 1.0000e-05, 1.0000e-05,\n",
      "        8.5392e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.2570e+02, 3.4871e+00, 1.4938e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.4241e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 4.0559e+01, 1.0000e-05, 9.8597e+00, 2.4924e+01, 3.0338e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1948e+02,\n",
      "        3.3448e+02, 1.0000e-05, 1.5923e+00, 1.0000e-05, 1.6733e+01, 1.0000e-05,\n",
      "        1.0000e-05, 9.0053e+00, 1.0000e-05, 3.4092e+01, 1.0000e-05, 1.0000e-05,\n",
      "        3.5089e+00, 1.0000e-05, 1.0000e-05, 1.2973e+02, 1.2055e+01, 1.0000e-05,\n",
      "        3.1888e+00, 6.5231e+00, 5.2969e+01, 8.8791e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 2.4229e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.9463e+01, 5.3497e+01, 8.1736e+00, 1.0530e+02, 1.0000e-05, 1.0000e-05,\n",
      "        9.5193e+00, 5.7864e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 7.2723e+00, 1.0000e-05, 1.0000e-05, 5.7232e+00,\n",
      "        1.0000e-05, 2.6920e+01, 3.5792e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.4522e+00, 1.0000e-05, 1.0000e-05, 1.9378e+01, 4.4807e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 9.6712e+00, 4.1172e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.2862e+02, 4.2762e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        4.6274e+00, 1.0000e-05, 1.4221e+02, 1.0000e-05, 1.2304e+00, 2.9669e+00,\n",
      "        3.6108e+00, 1.0000e-05, 1.9098e+00, 2.8035e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([8.4189e+01, 1.0000e-05, 1.4226e+01, 2.7554e+00, 1.0000e-05, 8.3665e+01,\n",
      "        1.0000e-05, 1.2183e+00, 1.0000e-05, 3.8368e+02, 1.0660e+00, 7.2962e+00,\n",
      "        7.5210e-01, 1.0000e-05, 1.0000e-05, 2.1782e+01, 1.0000e-05, 1.3089e+02,\n",
      "        1.0000e-05, 1.4618e+01, 1.0000e-05, 2.1313e+01, 1.3232e+02, 3.0149e+00,\n",
      "        3.6640e+00, 1.1847e+01, 2.3985e+01, 2.6270e+01, 1.5034e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.5909e-01, 4.5230e+00, 1.0000e-05, 1.0000e-05, 9.3754e+00,\n",
      "        1.0000e-05, 1.0000e-05, 6.8361e+00, 1.0000e-05, 1.0000e-05, 1.7747e+01,\n",
      "        6.7044e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.6555e+01,\n",
      "        1.2288e+01, 1.0000e-05, 7.8210e+01, 1.0000e-05, 1.6804e+02, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 9.6270e+01, 2.3299e+02, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 3.0486e+01, 1.3536e+02],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.9479e+02, 1.0000e-05, 1.0000e-05, 1.9954e+01, 2.8317e+02, 1.0000e-05,\n",
      "        4.3190e+00, 7.7559e+00, 1.0000e-05, 8.7569e+01, 2.6652e+00, 4.6960e+01,\n",
      "        1.0000e-05, 4.9136e-01, 1.1587e+01, 5.6035e+01, 1.0000e-05, 9.9252e+00,\n",
      "        4.1269e+00, 1.0000e-05, 1.1117e+01, 5.2745e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5270e+01, 3.7188e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0138e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        9.0046e+00, 1.0000e-05, 3.7998e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.1796e-01, 3.1246e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 5.1293e+00,\n",
      "        6.3047e+00, 1.1643e+01, 6.0583e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        5.9196e+01, 3.9234e+01, 1.0000e-05, 2.9033e+00, 1.2397e+00, 2.6072e+01,\n",
      "        1.0000e-05, 3.8657e+01, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.3257e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.7495e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 7.3711e+00, 1.0000e-05,\n",
      "        1.0000e-05, 2.0804e+00, 1.0000e-05, 1.3323e+01, 1.0000e-05, 2.6149e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.8890e+02, 2.4416e+02, 1.5521e+01,\n",
      "        1.0000e-05, 1.7438e+01, 1.0000e-05, 1.7775e+01, 1.0000e-05, 1.0000e-05,\n",
      "        2.7918e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1115e+00, 1.1188e+00,\n",
      "        1.0069e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 6.9667e-01, 3.9801e+01,\n",
      "        1.2453e+00, 1.0000e-05, 1.0823e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.1427e+01, 1.0000e-05, 1.3983e+01, 1.0000e-05, 2.6421e-01, 4.9605e+01,\n",
      "        1.0000e-05, 1.9348e+02, 1.5952e+02, 1.5632e+01, 2.0043e+02, 1.0000e-05,\n",
      "        1.0000e-05, 1.0929e+01, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([2.8875e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.3712e+01, 6.2381e+00,\n",
      "        1.0000e-05, 6.5611e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1444e+01,\n",
      "        1.0000e-05, 4.0716e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.2461e+01,\n",
      "        4.7019e+01, 1.0000e-05, 5.7723e-01, 1.0000e-05, 2.9616e+01, 1.0000e-05,\n",
      "        3.3811e+00, 4.3623e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.2655e+01,\n",
      "        4.6129e+01, 4.5070e+01, 2.4345e+01, 4.2908e+01, 1.0000e-05, 1.0000e-05,\n",
      "        9.2021e+00, 1.0000e-05, 1.0000e-05, 1.1333e+01, 5.6257e-01, 1.0000e-05,\n",
      "        2.4066e+00, 1.0000e-05, 1.0000e-05, 8.1495e+00, 2.4107e+00, 1.0000e-05,\n",
      "        1.0000e-05, 9.7077e-01, 1.0000e-05, 4.8496e+00, 2.1492e+01, 1.0000e-05,\n",
      "        8.1438e+01, 1.0000e-05, 5.2366e+00, 1.0000e-05, 1.1243e+02, 1.0000e-05,\n",
      "        1.0000e-05, 3.1592e+01, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 5.6182e-01, 1.0000e-05, 8.7838e+00,\n",
      "        2.7670e+00, 4.0749e+00, 1.9254e+00, 1.0000e-05, 9.9823e+00, 1.0000e-05,\n",
      "        8.6745e+00, 4.0238e+01, 3.6921e+00, 1.7816e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.1844e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 2.0815e+01, 1.0000e-05, 6.9342e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.2699e+02, 1.0000e-05, 2.7605e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.5491e+01, 3.4055e+00, 7.5691e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 9.1252e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.3029e+00, 5.2754e+00, 4.6512e-01, 9.8813e+00, 1.0000e-05, 1.2686e+01,\n",
      "        1.0297e+01, 2.4456e+01, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([8.2743e+01, 1.0000e-05, 2.3789e+01, 1.9058e+00, 1.0000e-05, 1.0000e-05,\n",
      "        2.4287e+00, 5.3414e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.4997e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 7.9184e+00, 1.0000e-05, 2.3223e+02,\n",
      "        4.7297e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.5554e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.8407e+01, 1.0000e-05, 1.7602e+01, 1.3799e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 7.9422e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 8.4401e+01, 1.0000e-05, 3.9689e+02, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 4.7464e+01, 1.0000e-05, 1.5135e+02, 5.7737e+00,\n",
      "        1.0000e-05, 1.0000e-05, 5.0561e+01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.3485e+01, 6.5631e-01, 1.0000e-05, 1.8973e+01,\n",
      "        1.3472e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 6.5865e+01,\n",
      "        1.0000e-05, 4.9762e-01, 1.0000e-05, 1.6417e+01, 5.5256e+01, 1.4918e+01,\n",
      "        1.0000e-05, 1.0000e-05, 6.1906e+00, 1.0000e-05, 4.2704e+00, 3.6152e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.3988e+01, 1.0000e-05, 1.0000e-05, 3.0659e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 2.5537e-01, 3.7396e+01, 9.5129e+01, 1.0000e-05, 1.9649e+00,\n",
      "        9.6326e+00, 7.5218e+01, 1.7888e+01, 2.3924e+00, 1.0000e-05, 1.0000e-05,\n",
      "        3.6531e+01, 1.0294e+01, 1.0000e-05, 1.0000e-05, 1.6130e+02, 1.0000e-05,\n",
      "        1.4830e+01, 2.6506e+00, 1.0000e-05, 1.0000e-05, 4.5034e+00, 4.7368e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss Calculation\n",
      "tensor([8.8794e+00, 1.0000e-05, 1.0000e-05, 9.5122e-01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 2.1000e+02, 1.0000e-05, 1.0000e-05, 1.5998e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.9132e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 8.8608e+01, 3.1843e+01, 1.0000e-05,\n",
      "        3.1778e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.3716e-01, 1.3948e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        7.8348e+01, 1.0000e-05, 1.0000e-05, 2.2640e+00, 3.3527e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        7.0931e+01, 1.0000e-05, 7.6804e+00, 3.8084e-02, 4.2671e+01, 1.4026e-01,\n",
      "        4.3345e+01, 1.0000e-05, 7.1119e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.1352e+01, 1.8283e+01, 5.9960e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([4.0058e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.8495e+01, 2.8472e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.5384e+02, 1.0000e-05, 1.0000e-05, 1.6883e+00,\n",
      "        1.3445e+01, 9.9205e+00, 1.0000e-05, 1.0784e+00, 1.0000e-05, 3.2460e+01,\n",
      "        7.7235e-02, 1.0000e-05, 1.0000e-05, 2.9361e+01, 6.6287e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0664e+01, 1.0000e-05,\n",
      "        4.1058e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.8685e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.3436e+01, 1.0000e-05,\n",
      "        3.7562e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.5884e+00, 1.0000e-05,\n",
      "        1.0000e-05, 9.8385e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.3322e+00, 2.5361e+00, 1.0000e-05, 1.4169e+02, 3.1685e+00,\n",
      "        1.0000e-05, 1.0000e-05, 9.4903e+01, 1.6099e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.7464e+02, 1.0000e-05, 5.0170e+00, 1.0000e-05, 4.5229e+01, 1.0000e-05,\n",
      "        3.8341e+00, 1.0000e-05, 1.0815e+01, 8.1455e+00, 1.0000e-05, 8.5033e+00,\n",
      "        8.8633e+01, 1.7481e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5270e+01,\n",
      "        1.0000e-05, 4.7016e+00, 1.0358e+01, 3.2371e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 9.9430e+01, 1.0000e-05, 1.0000e-05,\n",
      "        5.4714e+01, 1.0000e-05, 1.0000e-05, 2.2487e+01, 7.0566e+00, 1.0000e-05,\n",
      "        1.9611e+01, 1.0000e-05, 6.7174e-01, 1.0000e-05, 1.0000e-05, 4.6911e-01,\n",
      "        1.0000e-05, 1.0000e-05, 1.3210e+00, 1.5868e+02, 1.0000e-05, 1.0000e-05,\n",
      "        4.1232e+00, 2.5155e+01, 1.3143e+01, 8.6943e+00, 4.7637e+00, 1.0709e+02,\n",
      "        1.0000e-05, 1.3349e+01, 3.1230e+01, 1.1436e+01, 2.7226e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.6631e+01, 1.0000e-05, 1.1753e+02, 1.0000e-05,\n",
      "        1.0000e-05, 4.3158e+01, 1.0000e-05, 5.2858e-01, 5.8984e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 4.4692e+00, 1.0000e-05, 2.8510e+01,\n",
      "        8.2961e-01, 1.0000e-05, 1.0000e-05, 2.7386e+01, 4.0174e+01, 7.7390e-02,\n",
      "        1.4929e+00, 4.5256e+01, 1.0000e-05, 1.0869e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.4803e-01, 1.0000e-05, 1.0000e-05, 7.6122e+01, 1.0000e-05,\n",
      "        5.6285e+01, 1.0000e-05, 8.9665e+01, 3.7538e+00, 5.0539e+00, 1.0000e-05,\n",
      "        3.2707e+01, 1.4235e+01, 3.5707e+01, 2.1494e+01, 1.6367e+00, 8.3232e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 8.6469e+00, 1.0000e-05, 1.0000e-05,\n",
      "        8.2793e+01, 1.0000e-05, 1.2746e-01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.3674e+02, 1.5390e+01, 1.8496e+01, 1.7380e+02, 1.6252e-01, 1.2889e+01,\n",
      "        1.0000e-05, 4.5393e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.1491e+01, 1.0000e-05, 1.4588e+02, 7.6464e+00, 7.1973e+01,\n",
      "        1.0000e-05, 1.0000e-05, 7.0814e+01, 6.0776e+00, 1.0000e-05, 1.0000e-05,\n",
      "        9.9350e+01, 1.0000e-05, 1.0000e-05, 1.3889e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0474e+01, 3.3245e-01, 4.4966e+01, 1.0000e-05, 1.1410e-01, 1.0000e-05,\n",
      "        1.0213e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 9.0812e+00, 1.0000e-05,\n",
      "        3.9189e+01, 2.8813e+01, 9.4744e+00, 2.3576e+02, 2.4050e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 5.5917e-01, 1.0000e-05, 5.9638e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.5657e+01, 1.0000e-05, 1.0990e+02, 2.2187e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.8479e+01, 5.6336e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([2.0363e+00, 1.9246e+00, 1.0000e-05, 1.0000e-05, 7.6793e+00, 4.2962e+01,\n",
      "        1.9476e+01, 1.0000e-05, 1.0000e-05, 5.8280e+01, 1.0000e-05, 1.1824e+02,\n",
      "        1.0000e-05, 4.9237e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.4450e+01, 1.0202e+02, 5.3158e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 5.9439e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5420e+01,\n",
      "        1.0000e-05, 6.8667e+00, 1.0000e-05, 1.0000e-05, 1.8428e+01, 1.0000e-05,\n",
      "        1.0000e-05, 2.3169e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 9.0818e+00,\n",
      "        1.0000e-05, 3.7124e+00, 1.0000e-05, 4.6313e-01, 7.8679e+01, 1.0000e-05,\n",
      "        1.9976e+01, 5.9956e+00, 1.0000e-05, 9.4561e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.8440e+02, 1.0000e-05, 1.0000e-05, 5.0751e+01, 5.4801e+01, 1.0000e-05,\n",
      "        2.8620e+01, 1.0000e-05, 1.0000e-05, 3.8222e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     33\u001b[0m     loss_fn_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_cap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m loss_cap\n\u001b[0;32m---> 34\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_triplet_capped_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_tripletloss_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcap_calc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTripletLoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCappedBCELoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     36\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network, embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:233\u001b[0m, in \u001b[0;36mtrain_triplet_capped_loss\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, cap_calc, loss_fn, loss_fn_args, print_dist)\u001b[0m\n\u001b[1;32m    231\u001b[0m anchor_output, anchor_embeds \u001b[38;5;241m=\u001b[39m network(anchor_data\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m    232\u001b[0m _, pos_embeds \u001b[38;5;241m=\u001b[39m network(pos_data\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m--> 233\u001b[0m _, neg_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneg_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m cap \u001b[38;5;241m=\u001b[39m cap_calc(anchor_embeds, pos_embeds, neg_embeds) \n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_dist:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/models.py:42\u001b[0m, in \u001b[0;36mConvNetWithEmbeddings.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 42\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1_drop(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     43\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)), \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     44\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m250\u001b[39m) \n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# capped smote using triplet loss\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-3, 1e-3, 1e-2]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['print_loss']=False\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "start_epoch = 5\n",
    "\n",
    "loss_caps = [1, 5, 10]\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10): \n",
    "            print(loss_cap)\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(2)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                _, _ = train.train_triplet_capped_loss(epoch, train_loader_tripletloss_smote, network, optimizer, verbose=False, cap_calc=loss_fns.TripletLoss,loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args, print_dist=True)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "    \n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "        \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "\n",
    "\n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"triplet_loss_capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm, \"margin=0.5\"]\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "25ce4afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006964887678623199, AUC: 0.3482715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00073517307639122, AUC: 0.841398\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010314007997512817, AUC: 0.826527\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009717654287815094, AUC: 0.8537300000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006961984634399415, AUC: 0.47539049999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007382735610008239, AUC: 0.825611\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009567088186740875, AUC: 0.831588\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015623956918716432, AUC: 0.8068925\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006931769251823425, AUC: 0.509471\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006451610028743743, AUC: 0.8464630000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012270017266273498, AUC: 0.8370259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013078323006629944, AUC: 0.864949\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006944571435451508, AUC: 0.445826\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008669923841953278, AUC: 0.8259850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009103761315345764, AUC: 0.8520369999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009062850177288056, AUC: 0.8787030000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006883836388587952, AUC: 0.6776545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007697112262248993, AUC: 0.839264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011312421560287476, AUC: 0.836353\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015704461336135864, AUC: 0.812537\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000699282020330429, AUC: 0.30072699999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007561590373516083, AUC: 0.8454740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006398472785949707, AUC: 0.782464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001031688392162323, AUC: 0.865548\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006867203414440155, AUC: 0.7741625000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007571307122707367, AUC: 0.8070809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008658646643161774, AUC: 0.851773\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001038480818271637, AUC: 0.8632489999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006985587179660798, AUC: 0.4241545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007187469601631165, AUC: 0.8148850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008026745319366455, AUC: 0.847086\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012267091274261474, AUC: 0.839636\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006906501948833466, AUC: 0.6274955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006881701350212097, AUC: 0.8297220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000837628573179245, AUC: 0.8645340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010912498831748961, AUC: 0.8570349999999998\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006931628286838532, AUC: 0.517671\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000721651703119278, AUC: 0.84447\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012107672691345215, AUC: 0.854232\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001070845365524292, AUC: 0.8673435\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006956841349601746, AUC: 0.4125635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00058995121717453, AUC: 0.7727959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006098356246948242, AUC: 0.80277\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006457704603672027, AUC: 0.810598\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006939020454883576, AUC: 0.519663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006275418400764466, AUC: 0.793617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005780983865261078, AUC: 0.807573\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005679846704006195, AUC: 0.817343\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006914187371730805, AUC: 0.576268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006320301592350006, AUC: 0.7865439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005776986479759216, AUC: 0.799199\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006131931841373443, AUC: 0.8158160000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0007014695405960083, AUC: 0.2262525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006065096855163574, AUC: 0.7679549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006076382398605346, AUC: 0.782258\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000623035877943039, AUC: 0.7969729999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0007048844397068023, AUC: 0.2643715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006356420814990997, AUC: 0.7727170000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00057607901096344, AUC: 0.7863129999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005922649502754212, AUC: 0.805336\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006923337280750275, AUC: 0.5283145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00060067218542099, AUC: 0.7647740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006089491248130798, AUC: 0.7924229999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006275667548179627, AUC: 0.8096549999999998\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0007001697123050689, AUC: 0.291281\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005993172526359558, AUC: 0.768286\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005809304416179657, AUC: 0.796477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005988768637180329, AUC: 0.8078699999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006949147582054139, AUC: 0.418945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006285334825515747, AUC: 0.768832\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005816346108913422, AUC: 0.794821\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006180701851844787, AUC: 0.807817\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006933906674385071, AUC: 0.5035585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005996799767017364, AUC: 0.8007094999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005933422446250915, AUC: 0.804215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006146812736988067, AUC: 0.8228629999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006925413310527801, AUC: 0.5801085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005885217487812042, AUC: 0.7805460000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005701355338096619, AUC: 0.8124170000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005963357090950012, AUC: 0.8258099999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006931113302707672, AUC: 0.5052620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006038292646408081, AUC: 0.8442369999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009694813191890717, AUC: 0.871629\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016370036602020265, AUC: 0.834625\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000694307565689087, AUC: 0.5696005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012186483740806579, AUC: 0.832325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00117341548204422, AUC: 0.863417\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014034479260444641, AUC: 0.851683\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006916224658489227, AUC: 0.5674680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010546446442604065, AUC: 0.846184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010310733914375306, AUC: 0.8756970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012629610896110535, AUC: 0.8824119999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000692455530166626, AUC: 0.542134\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010450857281684876, AUC: 0.844635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011799923181533814, AUC: 0.8730639999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001465002477169037, AUC: 0.872712\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000696535885334015, AUC: 0.43334100000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011155704259872437, AUC: 0.830578\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008758077919483185, AUC: 0.85151\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014336732625961303, AUC: 0.872911\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006861005127429962, AUC: 0.753309\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008633033633232116, AUC: 0.867355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012579433917999268, AUC: 0.8600350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011751952171325684, AUC: 0.863558\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006959941685199737, AUC: 0.4150395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009759220480918884, AUC: 0.849207\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010458824634552001, AUC: 0.8547909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008734932541847229, AUC: 0.774424\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006905612051486969, AUC: 0.618195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010072756111621857, AUC: 0.8289580000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012035175561904907, AUC: 0.8549620000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011508205533027648, AUC: 0.8504689999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006850793659687042, AUC: 0.7759304999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107933908700943, AUC: 0.8351950000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012307174801826476, AUC: 0.8374840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001549645185470581, AUC: 0.8352599999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006924456655979156, AUC: 0.5842160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006634254157543183, AUC: 0.874015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000829733669757843, AUC: 0.874533\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013509661555290223, AUC: 0.861971\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006873879432678223, AUC: 0.668617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007288475334644317, AUC: 0.83084\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000798613578081131, AUC: 0.8603899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010380207896232604, AUC: 0.861265\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006963967382907867, AUC: 0.34070750000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007012554407119751, AUC: 0.813416\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008462599813938141, AUC: 0.8528180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006368292272090912, AUC: 0.868417\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006908078789710999, AUC: 0.6734819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006440731883049011, AUC: 0.8425530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001008029878139496, AUC: 0.8353280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021165251731873, AUC: 0.8578000000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006938815414905548, AUC: 0.4701365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006398897767066955, AUC: 0.823039\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009389999508857727, AUC: 0.8418840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012309846878051757, AUC: 0.839351\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006928956806659699, AUC: 0.5152815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006695924997329711, AUC: 0.836996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000713811069726944, AUC: 0.869234\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009120012223720551, AUC: 0.884378\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006952646076679229, AUC: 0.454125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007137651145458221, AUC: 0.819804\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005585785508155823, AUC: 0.794235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010339073836803437, AUC: 0.8582479999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006922257542610168, AUC: 0.593245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006762655973434448, AUC: 0.840071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009531191289424896, AUC: 0.8400770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009395413696765899, AUC: 0.8632420000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006843304336071014, AUC: 0.754032\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007121754586696625, AUC: 0.8400759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007208648920059204, AUC: 0.879004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008298051655292511, AUC: 0.8793030000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006933661103248597, AUC: 0.539899\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007351889312267304, AUC: 0.838316\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008324334919452667, AUC: 0.855024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009299099445343017, AUC: 0.856663\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0007029947936534882, AUC: 0.2404185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007611706256866455, AUC: 0.838189\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000869436115026474, AUC: 0.852241\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012457705140113831, AUC: 0.848675\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006937456727027893, AUC: 0.48097750000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006296309232711792, AUC: 0.8038609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005941517353057861, AUC: 0.827549\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005852521359920502, AUC: 0.8455324999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006981613039970398, AUC: 0.35741700000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006273975074291229, AUC: 0.731024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006015610098838806, AUC: 0.759389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006258268058300018, AUC: 0.7851140000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006868114173412323, AUC: 0.6472709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006055816113948822, AUC: 0.7690710000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006437261402606965, AUC: 0.7876675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006609744429588318, AUC: 0.796758\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006866777837276458, AUC: 0.678727\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000609272301197052, AUC: 0.7902450000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005601440370082855, AUC: 0.8025740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005625659823417663, AUC: 0.8274110000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.000690476804971695, AUC: 0.639091\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006030465364456176, AUC: 0.76774\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005807923972606659, AUC: 0.8022050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006262666881084442, AUC: 0.8161539999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0007070432305335999, AUC: 0.2567175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006206088662147522, AUC: 0.770519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005693963170051575, AUC: 0.794623\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005815098285675049, AUC: 0.8173400000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006920393109321594, AUC: 0.585516\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006010463535785675, AUC: 0.7736589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005691076517105102, AUC: 0.7999640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005890714228153228, AUC: 0.8220719999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006970245838165283, AUC: 0.39544100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006008437871932984, AUC: 0.7631329999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005794459283351898, AUC: 0.790374\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005711245238780975, AUC: 0.8116605\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006910800039768219, AUC: 0.584476\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005670577585697174, AUC: 0.800147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005715482532978058, AUC: 0.82222\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006515784859657288, AUC: 0.8217369999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006903544068336487, AUC: 0.6095495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006156922280788422, AUC: 0.7991219999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000538288563489914, AUC: 0.8264360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005560230016708374, AUC: 0.842511\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006966787576675415, AUC: 0.3983555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007222783267498016, AUC: 0.869363\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010071758329868318, AUC: 0.8470485000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012444838881492614, AUC: 0.867406\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006997887194156646, AUC: 0.35265399999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009627985954284668, AUC: 0.836147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008971806466579437, AUC: 0.853533\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018746265172958374, AUC: 0.829051\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006894789934158325, AUC: 0.6815665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009019197821617126, AUC: 0.855526\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008880127966403961, AUC: 0.875398\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011336479783058166, AUC: 0.887485\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006959426403045654, AUC: 0.37305950000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008238074481487274, AUC: 0.8587229999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009622399508953095, AUC: 0.867203\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014156073331832886, AUC: 0.8529399999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006926114559173584, AUC: 0.5274565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008006506264209747, AUC: 0.837974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010012348294258118, AUC: 0.8614569999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001378297448158264, AUC: 0.868375\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0007036321759223938, AUC: 0.276433\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006420859694480897, AUC: 0.8778259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007337818443775177, AUC: 0.8765740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013255744576454163, AUC: 0.847181\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006944135427474975, AUC: 0.474867\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009478649497032165, AUC: 0.805874\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001002079963684082, AUC: 0.872723\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017056990265846253, AUC: 0.824271\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006942294836044311, AUC: 0.467659\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006910409927368164, AUC: 0.8614120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001030386745929718, AUC: 0.8703960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001048705518245697, AUC: 0.8718370000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006894182562828064, AUC: 0.66474\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005513716340065002, AUC: 0.8781209999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007959308028221131, AUC: 0.8807670000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012150654196739197, AUC: 0.872276\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006941581666469574, AUC: 0.4895235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009898362159729004, AUC: 0.8593590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012273207306861878, AUC: 0.8441259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014980888962745666, AUC: 0.8405285\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006946149468421936, AUC: 0.431736\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007163270711898804, AUC: 0.8414879999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001023943543434143, AUC: 0.852204\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000873123973608017, AUC: 0.8733000000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006913607120513916, AUC: 0.5953550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006977505087852478, AUC: 0.830875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009022049903869629, AUC: 0.8551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008309317231178283, AUC: 0.869514\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006998177468776703, AUC: 0.32885600000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006329614818096161, AUC: 0.844735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007288149893283845, AUC: 0.8794120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008934969007968902, AUC: 0.880472\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006871189177036285, AUC: 0.647358\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006863475143909454, AUC: 0.836021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008715020120143891, AUC: 0.861773\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0008677043914794922, AUC: 0.873766\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006948215365409851, AUC: 0.44974300000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006209433376789094, AUC: 0.859944\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011180386543273926, AUC: 0.8403849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008133378028869629, AUC: 0.876189\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006891251504421235, AUC: 0.614881\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007206940054893493, AUC: 0.822441\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000845925122499466, AUC: 0.8559945000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010710387229919433, AUC: 0.867974\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006960428357124329, AUC: 0.439736\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005857323706150055, AUC: 0.841833\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009211321473121643, AUC: 0.846067\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010535757541656494, AUC: 0.8533420000000002\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006958933174610138, AUC: 0.37558400000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007239218056201935, AUC: 0.830268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007279396951198578, AUC: 0.863995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001042657732963562, AUC: 0.8694850000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006966108083724976, AUC: 0.4870975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007084191739559173, AUC: 0.813611\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001110625147819519, AUC: 0.820113\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008412255644798279, AUC: 0.878032\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006922284066677093, AUC: 0.5504545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006489351093769074, AUC: 0.831129\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011578829884529114, AUC: 0.8231729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010449389815330506, AUC: 0.857415\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006993962824344635, AUC: 0.307477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005656020939350128, AUC: 0.8104359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005565700232982635, AUC: 0.817501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005500772297382354, AUC: 0.8309770000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006847118139266968, AUC: 0.717801\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006017836928367614, AUC: 0.755253\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006175267100334168, AUC: 0.784343\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006482227146625518, AUC: 0.799193\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006961579024791718, AUC: 0.41039400000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006349701881408691, AUC: 0.7525470000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005696538984775543, AUC: 0.798821\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005563447773456573, AUC: 0.827493\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0007002742290496826, AUC: 0.36772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005938598811626435, AUC: 0.7970849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005595190823078156, AUC: 0.8116250000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005830189883708954, AUC: 0.823024\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006892826557159424, AUC: 0.685903\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006198147535324096, AUC: 0.7771329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005677554905414581, AUC: 0.791501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005808756649494172, AUC: 0.8144760000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006931638419628144, AUC: 0.506908\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005789222419261932, AUC: 0.8014009999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005544031262397766, AUC: 0.8228660000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005623725652694702, AUC: 0.83096\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006979638040065765, AUC: 0.33687649999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005897727608680726, AUC: 0.8049669999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005656885504722595, AUC: 0.8137570000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005767394602298737, AUC: 0.82622\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000693691611289978, AUC: 0.49163749999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000623263955116272, AUC: 0.729361\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006053521037101745, AUC: 0.777497\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006119861006736756, AUC: 0.8077430000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006916729807853699, AUC: 0.665225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000604123443365097, AUC: 0.7826569999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000572631984949112, AUC: 0.804999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005838558971881867, AUC: 0.8272949999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000702115386724472, AUC: 0.256207\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006290761232376099, AUC: 0.772369\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005777866840362549, AUC: 0.7912410000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000571781724691391, AUC: 0.817149\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000689954161643982, AUC: 0.6224354999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006696927547454834, AUC: 0.8391649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010561627745628358, AUC: 0.8624040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011525577306747436, AUC: 0.8789560000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000687600165605545, AUC: 0.6463235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001127226710319519, AUC: 0.830285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011954184770584107, AUC: 0.8479339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013237202167510986, AUC: 0.8752679999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006991429030895233, AUC: 0.27942599999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008747574388980866, AUC: 0.847294\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010428960919380188, AUC: 0.8798949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001121375322341919, AUC: 0.8715649999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006881274878978729, AUC: 0.7000775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008347879946231842, AUC: 0.8535305000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009024002850055694, AUC: 0.8643719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001316437244415283, AUC: 0.8608529999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006976059079170227, AUC: 0.4164485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000802900105714798, AUC: 0.859806\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010028988122940063, AUC: 0.871806\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001041642963886261, AUC: 0.869907\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006893242597579956, AUC: 0.5960265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007124507427215576, AUC: 0.847674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001251802146434784, AUC: 0.8570409999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013170126080513, AUC: 0.8638170000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006925864517688752, AUC: 0.5448594999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009432995319366455, AUC: 0.8400119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012971652150154114, AUC: 0.8591899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013174834847450257, AUC: 0.8662359999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006914799213409424, AUC: 0.601997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009482887387275696, AUC: 0.8457029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013148759603500365, AUC: 0.8533959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011871318817138672, AUC: 0.869723\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006934074759483337, AUC: 0.49730749999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009023742377758026, AUC: 0.849256\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006607531905174255, AUC: 0.8573689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001261540174484253, AUC: 0.8524685000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000690977931022644, AUC: 0.595788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001017330527305603, AUC: 0.828994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014169846773147584, AUC: 0.815164\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00099807870388031, AUC: 0.8697720000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# capped smote using triplet loss w/ average \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-3, 1e-3, 1e-2]\n",
    "\n",
    "loss_fn_args = {}\n",
    "cap_aucs = []\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "loss_caps = [1, 5, 10]\n",
    "cap_calc = loss_fns.TripletLossWithAverage\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10): \n",
    "            print(loss_cap)\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(2)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                _, _ = train.train_triplet_capped_loss(epoch, train_loader_tripletloss_smote, network, optimizer, verbose=False, cap_calc=cap_calc, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "        \n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "        \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "\n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"triplet_loss_capped_smote_average\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7278387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a37a386d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006953137814998627, AUC: 0.41351000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012100113034248352, AUC: 0.751849\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001347662389278412, AUC: 0.779907\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013431642651557923, AUC: 0.8068489999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000698688566684723, AUC: 0.35026250000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011922909617424011, AUC: 0.7699495000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001296019196510315, AUC: 0.8160130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014232771992683411, AUC: 0.8320620000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006938562393188477, AUC: 0.47384050000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011157945990562439, AUC: 0.7417819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013826609253883362, AUC: 0.776867\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012448267340660095, AUC: 0.806332\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000687132328748703, AUC: 0.732907\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013641536831855774, AUC: 0.7088535000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014020829200744628, AUC: 0.7531479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014276145100593567, AUC: 0.771426\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0007051105201244354, AUC: 0.322938\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012302497029304505, AUC: 0.7643880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013913211822509765, AUC: 0.8029550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013240280747413636, AUC: 0.8284419999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006952855587005615, AUC: 0.485296\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012617493271827698, AUC: 0.7613989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001308695673942566, AUC: 0.806971\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014748266935348511, AUC: 0.815221\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006956429183483123, AUC: 0.476747\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011212791800498962, AUC: 0.774918\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011230679154396058, AUC: 0.825895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001202311098575592, AUC: 0.838972\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006950486600399017, AUC: 0.48097199999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011566267609596252, AUC: 0.79585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001257402002811432, AUC: 0.818791\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012450679540634156, AUC: 0.844643\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0007013601660728454, AUC: 0.330824\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011272501945495605, AUC: 0.7583740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013851812481880188, AUC: 0.788767\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013489682078361512, AUC: 0.8201449999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0007009153962135315, AUC: 0.24535099999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012266036868095397, AUC: 0.7551239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014567877650260926, AUC: 0.787479\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014886544346809387, AUC: 0.8071490000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006931928098201751, AUC: 0.5002415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014012029767036438, AUC: 0.649479\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013114124536514281, AUC: 0.693584\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012299007773399354, AUC: 0.7121620000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0007101229131221771, AUC: 0.2119795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014262165427207946, AUC: 0.616976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014142146110534667, AUC: 0.6947409999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011929526925086975, AUC: 0.720495\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006943486928939819, AUC: 0.47322600000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001498607873916626, AUC: 0.60515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013810985684394837, AUC: 0.6721889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012916117310523988, AUC: 0.6959690000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006934674978256225, AUC: 0.49733550000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015681596398353576, AUC: 0.549493\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001625352680683136, AUC: 0.65735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001388828694820404, AUC: 0.699155\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006907029747962952, AUC: 0.5882499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014448300004005431, AUC: 0.607761\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013494152426719666, AUC: 0.6913860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011988258361816406, AUC: 0.715993\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006981097161769867, AUC: 0.360881\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014119338393211364, AUC: 0.656449\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013790208697319031, AUC: 0.687104\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013106749653816223, AUC: 0.7021760000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006989958882331848, AUC: 0.4668105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001303554654121399, AUC: 0.682885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012563939094543458, AUC: 0.707837\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012131325006484986, AUC: 0.7160259999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006810896396636963, AUC: 0.7282065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017621628642082215, AUC: 0.254372\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019957852959632872, AUC: 0.526392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016850491166114808, AUC: 0.6527689999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000695532888174057, AUC: 0.4225245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001594462215900421, AUC: 0.567238\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001725798487663269, AUC: 0.651816\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001516685664653778, AUC: 0.686813\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006976980865001679, AUC: 0.3722295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001732854187488556, AUC: 0.531613\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018189577460289, AUC: 0.639831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015425624847412109, AUC: 0.685395\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006946506798267365, AUC: 0.45809099999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010704345107078552, AUC: 0.827189\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012367932200431824, AUC: 0.8512869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011861377358436584, AUC: 0.8723659999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006960716247558594, AUC: 0.40448150000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010685625076293946, AUC: 0.821029\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015051649808883666, AUC: 0.8280540000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012095825672149658, AUC: 0.859088\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006914547681808471, AUC: 0.5743914999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013932805061340331, AUC: 0.806651\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013861716389656066, AUC: 0.843584\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011066436171531678, AUC: 0.8695819999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006953510642051697, AUC: 0.44617850000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001377259373664856, AUC: 0.813183\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011593254208564759, AUC: 0.8457060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015679771900177003, AUC: 0.840825\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006900211274623871, AUC: 0.6015820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014498870372772216, AUC: 0.80487\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016269005537033082, AUC: 0.8309340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001314681053161621, AUC: 0.862953\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000698762983083725, AUC: 0.298201\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010476409792900085, AUC: 0.8333949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015019388198852538, AUC: 0.857112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012248939275741578, AUC: 0.8620990000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006974970698356628, AUC: 0.30638350000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012795483469963073, AUC: 0.8072629999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016676595211029053, AUC: 0.8269390000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014754130244255066, AUC: 0.8483880000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006972302198410034, AUC: 0.3896045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013244966864585875, AUC: 0.810084\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015768024921417236, AUC: 0.822819\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010522279143333436, AUC: 0.865938\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006987023949623108, AUC: 0.3143995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010556061267852784, AUC: 0.829197\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001492241322994232, AUC: 0.8176049999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001694005012512207, AUC: 0.8349770000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006853301227092743, AUC: 0.7056475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014562657475471496, AUC: 0.7940020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001479567587375641, AUC: 0.808318\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013639997839927673, AUC: 0.8473469999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006961835324764252, AUC: 0.38450400000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012881333231925965, AUC: 0.775539\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014020306468009948, AUC: 0.789855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013349701166152953, AUC: 0.805526\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006926476657390595, AUC: 0.5199125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001334111452102661, AUC: 0.740914\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001407979667186737, AUC: 0.787825\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0012890581488609314, AUC: 0.818852\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006957947611808777, AUC: 0.4403390000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013087846636772155, AUC: 0.7569419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014033986926078796, AUC: 0.7918499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011560410261154176, AUC: 0.82701\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006878388524055481, AUC: 0.7391975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001277011215686798, AUC: 0.733312\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013133590817451476, AUC: 0.7746850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013910502791404725, AUC: 0.7918069999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006964699029922485, AUC: 0.3996565000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001165420174598694, AUC: 0.743013\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013641301393508911, AUC: 0.789986\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014104530215263366, AUC: 0.8106280000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006948442161083221, AUC: 0.46871799999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012675276398658752, AUC: 0.740739\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013235929608345033, AUC: 0.790343\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00137513667345047, AUC: 0.805365\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006947003304958344, AUC: 0.424541\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00122288978099823, AUC: 0.748015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013375158905982972, AUC: 0.782779\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001588438630104065, AUC: 0.798579\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006849124431610108, AUC: 0.694234\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012758437395095825, AUC: 0.774427\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012920546531677247, AUC: 0.8121820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012923317551612854, AUC: 0.8296669999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006977393925189972, AUC: 0.36523649999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001291788637638092, AUC: 0.737293\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013498828411102296, AUC: 0.7949120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014673247933387756, AUC: 0.8174250000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006943438947200775, AUC: 0.4504785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011255612969398498, AUC: 0.768319\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012107766270637512, AUC: 0.80548\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013772826790809632, AUC: 0.813829\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006917154788970947, AUC: 0.5690235000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013541932702064515, AUC: 0.598478\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012371926307678223, AUC: 0.688711\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010923356413841247, AUC: 0.723258\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006968607604503632, AUC: 0.3831825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015744782090187073, AUC: 0.5828990000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014835953712463378, AUC: 0.665832\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012981435656547547, AUC: 0.6980459999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006955974400043488, AUC: 0.43256\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014091846942901611, AUC: 0.645457\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001369967758655548, AUC: 0.683918\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013020054697990416, AUC: 0.7027850000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0007000738978385925, AUC: 0.3881845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016745031476020813, AUC: 0.57461\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016251876950263977, AUC: 0.659213\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013794299364089965, AUC: 0.6950369999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006960032284259796, AUC: 0.4766595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001597978949546814, AUC: 0.5385105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015363540053367614, AUC: 0.6627500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012637528777122498, AUC: 0.704556\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006979741156101227, AUC: 0.32655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001522919237613678, AUC: 0.553407\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015850431323051453, AUC: 0.659846\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013161882162094115, AUC: 0.6960270000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006918782293796539, AUC: 0.559968\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012581725716590882, AUC: 0.679128\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001229236662387848, AUC: 0.710427\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011915088295936585, AUC: 0.724267\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006963012814521789, AUC: 0.36674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014103291034698487, AUC: 0.610051\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013099029064178468, AUC: 0.704035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011947248578071594, AUC: 0.7228699999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006930817067623138, AUC: 0.55144\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00126514995098114, AUC: 0.663859\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001268758773803711, AUC: 0.6927850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011619875431060792, AUC: 0.7084585000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006888798475265503, AUC: 0.6461855000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012820826768875121, AUC: 0.685115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012846903204917907, AUC: 0.709021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012085999846458435, AUC: 0.723304\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006979137063026429, AUC: 0.32278750000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00133148592710495, AUC: 0.820056\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013782886266708375, AUC: 0.858347\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00192118239402771, AUC: 0.85137\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006899331212043762, AUC: 0.657114\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012319467067718505, AUC: 0.8096279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012445133924484254, AUC: 0.847517\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001337042450904846, AUC: 0.847077\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006820880770683289, AUC: 0.710675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012821065187454224, AUC: 0.816725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012086466550827026, AUC: 0.8384739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012879958152770996, AUC: 0.850204\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006903654634952545, AUC: 0.593124\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001118379533290863, AUC: 0.8279394999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014810474514961242, AUC: 0.8571609999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013770051002502442, AUC: 0.854878\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0007069114446640014, AUC: 0.242749\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014144913554191589, AUC: 0.8204989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001513637661933899, AUC: 0.8455180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001306992769241333, AUC: 0.8610800000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006921878755092621, AUC: 0.5970345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013711398243904114, AUC: 0.7955110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015437301397323608, AUC: 0.8171569999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012172499895095825, AUC: 0.8463309999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006925551295280457, AUC: 0.5933379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014085525274276733, AUC: 0.78868\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013914011716842651, AUC: 0.826568\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012870569229125977, AUC: 0.854255\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006961119771003723, AUC: 0.38871350000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008103238046169282, AUC: 0.858504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000931158035993576, AUC: 0.8774884999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013895972967147827, AUC: 0.8711614999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006905738711357116, AUC: 0.644471\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012654099464416504, AUC: 0.7992060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011641114950180053, AUC: 0.843349\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015687112212181091, AUC: 0.84473\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006835061609745026, AUC: 0.7417645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013023046255111693, AUC: 0.825409\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014262330532073975, AUC: 0.8375239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012669787406921387, AUC: 0.852042\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000697836846113205, AUC: 0.37008850000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011396661400794982, AUC: 0.770827\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013007211685180664, AUC: 0.811997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012499591708183288, AUC: 0.815539\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006927756369113922, AUC: 0.518756\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011860894560813903, AUC: 0.7321660000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001197644829750061, AUC: 0.792614\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012735504508018494, AUC: 0.824929\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000690514475107193, AUC: 0.6386790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001202582597732544, AUC: 0.767078\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001092694878578186, AUC: 0.817475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011756229400634766, AUC: 0.840884\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006964456140995025, AUC: 0.36736450000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001126945436000824, AUC: 0.7768120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001062393069267273, AUC: 0.8200779999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012183101773262023, AUC: 0.8248995\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006926549971103668, AUC: 0.532581\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011874104738235473, AUC: 0.76892\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012728224992752076, AUC: 0.8023020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012980905175209046, AUC: 0.826338\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0007008664906024933, AUC: 0.3223985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012494014501571655, AUC: 0.74944\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012630877494812013, AUC: 0.7997829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013315749168395995, AUC: 0.8174549999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006991316080093384, AUC: 0.33853449999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011583274602890014, AUC: 0.7944675000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012185652256011964, AUC: 0.824557\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001334098756313324, AUC: 0.8564489999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006847937405109406, AUC: 0.705627\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011758599281311035, AUC: 0.7457389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012690225839614869, AUC: 0.793452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013103622794151306, AUC: 0.813781\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006925979852676392, AUC: 0.57975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011308208107948304, AUC: 0.750285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012988250255584718, AUC: 0.7969920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011933872103691101, AUC: 0.824703\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000695866584777832, AUC: 0.42814450000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001182684600353241, AUC: 0.750788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001265819251537323, AUC: 0.814612\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001408158242702484, AUC: 0.8340939999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006949114203453064, AUC: 0.45558\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014354689121246337, AUC: 0.5758650000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013175629377365113, AUC: 0.675001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001132315695285797, AUC: 0.706466\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000690006971359253, AUC: 0.6346470000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013463624715805054, AUC: 0.507289\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012996394634246825, AUC: 0.6847089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001164216935634613, AUC: 0.7226600000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000693820834159851, AUC: 0.471494\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013598218560218812, AUC: 0.5849859999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014249856472015382, AUC: 0.669624\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001213870644569397, AUC: 0.704044\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000694388210773468, AUC: 0.4512545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015957908034324646, AUC: 0.6283139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015082186460494995, AUC: 0.693443\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012417213916778565, AUC: 0.7187049999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006957093477249146, AUC: 0.38944199999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000748483657836914, AUC: 0.582685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010371949672698974, AUC: 0.700732\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010211527645587921, AUC: 0.7242780000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006927612721920013, AUC: 0.5246435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021042466163636, AUC: 0.674104\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010914167761802674, AUC: 0.7199840000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001138194501399994, AUC: 0.738764\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006960340440273284, AUC: 0.6067405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001342705488204956, AUC: 0.590044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013636059165000915, AUC: 0.6818770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012205939888954164, AUC: 0.710601\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006931335330009461, AUC: 0.5170950000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013164325952529908, AUC: 0.626445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001208702564239502, AUC: 0.710852\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011207187175750733, AUC: 0.734961\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006902595460414886, AUC: 0.606362\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001275633454322815, AUC: 0.6548795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012172811031341553, AUC: 0.7008749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011490079164505004, AUC: 0.7142949999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006903997659683228, AUC: 0.6133195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011215248703956603, AUC: 0.6702289999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011986317038536073, AUC: 0.7079549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011585350036621093, AUC: 0.7373209999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006897677779197693, AUC: 0.6272169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013875076174736023, AUC: 0.811742\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012797714471817017, AUC: 0.850464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015719639658927918, AUC: 0.8589289999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006917991638183594, AUC: 0.5485805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013168479800224305, AUC: 0.816217\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014584704637527466, AUC: 0.8348270000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014950295686721802, AUC: 0.848802\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006945172548294068, AUC: 0.4559815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012278074026107787, AUC: 0.828437\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010732338428497315, AUC: 0.845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011716055870056152, AUC: 0.846575\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006956190168857574, AUC: 0.4005905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011252890825271607, AUC: 0.8047855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012148600220680237, AUC: 0.841369\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001485383629798889, AUC: 0.8348699999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006944859623908996, AUC: 0.46946450000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014134332537651061, AUC: 0.806386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011645344495773316, AUC: 0.8577410000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015893993973731995, AUC: 0.8401670000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000698531299829483, AUC: 0.3484005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012929802536964417, AUC: 0.8213739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014258021712303163, AUC: 0.832266\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017997503876686096, AUC: 0.828209\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006885204911231995, AUC: 0.683658\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001426667034626007, AUC: 0.8046515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001412131667137146, AUC: 0.8346359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011856539249420167, AUC: 0.837064\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000691462904214859, AUC: 0.6056925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013597025275230408, AUC: 0.824577\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014291183352470398, AUC: 0.856176\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015407857894897462, AUC: 0.843618\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0007023155391216278, AUC: 0.270157\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012542378306388854, AUC: 0.8123429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018739871978759766, AUC: 0.817189\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015053725242614746, AUC: 0.834164\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006912327408790588, AUC: 0.64205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011190905570983886, AUC: 0.82108\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013176281452178956, AUC: 0.862773\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014106485247612, AUC: 0.85267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# capped smote using triplet loss w/ positive dist squared \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-3, 1e-3, 1e-2]\n",
    "\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "loss_caps = [1, 5, 10]\n",
    "cap_calc = loss_fns.TripletLoss\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    learning_rate_aucs = []\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10): \n",
    "            print(loss_cap)\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(2)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                _, _ = train.train_triplet_capped_loss(epoch, train_loader_tripletloss_smote, network, optimizer, verbose=False, cap_calc=cap_calc, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "\n",
    "\n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"triplet_loss_capped_smote_pos_squared\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f81e35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1276f1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
