{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14d7a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "523773ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2e1dda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "n_epochs = 50\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "NUM_CLASSES_REDUCED = 2\n",
    "nums = (6, 8)\n",
    "ratio = (10, 1)\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec790a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MNIST = torchvision.datasets.MNIST('mnist', train=True, download=True)\n",
    "\n",
    "test_MNIST = torchvision.datasets.MNIST('mnist', train=False, download=True)\n",
    "\n",
    "\n",
    "    \n",
    "reduced_train_MNIST = class_sampling.Reduce(train_MNIST, NUM_CLASSES_REDUCED, nums=nums, CIFAR=False)\n",
    "reduced_test_MNIST = class_sampling.Reduce(test_MNIST, NUM_CLASSES_REDUCED, nums=nums, CIFAR=False)\n",
    "\n",
    "ratio_train_MNIST = class_sampling.Ratio(train_MNIST, NUM_CLASSES_REDUCED, ratio, nums=nums, CIFAR=False)\n",
    "\n",
    "smote_train_MNIST = class_sampling.Smote(ratio_train_MNIST, CIFAR=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baeb801c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5918  585]\n"
     ]
    }
   ],
   "source": [
    "targets = ratio_train_MNIST.labels \n",
    "\n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "print(class_count)\n",
    "\n",
    "weight = 1. / class_count\n",
    "\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= class_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b817686",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_reduced = DataLoader(reduced_train_MNIST, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_MNIST, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_MNIST, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_MNIST, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_MNIST, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_MNIST, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_MNIST, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71d50474",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "            \"mean_40\", \"variance_40\",\n",
    "            \"mean_50\", \"variance_50\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c6863b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = enumerate(train_loader_smote) # enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i].reshape(28, 28).int())\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7fa8f3d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.014997146638037008, AUC: 0.529106990521835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00034093719523392357, AUC: 0.9943033484372387\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002675596971689544, AUC: 0.9967570186005239\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00022661456872957833, AUC: 0.9970763868943257\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00020036267166799145, AUC: 0.9972650070946916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00018535045363147806, AUC: 0.9973175206732027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013857130184923887, AUC: 0.5795264561265128\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002917447831196321, AUC: 0.99599717927064\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00022872275910022096, AUC: 0.9971503345865145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002035129637945266, AUC: 0.996977789971407\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00018738947546506767, AUC: 0.9976363531141624\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00017666397519857, AUC: 0.9978555169265196\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03356571868833301, AUC: 0.47016264205458846\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003127128379438728, AUC: 0.9945262632194897\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00024516865857886476, AUC: 0.9957260377326137\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002176467653201974, AUC: 0.9957399699065044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00020500283310378808, AUC: 0.9958374951237392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00019618590395149483, AUC: 0.9958846501738307\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019818636694803493, AUC: 0.436267806389938\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00029352622002548314, AUC: 0.993594950980182\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002315018642269553, AUC: 0.9958862577323565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00018812205418790103, AUC: 0.9966369875639274\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00016508893134924689, AUC: 0.9967505883664205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00015293520645818848, AUC: 0.9968052453562992\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006975224299460465, AUC: 0.6369302276731554\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00029850922386098354, AUC: 0.9942068949256879\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002143787023443613, AUC: 0.995828921478268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001765907072996254, AUC: 0.9970281601385501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00016321008157285844, AUC: 0.998082182678664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00015150627996733963, AUC: 0.9981175489662327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027448052945344345, AUC: 0.18931359394357683\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00029472578664003693, AUC: 0.9966648519117086\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00022560808475960363, AUC: 0.9967007540521192\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00019087643973822425, AUC: 0.997017443081711\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001757499254761769, AUC: 0.9974879218769425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00016077901728404976, AUC: 0.9976358172613204\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.043210438319614956, AUC: 0.49067348128587535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002596618040748264, AUC: 0.9950722972654358\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00022399158643145986, AUC: 0.9959259108426609\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00020294097362097747, AUC: 0.996061481611674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00019352420331528468, AUC: 0.9961000630162943\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00018441369352133378, AUC: 0.9961622219459603\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017209234691801526, AUC: 0.4487799702494502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000376386749201433, AUC: 0.9947320307107981\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00025645756851071894, AUC: 0.9960255794712632\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00021375564561373946, AUC: 0.9966895011424383\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001912864453427293, AUC: 0.9978265808730544\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00017350790231617834, AUC: 0.9975200730474594\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.042829643865549785, AUC: 0.21297042520994713\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003425444844593419, AUC: 0.9948263408109812\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00026151910424232483, AUC: 0.9949211867640061\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00021547325125145369, AUC: 0.9952191209441299\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001836659668271833, AUC: 0.9958364234180552\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00017789771344597541, AUC: 0.9963036870962348\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017169658194911157, AUC: 0.3512220659913492\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002892040194438358, AUC: 0.9959784244211718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002445360225303326, AUC: 0.9959366278994998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002200057726222289, AUC: 0.9960529079662026\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00020412793749361057, AUC: 0.9960577306417803\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00019668154881237457, AUC: 0.9960673759929354\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023908237986436296, AUC: 0.32199986710849515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008322630177867091, AUC: 0.9791253166890296\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006110757959555395, AUC: 0.9860962263099459\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005102617739397053, AUC: 0.9885375718578662\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00045150086499642634, AUC: 0.9899238231599886\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00039884013819891967, AUC: 0.9909939212853609\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019079573653005912, AUC: 0.32422579981395194\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007571980017806186, AUC: 0.9836441637051867\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005510994859857342, AUC: 0.9889244576097534\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047686242538949716, AUC: 0.9908074444963626\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004355968217178408, AUC: 0.9922901493100359\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003953395762305329, AUC: 0.9926298800118316\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020396662054594998, AUC: 0.32706635572912424\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009568810894869376, AUC: 0.9812140710669474\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006778868952646512, AUC: 0.9875542818928893\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005919841999346178, AUC: 0.9897041234947893\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005447649807663438, AUC: 0.9907999425565753\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005056547853270426, AUC: 0.9912323758000283\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015319022085849296, AUC: 0.5994108833855611\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009089302017081598, AUC: 0.9790347575587401\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005747834779707788, AUC: 0.9883671706541264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000458234693569673, AUC: 0.990918901887488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00038889249474365517, AUC: 0.9922644283736224\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003466467155186039, AUC: 0.9927327637574859\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022831059884333958, AUC: 0.5689749778156923\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007170155551863013, AUC: 0.9835755745414171\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046281468732510046, AUC: 0.991119310850377\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00038617668870072927, AUC: 0.9947089890385943\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00034435577767729514, AUC: 0.9953611219472464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00031480544842548254, AUC: 0.995754437933237\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014110454614611639, AUC: 0.5700257852387546\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007410957082704975, AUC: 0.9839426337381523\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005572757859160935, AUC: 0.9905218349316037\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004801149200455249, AUC: 0.9923335533902338\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00042885151647386097, AUC: 0.9939652252939689\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00038886230678044986, AUC: 0.9943676507782727\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023799740256236452, AUC: 0.48671835145944886\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010532490276648637, AUC: 0.9761122161587497\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007937546842586921, AUC: 0.9851220458432823\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006709727256194404, AUC: 0.9895728395485118\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005924375592798426, AUC: 0.9911461034924746\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005356211284672992, AUC: 0.9922660359321482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.038471383831269, AUC: 0.5511171460048956\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001127833812873556, AUC: 0.9735224393736095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007559132008325486, AUC: 0.9828484222348921\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005925674739584913, AUC: 0.987316363231064\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000514206015668794, AUC: 0.9897143046987864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004603581090397963, AUC: 0.9916556995451681\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009679813069092809, AUC: 0.6394594530871553\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008146941785239779, AUC: 0.9844082898578062\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005787516911330924, AUC: 0.9914836907829023\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004931627068953978, AUC: 0.9918829011501546\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00044094940828734056, AUC: 0.9935252901107287\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00041390117836294707, AUC: 0.9944410626176197\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00926855286702853, AUC: 0.7516295284923673\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008486442067361519, AUC: 0.9835257402271158\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006221389671783763, AUC: 0.9889089178773369\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0005274283898296317, AUC: 0.9901654927917076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004735380781363256, AUC: 0.9920098982736966\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00043891538435874764, AUC: 0.9925693286406915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010137769252863978, AUC: 0.7027436737213479\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0043213530356839575, AUC: 0.8707233584684039\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025608429750793964, AUC: 0.9291307823880175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017684388358153666, AUC: 0.9540056071641383\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001364466134559047, AUC: 0.9649016388523317\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011105834512236696, AUC: 0.9711169959660999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.048457402373446194, AUC: 0.3141201510676332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006812155123329558, AUC: 0.7461391802737565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003984872598825775, AUC: 0.8655668465703275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002915234669395115, AUC: 0.9103711102442202\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023524123195782457, AUC: 0.9318936396411073\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019868568480631826, AUC: 0.946382028781728\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03804946883618215, AUC: 0.5005385321061588\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004113984527548401, AUC: 0.8280314266974746\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020528918837908632, AUC: 0.9288955429904018\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014282616152279618, AUC: 0.9560632820772228\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011652547627008726, AUC: 0.9674324718248577\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001012749090698195, AUC: 0.9738155508781556\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025375956580752417, AUC: 0.31314436304244386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006008985373297587, AUC: 0.7674800555572227\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002995263962518601, AUC: 0.8973836449138992\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020630967419586815, AUC: 0.9351055415757503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016547483440264906, AUC: 0.9556088788672499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014058804413299877, AUC: 0.9638819108941027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007868396816293153, AUC: 0.813069343644571\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036104020371446944, AUC: 0.8969195963527712\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025985039786028813, AUC: 0.9319134661962593\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002009116406272904, AUC: 0.9497364675723294\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016657850140123387, AUC: 0.9599294603318858\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013986716729513606, AUC: 0.9665317031975411\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013424278046033397, AUC: 0.6627717309761524\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032091814538706903, AUC: 0.8757726997980906\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019917528821814874, AUC: 0.9312682993745525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014864930084773473, AUC: 0.9541143852910537\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012210501400333508, AUC: 0.9657750789847089\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001054812909159848, AUC: 0.9731430555615095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02451856881688594, AUC: 0.3331541798665083\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005443113931217549, AUC: 0.7697917247173912\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002766030668965531, AUC: 0.8986118196276467\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019197813225582273, AUC: 0.9386523515366115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015408877383601344, AUC: 0.953459573118192\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013139311447893857, AUC: 0.9613976971188264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01599828895821581, AUC: 0.4285365215862959\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004220162859614592, AUC: 0.8312224303712816\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023152986175031644, AUC: 0.9169744248155595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001618220507481577, AUC: 0.9473208429608228\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012819207356336447, AUC: 0.9608420177217253\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010830853172957774, AUC: 0.969072181521222\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.031331054418970584, AUC: 0.34977365575956065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009803225535043278, AUC: 0.7071553501691152\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004880177061503472, AUC: 0.8470456289411976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031422458080031116, AUC: 0.9068585948652437\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023064981089368866, AUC: 0.9318400543569122\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00186046544562709, AUC: 0.9486770865037962\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01761124296958402, AUC: 0.49919943585412796\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004801756353358551, AUC: 0.8236894111191608\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026883992595949035, AUC: 0.9146729368593879\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019362757665029964, AUC: 0.9434766346726796\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015456293927463192, AUC: 0.9582233048831197\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013225684501616357, AUC: 0.9674581927612711\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0093171685378744, AUC: 0.7798191389487853\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006876715468570559, AUC: 0.8043590556986879\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005815882860503582, AUC: 0.8217378350687821\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005246520289229557, AUC: 0.8363971612659845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004839681937334207, AUC: 0.8475402211143166\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004531451880808449, AUC: 0.857177534476772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.032215298085972885, AUC: 0.22593270545669666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025730607919317843, AUC: 0.27731134764846344\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021026573812986258, AUC: 0.3395163606589704\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017496103085346104, AUC: 0.4057402699840959\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014650556118098353, AUC: 0.4689805506852486\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012382540396775272, AUC: 0.5310719628932623\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016610427426008458, AUC: 0.4445269062429\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014218790930990847, AUC: 0.4941688493739096\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012211926481985404, AUC: 0.5437454184582013\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01047490990680197, AUC: 0.5911324928302889\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009051944898522419, AUC: 0.6373792723547088\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007903894282275845, AUC: 0.6784223849309607\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018394702216359644, AUC: 0.4904387777411017\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013296806540795241, AUC: 0.5152707342898664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010588987520269232, AUC: 0.5647578159495527\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00883881201655228, AUC: 0.6168834370029965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007559715837672137, AUC: 0.6652720203366871\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006553021276959722, AUC: 0.7076204704359271\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030711773266209826, AUC: 0.2610814367715081\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024982199659011873, AUC: 0.3078415633185152\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02052583951140536, AUC: 0.3623442275788454\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016984845293728214, AUC: 0.4185246470873183\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014173884322677834, AUC: 0.48392816571141967\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01187465304420108, AUC: 0.5437089804649489\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014291684820044856, AUC: 0.490287667239672\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011422285135241522, AUC: 0.5417616912373057\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009641479507983348, AUC: 0.6001117789028306\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00831049767093382, AUC: 0.6499996784882948\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007280988476044396, AUC: 0.6906382221688752\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006447658282135831, AUC: 0.7287523631110331\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014613781409727614, AUC: 0.516023071679963\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012573286613322193, AUC: 0.5615287667239671\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010946848871298211, AUC: 0.6067509956145803\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009594809194529279, AUC: 0.6474291924054648\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008491581024343676, AUC: 0.6836919617786885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007537402474855537, AUC: 0.7178397199847388\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024030071361217934, AUC: 0.47882577495038003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017847618948105205, AUC: 0.5318928894471284\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014380257568991209, AUC: 0.5849546454154574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01227407721999269, AUC: 0.6243617992652386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010801579888069358, AUC: 0.6614438876338025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009596453937190906, AUC: 0.6918781856451454\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013063472743853772, AUC: 0.5703901651712799\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009833419298286517, AUC: 0.6174953809485024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008317308643096228, AUC: 0.6553421313225276\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007353524984039875, AUC: 0.6885971586938908\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006614174902068903, AUC: 0.71900841503303\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006053295935162847, AUC: 0.744837593720662\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018981957287521837, AUC: 0.5660486854458081\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011606936622603833, AUC: 0.638073201785033\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.00833857972676216, AUC: 0.6963016508554354\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006892785769318449, AUC: 0.7331501073849096\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006088448360593176, AUC: 0.7655322304767376\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005489326165082785, AUC: 0.7905281579951386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009097732125355343, AUC: 0.6782385874061722\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008991921663777921, AUC: 0.6821578150921881\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008889611956989295, AUC: 0.6863058519417164\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008770341952148184, AUC: 0.6901023693269259\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008677660801889487, AUC: 0.6939915892537927\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008580556320601122, AUC: 0.6975367916561283\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024767556792707424, AUC: 0.4853594286522657\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023870162095095553, AUC: 0.48406159306906493\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023036159096790892, AUC: 0.48444847882095227\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02227768197069504, AUC: 0.48439060671402173\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021459959555363308, AUC: 0.4876673468425407\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02072515546905328, AUC: 0.48994150630377276\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03297822371773098, AUC: 0.19120890544555091\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.032434461526495574, AUC: 0.19303187681386189\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03188429411894046, AUC: 0.1949395129312008\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03131952641173179, AUC: 0.19668639319595496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03074960728362974, AUC: 0.1983336048321066\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030263860279975716, AUC: 0.20102894462711074\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024641912669622134, AUC: 0.5257809519318567\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02339874291271897, AUC: 0.5320332828917191\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02218922719698762, AUC: 0.5386114123794867\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021103361378545345, AUC: 0.545319218255006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02001717668142378, AUC: 0.5519311064718164\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019000960679774945, AUC: 0.5584310014446593\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016481639435572654, AUC: 0.48704843681008947\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016103872354479805, AUC: 0.4925328906474388\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015710857837590124, AUC: 0.4973887891011819\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015343910912302463, AUC: 0.5035559194591744\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015056081440137781, AUC: 0.5114570696137144\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014698399273258313, AUC: 0.5164271047227926\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01327855542579793, AUC: 0.47983800096882195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013045216692653995, AUC: 0.4868458844358327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012863725855730581, AUC: 0.4943446091060688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0126326646864044, AUC: 0.5007475147145191\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012422062595438514, AUC: 0.5086470573105332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012174431581674895, AUC: 0.5138062484728194\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.035999908210327905, AUC: 0.5225486875892194\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.034655183985613396, AUC: 0.5225936992279431\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03326465081477511, AUC: 0.5272684794211074\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03193605809971906, AUC: 0.5360661113802283\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03067100319556321, AUC: 0.5423087969889357\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029459323448670825, AUC: 0.5450668315664479\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03719437048301934, AUC: 0.2513106960514076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.035909210426219994, AUC: 0.2557379122315913\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.034742730498067094, AUC: 0.26021335516755045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03352991147564558, AUC: 0.2652568021159757\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.032477679953565264, AUC: 0.26982601929927597\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03139533167300017, AUC: 0.2746283324688241\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030752007265268645, AUC: 0.384290080720872\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02967609283099757, AUC: 0.3789508430036909\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02860425767444429, AUC: 0.3742090812052832\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02757177688567041, AUC: 0.3744887963887805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026676153297503297, AUC: 0.37201101284760774\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025791436742304767, AUC: 0.3677708093092642\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017819082761650007, AUC: 0.5461642581867597\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016931602920311085, AUC: 0.5525066124240697\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016129887128715437, AUC: 0.5562516879364521\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015346918540464919, AUC: 0.5605342238493096\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014612096190205766, AUC: 0.5629225199658768\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013914504653425197, AUC: 0.5695044004235382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 class normal\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(NUM_CLASSES_REDUCED, shape=28*28)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7fd6390d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.015434165919049185, AUC: 0.586560060529937\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047162366456373386, AUC: 0.9941243735880276\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047194522730311994, AUC: 0.9953359368636747\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004681238353129006, AUC: 0.9955384892379315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004896215060976475, AUC: 0.9955449194720349\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004768311730576351, AUC: 0.9955829650238133\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01628745120504628, AUC: 0.5384179695035431\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00043066131340544167, AUC: 0.9957581889031306\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004368295303042631, AUC: 0.9965710776643675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005211861298938222, AUC: 0.9964960582664946\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005566760373164902, AUC: 0.996480518534078\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005875051515196175, AUC: 0.9964006764606277\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010564863311578028, AUC: 0.7375891123276161\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005165793726656502, AUC: 0.9948579561286562\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005178344792707613, AUC: 0.9950948030847977\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000562142310799032, AUC: 0.9955181268299375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000529460346723442, AUC: 0.995691207297887\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005398979776887914, AUC: 0.9957587247559726\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01622329636883785, AUC: 0.5611525980289189\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00037197222620804116, AUC: 0.9954891907764722\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004169322997640132, AUC: 0.9964944507079687\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004149286797574835, AUC: 0.9965362472296408\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000458375236509256, AUC: 0.9965399981995344\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00048165490723544766, AUC: 0.9964472956578773\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01752576966216599, AUC: 0.45997929464618714\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000440759549599997, AUC: 0.9951912565963486\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00043792931621365914, AUC: 0.9948086576671967\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000473011184799992, AUC: 0.9948568844229723\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004832323015846821, AUC: 0.9948547410116044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000496385075290751, AUC: 0.9941859966648521\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018851644504144324, AUC: 0.7031466350584936\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00059058528015579, AUC: 0.9950315724494476\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000651972264236545, AUC: 0.9952973554590545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000696890488050986, AUC: 0.995312359338629\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006958131772884424, AUC: 0.995249128703279\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007051626224201905, AUC: 0.9952062604759231\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016097408397350747, AUC: 0.4212537456113652\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005087466065928063, AUC: 0.9952727062283248\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000544123829768558, AUC: 0.9958632160601527\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005856433393545525, AUC: 0.9958728614113077\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005634833690295802, AUC: 0.9959746734512781\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005798915219849928, AUC: 0.9959843188024332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02290503184000651, AUC: 0.41001423225148215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005981546072732835, AUC: 0.9918646821535282\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005572045349186252, AUC: 0.9930178374694028\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005620191988244067, AUC: 0.9935611922511393\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005300259429722345, AUC: 0.9946023543230463\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005342742856244864, AUC: 0.9941581323170705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014258610289042534, AUC: 0.42725958426393107\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00040867688678066183, AUC: 0.9947813291722574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004248183922491212, AUC: 0.9949978137204049\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046453092779432027, AUC: 0.9954436432849065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005077819955028115, AUC: 0.994996206161879\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005159002283345098, AUC: 0.9950626519142807\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026972654196539773, AUC: 0.47690956518757\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004832349547935075, AUC: 0.9936142416824922\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005140429667310932, AUC: 0.9950428253591286\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005611616261997579, AUC: 0.9954232808769125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005213462171100435, AUC: 0.9956526258932666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005504900346631589, AUC: 0.9956049349903332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02143407016067031, AUC: 0.7418003798124944\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012108057922458055, AUC: 0.9773784364242755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008539579978393965, AUC: 0.9874154960068247\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007058345938321226, AUC: 0.9909328340613788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006465396649101999, AUC: 0.99247823365756\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006078562195997061, AUC: 0.9935435091073549\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01687634509542714, AUC: 0.4287492551645497\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007437798300638455, AUC: 0.9835530687220552\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005491787941806312, AUC: 0.9903675093131225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005078372501191639, AUC: 0.9925559323196427\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004767034950957288, AUC: 0.9931378685059995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047136334158619, AUC: 0.994322103286707\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020184263185931535, AUC: 0.5116135386435635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009376657798917151, AUC: 0.9813528569530121\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007514007096458419, AUC: 0.9884598731957834\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006283529673550687, AUC: 0.9908996111851779\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006020242197913413, AUC: 0.9922413867014185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000569853723419379, AUC: 0.9927493751955863\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010291692879876241, AUC: 0.6680718514358712\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000562272888779887, AUC: 0.9875982218259293\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00043736479975915596, AUC: 0.9927397298444313\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00040437273418928035, AUC: 0.9933720361979312\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00038897634848304415, AUC: 0.9937187329866723\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003753718814000827, AUC: 0.9939202136552451\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02933506541123795, AUC: 0.22089086606679725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010854995830705694, AUC: 0.9793278690632863\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007933549444127527, AUC: 0.9880831686478933\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893780043900137, AUC: 0.9916230125218092\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006019612832099014, AUC: 0.9930124789409833\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005564244298214251, AUC: 0.993753563421399\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017301365949105526, AUC: 0.3093435588344986\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010187809450038965, AUC: 0.9777669297346886\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007021106918406043, AUC: 0.9875719650366738\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006120321543320366, AUC: 0.9913791994787223\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005390250874108656, AUC: 0.9929974750614086\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005012893312712881, AUC: 0.994131875527815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02022241114582828, AUC: 0.6648251190665015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010143872003377595, AUC: 0.9802623964196456\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007056809236791069, AUC: 0.9886720709211954\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005916092028035387, AUC: 0.9910893030912279\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005326633405241167, AUC: 0.9921326085745028\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004845525969136082, AUC: 0.992836183355982\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01726887191551319, AUC: 0.5546264462668204\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008246762905555235, AUC: 0.9831195637729184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007096679252620563, AUC: 0.9880772742666317\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00067290382972662, AUC: 0.9904387777411016\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006265074446581413, AUC: 0.991437607438495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000599560406884298, AUC: 0.9927075786739141\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023684749445313007, AUC: 0.29908144105833084\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009554870133567794, AUC: 0.9815157562169647\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007079363362882942, AUC: 0.9897159122573123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006195616216146181, AUC: 0.9925704003463753\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005743500793942754, AUC: 0.9932595071011219\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005607585544171541, AUC: 0.9941538454943349\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011804064361698632, AUC: 0.5213810642466123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007416657162502439, AUC: 0.9836104049761438\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005662420583314283, AUC: 0.9893729664384648\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005386052633911433, AUC: 0.9911000201480669\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0005322696166749326, AUC: 0.9914831549300605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005213938823151045, AUC: 0.9925859400787919\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010889808090083594, AUC: 0.6292696754446506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004488938837071137, AUC: 0.8496745229838001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028764699310002612, AUC: 0.9129426680327342\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002171325078908948, AUC: 0.9410058172184521\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017706683950641387, AUC: 0.9549363835506038\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015267168883211124, AUC: 0.9638208236701205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028033436208531475, AUC: 0.26128827596850046\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007363331490668697, AUC: 0.7411209184089029\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038302429714558287, AUC: 0.8827227111581708\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025852392919315315, AUC: 0.9272895920230801\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019614726119900343, AUC: 0.9495537417532246\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016043204698503388, AUC: 0.9608307648120442\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005754209453274745, AUC: 0.748241866825565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002346359239601941, AUC: 0.9099638620843389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015788197393989958, AUC: 0.9492809926566728\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012787914917829367, AUC: 0.965184033300039\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011168171036564291, AUC: 0.9731221573006735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001011970805825654, AUC: 0.9768243645857\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015986950994771952, AUC: 0.4664368572445161\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0053566171022182175, AUC: 0.8101655570940487\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031820097571821195, AUC: 0.9041128849030963\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022901818001986045, AUC: 0.9372778889970121\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018308650756227798, AUC: 0.9523889391399777\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015680325944477974, AUC: 0.9618456700946959\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027928124303403107, AUC: 0.1488845687241987\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004959578346268237, AUC: 0.7703618721412251\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024935119393943014, AUC: 0.9056223823588669\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017492613190202733, AUC: 0.9444074110591453\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013961558756620987, AUC: 0.9603426028730286\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011719049380679554, AUC: 0.9679860078105911\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022735910632842322, AUC: 0.6760415907541808\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004439260648644488, AUC: 0.8581308166825994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003086705015312811, AUC: 0.9077116725896267\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024071712177979525, AUC: 0.930883557034033\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020013180702122594, AUC: 0.9451849335328135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017536128900065926, AUC: 0.9541545742541999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.033143807642208124, AUC: 0.31149340043639856\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005023892621816315, AUC: 0.7975531887530918\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025717914721486977, AUC: 0.913009649637978\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001791121056361228, AUC: 0.9451308123957767\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014801170391572434, AUC: 0.958631088895843\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001296428450639697, AUC: 0.9655323376473061\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021373009336167488, AUC: 0.2911492114389578\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005715213206984242, AUC: 0.760967300116173\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029166127337185246, AUC: 0.8898768824510338\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020340771161744806, AUC: 0.9335965799728215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016408656448064136, AUC: 0.9516821492414467\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001409284878468168, AUC: 0.960728416919232\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010461951881708812, AUC: 0.5722849408204121\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003409418013278495, AUC: 0.8671186764006121\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002043864741828871, AUC: 0.9277402442631595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015190032081327577, AUC: 0.9524184110462848\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001229168897336561, AUC: 0.9637538420648768\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010525988989488432, AUC: 0.9705522070706853\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028475319129833275, AUC: 0.4751428583676636\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038394745339024386, AUC: 0.8451931856665795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002043709991881566, AUC: 0.9312779447257076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014271239439646404, AUC: 0.9581568591307181\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010996327945657892, AUC: 0.970662056903285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009199229394920617, AUC: 0.976383357696776\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02530237853403664, AUC: 0.3903527197746846\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016282825489715512, AUC: 0.46199838815465144\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011771297849730182, AUC: 0.5325669923223004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009382501645611434, AUC: 0.5973033741581751\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007839971694393435, AUC: 0.6558420820240662\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006703334812298571, AUC: 0.7043169376653106\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024845688979818214, AUC: 0.44596781453490114\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016216132458199133, AUC: 0.4755340309422865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012155734233974671, AUC: 0.5339189490425381\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01014754885718936, AUC: 0.5859031049457074\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00879619580618343, AUC: 0.6354266245986462\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007762235143910284, AUC: 0.6782632366369018\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05102077033949194, AUC: 0.4918555726552152\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.035853824260071936, AUC: 0.49778532020422417\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022362716943334103, AUC: 0.5303967883124064\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013425151753869857, AUC: 0.610104362699498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00955481657577104, AUC: 0.67522763028726\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007960590516558345, AUC: 0.7120091052114905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03078317642211914, AUC: 0.19578508871579653\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02601883771750251, AUC: 0.23882639653967666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021732183223432143, AUC: 0.2899756937150892\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018071984158786435, AUC: 0.349022390075148\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01499492337244638, AUC: 0.4159616629442756\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012453146849606595, AUC: 0.4856868347386967\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013844287173348183, AUC: 0.40772989158625306\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010922173288791569, AUC: 0.4893349208866864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008945100796148645, AUC: 0.5635601848477964\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0074502261775867785, AUC: 0.6286480861479896\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006269480377497387, AUC: 0.6837718038521389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00541697649235064, AUC: 0.7285047990980524\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008802508221896786, AUC: 0.7645044647258792\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006182037278485347, AUC: 0.8134187196975219\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004977234402058287, AUC: 0.8381569019989455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00439567175957974, AUC: 0.8495330578335254\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0040600618220264124, AUC: 0.8609901274472398\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038324547603757237, AUC: 0.8702335889708623\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.039993207153573046, AUC: 0.28895221478696637\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03225855797714328, AUC: 0.2606527544979488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026134270812166897, AUC: 0.26354046546321264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021270750965884506, AUC: 0.3074980816468258\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01742941202831071, AUC: 0.36939551512605406\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014362627428264104, AUC: 0.43976960471207555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01965548138193956, AUC: 0.33460151839261293\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01565995374328108, AUC: 0.40520656055351456\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012897913006768711, AUC: 0.4764894565594818\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01087370965298165, AUC: 0.543381038525676\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00934282346295027, AUC: 0.6014514110077034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008085668950841047, AUC: 0.6521430898560914\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01855959309801058, AUC: 0.4171496486948768\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01416365354944707, AUC: 0.4849800448401658\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011793362674752625, AUC: 0.5473029454759016\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010136768684624145, AUC: 0.5996525530172802\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008896262502571564, AUC: 0.6449272954864044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007944520956240826, AUC: 0.6839941827815478\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00945114497072208, AUC: 0.7456354786023243\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006690539681886787, AUC: 0.7752279517989651\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005590809551578624, AUC: 0.7955255215991563\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.005001779668819831, AUC: 0.8133662061190108\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004603371363495695, AUC: 0.8281755711119589\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004293489653624857, AUC: 0.8413216488834971\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014020731236870492, AUC: 0.43190435669794625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01384871346609933, AUC: 0.4386700346803959\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013656736654277668, AUC: 0.44526263219489615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013476063252482602, AUC: 0.45194471713400175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013284212313823818, AUC: 0.45871575364487105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013124383507801632, AUC: 0.46412197296729585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04839635931927225, AUC: 0.5068401615274807\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.046927758131955224, AUC: 0.5083978857390269\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04539537923429817, AUC: 0.5120277528903902\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04376915917880293, AUC: 0.514126152619463\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04220761127353455, AUC: 0.5151887488050482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.040702651993334914, AUC: 0.5172850051227532\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00464082109755364, AUC: 0.8258724755972616\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004527880785134515, AUC: 0.8279076446909843\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0044371104388503555, AUC: 0.8314844624109949\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00433427243499282, AUC: 0.835571412036541\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004253862313849091, AUC: 0.8382646084201772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00418866347081913, AUC: 0.8398067928993067\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01897424496478916, AUC: 0.39885777608210116\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018380860117404852, AUC: 0.40083132209899985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017843327413681376, AUC: 0.40761897004796954\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017337378507815533, AUC: 0.4102907323179279\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016842074028947093, AUC: 0.41357283097486636\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016439205371074794, AUC: 0.4190503187252703\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007737966057676706, AUC: 0.8301668002726419\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007555357170894773, AUC: 0.8317818607382766\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007404705752497134, AUC: 0.8332822486957342\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007239785500441525, AUC: 0.8353158102309312\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007122833289468264, AUC: 0.8369780257466574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006969450917056382, AUC: 0.8382999747077459\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04337097349621001, AUC: 0.3025752015878392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04186376330768593, AUC: 0.3049115199787374\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04038631052210711, AUC: 0.3070715427846343\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.038873694204642414, AUC: 0.31132460679118457\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03735342282439364, AUC: 0.31571324156674796\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03593970825953513, AUC: 0.3203660517933923\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021948955567480367, AUC: 0.49320324255271725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02097338919313798, AUC: 0.4971937386667124\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02004893620808919, AUC: 0.4998242402678407\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019116403646844266, AUC: 0.5136663908810707\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018271816443212283, AUC: 0.5172035554907768\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01742565557823418, AUC: 0.5207080330771242\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03195755427421743, AUC: 0.3131984841794807\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03137098039899554, AUC: 0.31758015286809876\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030787140192699236, AUC: 0.3223347751347134\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03023787808467636, AUC: 0.32714083927415516\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02964584970572967, AUC: 0.33261457605466554\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02916779429275797, AUC: 0.33751548614713234\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012466282577988524, AUC: 0.6794137126885667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012176150860993759, AUC: 0.6804291538240602\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011873648033379028, AUC: 0.6832616719466034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01161756920271532, AUC: 0.6847695618438482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011303830591047772, AUC: 0.6877601565547662\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011112300011929024, AUC: 0.691029394743498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030413311707553903, AUC: 0.22623867742944964\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029429865179594998, AUC: 0.23105653033141427\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028539160023564877, AUC: 0.23630252965409626\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027687830954605006, AUC: 0.24157746503024352\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02690355525994153, AUC: 0.24701101284760774\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026060298856494343, AUC: 0.25340266554637697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 class ratio\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(NUM_CLASSES_REDUCED, shape=28*28)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "    \n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e0ac8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.02059953010353736, AUC: 0.32781279873795943\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005200702241982486, AUC: 0.9947797216137316\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005176582568427298, AUC: 0.9949672701084137\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005116876422988702, AUC: 0.9951516034860443\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005180294746937959, AUC: 0.9951408864292053\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005392106886235823, AUC: 0.9951290976666824\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01974508234185955, AUC: 0.4221475481517364\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000617082012859684, AUC: 0.9935440449601969\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006120305192149697, AUC: 0.9944951837546565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006366217697876087, AUC: 0.9952791364624282\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006411895001650351, AUC: 0.9963738838185302\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006284191124681114, AUC: 0.996050228701993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012246092900973176, AUC: 0.5678904116635873\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006060551072993387, AUC: 0.9936142416824921\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005817237015096297, AUC: 0.9949774513124109\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005760042008405887, AUC: 0.9952223360611816\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005473579216447676, AUC: 0.9958551782675235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005496152007555122, AUC: 0.9959307335182382\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020917637748007447, AUC: 0.41253059719727525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046222650795002654, AUC: 0.9944105190056286\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004821941102266805, AUC: 0.9952255511782332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004664629574394621, AUC: 0.9959725300399103\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00048456089590400396, AUC: 0.9960400474979959\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004687713262457285, AUC: 0.996112387631659\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007805033006529877, AUC: 0.7128037749761009\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000568464722322381, AUC: 0.9935215391408351\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006042515577490039, AUC: 0.9945852070321041\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005913685012307967, AUC: 0.9948617070985497\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005627724478950659, AUC: 0.9951473166633087\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005678194064037647, AUC: 0.9951880414792966\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015200342450823103, AUC: 0.6629357019457889\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005514146470875474, AUC: 0.995143029840573\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006412698677114325, AUC: 0.9952416267634917\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006309525875324542, AUC: 0.9954238167297544\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006594087458051756, AUC: 0.9955492062947706\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006325702119317854, AUC: 0.9956729883012607\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01261024939100688, AUC: 0.5841213942462266\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004073684537632865, AUC: 0.9964617636846099\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004374620120964682, AUC: 0.9962629622802468\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004943823653965509, AUC: 0.996154720006173\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005526162575984347, AUC: 0.9964687297715552\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005282025252069746, AUC: 0.9965298169955373\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007669163786846659, AUC: 0.6607108409460161\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047736892603939365, AUC: 0.9918523575381635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005487542036404028, AUC: 0.9939909462303824\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005776005224411532, AUC: 0.9941784947250646\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006030472631780257, AUC: 0.9946189657611467\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006322643524864939, AUC: 0.9954993719804692\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03303867640209, AUC: 0.4312377557625615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000605412274907588, AUC: 0.9943017408787128\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005586184329868103, AUC: 0.9959210881670831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005735881456924028, AUC: 0.9962586754575112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000565989047103787, AUC: 0.9963101173303383\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005607829269168293, AUC: 0.9964376503067223\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020576018971192416, AUC: 0.30968328953629437\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003610113886325749, AUC: 0.9959462732506548\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003693705852727712, AUC: 0.9966830709083349\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00039779109490830955, AUC: 0.9965962627479391\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003978151357568816, AUC: 0.9966632443531828\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00041475887439265755, AUC: 0.9966487763264501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.049437396521400466, AUC: 0.3479731902106116\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006995496974475142, AUC: 0.9859928067114496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005708400846514889, AUC: 0.9902914182095656\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005212198503269172, AUC: 0.9928983422856481\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005073204451466199, AUC: 0.9936008453614434\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005033020215498488, AUC: 0.9939786216150176\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021541533262833305, AUC: 0.3301051771958178\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009412846950270375, AUC: 0.9787646877263978\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000678781026638813, AUC: 0.986980383499162\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000593210938801183, AUC: 0.9896607194145914\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005556317528335698, AUC: 0.9914649359334342\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005463160589861821, AUC: 0.992416074727894\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018285044478580325, AUC: 0.5002807868891813\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007786210279287018, AUC: 0.9827466101949218\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005811507780853019, AUC: 0.988413253998534\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005321048709176342, AUC: 0.9921851221530139\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005100715913140749, AUC: 0.9933361340575205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005201459486292015, AUC: 0.9936887252275232\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01893606738767762, AUC: 0.42892287148534114\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008950730164845785, AUC: 0.9811631650469621\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007115444534807225, AUC: 0.9884609449014674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006359510170006604, AUC: 0.9916974960668402\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006037186329902823, AUC: 0.993911104156932\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005934974390774286, AUC: 0.9944030170658413\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014191178308017012, AUC: 0.4908406673725635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008048738196769857, AUC: 0.9811192251139224\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006260865408441294, AUC: 0.9913770560673546\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005655441101540197, AUC: 0.9934325875690715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000547960030366175, AUC: 0.9940809695078301\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005412412417848165, AUC: 0.9948113369314067\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0317227480080804, AUC: 0.4776511855208275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011751854765242424, AUC: 0.9786173281948618\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000885484566599686, AUC: 0.9864097002224862\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007618220202917884, AUC: 0.9888896271750266\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007121175465870101, AUC: 0.9916272993445449\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006542243911859658, AUC: 0.9926673897107681\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009516305804993054, AUC: 0.7067840041496443\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001021255123936118, AUC: 0.9788123786293312\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007513182750646619, AUC: 0.987347978548739\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006885052837940477, AUC: 0.9887867434293725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006267631090946079, AUC: 0.9898777398155809\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000610499864532834, AUC: 0.9917162509163084\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017798137467346823, AUC: 0.5915370617259605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005518806554515909, AUC: 0.9887186901184449\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00041028903508038254, AUC: 0.9937117668997268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00036859405892235894, AUC: 0.9951826829508772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003501267382570429, AUC: 0.9954318545223837\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00035615950267507424, AUC: 0.9960400474979959\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034323661717322053, AUC: 0.8833121492843149\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000829787512250075, AUC: 0.9822579124030643\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005738624442931782, AUC: 0.9896821535282695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005079480061619919, AUC: 0.9925720079049012\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047988205469419746, AUC: 0.9931721630878841\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046463844445428, AUC: 0.9935317203448321\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04193415552933023, AUC: 0.5062726933678564\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010434732488963914, AUC: 0.9800952103329577\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008436107610817035, AUC: 0.9875430289832086\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000793235364908017, AUC: 0.9898386225581187\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0007510473701030818, AUC: 0.9916653448963233\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007364097404183808, AUC: 0.992445546634201\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012717402499655018, AUC: 0.7021885301770887\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032795123431993566, AUC: 0.8978782370870182\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023585265467625967, AUC: 0.934192448333069\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019331971804300945, AUC: 0.952396441079765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016654847694973522, AUC: 0.9617974433389206\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001480100441176452, AUC: 0.9687217337625873\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01639600866329596, AUC: 0.5948764966369876\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036197650259819584, AUC: 0.869995670309037\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023232798398651693, AUC: 0.927713987473904\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018095176042236896, AUC: 0.9512207799445285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015453259396997299, AUC: 0.9613998405301943\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013573305453819765, AUC: 0.9686649333613407\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016275691689911838, AUC: 0.6045314931432271\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004559711393115436, AUC: 0.8376398040064646\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002579905354951973, AUC: 0.9152296879621731\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018650585203190521, AUC: 0.9441309109926995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015167086144164976, AUC: 0.9557069399373268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013033367585444796, AUC: 0.9638486880179017\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04230171108838194, AUC: 0.3614188097207992\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009186749132523625, AUC: 0.6475620839102683\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0048215534869681725, AUC: 0.8263504563322801\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003121311373345353, AUC: 0.8969270982925586\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002242882676252914, AUC: 0.9289362678063898\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001758875869075704, AUC: 0.9492627736600464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024734719199423463, AUC: 0.2973050888872694\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007260780156769367, AUC: 0.7533758729042797\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004060013940862494, AUC: 0.8725248957230369\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002960486441665555, AUC: 0.9117884410111758\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023780714404262123, AUC: 0.9354099059899775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020186859134808336, AUC: 0.9465749358048297\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020151301200345435, AUC: 0.3655416614867558\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0048983018097176565, AUC: 0.7944500649453644\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026395921874984205, AUC: 0.9058945956025772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018763858092250784, AUC: 0.9379916449824883\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015158520598836074, AUC: 0.954871545356728\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013002523724336803, AUC: 0.9653597930321983\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010327672859649974, AUC: 0.8110475708718967\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00249152598173722, AUC: 0.9195213333733437\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017055201481094519, AUC: 0.9525143287049938\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013233691639041307, AUC: 0.9657231012590399\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011144086687708, AUC: 0.9727524188397285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009777844569204264, AUC: 0.9762011677305131\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03184655487660789, AUC: 0.4165050177260121\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005582328908932135, AUC: 0.7703774118736417\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029242823582998714, AUC: 0.8943469668585734\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00212605695546784, AUC: 0.9344442991687851\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017325078478511076, AUC: 0.9527795758617585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014998055146100851, AUC: 0.9619903503620222\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015399368653386276, AUC: 0.4762397491351335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005026916292636785, AUC: 0.8251989085749316\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002720246774069271, AUC: 0.9092635024199115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018927443224944437, AUC: 0.9424547633030828\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014679233233133953, AUC: 0.959210881670832\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012127099940495462, AUC: 0.9679752907537521\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0059272342093489435, AUC: 0.7826709477736387\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035695033784238447, AUC: 0.8844449421921953\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026065037117241332, AUC: 0.9238279826640888\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002075678381613816, AUC: 0.9425329978180073\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017694519913714864, AUC: 0.9549888971291147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015223829025560777, AUC: 0.9635754030685076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02533743346946827, AUC: 0.38688950285716733\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01634863839633223, AUC: 0.44900824356012053\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012184010776180164, AUC: 0.509900417107852\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009828708680273336, AUC: 0.5697391039683117\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00822249011716981, AUC: 0.6245230909706653\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00701993247243435, AUC: 0.6729995541704354\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027668761417238855, AUC: 0.2700516133457365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020644878008350822, AUC: 0.33511111444530656\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016396949009865706, AUC: 0.4037517200876226\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01365613888015905, AUC: 0.4715381762998718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01156817114377861, AUC: 0.5345968028876038\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009992070326400346, AUC: 0.592375671423611\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006015322470023272, AUC: 0.8080194664620424\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005547392442359687, AUC: 0.8228079331941545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005233749346209856, AUC: 0.8364727165166992\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004982540079278729, AUC: 0.8464583342264214\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004736871205995295, AUC: 0.8556031988271253\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004510807941665807, AUC: 0.8629926095176037\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024058848434353467, AUC: 0.5771997830867697\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013871452576378611, AUC: 0.6545206689158196\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008841436595403382, AUC: 0.7159052912253026\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006778048679201746, AUC: 0.7552545729681532\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0058733740701932095, AUC: 0.7785502394190499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00536137955035729, AUC: 0.8009815752358825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01628287534536042, AUC: 0.5943610062030324\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0117496797510309, AUC: 0.6192492273002019\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009785826902211824, AUC: 0.6493882703956309\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008717300482171416, AUC: 0.6785231252652473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007941387948535737, AUC: 0.7070267454870474\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007280707606124088, AUC: 0.7346467443724736\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021741300389386604, AUC: 0.44206091146425003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01455959079181679, AUC: 0.479004749799591\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011198460438730308, AUC: 0.533519202822444\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009463431672279879, AUC: 0.5865943551118218\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008167188113273794, AUC: 0.6371376027229898\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007175172584644262, AUC: 0.6819879497412902\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017544172812199246, AUC: 0.4463037942668033\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01423069951943976, AUC: 0.4882632152027882\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011779168624562013, AUC: 0.5453910225358272\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00990159319054266, AUC: 0.5966174825204802\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008399602048885749, AUC: 0.6476328164854055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0072153115618056145, AUC: 0.6925983718647251\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01010748120815364, AUC: 0.6472309268539437\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008826762499522965, AUC: 0.6914162804953852\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007909852772272399, AUC: 0.7292206984948966\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00714042838315786, AUC: 0.7603098086791014\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006488267679392181, AUC: 0.7873264372644926\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00593362091490941, AUC: 0.808960424052505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01552720050140444, AUC: 0.508838892627951\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01143161998772473, AUC: 0.5780898346572472\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009323703082698719, AUC: 0.6371451046627771\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007946039332119327, AUC: 0.685994521440544\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006923506472174919, AUC: 0.725782130808109\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006098156636792927, AUC: 0.760577199247234\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013018863294929205, AUC: 0.6746644489503715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009130722740915745, AUC: 0.7181274729608657\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0075194277871963155, AUC: 0.7495016568569874\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.006660650729145816, AUC: 0.7722293192954177\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0060753738411218235, AUC: 0.7909557685630142\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005611769407679082, AUC: 0.8074809343558835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018042430127382773, AUC: 0.35864523541087057\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017737667012658918, AUC: 0.36503742396248173\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017420577213137292, AUC: 0.3713347665610679\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017151203708372253, AUC: 0.3773207786584817\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01686673529646658, AUC: 0.3826734126967116\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01656608700011828, AUC: 0.38866853429243853\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010033122747828008, AUC: 0.6107168424978459\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00991231758402001, AUC: 0.6142941960706982\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00978261787698876, AUC: 0.6188076845584358\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009670870644705636, AUC: 0.6247299301676577\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00954044284781067, AUC: 0.628655588087777\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009452651499714663, AUC: 0.6327645076798429\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004205402627001144, AUC: 0.8212491372769244\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0041637183716578515, AUC: 0.823404337407244\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0041195161594367175, AUC: 0.8257690559987654\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004063371545779779, AUC: 0.8278776369318352\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004017293823431738, AUC: 0.829850647095892\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003982972531091599, AUC: 0.8318466989321524\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019648731618687725, AUC: 0.6270619617358203\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018621899820015792, AUC: 0.6337038577117798\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017523318344021436, AUC: 0.6401314125509596\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01656031312409395, AUC: 0.6469538909346559\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015676029967471927, AUC: 0.6534355669108727\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014792663463647816, AUC: 0.6596916488406288\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0365165963182785, AUC: 0.20552796508811563\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03527913379866637, AUC: 0.20959615986419347\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03393675328288266, AUC: 0.2139381754425073\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03275852716734197, AUC: 0.21846881122118716\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03160234317029238, AUC: 0.22284619308706963\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030463399353975094, AUC: 0.22758956244400336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027765503087645978, AUC: 0.5330530108499484\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02643214693721037, AUC: 0.537481834588658\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02515975288722826, AUC: 0.5409413005362815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024017515636625745, AUC: 0.544347717052552\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022839124651922695, AUC: 0.5482846278823523\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021747514081050644, AUC: 0.5539769926223781\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04729763173168491, AUC: 0.6385870846604622\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.045589565490343556, AUC: 0.646595405383392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.043853441874186196, AUC: 0.6576195059008115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04208702774521727, AUC: 0.6700743335062351\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0404557648652829, AUC: 0.6832080866624085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.038748427207425516, AUC: 0.6962850394173349\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02766077661613006, AUC: 0.5418897600665316\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026439566049516573, AUC: 0.5475081771143682\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02519796501775706, AUC: 0.5502571021935672\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024009894140018438, AUC: 0.5572628422492102\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022800698290206877, AUC: 0.5656441165501366\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02177845419810672, AUC: 0.5686684699900975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009296379227569138, AUC: 0.6616850214126796\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00906989243706808, AUC: 0.6666132600000858\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008837668298440937, AUC: 0.6710093967154365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008625524878255082, AUC: 0.6755528929623231\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008449644282244253, AUC: 0.6799490296776738\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008258050519733943, AUC: 0.6843526683328117\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01907605512788824, AUC: 0.3852138910203924\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018754232497442336, AUC: 0.38922689295374946\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01839506157189916, AUC: 0.3932613290007845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018077005263934718, AUC: 0.3977480248464246\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017768536048399487, AUC: 0.4023226005581443\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017414361546992267, AUC: 0.4091493657645763\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 class oversampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(NUM_CLASSES_REDUCED, shape=28*28)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "   \n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "    \n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d20a41e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.025259452330152934, AUC: 0.47413920599469295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009826663303079072, AUC: 0.9834807285883921\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008517812868082745, AUC: 0.9909269396801174\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006412443228636716, AUC: 0.9927397298444312\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000599567070757627, AUC: 0.9933452435558338\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005468863446273171, AUC: 0.9936769364650002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015795771873268773, AUC: 0.4420485868488852\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099562854747101, AUC: 0.9834684039730273\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007424404532272623, AUC: 0.990677768108611\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005826917370900851, AUC: 0.9929026291083838\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005142473255131802, AUC: 0.9938403715817947\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004954260637054285, AUC: 0.9947995481688836\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017985747714467176, AUC: 0.474496619840273\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011684125748233519, AUC: 0.980765026385394\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008966049052173306, AUC: 0.9882444603533199\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007349248998653815, AUC: 0.9900599297818437\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007689159294092877, AUC: 0.9915758574717177\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006269615444337359, AUC: 0.9916932092441045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013101774219647205, AUC: 0.6073784792925028\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008287856791083611, AUC: 0.9841741221658742\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005542039716959494, AUC: 0.9921122461665088\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047620453758022554, AUC: 0.9929910448273054\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004437259296205967, AUC: 0.9937712465651833\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00043863704424220335, AUC: 0.9942406536547308\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01990559086296129, AUC: 0.4927965302456778\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001485775778259056, AUC: 0.9764980302049531\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010664010455149302, AUC: 0.9873608390169457\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008447182598074524, AUC: 0.9897968260364466\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007650676968181603, AUC: 0.9921052800795634\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000801586185429654, AUC: 0.9936437135887994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028711092891653626, AUC: 0.20350940743249327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009328041140830788, AUC: 0.9840251550758123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006410830941506301, AUC: 0.9900674317216308\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005532222381536511, AUC: 0.992635774393093\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005592739915255434, AUC: 0.9935869131875528\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005538520667370308, AUC: 0.994027384223635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007732406906459642, AUC: 0.7111420953132166\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009744769544581696, AUC: 0.9848787686530375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006566880035597839, AUC: 0.9900727902500505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005160567830808415, AUC: 0.9916444466354872\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004921243458554364, AUC: 0.9932219974021854\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005134509155222101, AUC: 0.9948220539882455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009792498179844447, AUC: 0.6666861359865908\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013591752042434724, AUC: 0.9764160447201348\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008138457378738909, AUC: 0.9851986727996811\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000806694908171707, AUC: 0.9898595208189547\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007209052210268767, AUC: 0.9916916016855787\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006840733761125964, AUC: 0.9927456242256927\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021309424137723618, AUC: 0.3682096727868206\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014855835748755414, AUC: 0.977771752410266\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009846074919276108, AUC: 0.9884186125269535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000844753677060145, AUC: 0.9909328340613788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000771215335922952, AUC: 0.9920999215511439\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007228074483496309, AUC: 0.9928619042923955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030085107554560123, AUC: 0.12351568762780091\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001005440518475961, AUC: 0.9832765686556095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007610864952731083, AUC: 0.9894072610203496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006230243059418956, AUC: 0.9915378119199394\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006238604369370834, AUC: 0.9924198256977876\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006563727653298072, AUC: 0.9934620594753786\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04829688891614199, AUC: 0.5415419915721065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0050745306548124515, AUC: 0.861063539286587\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030682147413060284, AUC: 0.9233730436012741\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022957765537759533, AUC: 0.9456543406223608\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018605631207333836, AUC: 0.9599846531746065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015996128752611685, AUC: 0.9669459174443676\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03497630370082816, AUC: 0.4612278317679286\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036515312165207, AUC: 0.8542030153511122\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001678060794222182, AUC: 0.9494294238938925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001148819614888225, AUC: 0.9694413841293249\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009214099704849053, AUC: 0.9781114831120619\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008152827589655022, AUC: 0.9824443891920627\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022637907269084923, AUC: 0.23486912330188234\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0053579634514407835, AUC: 0.7767406643717876\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024183097586621904, AUC: 0.9166486262876543\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015476460165612198, AUC: 0.9549106626141903\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011721057798057856, AUC: 0.969924723392763\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009290600045127158, AUC: 0.9770424566923733\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03871967481530231, AUC: 0.5279672315270091\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004855744833778397, AUC: 0.8261039640249837\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030198566168238164, AUC: 0.9053646371418895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023148172143576803, AUC: 0.9340734890021564\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019551743878587677, AUC: 0.9487644305170336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016744894517381245, AUC: 0.9582018707694417\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015652323361509336, AUC: 0.5297575158719612\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035859937500015795, AUC: 0.8602447561440887\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002150102555134775, AUC: 0.9302689338243174\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001585082361169977, AUC: 0.954376417330767\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001325318840473088, AUC: 0.9653801554401924\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001151876790182931, AUC: 0.9707836954984075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015307726080126398, AUC: 0.6970298748676445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002630889292335905, AUC: 0.940459247319664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020344766400615623, AUC: 0.9581043455522071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017110021094604556, AUC: 0.9661689308235414\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014493091506247195, AUC: 0.970892473625323\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013437612333159516, AUC: 0.9748952943546831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007403325343477553, AUC: 0.794117300330514\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026337531536015416, AUC: 0.9275060765712277\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017764529456263003, AUC: 0.952890497400042\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014343560976024493, AUC: 0.963109211096012\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012304376864778824, AUC: 0.9702692767701363\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011163913561937479, AUC: 0.9739848803762116\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.034525970000904786, AUC: 0.5046399497584375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006208922305215713, AUC: 0.8046248387082945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035058830095374065, AUC: 0.8957878751505746\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002431924175278247, AUC: 0.9360936542163045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019098643066967002, AUC: 0.9554213303725678\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001589568875591207, AUC: 0.9650581078821809\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0309543698470785, AUC: 0.49960561230832545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004397869356917545, AUC: 0.8441327328923622\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025987869464092375, AUC: 0.9274562422569265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001891297459849166, AUC: 0.9534718977335568\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014979482067297705, AUC: 0.9659181516935094\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012266228658071956, AUC: 0.9737737543564836\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016187699438375468, AUC: 0.6766149532950663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004343757965056299, AUC: 0.8662950705825365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028181430222331614, AUC: 0.9266663951678935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020248473431012646, AUC: 0.9519618644249441\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001582998358191417, AUC: 0.9655462698211967\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0014023429735353522, AUC: 0.9720852820515018\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028508821135969142, AUC: 0.2438108996754875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024683232633223445, AUC: 0.29533207872321277\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0211319479142657, AUC: 0.3532984957539021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01803111437685001, AUC: 0.4147945754545104\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015439861803074554, AUC: 0.47698565629112666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013303249765873942, AUC: 0.5370472579338372\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026503448407348886, AUC: 0.5624097087961316\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01763462576066485, AUC: 0.5753328717854188\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01429747103657535, AUC: 0.6005844011094297\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012676069701927296, AUC: 0.6292895019998028\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011549829696276173, AUC: 0.6582068006155878\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010552035602229969, AUC: 0.6853113090670588\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02109317996733924, AUC: 0.553629224127953\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013888498764354003, AUC: 0.5771242278360547\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010829893451793348, AUC: 0.6119096509240246\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009212090115122668, AUC: 0.6500698752105902\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008102929370003457, AUC: 0.6891935629069803\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007137821574635634, AUC: 0.7250394387691674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01831988320834395, AUC: 0.5060267369134019\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01398420136414206, AUC: 0.5453218975192156\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012103377415279918, AUC: 0.589257543736309\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01086805572667724, AUC: 0.6273952622035126\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009786624592530309, AUC: 0.6630375139857592\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008935120288382899, AUC: 0.6955734268432265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.033907815289546735, AUC: 0.648997097821008\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023227082523006336, AUC: 0.7065739498356005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015249779505759292, AUC: 0.7511467250817712\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010553885690914177, AUC: 0.7838316050293005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008183475606930182, AUC: 0.8079374809772242\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006771350499265682, AUC: 0.82643833619836\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03781847035662728, AUC: 0.29181313311013274\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029127805129341457, AUC: 0.3347788856832981\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022386902360935882, AUC: 0.38884858084733337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01764721505143381, AUC: 0.4495274849639692\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01439108759720133, AUC: 0.5110910821226632\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012017628667764289, AUC: 0.5670523378187788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012935475039432754, AUC: 0.6161980812181435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010881806505886417, AUC: 0.6556507825594904\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009408064263701192, AUC: 0.6919682089225929\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008377486380977907, AUC: 0.7237855431190066\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00750540576366164, AUC: 0.7503097229426466\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006785090912449681, AUC: 0.7738786743429372\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02564582568024503, AUC: 0.3779359377210393\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018740898827341528, AUC: 0.3740644009379568\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015678882105256708, AUC: 0.42597728841314686\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013728976002884702, AUC: 0.4827632216330223\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012125058697370763, AUC: 0.5383488444869315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010691856498797241, AUC: 0.591923411625006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017725536788719288, AUC: 0.41052972268543725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014306321647596655, AUC: 0.465678625473158\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012287120641388509, AUC: 0.5260172630351563\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010745276575503141, AUC: 0.5833642341805523\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00936093290893681, AUC: 0.6355064666720966\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00823620683658197, AUC: 0.6784422114861128\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015090189118316208, AUC: 0.40543644142271074\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013009751065177207, AUC: 0.4701326342954392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011625408385851368, AUC: 0.5277946869119015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010417668715767239, AUC: 0.5781246650919738\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009336050993167096, AUC: 0.6220967493023196\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008414739900987834, AUC: 0.6620944129839287\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008780707730516391, AUC: 0.7387272637639162\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008231138097079892, AUC: 0.7489631247508284\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007744180242960991, AUC: 0.7538527819336143\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0073633186565422865, AUC: 0.7609340772399721\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006984353312300846, AUC: 0.7685415800371239\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006675069129738502, AUC: 0.7742677035061923\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015601363488112424, AUC: 0.4007412988215524\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015371393713151446, AUC: 0.4079576290440814\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015182356409898208, AUC: 0.41520182361439173\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014989140117637365, AUC: 0.4223008020645338\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014796875525211943, AUC: 0.4291913337591577\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01460929390806589, AUC: 0.4355524428459359\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013602202476675219, AUC: 0.6168432480398502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012699128678126365, AUC: 0.6264821689608313\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011856846681045943, AUC: 0.6351983512879759\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011144829092558867, AUC: 0.641401383786379\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010493276035316737, AUC: 0.6483358554140428\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009931904929024833, AUC: 0.6531504931989557\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007426709615419123, AUC: 0.8378986209291259\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007230492358869154, AUC: 0.8396974789195493\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00703330884068649, AUC: 0.841468472562191\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006853320830603811, AUC: 0.8432126735627355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006688776963986225, AUC: 0.8449600896803315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006532589594523112, AUC: 0.8466142674034286\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020180776252509643, AUC: 0.6635363929816138\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01928017697225693, AUC: 0.6680889987268137\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01850491675777712, AUC: 0.672709121930099\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01776377063853894, AUC: 0.6772954864043417\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017014518287611302, AUC: 0.6819016774337364\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01632301950553436, AUC: 0.6864971514062922\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017685800358869026, AUC: 0.5525591260025806\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017244762515429383, AUC: 0.5570640408448471\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01683574483014535, AUC: 0.5615448423092257\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016416267825456386, AUC: 0.5657555739412619\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01604255978365122, AUC: 0.5698564557406987\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015635342825026738, AUC: 0.5741234519211396\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.038534841675689255, AUC: 0.2642402892747982\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03746674421164313, AUC: 0.26708620371839004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.036448587295184715, AUC: 0.2706405156190387\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0354960798970414, AUC: 0.27453991674990247\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.034609130203847316, AUC: 0.2783444719277413\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.033658252739758227, AUC: 0.2823778362690924\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016514028328052467, AUC: 0.5517483806527117\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01572184039445644, AUC: 0.5542636738928208\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0149802401445914, AUC: 0.5599544310743207\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014391729303521892, AUC: 0.5641758797631959\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013815609317882215, AUC: 0.5650680747450412\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013320472176277366, AUC: 0.5692788063770775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008674091433886415, AUC: 0.6835199530164229\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008602780584963212, AUC: 0.687269315351541\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00851636396925395, AUC: 0.6913659103282419\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008483525882349744, AUC: 0.6954796525958855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008406486817275021, AUC: 0.6988008685102862\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00837176507550984, AUC: 0.7025143287049938\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024710182324206114, AUC: 0.3794186425347126\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023854589363556226, AUC: 0.3766884723049817\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023179766060649486, AUC: 0.37632516407814026\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022465878638668335, AUC: 0.3770158783914126\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021829161831557628, AUC: 0.37807954628268164\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021287006867845112, AUC: 0.3792278789229786\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017266795748756045, AUC: 0.3482277203105374\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.017274150690430194, AUC: 0.3486665837880938\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01725076098866591, AUC: 0.3494424987032361\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01723363187248909, AUC: 0.35021787776553653\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017191434745709593, AUC: 0.35110739348317205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017156970180092886, AUC: 0.3516603936160636\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02348236218249082, AUC: 0.29716523129552075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02347729171531788, AUC: 0.2976919746391567\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023424259130505547, AUC: 0.2978248661439601\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02338111770819433, AUC: 0.2983928701564261\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023352496619056716, AUC: 0.2988397714266117\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023319610650988594, AUC: 0.29930274828205583\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03364902401562803, AUC: 0.2972916925662208\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.033554815604326396, AUC: 0.294843916784197\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03342587903419637, AUC: 0.2926796071555645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03324367096705466, AUC: 0.2904917200018863\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03316045350416353, AUC: 0.29058549424922725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.032995050244696636, AUC: 0.28929462475297185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04275538166117224, AUC: 0.6219081291019535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04264639425968778, AUC: 0.6221996330479739\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04246624656345533, AUC: 0.624083691640267\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04231444254178191, AUC: 0.6257357259519961\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04220120013377188, AUC: 0.6263760700981253\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.042049585662273146, AUC: 0.6266943666862432\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.034039819215889056, AUC: 0.3034770419208395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03391259550801469, AUC: 0.3040493327560412\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03380225163809261, AUC: 0.3044078183073052\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03366973138497236, AUC: 0.30492705971115386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0335590814704974, AUC: 0.30532037569714454\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.033526372218477554, AUC: 0.30575870332185895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03133498462337391, AUC: 0.6822982085367787\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.031212900489507006, AUC: 0.6835542475983076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.031085180199664574, AUC: 0.683876295156319\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03094319724641725, AUC: 0.6852604030470735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030901740056387385, AUC: 0.6857544593673507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030774278423554163, AUC: 0.6858937811062575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01965352170956061, AUC: 0.449014137941382\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01953600257573414, AUC: 0.44908754978072907\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01946151231880267, AUC: 0.45036770222014544\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01936693961575905, AUC: 0.4508279998113798\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019314645486835615, AUC: 0.4513252712487086\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019202658848732894, AUC: 0.4509003399450429\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002047474031369385, AUC: 0.9438056483176365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020391887389354826, AUC: 0.9438179729330014\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002043644957414078, AUC: 0.9438260107256305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020507714763191177, AUC: 0.943841550458047\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002039890481818537, AUC: 0.943852267514886\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002033891823474418, AUC: 0.9438651279830929\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013975702704356571, AUC: 0.5464155731696339\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013910998962433935, AUC: 0.5474010065459782\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01381743225745286, AUC: 0.5490401803895006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01371115878008414, AUC: 0.5503465896181727\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013632021582151299, AUC: 0.5508947670754867\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013555922123215955, AUC: 0.5517628486794443\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017059737851160654, AUC: 0.6087829495912515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016993810918266974, AUC: 0.6098503684524141\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01694564789718723, AUC: 0.6104355197558226\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016819403531882087, AUC: 0.611000308651237\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01677077453328956, AUC: 0.6119894929974751\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016692249424462485, AUC: 0.612116490121017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 class undersampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(NUM_CLASSES_REDUCED, shape=28*28)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "    \n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1558d4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.017008969996039666, AUC: 0.4984679967248675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005603974095042448, AUC: 0.9940472107787871\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005973111028256624, AUC: 0.995270026964115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005974054922721894, AUC: 0.9954104204087056\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005906756743634463, AUC: 0.9955352741208798\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006257031521688584, AUC: 0.9964397937180899\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03125310487135103, AUC: 0.4181972410008873\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006459708601298046, AUC: 0.9945803843565264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007238506530382619, AUC: 0.9947990123160417\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008245322768485817, AUC: 0.995249128703279\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000738330108285197, AUC: 0.9955272363282506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007196388999867883, AUC: 0.995691207297887\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022401894595065225, AUC: 0.32051448303061214\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006671747497396687, AUC: 0.9946870190720744\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006203039956142196, AUC: 0.9953670163285078\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005942089926628839, AUC: 0.9955095531844662\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006852501359291945, AUC: 0.995354691713143\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000627228145643791, AUC: 0.9951205240212112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02969129238563048, AUC: 0.4617513599945128\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00048264325837417665, AUC: 0.9952260870310752\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004770141827640573, AUC: 0.9955824291709714\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005499457257875004, AUC: 0.9959194806085574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005612332319867784, AUC: 0.9959114428159281\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047227906884613987, AUC: 0.9961702597385896\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019041645847739146, AUC: 0.540618717125428\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00044083002931583, AUC: 0.9951880414792968\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005507778306925519, AUC: 0.9959569903074937\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005961093843353461, AUC: 0.9965437491694281\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006137743869923657, AUC: 0.996555002079109\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006340771111395542, AUC: 0.9965592889018446\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008679495588346051, AUC: 0.6659718441482726\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047903851939531093, AUC: 0.9964660505073455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00040105198110852925, AUC: 0.9970533452221217\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004984101215011091, AUC: 0.9968518645535488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005108197807772066, AUC: 0.9969059856905857\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004932245636825482, AUC: 0.9969493897707836\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023510699933606893, AUC: 0.2995519198535621\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000459339986429945, AUC: 0.9946414715805088\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005903905469685114, AUC: 0.9951912565963486\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005857869211437786, AUC: 0.9957351472309268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006460395658978765, AUC: 0.9956853129166257\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006257574812472483, AUC: 0.9957496152576595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018891543828675958, AUC: 0.39205994692913454\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005221897215576646, AUC: 0.994922794322532\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005133651181531002, AUC: 0.9961809767954285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006581976331045416, AUC: 0.996617696861617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005772712777366797, AUC: 0.9968347172626064\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005728965220244035, AUC: 0.9969386727139447\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01796896956228568, AUC: 0.5029905947109181\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005525995670638469, AUC: 0.9937583860969765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046807869991160326, AUC: 0.9933447077029918\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005316570648001835, AUC: 0.9950717614125939\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005202886357317306, AUC: 0.995227694589601\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000525656138888057, AUC: 0.9956954941206225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008751507378019408, AUC: 0.6586847813506064\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004702288171519404, AUC: 0.9961252480998659\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004187753235084423, AUC: 0.9969777899714068\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00042190729769613924, AUC: 0.9972253539843874\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000395142891700717, AUC: 0.9972982299708925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047323658414508986, AUC: 0.9970956775966358\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007892978117332696, AUC: 0.7463637026145332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008269447223987144, AUC: 0.9842748625001607\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006167486957881762, AUC: 0.9896419645651233\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005577032610496378, AUC: 0.9922928285742457\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005400142252568627, AUC: 0.9929042366669096\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005209215185903862, AUC: 0.9933039828870036\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02806110954679564, AUC: 0.3493562263956823\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000794578474747715, AUC: 0.985303699956703\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000572952393666064, AUC: 0.9911262769373225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00048428689039025003, AUC: 0.9925119923866029\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00045377258249938365, AUC: 0.9936281738563829\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00044680197046410223, AUC: 0.994753464824476\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014833721314898188, AUC: 0.6116036789512717\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009544241564120811, AUC: 0.9816406099291388\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007496831579978421, AUC: 0.9885890137306933\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006580656359654776, AUC: 0.9909103282420169\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000621991991749955, AUC: 0.9930933927201175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000602215435934363, AUC: 0.9935660149267167\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010429812761073774, AUC: 0.6872076922747168\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008271394864372585, AUC: 0.9824529628375338\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007318764246275213, AUC: 0.9888489023590387\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006445545093860191, AUC: 0.9914579698464888\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006045127137107139, AUC: 0.9930628491081265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005580135630771487, AUC: 0.9936313889734346\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03039028925925308, AUC: 0.5705825363415399\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008411152639250824, AUC: 0.9854360556086645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000675338025418868, AUC: 0.9917810891101843\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006347071248305263, AUC: 0.9927638432223189\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006075467042794632, AUC: 0.9934068666326579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005814572391302689, AUC: 0.9943113862298679\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01326370880964133, AUC: 0.5576925962284534\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008815089491336736, AUC: 0.9823029240417879\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936406003268856, AUC: 0.9888998083790237\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006298934327396053, AUC: 0.9909113999477007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005951125182473635, AUC: 0.9917033904481014\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005653822181387718, AUC: 0.9927456242256927\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01905163672153007, AUC: 0.6366923090113301\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009559691569326333, AUC: 0.9841435785538831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007844825215468002, AUC: 0.9894592387460186\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007550228333127671, AUC: 0.9914167091776587\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007326969757336761, AUC: 0.9921760126547007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006917849892661685, AUC: 0.9930381998773967\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030554672699290526, AUC: 0.6424393307412346\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011999923311652111, AUC: 0.9761106086002238\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000890545778393005, AUC: 0.9863930887843857\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007885112648918515, AUC: 0.9883366270421351\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007512930695808205, AUC: 0.9905523785435949\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007195305657682952, AUC: 0.9915758574717176\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01054781070653943, AUC: 0.683023217431936\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012344811767278003, AUC: 0.9735358356946583\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009005731183796442, AUC: 0.9830884843080854\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007621164339176123, AUC: 0.9874390735318703\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006848015585301085, AUC: 0.9885311416237628\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006334982796979, AUC: 0.9896065982775547\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012937709658289054, AUC: 0.5659179373523726\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008261655428394767, AUC: 0.9812906980233461\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006777105802828234, AUC: 0.9887856717236885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006244132607619955, AUC: 0.9900309937283783\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0005815412625516177, AUC: 0.9907201004831249\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005510169742763907, AUC: 0.9916015784081312\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0429279197076833, AUC: 0.511008560785003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005785764127537824, AUC: 0.8119687019072075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032533862575981187, AUC: 0.9013537786199003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022744600570473363, AUC: 0.9369461960878456\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017831691427013642, AUC: 0.9540120373982417\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015013299990391386, AUC: 0.9615005808644808\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03230802267481328, AUC: 0.14692977755676823\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007659684056821076, AUC: 0.7011864853626437\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003727853915212564, AUC: 0.8705133041543599\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024848000356622857, AUC: 0.9243290050713113\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019553461060020493, AUC: 0.9446351485169737\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016293079586502928, AUC: 0.957464001406078\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01649578807270058, AUC: 0.4586128698992168\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00536702459149726, AUC: 0.8048456100791775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035690578861513, AUC: 0.8881803723534227\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002823893327890716, AUC: 0.9221234347738486\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023303989544665098, AUC: 0.9408204121351378\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020202435321689392, AUC: 0.9503709173371971\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0201811731231879, AUC: 0.39100270927196884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0059308119688962066, AUC: 0.7738240173530584\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031613961756846425, AUC: 0.888915026599735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022704676564929403, AUC: 0.9284229207838026\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018350819499843116, AUC: 0.9461607215580029\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015970877608897523, AUC: 0.9565305457554025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01731543215165227, AUC: 0.4663157545022355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005455929546869566, AUC: 0.7915821805352526\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002866207812883839, AUC: 0.901608844572668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018838842092833905, AUC: 0.9417635131369683\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014361937223754315, AUC: 0.9595281065532659\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012082772842351942, AUC: 0.970893545331007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010298960450766743, AUC: 0.6292278789229787\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003941133155585816, AUC: 0.8584566152105044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024037062504770348, AUC: 0.9249795304214375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017544229825337727, AUC: 0.9473942548001697\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014827055837303461, AUC: 0.9607541378556457\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013066912783352238, AUC: 0.9671152469424237\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015357928740065044, AUC: 0.4890219828269881\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0064481220383575, AUC: 0.7789526649033536\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003967860347242336, AUC: 0.8755438906345783\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029095971559639057, AUC: 0.9157280311051857\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023351556025676848, AUC: 0.9380982796980362\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001983741857497095, AUC: 0.9500210054314043\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027136541054609154, AUC: 0.36476467486592956\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006250277562664656, AUC: 0.7621231346962573\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030155363290206246, AUC: 0.8985828835741813\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00202643643008009, AUC: 0.937016928662983\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015871816046736501, AUC: 0.9550365880320483\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001339278791261756, AUC: 0.9643234536358687\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013148997387777452, AUC: 0.4339202350893588\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035827885503354278, AUC: 0.8423194068752063\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020717734135455965, AUC: 0.92358363376816\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015279940690066257, AUC: 0.9487799702494503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012698705398764916, AUC: 0.962150570361765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011104394683679932, AUC: 0.9686049178430423\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023326788876614463, AUC: 0.39854698143377076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004421475511159956, AUC: 0.8156864489246505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023338594792052087, AUC: 0.9202458064156591\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016381472164059279, AUC: 0.9527099149923053\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013201475513647803, AUC: 0.9653260343031554\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011295307867275261, AUC: 0.9728011814483459\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007682041845459869, AUC: 0.6981133693140655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006770150755256353, AUC: 0.7302307811019707\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006163383122556698, AUC: 0.7599690062716216\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005596759650030985, AUC: 0.7830192521209056\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005154129140865729, AUC: 0.8031539226571442\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004788349133840999, AUC: 0.8202653114591059\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010572044498925378, AUC: 0.6578531377399013\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008880604868349821, AUC: 0.6975550106527544\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007889147624219179, AUC: 0.729510059029549\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007176146991011025, AUC: 0.7566938737016285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006598946964271814, AUC: 0.7764588057769223\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0060794861913961405, AUC: 0.7931222216030145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00853373890831357, AUC: 0.7126912458792917\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007833665942553407, AUC: 0.7367901557402701\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007218653864495256, AUC: 0.7587970961062789\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0066770431171046035, AUC: 0.777727705306658\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006214353115168664, AUC: 0.7948948228041822\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005797951611426059, AUC: 0.8083141855251144\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015555935123198767, AUC: 0.5039556656792684\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013500487088663485, AUC: 0.5496612338333198\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011732532370904958, AUC: 0.5926119825269105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010257643942507158, AUC: 0.6311799908261995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008986377321168255, AUC: 0.6689410047455128\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007948787316032078, AUC: 0.7044546518456916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01571848081505817, AUC: 0.6496486948768182\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007899324839653188, AUC: 0.7913898093649929\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004879064194657541, AUC: 0.85241541027037\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037643459025870694, AUC: 0.8775115422702155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003263869887800197, AUC: 0.892012256026201\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029854394387507782, AUC: 0.9040914507894185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01110870201395165, AUC: 0.6769884427259047\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01002667902912906, AUC: 0.7041599327826196\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00916935642313513, AUC: 0.729237309932997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008390759335788388, AUC: 0.749673129766411\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007716207524017271, AUC: 0.7710198994311387\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0071319502086126035, AUC: 0.7894644901038698\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013644918398333879, AUC: 0.5951942573722634\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011760998463285143, AUC: 0.6373926686757576\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010204565451011895, AUC: 0.6750470478795232\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008954591632629773, AUC: 0.7086042962537455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007893487780237296, AUC: 0.7393177735957441\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007002498298945141, AUC: 0.7659400144894608\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.033568261819843424, AUC: 0.3794465068824939\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024222187373949135, AUC: 0.3378268166483048\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018737938093102497, AUC: 0.3671813711831202\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015391018573295008, AUC: 0.41825993578339543\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013072926065196161, AUC: 0.4776924461896576\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011204362162398502, AUC: 0.5361288061627363\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02783958857597525, AUC: 0.4776506496679856\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01791087045926238, AUC: 0.4740288203092514\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013137085837607058, AUC: 0.5081396046692073\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01095816511545122, AUC: 0.5590868853232049\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009541967147132131, AUC: 0.6057296600978253\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008412535639776699, AUC: 0.6495709962147356\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006909662151929014, AUC: 0.8370010674188612\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004766638728155606, AUC: 0.8784792925027757\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00360821949522441, AUC: 0.9026848370793019\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0030176374236002227, AUC: 0.9169010129762123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026602653736406724, AUC: 0.9279695892795138\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024775044518227903, AUC: 0.9342497845871575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01665117034754151, AUC: 0.49944217719153094\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016447709954303245, AUC: 0.5040435455453481\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01626822074747974, AUC: 0.5080131433985073\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016035122407395894, AUC: 0.5119473749640978\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01585993065843918, AUC: 0.5164737239200421\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015698735511574437, AUC: 0.5205172694653903\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01010664501545592, AUC: 0.7519890857493151\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009418974752011507, AUC: 0.7578357761078223\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00879349965239657, AUC: 0.7632612861325572\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00822822672486552, AUC: 0.7684858513415612\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007715740312453876, AUC: 0.7737773981558089\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007257652578886992, AUC: 0.7784950465763291\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03765357947497634, AUC: 0.22999286244014522\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03629916252309985, AUC: 0.23417144290166458\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03496262647103572, AUC: 0.2380295833636983\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03367201487223307, AUC: 0.24280617559683287\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.032367919542774654, AUC: 0.24745791411779333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03116713695644592, AUC: 0.2522870199294389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030962547160083464, AUC: 0.22777068070458223\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029943999296389752, AUC: 0.23267694932546845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02898198279781618, AUC: 0.2378993711231047\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028054827735537573, AUC: 0.24318716696745873\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02713260591400336, AUC: 0.2486185713734551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02631832304454985, AUC: 0.2542053731036168\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029282066392602387, AUC: 0.6284090957804804\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027812505607526, AUC: 0.6342354237309933\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026406334547275836, AUC: 0.6404405996407643\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02502740935015629, AUC: 0.6466543491960064\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02379742122831799, AUC: 0.6521907807590247\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022552193074986553, AUC: 0.6590989955974331\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013687363322477162, AUC: 0.4806675011681592\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013183911147818556, AUC: 0.4883007249017246\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012716345165086828, AUC: 0.495913050374454\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012257992604257651, AUC: 0.5037075658134461\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011892965368108967, AUC: 0.5110155268719483\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01149510251315731, AUC: 0.5180394859242176\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027580263204949735, AUC: 0.5522494030599341\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026015785169897613, AUC: 0.5612699498013058\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02460132918742873, AUC: 0.5690167743373644\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023266193042383924, AUC: 0.5763922528539522\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021872709996952032, AUC: 0.5841031752496002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020612139139116182, AUC: 0.5913741624620081\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014465718535903078, AUC: 0.6655533430787104\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014136679671072318, AUC: 0.6695845640086937\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013791202264789715, AUC: 0.6737701105571583\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013438336844276444, AUC: 0.6770993642641883\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013146590001834846, AUC: 0.6800599512159573\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012860915182046516, AUC: 0.6836849956917432\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02604610924888595, AUC: 0.438637347657037\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025357137802471532, AUC: 0.4434600232345792\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02466423713889428, AUC: 0.44884641600185193\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024024164701347273, AUC: 0.4541985141872399\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023395352728865407, AUC: 0.4588325695644159\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022815714218108057, AUC: 0.46391299035893563\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03366526214725976, AUC: 0.5326902384759488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03228078273512562, AUC: 0.5356535046919274\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030898257071927466, AUC: 0.541357122341634\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029654124755543458, AUC: 0.5439479708324582\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028264643983070895, AUC: 0.546594548018845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02710604420853451, AUC: 0.5534588229242132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 class weighted\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['pos_weight'] = torch.tensor([weight[1]])\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(NUM_CLASSES_REDUCED, shape=28*28)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "    \n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2bdd2645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.047167061278538674, AUC: 0.2392877658365949\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00045623506880201415, AUC: 0.9964054991362052\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00040390655614327693, AUC: 0.9967473732493687\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003926855806978593, AUC: 0.9968084604733509\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004194700168773501, AUC: 0.9972532183321687\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00041026875037337437, AUC: 0.9972462522452233\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014488663979445432, AUC: 0.5211817269894072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00037133988324653043, AUC: 0.9954500735190099\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003817048594818352, AUC: 0.9955213419469892\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00039651810999489226, AUC: 0.9960529079662027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00036910817983481207, AUC: 0.996154184153331\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003813507332318071, AUC: 0.9960989913106103\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015022079396692123, AUC: 0.4698384510852092\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000366702305604212, AUC: 0.995796770307751\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00031577879042358873, AUC: 0.9962040184676323\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00031383795274217186, AUC: 0.9962565320461434\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003252887315508248, AUC: 0.996713078667484\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00036067246434357843, AUC: 0.997629922880059\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024926901357267706, AUC: 0.43631121047013577\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005504578105164364, AUC: 0.9924921658314507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000498269378028301, AUC: 0.994080433654988\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00045212805579661336, AUC: 0.9948467032189753\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004568027166599566, AUC: 0.9953841636194501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004613064769385517, AUC: 0.995417922348493\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016710107617743514, AUC: 0.5424186468215353\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005595467784143135, AUC: 0.993890205896096\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005199610261443239, AUC: 0.9949147565299027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000490341913996276, AUC: 0.995195543419084\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004416391808794152, AUC: 0.9959216240199251\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00048344429605495857, AUC: 0.9963894235509467\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02391877085525797, AUC: 0.5011547628744004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00031467152308233036, AUC: 0.9964097859589408\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000306111770078509, AUC: 0.9970871039511645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002650488129437093, AUC: 0.9977306632143453\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00029807115903058656, AUC: 0.9981497001367496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003017217507026704, AUC: 0.9981314811401234\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010594974640240087, AUC: 0.6260797434765275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004781737584258212, AUC: 0.9935863773347109\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004361345746995993, AUC: 0.9947111324499621\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00042414096016321124, AUC: 0.995375054121137\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00040467462924696643, AUC: 0.9945203688382281\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00041186686134733275, AUC: 0.9944780364637141\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0114503400419563, AUC: 0.5569825912128707\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00038486245564545656, AUC: 0.9970094052890819\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00035172213431964504, AUC: 0.9976170624118522\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00034068680636384227, AUC: 0.9972666146532175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003138322915349688, AUC: 0.996880264754172\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00030962550553722656, AUC: 0.9964215747214636\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028141674294481615, AUC: 0.43887955314159816\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00040787111343063926, AUC: 0.994964054991362\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003662665006290065, AUC: 0.9958669670300463\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00035974741228865785, AUC: 0.9959360920466578\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003617118666137474, AUC: 0.9959982509763239\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003606103067565902, AUC: 0.9960277228826312\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03999842373233898, AUC: 0.3949642693324988\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00038356421897129984, AUC: 0.9952544872316984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00034218091772209784, AUC: 0.9963642384673751\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003303526659683164, AUC: 0.9965169565273306\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003106286807089859, AUC: 0.9966380592696111\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003449409644796241, AUC: 0.9970838888341128\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025964936361056182, AUC: 0.767855688399429\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007594893561140104, AUC: 0.9813807213007936\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004979779415495894, AUC: 0.9886774294496149\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00040858618406035144, AUC: 0.9926057666339438\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003606008354181088, AUC: 0.9931362609474736\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003383329550795427, AUC: 0.9934041873684482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029280705975202794, AUC: 0.42188176514212955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010533488932110014, AUC: 0.9794286093975729\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006994008709431681, AUC: 0.9899543667719797\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000561271796068543, AUC: 0.9928243945934592\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004928336472985167, AUC: 0.994227793186524\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00045162783337922817, AUC: 0.995259309907276\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014252784582892314, AUC: 0.5056414587200404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000941470882413797, AUC: 0.9813887590934227\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006831416061946324, AUC: 0.9894881747994839\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005960808777661057, AUC: 0.9916374805485418\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005653843468760852, AUC: 0.9930489169342359\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005365548420149841, AUC: 0.9941131206783468\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03465989114828485, AUC: 0.2273859383640627\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047952936302801094, AUC: 0.9893665362043614\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00032787647399102677, AUC: 0.9948279483695069\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000285847432618309, AUC: 0.9960411192036798\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002699758217324875, AUC: 0.9963819216111595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002575533514684278, AUC: 0.9965801871626806\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01677265621366955, AUC: 0.4891934557364118\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009305943304954355, AUC: 0.9801552258512558\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006358535578532249, AUC: 0.9878532877786971\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005148690093624913, AUC: 0.9907399270382771\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004499383409571203, AUC: 0.9914697586090119\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00041156365511086664, AUC: 0.9923791008817994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01204266015046872, AUC: 0.5752637467688074\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007729561185738068, AUC: 0.9806385651146939\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004856147751304674, AUC: 0.9904050190120588\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00038356938655825627, AUC: 0.9938076845584359\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003375767982771184, AUC: 0.9958042722475383\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00031760584863816727, AUC: 0.9960132548558984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04167308175539131, AUC: 0.4256793542330231\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006095293145742475, AUC: 0.9886077685801614\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004775479713582104, AUC: 0.9919268410831944\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00044166694701828574, AUC: 0.994237438537679\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004213243167592872, AUC: 0.9951687507769866\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00040853848800402495, AUC: 0.9954088128501799\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026900875889243052, AUC: 0.2918340313709688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009870903092141477, AUC: 0.9807591320041325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006416717979972161, AUC: 0.9880890630291548\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005265582298886949, AUC: 0.9922483527883638\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046763197358844194, AUC: 0.9933656059638278\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004403882083438692, AUC: 0.9937342727190889\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0315212305041327, AUC: 0.547362425141358\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007200195428994378, AUC: 0.9853556776823722\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00048762070898190294, AUC: 0.9911755753987816\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004172908068937298, AUC: 0.9936463928530092\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003882805136173161, AUC: 0.9941163357953985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003728077093266552, AUC: 0.9948509900417107\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02513281899209348, AUC: 0.3667012470367338\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011389249353428558, AUC: 0.977657615754931\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000894569770643183, AUC: 0.9854033685853056\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007802172724011028, AUC: 0.9880327984807501\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.000725286470930522, AUC: 0.989414762960137\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006725180963551776, AUC: 0.9899463289793504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024510826876938466, AUC: 0.3556107007669126\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0056829983156413515, AUC: 0.7760097610953689\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030601203565024933, AUC: 0.8936482147526716\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021707558113595714, AUC: 0.9326861659943498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017243282518525054, AUC: 0.9504523669691735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001447187632507419, AUC: 0.9613580440085222\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018804269794598375, AUC: 0.3043601274043717\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004782377069287665, AUC: 0.7888964860914037\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024597099355535726, AUC: 0.9102253582712101\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017469313080513205, AUC: 0.9440339216283067\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014364365218342214, AUC: 0.9574061292991474\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012296164874951538, AUC: 0.9667819464747314\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027304877405581268, AUC: 0.35814099788659637\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005356248121083893, AUC: 0.7852542943246753\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002351985338064948, AUC: 0.9155565581957621\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001509893076266808, AUC: 0.9542113746554466\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001162366837448215, AUC: 0.9666688815250801\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000983693554041055, AUC: 0.9734699257950984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023366404863124555, AUC: 0.385091716572428\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004716761857579707, AUC: 0.8208150964749458\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002417656945886079, AUC: 0.9272440445315144\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018307087214096732, AUC: 0.9550355163263644\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001540128365313291, AUC: 0.9669603854711004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001387988003144353, AUC: 0.9725900554286179\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013819874690432972, AUC: 0.4691418423906753\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005026581124489352, AUC: 0.800312295036288\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030386760121300107, AUC: 0.8965423559520389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022929086201432824, AUC: 0.9298916934235854\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00190459392332389, AUC: 0.9453767688502313\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016527044476929659, AUC: 0.9553554204730079\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019333746615897547, AUC: 0.4435221821642453\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005025546990072752, AUC: 0.8098161810410979\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002758859600833237, AUC: 0.9092827931222216\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019823808230721926, AUC: 0.9393489602311454\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016010350568941168, AUC: 0.9548844058249348\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013755515495442456, AUC: 0.96502756427019\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011095752380402685, AUC: 0.6162098699806664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031781239786009855, AUC: 0.8775292254139999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002052800070424998, AUC: 0.9328721069305062\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015675309528722032, AUC: 0.9536396196730869\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013140710861292931, AUC: 0.963635954439648\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011379619439442952, AUC: 0.9695169393800397\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016645586515312116, AUC: 0.6134507636974703\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00330123323831499, AUC: 0.8890211254624409\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021659998419862356, AUC: 0.935613530069918\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001626543685269405, AUC: 0.9536771293720234\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013504355343725864, AUC: 0.9653983744368186\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011694188196960196, AUC: 0.9722481813154543\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022285592728766842, AUC: 0.3476522143582841\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005222225287933034, AUC: 0.7975290753752041\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00286959355415518, AUC: 0.9029532993531184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021168008971164934, AUC: 0.9349972993016766\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001757803725900117, AUC: 0.9484096959356634\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001536690979023651, AUC: 0.9582468824081657\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011419931060285549, AUC: 0.5588580761596926\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003939712022895892, AUC: 0.8464272547615883\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002404920926498824, AUC: 0.9180948931080751\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001749817567335646, AUC: 0.9455905741341689\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013631825985128587, AUC: 0.9605103248125586\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001118900952378662, AUC: 0.9695051506175169\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013230142139253161, AUC: 0.7178943769746178\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010303228291418736, AUC: 0.7436431777359575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008750392289882367, AUC: 0.7628122414510038\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007786720189001743, AUC: 0.7776559010258366\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007081309707515235, AUC: 0.7929411033424356\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006519167072777916, AUC: 0.806685728738431\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02108461071985849, AUC: 0.3830045697530361\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01455146678979846, AUC: 0.44816856215678624\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011358794711884997, AUC: 0.5126037946954856\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009496953423225608, AUC: 0.5706179026291084\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00817964091804457, AUC: 0.6239792003360869\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007155972978343134, AUC: 0.6682122448804619\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.054502191010469235, AUC: 0.4851600913950607\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03930606802551396, AUC: 0.4533186438207594\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02612264605535977, AUC: 0.4105200773342822\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01696836973075788, AUC: 0.41204565037531127\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012058631727167292, AUC: 0.47398005770063406\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009408978942018118, AUC: 0.5476185627998097\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012288130588413024, AUC: 0.5874844066822993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010066089669616573, AUC: 0.6312287534348167\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008820126515737972, AUC: 0.6683237022715873\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00795916790300768, AUC: 0.6996319762681493\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007261314994306545, AUC: 0.7279576933464225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006750794671337056, AUC: 0.7519151380571261\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02554363542955608, AUC: 0.3932597214422586\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01760307552898399, AUC: 0.4058919163383675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013312763308886415, AUC: 0.465584315372975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010999613420316644, AUC: 0.5302354966069798\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009500997653905895, AUC: 0.5906636215935834\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008346044252131049, AUC: 0.6398393727520972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01648388047149216, AUC: 0.39621441401276614\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01426156411259811, AUC: 0.44927884924530487\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012498491792698577, AUC: 0.501211027422805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01097894159163007, AUC: 0.5509210238647422\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009778727162205162, AUC: 0.595146030616488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008736709136646974, AUC: 0.63654655703832\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012740049302948187, AUC: 0.6607445996750588\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009836007349239373, AUC: 0.7036037175326764\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008388727348043312, AUC: 0.7358229413605519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0074769776800404425, AUC: 0.7602465780437513\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006816844762482258, AUC: 0.7800200837645161\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006290378531067021, AUC: 0.797066634372602\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017059327159115494, AUC: 0.5851245107663554\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012302413490248023, AUC: 0.6204088128501798\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010194721182434209, AUC: 0.6539992841006032\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00910790020881479, AUC: 0.6836533803740682\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008300267391323304, AUC: 0.710096110565732\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007636374815156988, AUC: 0.7351815255087386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016138667645661728, AUC: 0.5672414938719867\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012125443227542854, AUC: 0.6007119340858136\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009960009197764269, AUC: 0.6345505052020595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008582508095056127, AUC: 0.6709868908960746\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007577834909253485, AUC: 0.7041851178661911\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006727562681241559, AUC: 0.733303361297707\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029025785671257825, AUC: 0.4100699609470449\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020185097404148266, AUC: 0.3898795617152435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015473185118681156, AUC: 0.41074352796937497\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.012929203594199865, AUC: 0.45976924033214306\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01108596438453311, AUC: 0.5192505133470225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009620982914484313, AUC: 0.5762765086400912\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011681524122723882, AUC: 0.5932619720241948\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011517065652408955, AUC: 0.5975970215155633\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011362241168446668, AUC: 0.6021967823108547\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011212440751354146, AUC: 0.6058914876560939\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011072680570077206, AUC: 0.6099762938702722\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010949866371865598, AUC: 0.6135316774766046\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010699937062233872, AUC: 0.6485223322030411\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010165640532847024, AUC: 0.6588557184071882\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009680783526497597, AUC: 0.6638536178640477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009260127756659782, AUC: 0.66770586394482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008841402535606368, AUC: 0.6738649565101833\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00850368829494184, AUC: 0.6834770847890669\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027324782148404645, AUC: 0.26492725261817696\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026449447339612752, AUC: 0.2701528895328649\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025546521133517627, AUC: 0.2755467842399249\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02468386940334154, AUC: 0.28125040188963146\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023834513828127527, AUC: 0.2872744595388236\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02308531577542702, AUC: 0.29350910735490177\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0348701220368253, AUC: 0.25629787845142815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03369629012871973, AUC: 0.2599197078101624\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03264338629586356, AUC: 0.2639766496765592\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03156192850622331, AUC: 0.26882343863198915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03051394507998512, AUC: 0.2735560909320839\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029518748415676456, AUC: 0.2787029574790053\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015163411264834196, AUC: 0.45210332957521876\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014994294006631981, AUC: 0.4574645372589198\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014812057309515975, AUC: 0.46245707818735987\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014605214630348095, AUC: 0.46870458647164487\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01445269387207663, AUC: 0.4744719706095433\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014282780898036918, AUC: 0.4795572140796406\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012704814196126554, AUC: 0.58271478053611\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012367123649233864, AUC: 0.5867979791917624\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012114899005455506, AUC: 0.5901567048050995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011827047320379726, AUC: 0.594503007206149\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01156320581771819, AUC: 0.5967005397109824\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011288790969374757, AUC: 0.601234390606714\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027286003341832763, AUC: 0.48576667681214714\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026097600751288436, AUC: 0.49153245339151974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024979618025122222, AUC: 0.4974841709070489\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02397394279021901, AUC: 0.5027842913667677\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022925239665661293, AUC: 0.508648664869059\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021957557393897393, AUC: 0.5143603203113949\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016598303619132032, AUC: 0.6850476694688197\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0156865850492047, AUC: 0.6915116623012523\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014833443900319607, AUC: 0.7042349521804925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014060573301453522, AUC: 0.7170236161064504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013359501248314268, AUC: 0.7295481045813275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012653278267901876, AUC: 0.7371647168767925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04159988577074639, AUC: 0.26983352123906323\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04030320066842974, AUC: 0.27365040103226695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0390141489096063, AUC: 0.2776918031662473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03774726069985463, AUC: 0.2808699463718476\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03656723780661636, AUC: 0.28599430709940715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0353102990065549, AUC: 0.29086199431567306\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021299612941702455, AUC: 0.2341237519987311\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02092288493122867, AUC: 0.23945334436475718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02050143986261656, AUC: 0.2450980182018493\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020162381987640823, AUC: 0.2508573645471186\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019842612817420724, AUC: 0.25681283303254127\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019443005508517627, AUC: 0.2630244391764156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 class SMOTE\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(NUM_CLASSES_REDUCED, shape=28*28)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "    \n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd32789c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.00995381722539108, AUC: 0.7384561222258899\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030169588191662267, AUC: 0.9752157343541688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021965423479336885, AUC: 0.9844002520651768\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001858554456544959, AUC: 0.9876357315248658\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017254860998434064, AUC: 0.9891012890475965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015510958050595554, AUC: 0.9901403077081359\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01969920527614175, AUC: 0.49038733586827454\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016330346064044327, AUC: 0.9859113570794734\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001119328396660941, AUC: 0.9908926450982325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009483712615433687, AUC: 0.9923185495106592\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000913108882203112, AUC: 0.9943087069656583\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008726360326474744, AUC: 0.9948933224162247\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008244756586062982, AUC: 0.7884356526473275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002420156885624919, AUC: 0.9831463564150159\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019199492151445977, AUC: 0.9883328760722414\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001760905327017016, AUC: 0.9898289772069635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017173529658505143, AUC: 0.9903782263699613\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015599583246693107, AUC: 0.9910866238270181\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012901468553404877, AUC: 0.6308391884187198\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002436935161211476, AUC: 0.9743669434525213\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018055564868524208, AUC: 0.9842030582193396\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014883343971047096, AUC: 0.9880815610893674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012551850907304026, AUC: 0.9903235693800826\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011446636409246157, AUC: 0.9917537606152448\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018447417896973666, AUC: 0.33478531591740146\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002087759490338912, AUC: 0.9854891050400175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001138822879356874, AUC: 0.992878515730496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009174461999048111, AUC: 0.9947545365301599\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007960867314111619, AUC: 0.995482224689527\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007598168731476209, AUC: 0.995849819739104\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026954060509091333, AUC: 0.5632890433097701\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002334326690768603, AUC: 0.9827846557467003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015394693822840973, AUC: 0.9903015994135627\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012376786018750683, AUC: 0.9925007394769219\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011391930636905489, AUC: 0.9929792560647824\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010894668398436552, AUC: 0.9935686941909265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01609354878064268, AUC: 0.4568150836144775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002858357770102365, AUC: 0.9753330861265556\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020232643646729905, AUC: 0.9844602675834752\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016603419869582845, AUC: 0.9884100388814822\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015970654245735943, AUC: 0.9898691661701096\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001459824804440295, AUC: 0.9907372477740674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010885211498347376, AUC: 0.6319607284169193\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025643337340581986, AUC: 0.980668037021001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016526244195104879, AUC: 0.9890118016229911\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013336282585965427, AUC: 0.9913668748633575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011251594709313433, AUC: 0.9925811174032142\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009988276731400262, AUC: 0.993225748372079\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02269862058493415, AUC: 0.5611375941493444\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00382194299381959, AUC: 0.9658811778474149\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025325904968609226, AUC: 0.9816716893939718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018602146865418238, AUC: 0.9864032699883826\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016206914470309303, AUC: 0.9885713305869089\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00143126646677653, AUC: 0.9899045324576783\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017541348317148275, AUC: 0.45317985793469456\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037505197228852264, AUC: 0.9654878618614241\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002373727582256246, AUC: 0.9807837812348621\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016995240812716277, AUC: 0.9857527446382566\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015701459061284984, AUC: 0.9879663527283482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013447503865875812, AUC: 0.9893670720572032\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012501404152153442, AUC: 0.5028635975873761\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01407681735652821, AUC: 0.7987058082161245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008497557778289352, AUC: 0.8921188907417489\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005966387180067738, AUC: 0.9324278849245304\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00497585127812735, AUC: 0.951942037869792\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004322129870547024, AUC: 0.9632506762462867\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.033741787106847665, AUC: 0.11035782109373996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029963302809752786, AUC: 0.3696436149918765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0180457383702754, AUC: 0.6067927921362524\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01132280387246584, AUC: 0.7719287058510844\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00789334487717591, AUC: 0.8645251486455783\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005924822874444366, AUC: 0.9137936023457494\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02208925527568683, AUC: 0.459284829363021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013539695344849896, AUC: 0.7698865706704162\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006918879029173288, AUC: 0.8851388716225195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004734827371364301, AUC: 0.9269123516223481\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037712512796216377, AUC: 0.9476343168733631\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00332915573139862, AUC: 0.9585946509025907\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0289459662901442, AUC: 0.382048072430157\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021252119763297325, AUC: 0.619575025828107\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013044563139447515, AUC: 0.7896327478962417\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008721078157918546, AUC: 0.8739229357876821\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006498064560426195, AUC: 0.9145807701705726\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005247130897474585, AUC: 0.9364055205703189\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01735672348527928, AUC: 0.3962138781599242\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020255253675314704, AUC: 0.6708223840735962\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012780400783625696, AUC: 0.8151934643100573\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008590581994619428, AUC: 0.8939300733475369\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006209293507641146, AUC: 0.9319381154269889\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004875649576601775, AUC: 0.9516253488402001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009060546231319199, AUC: 0.7040538339199136\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016070203504700592, AUC: 0.8350023363183908\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010781638617347733, AUC: 0.8986000308651236\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007859244356491057, AUC: 0.9298407874036001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0060318752845622, AUC: 0.9478561599499299\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004933324413023133, AUC: 0.9584403252841093\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03631723099860592, AUC: 0.41427747746202953\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018668564703647146, AUC: 0.7187292357023745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012039569100484591, AUC: 0.8430722801181448\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008134908804488725, AUC: 0.9032914224963884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005855855487641834, AUC: 0.9345128883325545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0045585696494850805, AUC: 0.951721802351751\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03185423659488528, AUC: 0.3660057100478838\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022725554480068924, AUC: 0.6182514693084926\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013012105633753428, AUC: 0.7924411526408971\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00836746366868108, AUC: 0.8799711068147621\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0060594558222200065, AUC: 0.922478705208061\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004693458786168701, AUC: 0.9449657697204561\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03708247072208002, AUC: 0.3381585095574713\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022133228941733793, AUC: 0.5825690285630999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01251810529957647, AUC: 0.771973717489808\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007993706264851257, AUC: 0.868227355930605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005804910916472567, AUC: 0.9135471100384529\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004527683583846003, AUC: 0.9373143269902647\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006171950148746341, AUC: 0.8275180796748874\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012548981739620739, AUC: 0.885164056706091\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0093006297915125, AUC: 0.9230992227990381\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007210223077493671, AUC: 0.9448805691185863\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005824784067600164, AUC: 0.9572073278947842\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0049528327787885015, AUC: 0.965035066209977\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014483087551519738, AUC: 0.5431533010678475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01677851696685728, AUC: 0.6238988224097946\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018930261426337264, AUC: 0.6546519528620972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018974582601037824, AUC: 0.6788419577062069\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01831426975889976, AUC: 0.7005718621529281\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01748123820524038, AUC: 0.7206106150304579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0290557573053901, AUC: 0.5020410634749842\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016534687816232876, AUC: 0.5955232710172202\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020092488322445572, AUC: 0.62957189644751\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02080388197494096, AUC: 0.6554680567403857\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020403145262913674, AUC: 0.6786093975728009\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0196403904237609, AUC: 0.6999920693779391\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03337555445005681, AUC: 0.16640159812751582\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04327310242267869, AUC: 0.21561753824917584\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04474187538984153, AUC: 0.24185396509668927\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04436655222258953, AUC: 0.2634274005135614\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04327553695773486, AUC: 0.2818703836277666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04181829693401329, AUC: 0.29965801871626807\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019217054789604362, AUC: 0.3680060487068799\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026283172346790385, AUC: 0.44486235012196007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028218746185302734, AUC: 0.4848016058437968\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02797081949301141, AUC: 0.5183904695356942\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026952406881265264, AUC: 0.5494817231312669\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025480686013989815, AUC: 0.5786096119139378\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010436690865589718, AUC: 0.6411800765626541\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012477335228929855, AUC: 0.7229008500769485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01402296881744827, AUC: 0.744004878404273\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014527521765256768, AUC: 0.7593570623261158\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014486395794412365, AUC: 0.7716146960857021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014350767708219603, AUC: 0.7826286153991248\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010383463794400232, AUC: 0.6506796757447282\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014846182264403034, AUC: 0.6856129942170761\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01651175866215866, AUC: 0.7071848220754224\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016997142854931438, AUC: 0.7261068576303301\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016906898707830142, AUC: 0.7432905865659548\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01647774773354856, AUC: 0.7592155971758412\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02496645810934821, AUC: 0.4202318742417682\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02265584641608639, AUC: 0.4625856828694276\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024450397886351276, AUC: 0.4944014095073155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024158124351106567, AUC: 0.5203173963553434\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0232385858492328, AUC: 0.5450437898942441\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022075745876778234, AUC: 0.5684085813617521\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0376147947449615, AUC: 0.35095038860048094\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04035637739035407, AUC: 0.38120196079271934\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04077690432530753, AUC: 0.4010172630351562\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04009957836775059, AUC: 0.4216513484200915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.038856095655611086, AUC: 0.4415084471842005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03747728201666727, AUC: 0.4611892503633082\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013005842086444484, AUC: 0.5119623788436725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016187392406582093, AUC: 0.6092705756774253\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01803820868703396, AUC: 0.6404898981022236\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01839881506025421, AUC: 0.6650646452868527\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01815954844156901, AUC: 0.6865469857205935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017650623005616244, AUC: 0.7062695854213734\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04105577804533838, AUC: 0.09269718312878043\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.045124650248336, AUC: 0.12728219725386136\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04696177646486902, AUC: 0.16055865873890246\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.046710452678040686, AUC: 0.18330775529101098\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04551864410779491, AUC: 0.20350994328533523\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04403284124212482, AUC: 0.22242608445898154\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029751194189794314, AUC: 0.4901751381428626\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024380373905410924, AUC: 0.5050161184534858\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021045390616786157, AUC: 0.5118980765026384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019091223090825366, AUC: 0.5204856541477154\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01800427328232159, AUC: 0.5315869174743755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01765695417889897, AUC: 0.5407826880950646\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01906613336093184, AUC: 0.5150596082701384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019659642600618286, AUC: 0.5236504010322669\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02032505256542261, AUC: 0.5306100577435022\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02091000934071669, AUC: 0.5368886454926203\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021444699779060317, AUC: 0.5419588850831429\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021889654992776876, AUC: 0.546981969623574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014156564175465586, AUC: 0.49595216763191624\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014697908614733204, AUC: 0.5063600373810943\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01567043774369834, AUC: 0.5143731807796016\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016775193421737007, AUC: 0.5223798939440055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017771797890988938, AUC: 0.528084583299396\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0187131909356601, AUC: 0.5332496688429438\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023258467885524835, AUC: 0.5235378719354576\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014870941762351595, AUC: 0.563311549129132\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01045181341546416, AUC: 0.6104290895217191\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008926940507276705, AUC: 0.6362266528916762\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008811301572969488, AUC: 0.6501395360800436\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009220596672832102, AUC: 0.6583241523879746\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04207033943192066, AUC: 0.4471675890480252\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0313130175351603, AUC: 0.41420835244541804\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022776514847086083, AUC: 0.40317139145979175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01783699525315816, AUC: 0.41570445358013997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016039830063687595, AUC: 0.42930064773891535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01585012439862048, AUC: 0.44427505540718387\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03627318032780049, AUC: 0.5180518105395824\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03587645773561845, AUC: 0.5202627393654645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03551366857366779, AUC: 0.5219035207675127\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03521002104070122, AUC: 0.5244322103286706\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.034800638076434715, AUC: 0.5260660256437736\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03444364993962195, AUC: 0.5281081608244418\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018640805475460077, AUC: 0.5614971514062921\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01418521833715972, AUC: 0.5746057194788938\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012334641462527447, AUC: 0.5809920136492436\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011653619276564067, AUC: 0.5935282908866436\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011660955954289091, AUC: 0.605237211336074\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01204419333495462, AUC: 0.6142416824921872\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011888659025077741, AUC: 0.7527189173200499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01252005608679098, AUC: 0.7541689351103642\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013116929842078167, AUC: 0.7541544670836317\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013749641908128316, AUC: 0.7547787356445024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014354656942142463, AUC: 0.7546747801931643\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014890439762091784, AUC: 0.7551731233361769\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016744325373236932, AUC: 0.282550916737042\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016694534392583938, AUC: 0.3004226807217295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01799844757617137, AUC: 0.31775966357015173\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019321081307610618, AUC: 0.33079696321477414\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02042396122871225, AUC: 0.34029870580821614\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02130939057154685, AUC: 0.34834989475850187\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025291099311402124, AUC: 0.5490074933661419\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017180564240639254, AUC: 0.5892296793885277\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011911138984727563, AUC: 0.6252480998658225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009059363023588129, AUC: 0.6644902110402833\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007914285975706996, AUC: 0.6891908836427705\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.007558618766674097, AUC: 0.7054850968607598\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0402261169307227, AUC: 0.3062731220501301\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.040239928425222206, AUC: 0.30676021228346184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.040261063269700074, AUC: 0.30729552927256903\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04023169089054716, AUC: 0.30778154780021694\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.040275593475278613, AUC: 0.3080189306092004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04026850113957565, AUC: 0.308349551812683\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012359107256429289, AUC: 0.7275204374273919\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011905916482518672, AUC: 0.7312328259164156\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011517731546121601, AUC: 0.7368421334659391\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01106380083546135, AUC: 0.7405620238947499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010651532660853542, AUC: 0.7442331517149434\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010298251118472398, AUC: 0.7485821333802026\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010768839537973977, AUC: 0.8453539415191643\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010801438219058587, AUC: 0.8454777235256545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010790037072223166, AUC: 0.845598290415093\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010809525199558424, AUC: 0.8457204648630574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010760490445123203, AUC: 0.845841567605338\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010764819001065525, AUC: 0.845945523056676\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01461709113348098, AUC: 0.5874710103612506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014243759230303715, AUC: 0.5899332541700067\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013848558469341903, AUC: 0.5920772013906453\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013504643864759995, AUC: 0.5939746563039872\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013199459691965803, AUC: 0.5960671616517985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0128943855964866, AUC: 0.5988862834532931\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026688451352326767, AUC: 0.4912789949972779\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026698237867335602, AUC: 0.4916648090434812\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02670239020084989, AUC: 0.4920811667016757\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026735573328306464, AUC: 0.49244554663420115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026702590610670006, AUC: 0.4928163568008299\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02676585693043458, AUC: 0.4931635894424129\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018007763177464962, AUC: 0.5108472690795762\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018001575154053744, AUC: 0.5120958062013178\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01799933411813424, AUC: 0.5120159641278674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017969453310127337, AUC: 0.5142638667998439\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01791839323182037, AUC: 0.5153961238548825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017940229017048396, AUC: 0.5151501674004277\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016185296495015083, AUC: 0.5124350010502716\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015746716880403442, AUC: 0.5161270271313011\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015463450433798211, AUC: 0.5190340287988751\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015123979892296326, AUC: 0.5168552511435099\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014816848881249596, AUC: 0.5179098095364659\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014585225972082798, AUC: 0.5196288254534387\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03702228261817316, AUC: 0.5028721712328474\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0369762693132673, AUC: 0.5030511460820583\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03697256321245592, AUC: 0.5033887333724862\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03695272165302411, AUC: 0.5035677082216973\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03690475163746078, AUC: 0.5037327508970176\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03688359507369205, AUC: 0.5039181559803321\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027540721261476633, AUC: 0.30282169389513575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027603782235218626, AUC: 0.30361368439553654\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02772621190325814, AUC: 0.30443032412666704\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02780768195047635, AUC: 0.3052164202458064\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02790582155342181, AUC: 0.305968757635903\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027996117530648998, AUC: 0.3066380378354975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012085500958049767, AUC: 0.6116192186836882\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011977842382269123, AUC: 0.6143108075087986\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011798294928256523, AUC: 0.6156290054999936\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011706234258647785, AUC: 0.617426255931891\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011595918525079763, AUC: 0.6183409567330982\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011547608405166531, AUC: 0.6206124369301205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['reduction'] = 'mean'\n",
    "\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(NUM_CLASSES_REDUCED, shape=28*28)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SigmoidFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "    \n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfffcb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES_REDUCED = 3\n",
    "nums = (6, 8, 9)\n",
    "ratio = (20, 2, 1)\n",
    "\n",
    "\n",
    "reduced_train_MNIST = class_sampling.Reduce(train_MNIST, NUM_CLASSES_REDUCED, nums=nums, CIFAR=False)\n",
    "reduced_test_MNIST = class_sampling.Reduce(test_MNIST, NUM_CLASSES_REDUCED, nums=nums, CIFAR=False)\n",
    "\n",
    "ratio_train_MNIST = class_sampling.Ratio(train_MNIST, NUM_CLASSES_REDUCED, ratio, nums=nums, CIFAR=False)\n",
    "targets = ratio_train_MNIST.labels \n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "smote_train_MNIST = class_sampling.Smote(ratio_train_MNIST, CIFAR=False)\n",
    "\n",
    "\n",
    "weight = 1. / class_count\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= max(class_count)\n",
    "\n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_MNIST, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_MNIST, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_MNIST, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_MNIST, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_MNIST, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_MNIST, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_MNIST, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79e81c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.03784423537580223, AUC: 0.3768751506805761\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002918882420256769, AUC: 0.9819564945039094\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002367155349647784, AUC: 0.9842413121132969\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00021068657571383375, AUC: 0.9871322735948673\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00019660147983012092, AUC: 0.98688013037752\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001768489342609744, AUC: 0.9873670457763642\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08897318705455336, AUC: 0.4928097071453914\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00043320110645962185, AUC: 0.9835154133727497\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003763560746568111, AUC: 0.9837975611065666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00031749168977734184, AUC: 0.9845187222918818\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002995065506199354, AUC: 0.9848456136028897\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00026802601598631317, AUC: 0.9863411018711238\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03703024046066788, AUC: 0.5825095251179914\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004063983609667444, AUC: 0.9816988241387914\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003299049903891679, AUC: 0.9848237682093199\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002794862249520959, AUC: 0.9858676095454367\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002609276418903173, AUC: 0.9856331005614528\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00023253646352103843, AUC: 0.9832756404385081\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02998655242037911, AUC: 0.48498984583995275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00030526970771249965, AUC: 0.9796537794636454\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00024279869968734684, AUC: 0.9837675565900971\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00020253814855670897, AUC: 0.9837501855542464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001852663245909638, AUC: 0.9860889586537763\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00017682860919870677, AUC: 0.9868627593416692\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06245741054582904, AUC: 0.4599115972193709\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003879383707970675, AUC: 0.9833372286565241\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003055124988526724, AUC: 0.985100388795366\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00026291253634838375, AUC: 0.9829945454947427\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002406041300325157, AUC: 0.9845966287556968\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002134914697736593, AUC: 0.9832464255145773\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.031851559340325554, AUC: 0.41572915712575687\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00039112979824224405, AUC: 0.9815282721504396\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00031349771750779914, AUC: 0.9834925151891284\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00028391254876430894, AUC: 0.982466308086375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00026630808082173286, AUC: 0.983779926873203\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00025297709465513257, AUC: 0.983779926873203\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016746408474846625, AUC: 0.45461606326004866\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002845898397274465, AUC: 0.9850822281669768\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00023625976514021825, AUC: 0.9866116689143734\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00020676396029695125, AUC: 0.9881545327349274\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00018119002353249257, AUC: 0.9889285966203333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001714257197248737, AUC: 0.9891983740710443\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04996147272498459, AUC: 0.4074400146548376\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003804263412770022, AUC: 0.9796398299954624\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000299430124535118, AUC: 0.9804138938808683\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002754851947711139, AUC: 0.9812055919996377\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00024422814715520657, AUC: 0.9795869272953718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00023100227472207368, AUC: 0.9832756404385081\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08535392525162838, AUC: 0.40946926747921\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00024399617341698857, AUC: 0.9834438236492439\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00020380808621270037, AUC: 0.9855678275782565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00018560676621725997, AUC: 0.9862689857525921\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00017468837482189086, AUC: 0.9878494868174894\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00016622589593921053, AUC: 0.9896673920390121\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026579497907081456, AUC: 0.5833672858124959\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00035084125866739166, AUC: 0.9832580062051445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002802148669076509, AUC: 0.9845363565252454\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00023644997408016325, AUC: 0.9858147068453462\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002117733623413195, AUC: 0.9863189932800411\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002001281403352192, AUC: 0.9881545327349274\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03854259606406463, AUC: 0.4171001529703945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008706716970698601, AUC: 0.9518590693125387\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006183040908144186, AUC: 0.9691387756262259\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005184895973633601, AUC: 0.9735478603621387\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047140765336045436, AUC: 0.9779024632128837\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004332730535826931, AUC: 0.9807457859446213\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.040780458836197166, AUC: 0.2767263914462914\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006725843242138107, AUC: 0.9632302546593857\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00042823186053665506, AUC: 0.9743074483843357\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003553688505886769, AUC: 0.9802238652765627\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003094192884358286, AUC: 0.9825452673402417\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002826328982467029, AUC: 0.9835709480479693\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03603336576781521, AUC: 0.71871002687773\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009038972027729012, AUC: 0.9556959626554312\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006711302424563474, AUC: 0.964410169109666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005699828525499112, AUC: 0.9703726455666484\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000515103096947383, AUC: 0.9739789778882506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004730229372071879, AUC: 0.9757966199122605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05422931477878419, AUC: 0.4888004194315565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000784579019731342, AUC: 0.9535008953979388\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006239240933988662, AUC: 0.9685471076172518\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005443576179933726, AUC: 0.9719010335239935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005067432972493929, AUC: 0.9752746992442021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047887080950381923, AUC: 0.9763093286673679\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028135802299138923, AUC: 0.42678582144470173\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007263207994971458, AUC: 0.9572377736859339\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005602550340404076, AUC: 0.9706737435213932\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004880396394006495, AUC: 0.9739868738136372\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00044185340161308955, AUC: 0.9762914312364914\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004194196147685228, AUC: 0.978848131876693\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03640774049470312, AUC: 0.36138939861473884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007128836102568345, AUC: 0.9582342394697307\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005616750574484847, AUC: 0.9649207722846702\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004952196582812348, AUC: 0.969561997226951\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000454878318638106, AUC: 0.9728996048878936\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00042876672152960556, AUC: 0.9747167205168777\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04286446647553248, AUC: 0.5940588951810641\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007334353566291339, AUC: 0.9588846005240789\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005345879485517162, AUC: 0.9722060794414317\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004519889221755634, AUC: 0.9755252632764723\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004083167854031508, AUC: 0.9778640363760018\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00037921735600766903, AUC: 0.9783791139087252\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08611499432275507, AUC: 0.20056676952425467\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008251129749963495, AUC: 0.9619108455272741\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005953293578716332, AUC: 0.969126668540633\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005067756835881726, AUC: 0.9737931604441511\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004629066853002803, AUC: 0.9761137729152914\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00043001744626371764, AUC: 0.9773905440503149\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.072798258428953, AUC: 0.4707566507379532\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006663546964288041, AUC: 0.9639008819222262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005058344328172431, AUC: 0.9708650881132634\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00043771472442559505, AUC: 0.9747080349989524\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004014009543318037, AUC: 0.9747080349989524\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000380196191550355, AUC: 0.9752123214336473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.036231617180111864, AUC: 0.599719747288276\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006651972192195514, AUC: 0.9644472799589833\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047324178819549363, AUC: 0.9750215032368031\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00040479719294288946, AUC: 0.9788878747011391\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.00037018958300564895, AUC: 0.9791518618065665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003441834289660384, AUC: 0.9814724742777069\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02674292091284022, AUC: 0.5338661503784254\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010156536823792831, AUC: 0.9490333808141647\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007460199420608577, AUC: 0.9631349771597197\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006313614093783112, AUC: 0.9682678550560768\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005685341617599793, AUC: 0.9716312560732826\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005159145895618437, AUC: 0.9744732628174556\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02538277121151195, AUC: 0.6891629476858095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008619942052232209, AUC: 0.9522533391868461\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005961156385928261, AUC: 0.9694222393476071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005003327477833561, AUC: 0.9724740145095525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00044484035736117706, AUC: 0.9744911602483322\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004163205177757214, AUC: 0.975769510568433\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05218204548876937, AUC: 0.5200882869737234\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012341357136777939, AUC: 0.9413459078577091\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009134099297846145, AUC: 0.957234878513292\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007932038787107652, AUC: 0.9628794123747049\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007303605392570197, AUC: 0.9654476937054737\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006772539559694749, AUC: 0.9685471076172518\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11144608099093367, AUC: 0.4970879827173985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012798544826332463, AUC: 0.9464095648081765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009174323617013608, AUC: 0.9623785475076775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007707210512884374, AUC: 0.9664686368579692\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006851140879644337, AUC: 0.9693174867374773\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006220595279707871, AUC: 0.9711172313306108\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026863210341509652, AUC: 0.5326093822543814\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011917575205803242, AUC: 0.9345793629988515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000770702182902077, AUC: 0.9584640108984827\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006325043248951902, AUC: 0.9675285332423723\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005557038515694731, AUC: 0.9711519734023121\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005034788130741061, AUC: 0.9721513343587508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04944880516988734, AUC: 0.5448751759475373\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011464647393938394, AUC: 0.9459589706661109\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008268180610127532, AUC: 0.9582563480608134\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007021144432747538, AUC: 0.9642069806297159\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006328153999351961, AUC: 0.9672808643827439\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005838030436865045, AUC: 0.9677735701268717\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029839400570638244, AUC: 0.438127528670105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012490061098764803, AUC: 0.9373882068564005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008693551052512461, AUC: 0.9512768764140286\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007036268731356883, AUC: 0.9621142972047371\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006203433965509824, AUC: 0.9680328196770672\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005595930495743685, AUC: 0.9719084030543544\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06058202054134967, AUC: 0.4952911332969068\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00156577620040961, AUC: 0.9306771966727624\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011182888070566644, AUC: 0.9527726278797756\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009283897261439599, AUC: 0.9576473090159888\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008181276389507564, AUC: 0.9628162449716114\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007409939997782314, AUC: 0.9661980698147195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04213703055642811, AUC: 0.3377366277239626\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010968729656028813, AUC: 0.939857525922323\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007160497092746707, AUC: 0.9595818107357211\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005700983530159346, AUC: 0.9688721565456695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004979268562546477, AUC: 0.9730038311029976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004533155561912145, AUC: 0.974534061442933\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023272571738825815, AUC: 0.41138666136060487\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000997100019082047, AUC: 0.9405539465414267\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006881503376252227, AUC: 0.963953784622317\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000578609991543798, AUC: 0.969330120218096\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005258074101847234, AUC: 0.973239656074546\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000483202826851122, AUC: 0.9763435443440434\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07364484624204405, AUC: 0.44196494840802353\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004090056213461919, AUC: 0.8015674991077605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002165175100363505, AUC: 0.8941811767455522\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016152450155538995, AUC: 0.9242875506523613\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013415122348327533, AUC: 0.9344914550295466\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011881515510387868, AUC: 0.9437549546931802\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06778460479601432, AUC: 0.4419991640846991\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0050179521391400675, AUC: 0.761562529938717\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023880150803738164, AUC: 0.8696672235926041\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016636674560603947, AUC: 0.9065601453692503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013153170371939236, AUC: 0.9244057263356484\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011151328009028534, AUC: 0.9326224894905233\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.041213168209404086, AUC: 0.3057702369935685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005286693572998047, AUC: 0.7794186388056307\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027409689396427264, AUC: 0.8654234268947854\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018744986302915052, AUC: 0.9041426762133669\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014779320058268132, AUC: 0.9183882415984722\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012463914095565034, AUC: 0.9299239253908747\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04147439520199337, AUC: 0.5518480676564999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004410087960952401, AUC: 0.8249994472852229\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026273784974690643, AUC: 0.8860491631898276\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001958259876306995, AUC: 0.9126123721781279\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016113505076486535, AUC: 0.9269613741857985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014135855389711444, AUC: 0.9376958847489675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014557591995711393, AUC: 0.6791669693438065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033211133302534896, AUC: 0.8443952616025359\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020929118558202053, AUC: 0.8881918541422551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016371708506428843, AUC: 0.9167182533791929\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013832066667927077, AUC: 0.9285939883582476\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012284285030734656, AUC: 0.9366301980192808\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029450983062401712, AUC: 0.4425318758507859\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0057422029287058736, AUC: 0.7853758513123555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033644656265969584, AUC: 0.8724994920288002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024600433516932035, AUC: 0.8980514961725817\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020033188800104864, AUC: 0.910297550052271\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017253696330750488, AUC: 0.9239938222279774\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1079312596585385, AUC: 0.42378905456295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0045601498792869465, AUC: 0.7931220173141853\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023103818241652484, AUC: 0.8843352209858959\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016311287515141533, AUC: 0.9192567933910051\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013246938753436108, AUC: 0.9349728432806201\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011541802880716179, AUC: 0.9436925768826255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027998113259293116, AUC: 0.4217124261862575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030529620654726466, AUC: 0.8177778339265805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015952815737135756, AUC: 0.899009535119497\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011819131679982402, AUC: 0.9249713377908464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009921159479983686, AUC: 0.942621889400194\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008915504076496268, AUC: 0.9472178443702579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021893393948790738, AUC: 0.5464930510592647\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035629296781091776, AUC: 0.8425755139984228\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021244631621027757, AUC: 0.8984823505011806\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015682003928544123, AUC: 0.9235566511590692\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012791727076416639, AUC: 0.9402073154169522\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010976115889550877, AUC: 0.9491523460899904\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.048453058932193156, AUC: 0.5723714201190285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0042965038420188515, AUC: 0.8211378133761187\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0022894155002296966, AUC: 0.8917973968713186\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001648501813188778, AUC: 0.9122288934018489\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013262001440987948, AUC: 0.9261691496720033\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011221100099635586, AUC: 0.937507698527252\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030508711321019105, AUC: 0.5289132995808843\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021972631605576998, AUC: 0.5165467012402919\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017099089948387627, AUC: 0.546776514780646\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01381555458024747, AUC: 0.5947982169947686\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011457330716057564, AUC: 0.6374733249320688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00964582359575974, AUC: 0.6690349178876398\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.033029048469592745, AUC: 0.6946495630394891\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012708336104750025, AUC: 0.757416642715693\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008430247321415823, AUC: 0.7750182395876432\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006657595700935538, AUC: 0.7872448168513788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005619126086087504, AUC: 0.7989842155187571\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004905813366975236, AUC: 0.8082803516739888\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05230818602228927, AUC: 0.4313725645018145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021868263122703867, AUC: 0.4811100513024592\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014865283129451146, AUC: 0.5403687081318556\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01147134788017701, AUC: 0.603901429373053\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009246035280457893, AUC: 0.6557268620434444\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007692901912249501, AUC: 0.6925729346627966\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04286259479645927, AUC: 0.2835047801932291\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020339719376082487, AUC: 0.3043321257831442\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01112007081447836, AUC: 0.46356688427837034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00783354066254046, AUC: 0.58272350469597\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006340848390280248, AUC: 0.6578337581341192\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005444720422925693, AUC: 0.7033240266692776\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04449516978648113, AUC: 0.5233990484883514\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02287264463651022, AUC: 0.5653951068423985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01634159788555209, AUC: 0.598721175924376\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012903275760571513, AUC: 0.6324278286099907\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010511096421247921, AUC: 0.6683432348237682\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008696594679286714, AUC: 0.7014682210058988\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07004083803988201, AUC: 0.46759038465790115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025010158560868634, AUC: 0.5324880482009396\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011253401358408898, AUC: 0.5931995553014823\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007569931591906867, AUC: 0.636833754975749\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005978803819476727, AUC: 0.7005646639441516\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005040680782521756, AUC: 0.7405346278439806\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030126063201914674, AUC: 0.6401029312833405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017222464469694353, AUC: 0.6470458184758336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011905206461740732, AUC: 0.6860753776621112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00967485991435487, AUC: 0.7147123198544624\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008298010211621609, AUC: 0.738510638969866\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007265203950033022, AUC: 0.7568054980907652\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04061641647717613, AUC: 0.7035069489407353\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014235509167476318, AUC: 0.702918439301916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00919783824724796, AUC: 0.723705778869872\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007361199976594543, AUC: 0.741684274580279\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006198217981488313, AUC: 0.7690241794291139\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005397088914369896, AUC: 0.790317911011868\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02208544145186553, AUC: 0.36802276553207486\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013682943264346105, AUC: 0.45272972668517475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009543806061133444, AUC: 0.5470786655254423\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007169609530285806, AUC: 0.6314326788137583\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005708437312909917, AUC: 0.684776761133518\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004720027655089481, AUC: 0.734125242010113\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06172914839001668, AUC: 0.7066684774655554\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02189509376544038, AUC: 0.7358899813340325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013807057118342871, AUC: 0.739002555121455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01113542947830776, AUC: 0.7467005559784262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009584922038223904, AUC: 0.7615498964580985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00845640380947576, AUC: 0.7781734513721539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class normal\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=28*28)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 3, nums, (1, 1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48046771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.023154299261617807, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017943383884527212, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018413019682914374, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01711023603722742, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017268284613159553, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01713702355222368, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04787962212119934, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014930541237289912, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016697131302518042, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015544821639646279, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015678672634878356, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015578278713751633, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0425627564784014, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01631622969475947, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015250972889183898, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015519895751392789, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014511457311584203, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014731402625765212, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019105560396123766, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018576992648428534, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020078522216215786, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020063737508027337, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019745972966385795, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02000387815016948, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037100845435491754, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017765871402723617, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0176740174227043, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017197622242771576, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016828775000547885, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01646545718860075, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04365342654812218, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016199205652137713, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01749112597131194, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01686672806050736, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01798459065686031, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017641801649273273, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04335888807808449, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01376075057179212, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016456694150611437, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01713614024422032, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015653528189505092, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015646048282855467, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012400601194732573, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014408006509199146, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015839820895540355, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016178976796347038, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015204462053337213, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01553238463701743, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02560395036818664, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014324270920946418, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01473300905302243, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015137386095033378, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015710755782401387, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014856393950928316, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.034208128461218415, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01853762156134031, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018632386177423577, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019567562260217546, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018829471519389795, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01850177138405404, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03574992896180216, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01212720245133043, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01315978658884328, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01425475678610907, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015163800052459936, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015637148403464755, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026418177459727174, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012367947009987104, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.013084992062109176, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013867020242192316, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014506760279121386, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015002217595976901, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021894103448758195, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009863119958736831, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010459990049048939, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01154608079585361, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012281122113603672, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012492890343379099, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0565180799580885, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01791835877004427, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01705087864735385, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017204718539521064, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017328711117015165, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01736665963721251, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03330282012202715, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015470939280831947, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014213569145630671, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013889335118682561, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014060035687407357, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01432392752036802, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.047126083866911896, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014827567109928696, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012013516106681733, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012269936139710863, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012890241130684882, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013546697868858216, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013514959743257202, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008423406386286253, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009579121195025608, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01142315895368517, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012573235377856802, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013724613092416649, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024384227831807114, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01166954222117829, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011586277471112377, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011778062316520973, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012186606860168935, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012769650456371132, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0493032188895445, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011250147321766553, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010857144978045607, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011739460070576647, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0126929401500174, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013470452239585547, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09620342235182548, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013561697891479202, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01282767704740263, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012729149292780809, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013026695030962111, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013286730660260515, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0868963781808842, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019827336528924645, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014293852893319595, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01340311427215296, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013216361428798765, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013327604732883094, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021214069131684523, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012730788795447845, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011135720023084198, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011098879204847017, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011689675417695472, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012012369492474724, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04460664170001567, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016985538823796713, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013142349038226229, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01207873659934855, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.011767812182164768, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011882062754392705, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10131313735147351, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01933097774501724, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012585519325648064, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0112442731938367, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011200005000483135, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011332142884047896, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024835580711014535, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01710791394889699, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01463944325841069, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014578280340283552, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014363177006684854, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014481733543294824, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04428664683809251, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019642600637031875, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014522003503285148, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013374838039446186, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01301107597934758, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013106934678104611, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0517595856013812, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018609298244296024, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014747213320195452, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014207856503525221, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014106670393906496, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014184415766026285, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04548361089836777, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017205059670218396, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012664537128239657, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01221269914302158, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012775290462282473, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013078333528136688, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03431478044444385, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01009097062013296, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010084921711361031, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011227694631068734, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01235725295174815, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012999389658814099, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.033813154887926085, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012515346601032878, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010761097230622655, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010842424475064451, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011278900553765892, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011874063693516436, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.062487878835108035, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.040919195264020526, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03476438421815563, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029781150882724837, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026299069666286912, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023732515145061535, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.038772903385311516, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03967657935574767, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030441164200259732, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02328331605871694, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018837672923949656, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016096511368685052, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012089235447815833, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018461560116702705, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0162955630761918, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01440351117188247, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01333779974155627, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012758917810154058, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.043301903686730966, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0358791072447257, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027289035489712392, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021487280447764365, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018175935793879567, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016159774575335604, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022462819555672364, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.030293819734896983, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02427670810547705, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02042776222579214, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01808262941424679, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016674943215098766, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04028465584888218, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.033884433044620375, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025282562447826458, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019476369464125856, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016028725910413755, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01389523680945393, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0473771417923259, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04331913273255868, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03530198674103063, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029157379134825728, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02462443378011372, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021130052402448345, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02757990388633523, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01862168401246653, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015092246591619228, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012694692173931559, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011293988128942278, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010465405985272203, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08726584477637017, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05645865317470806, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030919177156854997, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019639843704342963, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015781634440028072, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014188786584153058, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03966141136192781, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.032521492686655926, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025700714100951524, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021627804244792773, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019452655043824114, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018543051441118695, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07021289502467805, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.067028487956836, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06387287854580845, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06084324104856448, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05785962636273802, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05503052961498003, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05349864914319175, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04197147969930442, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03754725710788741, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03558209136163892, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03422026648808401, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03311522742267856, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04530235635549266, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.034162372847229235, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030043585452041183, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03072465807756815, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03247036264284011, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.033800206837788685, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05026472883539028, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028659683715239224, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022313358264081293, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020835842092359037, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019937053500126513, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01911315866085754, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012955346832223508, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011223363697994976, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01238357968088803, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014130705191707255, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01563564314804933, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016737422339975407, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07299215556082779, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.055911421167814015, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.051641353333494934, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0505487960198022, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.04982165194254917, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.048844181490122804, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026966390055241693, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015898222665485174, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012138242504297573, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012216264477638678, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012914014892163062, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013398969469326607, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03803188190375895, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025261686027394554, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01842607668734294, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01748198451172207, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018674494416808563, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019811911065738163, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05542861097972031, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05365113845612475, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0519137221994306, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05013723674982999, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04842493834847376, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04667036438507111, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03425382311268754, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022049055906916425, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018701360729104683, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019553429332163738, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021029881902472253, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02225212465537239, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class ratio\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=28*28)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network, average=False) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network, average=False)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 3, nums,ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "187a1744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.025290647115645788, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017568756228034815, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01691412220435741, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017181602070745827, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017222426205984957, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01736546016720354, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021052315461315373, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01802598420797826, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017972769116269046, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01782666076328105, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018057279382502695, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018472720797635064, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12931527737977755, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01767381345767709, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017261151025506842, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01712259099986917, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016897279362579627, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01713708840596842, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06277171514018869, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014950031408927993, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014906333928528144, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015037120584321242, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014996703406330357, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015219069710802521, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04206766558520367, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01822596721525948, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01816720194007539, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01815525458321933, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01813099733707411, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018010287007601315, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08045298742704836, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018362285246941477, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018709490794545328, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018714582462044892, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018561135259302083, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018786270494405934, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05609284173627566, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01826003034760943, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017726171182880838, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017888269875172003, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018036656540212727, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01810940104007559, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018191951426143185, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020544912733216143, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019371194200668154, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019309466406424326, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019633248078502873, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01964755416605514, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04443763154414753, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019324134377872244, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018838124954550495, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018950468502414344, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018772227713057314, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0187891389755358, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015380985839802243, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017173513511701814, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01669203509525635, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016562913883628183, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016270237688870707, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016292211110394896, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.046419097630599045, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01871119125973242, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0188148230040653, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018621347421531327, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01819162780595693, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017805755158664012, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03256971339148269, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017024112534093354, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.017295777088361143, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016930734166803914, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016982364232991674, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01682534748013836, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04028489969896241, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018842081032979654, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017563504371785745, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017517148860657715, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017289090018740967, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017103540212351706, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09677207295645747, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017804452895469627, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016770892384829712, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015267121471624234, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014834606011114086, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014226241243084204, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018808508194939937, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015224616975858883, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01455466936170467, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014864592112800938, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014465821677022789, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014072890672421057, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04754889785574634, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018068452385646555, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016862762578285215, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016026726793732137, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015245224253250117, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014995587273383376, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.031445645588832984, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018003316527116626, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01733404987728219, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017167351108876593, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017069868796619984, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016489846412116514, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015167908537189233, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01564659175723639, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013907491573060057, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014341252225469222, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01408829408533433, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013891797291101626, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.049816653015904586, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019362934427738027, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018708691147872682, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017944596649553855, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01787201039282816, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017519318866956724, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.050861339186454704, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02088785349856587, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02001165930246342, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01952140844423566, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019926128886825007, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019084239079002942, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.052617836711926345, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016231138987510392, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016338008555049435, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017584831525743597, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017597096017411436, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017317482988188276, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04786161093921006, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021336640115093918, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021080293366796343, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02107589628290295, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020169940054801726, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019655839231945714, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04408546144238381, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015892948434694167, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01608870232928087, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01637249583088999, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.016319567068463804, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016454276727276243, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04022635362943554, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01654918791282262, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016925641202229457, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01720189999577465, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017024854460933183, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01626286511516863, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.043015653525271084, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01785901435091965, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018854844249944547, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0185376309651333, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018477219053370252, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018700048737848588, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012878507142649018, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018294213459387153, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018850299299513148, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01866459578974885, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01823274897134697, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017835997756911314, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07084449786225455, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016285653423995478, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01638466401474689, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0167558112950278, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017675661789402188, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01779790137017596, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024993150666310488, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017941889005710948, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018554728357859186, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018832967784768746, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019661905002691275, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019302434963419584, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0638191744568639, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015926154849076102, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017845489750991504, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017518712808712127, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017631519736582144, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017315649572824467, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08749144501282381, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019500975623417776, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019328059975041372, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01939016456954214, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01931066166093989, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019459557427876827, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024687836098046742, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011777526948858143, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012414517835547673, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013627981012753587, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014186989811155027, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014403120428070411, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0882550900746916, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025347979123071115, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02355991728003436, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022418653506966767, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02228045666229964, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022236151826580296, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030309443650542372, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020864022341523597, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021683619323128626, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022343882976118216, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022920498269141755, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022907065112993695, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07081717368252056, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013287982013265278, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014582038939059976, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015600804689181009, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016230846172852892, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016671120086846323, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04250590698283694, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.015923848649913135, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014333278457553401, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014466521773197261, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014497019895198839, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015385938720288062, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05278348744381694, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01693824747313208, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016431114536610968, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01668852748047207, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016844343790510242, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017165428195345042, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014012926251496721, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014701559202316788, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015861661042619102, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016663142103437095, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01733516536169172, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017923704664223213, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025954449188948406, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014645765849985422, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016394998458971258, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017542185000277263, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017854735300841685, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017875590319537824, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037066619520242504, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020777570354172423, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02030453167979874, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02038576942607279, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02040180062323515, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01986664632922409, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.039888473401787895, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01610959236899917, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01662444906225013, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017376355920588633, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017778163781501696, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017896223538426955, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02250263845786802, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020433190484226905, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01896238732362272, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018034383092190853, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017431316226568808, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01701152701638905, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07003938272185004, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.042086278609944135, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03156454585029008, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02706494699404865, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024913348781944983, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023565994724453976, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05569769176079764, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03548302467239831, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02602938120562136, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022003844415845616, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019823394069781883, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018380583082482506, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06844744897950714, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.044082251182019166, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03246817058626789, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02628018425581853, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022903813994769862, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02071926035227641, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.034752212467342766, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025361473241739232, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020576539310376202, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01792074344223837, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01641214773469294, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01541209002004842, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0166845178977035, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01269673247274109, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011826196441598508, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.011508933543672533, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011337766027985604, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011173233698923058, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026033830172510545, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021426763472934842, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02122045268577931, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02061967353600298, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019305837839406106, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017935166915068614, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03707833470069239, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025680708017774036, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020761006707765747, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018798389713685554, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01826716555660576, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018052958177490648, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06533820869889401, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.038688900923899185, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028933587669312893, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024406759319804794, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02181420428377558, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020259300084067056, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04933356006548381, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.031765059361203274, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024698744497912064, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021945670606813558, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020339959983475472, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019339549464459566, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class oversampled\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=28*28)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network, average=False) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network, average=False)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5ddcb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.03402413838739016, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018554496181453016, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018292381989635686, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016142242662519957, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017255459136599708, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017447444055810828, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12580166232703455, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018920410737339553, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019741330735337285, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020207585708011132, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02041935393778656, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020234095324711182, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04277468813961357, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014704239931855628, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015221197886436175, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017247485692952612, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01732458901299947, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017534929811853166, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017921597566056602, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015153560294702887, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01689797427045452, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017323098025407893, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018447994010377926, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01768256676760469, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03657850818705372, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01740449985165614, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018765070453787773, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017835905016056336, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019059244445780027, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018154863515138547, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02898632738307316, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01779132130923461, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01790937587786029, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018191941698081474, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018392336526641795, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019521921437356555, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0454633998773569, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015814275654024594, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017992796925451027, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017099564677799124, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019407012275350778, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019138328399191256, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04643373512078693, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01738888112431033, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018357137805221437, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018298064474749832, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017877724656277228, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016135865269530918, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012203829096353447, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015354423043031508, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017858593450116284, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01800106091387456, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017543633832934762, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017701422993887935, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04721964966152364, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01676842632118596, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018306060292938867, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017377854690629583, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017284657265287967, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0183088859705972, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05603071444296424, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01736220223933651, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016022898152911393, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016267318621819946, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017201485580345757, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017724310528944236, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04209434122749025, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018006476850097822, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0156063701132859, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01549046544629836, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01540326569527033, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016356523974904072, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.061373640914548624, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018350040859934507, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011931814007595338, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011822794862686878, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012373820165110785, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012946896389283846, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03476313772918172, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015643680148366274, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013396708992053684, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013125888185004967, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013160181548148749, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013379667373548272, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04345372889731782, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02573345356837133, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021449850108987496, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020889618817497704, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01989639214778668, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01960433952178488, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028256964010675113, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015984708701052338, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015064429847693938, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015966027904148614, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015417200495133955, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015673961334591698, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00978926127153933, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01239496637712081, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013161519805171464, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013172896773342534, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013080754842696567, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01436102061967224, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025958856000903512, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016855709733544708, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01437713969365872, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014808459251115859, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015219324910288075, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01580217232391243, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.045972052985654724, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018988236080663863, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016216141559039197, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016066769441347144, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01677481182089309, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017248271720338864, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0690718167333204, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018390458362194113, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014087308308414277, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013431795840764362, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013211476969013649, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013393015247022, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06125606107533444, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030719876410968447, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0225399540055167, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020623282646897778, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019168687085641643, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018556626626967734, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08406912068532706, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03333288048125173, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025908434159980263, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02303137352822468, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02161484646659366, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020755455227215978, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05422013927097833, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02307504279724557, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018567154983888876, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017008478890386255, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.01681691130502254, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01665051961909829, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03294088499191788, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024021263472768815, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019416982890067155, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01742462072596182, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017294725160621454, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017194771272152794, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0387162173211838, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01752713179758559, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016053043794810307, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015495744216851497, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015617469831698204, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01607806469379984, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.039735211032217596, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015487924476579434, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012912896489335014, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013099537784248256, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012856585928389347, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013108812842552291, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05465634944932633, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021777039845352147, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015242652802271163, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0131787177212017, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012721212167230767, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013254513589754595, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02744650621877889, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01728843207750058, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013831143799139424, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012522059287882224, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01330318664945092, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013414212044952922, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027803320497203465, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020451601813821232, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01875581582441333, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018420327402871712, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01792209953404089, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01791686194561566, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009659149743228979, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012434326763341151, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01267193726478325, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012784374526308248, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012578288457378243, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01310631337923, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05202875198940159, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03277050512496082, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024856581002098897, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021022936714948013, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019146652377328675, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018142302641857243, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021751554269281548, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020688906854125604, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019887275656888536, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019091671318150184, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01828135036765536, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017605754640871715, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05460736671299887, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03675555891019477, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028143094454197524, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02380568184280266, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02133757336048073, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019918961250956307, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05650318305292165, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03873098062763649, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027888735473176243, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022324558505797946, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01971172564940078, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018093620826905375, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026359896642016295, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.02014952798718216, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017043732088952192, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015431308130227316, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014530877765446688, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01395583939122651, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03488179932885817, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03196184897009348, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029184859612084536, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027319251497924347, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025626722463577306, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024011451749727054, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018461942105259227, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017538208168649788, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01671714062838277, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016301061700939604, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01594447084366566, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015747335240047265, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07093914022902897, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0526334327404615, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.039924744804146255, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.031285376420680945, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025568737378942444, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021948133103501348, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021395141435536915, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02104408292695804, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02055944516033755, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01997350325028291, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01964976438167589, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018927374732449768, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021956721684917305, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021771395623947396, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021769636141852594, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021508003813358684, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02097937834583063, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02036160038102042, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04852241093266865, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.046877870426742206, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0453640296725909, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.043969850562859944, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.042550091839128515, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.041257574865502675, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012750580861267692, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012575900542496906, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012389057876706244, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012190548995042974, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012047885024438137, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011894572393215664, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06297580987488643, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06126290314540454, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05968163914114632, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05809946069908726, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.056572701078983684, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05505450085611094, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03186408389550981, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030905649962777063, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03009835833069906, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029314570344252392, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02863403600480677, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02795132452515022, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05378657642249062, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05132023779886266, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04896413886761917, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04667267577253366, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04445644302134691, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.042333301335360396, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020331268284605377, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020400302501731648, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020449090028287438, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02057736554708419, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.020648043158102182, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020679548458759578, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02599310266935108, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02461816372007223, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023395789962607716, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022309020197095616, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021276916301562566, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02030632099461612, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04432367313803965, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.042547034633601453, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04095827613060104, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03945275351775013, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.038058849387896866, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03676856986846457, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05961623024835103, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05819517834094948, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05679998880987547, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.055439135445416766, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05416069951684894, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.052775387211098874, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06336874055521945, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.061659961118052824, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06000675096352949, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.058323331934706124, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05689565975704148, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.055412029174265104, AUC: [[2.16158446e-314 2.39347716e-314]\n",
      " [5.73386054e-314 2.39347731e-314]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class undersampled\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=28*28)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network, average=False) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network, average=False)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "089f6544",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m learning_rate_aucs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m loss_fn_args\u001b[38;5;241m=\u001b[39m{}\n\u001b[0;32m----> 8\u001b[0m loss_fn_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mclass_weights\u001b[49m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m learning_rate \u001b[38;5;129;01min\u001b[39;00m learning_rates:\n\u001b[1;32m     12\u001b[0m     aucs \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'class_weights' is not defined"
     ]
    }
   ],
   "source": [
    "# 3 class weighted\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['weight'] = torch.from_numpy(class_weights).float()\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=28*28)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network, average=False) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network, average=False)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439e0fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 class focal loss\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['reduction'] = 'mean'\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=28*28)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network, average=False) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SoftmaxFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network, average=False)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e7be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 class smote\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=28*28)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network, average=False) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network, average=False)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18a510e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
