{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b520ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc6a2164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logistic_regression\n",
    "import reduceClasses\n",
    "import binaryRatio\n",
    "import train_with_dir\n",
    "import test \n",
    "import get_confidence_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82986e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f35e2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x108b19b30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = 10\n",
    "NUM_CLASSES_REDUCED = 2\n",
    "n_epochs = 20\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "\n",
    "ratio = (100, 1) # (10, 1)? \n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e32fb88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mnist = torchvision.datasets.MNIST('mnist', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "\n",
    "test_mnist = torchvision.datasets.MNIST('mnist', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a59eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_train_mnist = reduceClasses.Reduce(train_mnist, NUM_CLASSES_REDUCED)\n",
    "reduced_test_mnist = reduceClasses.Reduce(test_mnist, NUM_CLASSES_REDUCED)\n",
    "\n",
    "\n",
    "reduced_train_mnist_ratio = binaryRatio.Ratio(train_mnist, 2, ratio) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f647b327",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_reduced = DataLoader(reduced_train_mnist, batch_size=batch_size_train, shuffle=False) \n",
    "train_loader_reduced_ratio = DataLoader(reduced_train_mnist_ratio, batch_size=batch_size_train, shuffle=False)\n",
    "train_loader_normal = DataLoader(train_mnist, batch_size=batch_size_train, shuffle=False)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_mnist, batch_size=batch_size_test, shuffle=False) \n",
    "test_loader_normal = DataLoader(test_mnist, batch_size=batch_size_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aebb3fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.391519\n",
      "Train Epoch: 0 [640/60000 (1%)]\tLoss: 1.264056\n",
      "Train Epoch: 0 [1280/60000 (2%)]\tLoss: 0.899669\n",
      "Train Epoch: 0 [1920/60000 (3%)]\tLoss: 0.661826\n",
      "Train Epoch: 0 [2560/60000 (4%)]\tLoss: 0.528469\n",
      "Train Epoch: 0 [3200/60000 (5%)]\tLoss: 0.524239\n",
      "Train Epoch: 0 [3840/60000 (6%)]\tLoss: 0.430666\n",
      "Train Epoch: 0 [4480/60000 (7%)]\tLoss: 0.408026\n",
      "Train Epoch: 0 [5120/60000 (9%)]\tLoss: 0.749220\n",
      "Train Epoch: 0 [5760/60000 (10%)]\tLoss: 0.416634\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.408048\n",
      "Train Epoch: 0 [7040/60000 (12%)]\tLoss: 0.414640\n",
      "Train Epoch: 0 [7680/60000 (13%)]\tLoss: 0.446291\n",
      "Train Epoch: 0 [8320/60000 (14%)]\tLoss: 0.360250\n",
      "Train Epoch: 0 [8960/60000 (15%)]\tLoss: 0.369983\n",
      "Train Epoch: 0 [9600/60000 (16%)]\tLoss: 0.422143\n",
      "Train Epoch: 0 [10240/60000 (17%)]\tLoss: 0.590449\n",
      "Train Epoch: 0 [10880/60000 (18%)]\tLoss: 0.299529\n",
      "Train Epoch: 0 [11520/60000 (19%)]\tLoss: 0.627557\n",
      "Train Epoch: 0 [12160/60000 (20%)]\tLoss: 0.485572\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.374315\n",
      "Train Epoch: 0 [13440/60000 (22%)]\tLoss: 0.265171\n",
      "Train Epoch: 0 [14080/60000 (23%)]\tLoss: 0.599060\n",
      "Train Epoch: 0 [14720/60000 (25%)]\tLoss: 0.678561\n",
      "Train Epoch: 0 [15360/60000 (26%)]\tLoss: 0.316214\n",
      "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 0.565068\n",
      "Train Epoch: 0 [16640/60000 (28%)]\tLoss: 0.495589\n",
      "Train Epoch: 0 [17280/60000 (29%)]\tLoss: 0.228640\n",
      "Train Epoch: 0 [17920/60000 (30%)]\tLoss: 0.323359\n",
      "Train Epoch: 0 [18560/60000 (31%)]\tLoss: 0.440105\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.419452\n",
      "Train Epoch: 0 [19840/60000 (33%)]\tLoss: 0.292677\n",
      "Train Epoch: 0 [20480/60000 (34%)]\tLoss: 0.253476\n",
      "Train Epoch: 0 [21120/60000 (35%)]\tLoss: 0.320642\n",
      "Train Epoch: 0 [21760/60000 (36%)]\tLoss: 0.120480\n",
      "Train Epoch: 0 [22400/60000 (37%)]\tLoss: 0.328457\n",
      "Train Epoch: 0 [23040/60000 (38%)]\tLoss: 0.504898\n",
      "Train Epoch: 0 [23680/60000 (39%)]\tLoss: 0.596332\n",
      "Train Epoch: 0 [24320/60000 (41%)]\tLoss: 0.270609\n",
      "Train Epoch: 0 [24960/60000 (42%)]\tLoss: 0.371619\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.285583\n",
      "Train Epoch: 0 [26240/60000 (44%)]\tLoss: 0.372914\n",
      "Train Epoch: 0 [26880/60000 (45%)]\tLoss: 0.517124\n",
      "Train Epoch: 0 [27520/60000 (46%)]\tLoss: 0.221134\n",
      "Train Epoch: 0 [28160/60000 (47%)]\tLoss: 0.373895\n",
      "Train Epoch: 0 [28800/60000 (48%)]\tLoss: 0.217079\n",
      "Train Epoch: 0 [29440/60000 (49%)]\tLoss: 0.250801\n",
      "Train Epoch: 0 [30080/60000 (50%)]\tLoss: 0.487548\n",
      "Train Epoch: 0 [30720/60000 (51%)]\tLoss: 0.451506\n",
      "Train Epoch: 0 [31360/60000 (52%)]\tLoss: 0.566454\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.392212\n",
      "Train Epoch: 0 [32640/60000 (54%)]\tLoss: 0.430564\n",
      "Train Epoch: 0 [33280/60000 (55%)]\tLoss: 0.381347\n",
      "Train Epoch: 0 [33920/60000 (57%)]\tLoss: 0.178862\n",
      "Train Epoch: 0 [34560/60000 (58%)]\tLoss: 0.356837\n",
      "Train Epoch: 0 [35200/60000 (59%)]\tLoss: 0.375125\n",
      "Train Epoch: 0 [35840/60000 (60%)]\tLoss: 0.305694\n",
      "Train Epoch: 0 [36480/60000 (61%)]\tLoss: 0.303614\n",
      "Train Epoch: 0 [37120/60000 (62%)]\tLoss: 0.419039\n",
      "Train Epoch: 0 [37760/60000 (63%)]\tLoss: 0.324151\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.246208\n",
      "Train Epoch: 0 [39040/60000 (65%)]\tLoss: 0.172362\n",
      "Train Epoch: 0 [39680/60000 (66%)]\tLoss: 0.337878\n",
      "Train Epoch: 0 [40320/60000 (67%)]\tLoss: 0.274699\n",
      "Train Epoch: 0 [40960/60000 (68%)]\tLoss: 0.448082\n",
      "Train Epoch: 0 [41600/60000 (69%)]\tLoss: 0.277689\n",
      "Train Epoch: 0 [42240/60000 (70%)]\tLoss: 0.252125\n",
      "Train Epoch: 0 [42880/60000 (71%)]\tLoss: 0.490200\n",
      "Train Epoch: 0 [43520/60000 (72%)]\tLoss: 0.332981\n",
      "Train Epoch: 0 [44160/60000 (74%)]\tLoss: 0.225066\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.449534\n",
      "Train Epoch: 0 [45440/60000 (76%)]\tLoss: 0.680374\n",
      "Train Epoch: 0 [46080/60000 (77%)]\tLoss: 0.523756\n",
      "Train Epoch: 0 [46720/60000 (78%)]\tLoss: 0.413750\n",
      "Train Epoch: 0 [47360/60000 (79%)]\tLoss: 0.327124\n",
      "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 0.198700\n",
      "Train Epoch: 0 [48640/60000 (81%)]\tLoss: 0.295212\n",
      "Train Epoch: 0 [49280/60000 (82%)]\tLoss: 0.214101\n",
      "Train Epoch: 0 [49920/60000 (83%)]\tLoss: 0.297974\n",
      "Train Epoch: 0 [50560/60000 (84%)]\tLoss: 0.446539\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.435508\n",
      "Train Epoch: 0 [51840/60000 (86%)]\tLoss: 0.234210\n",
      "Train Epoch: 0 [52480/60000 (87%)]\tLoss: 0.146901\n",
      "Train Epoch: 0 [53120/60000 (88%)]\tLoss: 0.416313\n",
      "Train Epoch: 0 [53760/60000 (90%)]\tLoss: 0.151265\n",
      "Train Epoch: 0 [54400/60000 (91%)]\tLoss: 0.286268\n",
      "Train Epoch: 0 [55040/60000 (92%)]\tLoss: 0.279606\n",
      "Train Epoch: 0 [55680/60000 (93%)]\tLoss: 0.389960\n",
      "Train Epoch: 0 [56320/60000 (94%)]\tLoss: 0.233302\n",
      "Train Epoch: 0 [56960/60000 (95%)]\tLoss: 0.340218\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.451240\n",
      "Train Epoch: 0 [58240/60000 (97%)]\tLoss: 0.137249\n",
      "Train Epoch: 0 [58880/60000 (98%)]\tLoss: 0.061280\n",
      "Train Epoch: 0 [59520/60000 (99%)]\tLoss: 0.075249\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9063/10000 (91%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.246069\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.368521\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.390758\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.224682\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.212543\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.284067\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.137218\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.229660\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.588564\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.225056\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.271461\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.218576\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.273756\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.184197\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.249885\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.321270\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.502842\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.192247\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.575668\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.461249\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.229961\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.204620\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.499726\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.624610\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.191849\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.470697\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.421706\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.147273\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.273384\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.382151\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.346907\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.204644\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.198499\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.292199\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.088169\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.262757\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.454473\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.564171\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.206460\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.304238\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.233281\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.323621\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.474124\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.228801\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.288470\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.167131\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.189844\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.407031\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.378888\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.489107\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.360178\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.410049\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.373093\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.155084\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.315477\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.329127\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.260131\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.256863\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.403658\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.281571\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.207494\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.150334\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.300181\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.269003\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.443696\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.226933\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.222455\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.406453\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.326287\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.186827\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.411546\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.642186\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.489366\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.402514\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.289337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.172809\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.281974\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.207006\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.263407\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.389676\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.397663\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.200177\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.126119\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.404812\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.131283\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.264004\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.245760\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.383820\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.202636\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.300903\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.442992\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.124083\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.055349\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.066162\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9123/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.208912\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.349531\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.365097\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.205246\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.192188\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.281523\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.113111\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.215528\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.556448\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.215380\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.260346\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.198700\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.253884\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.171077\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.230557\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.299305\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.451687\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.179611\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.542470\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.454845\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.202932\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.196784\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.451837\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.626144\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.168075\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.454588\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.401709\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.135332\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.256643\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.364643\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.323707\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.186230\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.188301\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.286248\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.080954\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.244660\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.439868\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.550678\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.189317\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.287179\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.218687\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.312339\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.469904\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.233300\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.266593\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.155706\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.168600\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.389520\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.356305\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.460176\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.350245\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.410269\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.378863\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.145349\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.296381\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.315409\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.243684\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.241311\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.398844\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.269297\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.193144\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.144658\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.291260\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.270617\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.442300\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.206464\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.215829\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.369302\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.328165\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.174189\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.395395\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.622427\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.472577\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.400799\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.272013\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.162075\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.278196\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.203711\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.250522\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.362324\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.377613\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.184180\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.120134\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.402594\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.123412\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.252184\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.227389\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.379787\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.190311\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.280153\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.432966\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.118981\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.052323\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.062453\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9148/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.191086\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.343257\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.350229\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.196055\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.183583\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.280235\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.101423\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.206373\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.539961\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.213979\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.256667\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.188632\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.242661\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.166934\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.221591\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.289845\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.421317\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.172711\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.519567\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.448331\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.190724\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.193775\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.418768\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.626602\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.156281\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.448077\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.391260\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.131134\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.247629\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.355529\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.310849\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.177338\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.182507\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.281402\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.077123\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.234586\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.431746\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.540569\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.180335\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.279276\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.210621\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.306308\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.471545\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.234770\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.256442\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.150338\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.156308\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.381690\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.345000\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.442471\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.345636\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.414145\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.384877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.138774\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.283136\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.308743\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.233359\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.233128\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.396185\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.264195\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.184802\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.142904\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.287818\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.272144\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.440608\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.194314\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.213023\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.349038\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.329433\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.167231\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.386107\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.609218\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.463032\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.399997\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.260932\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.155654\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.276498\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.200868\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.243575\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.345494\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.363272\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.174387\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.118005\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.402464\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.118997\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.243048\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.215399\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.376596\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.183011\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.266585\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.424248\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.116294\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.050223\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.060239\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9159/10000 (92%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.180080\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.339804\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.340417\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.190289\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.178487\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.278791\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.094144\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.199071\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.529035\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.214859\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.254747\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.181932\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.234760\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.164837\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.216204\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.284800\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.400380\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.167876\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.502619\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.442760\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.183754\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.191864\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.395317\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.626015\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.148809\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.444628\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.384525\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.129217\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.242096\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.349629\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.302132\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.172273\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.178254\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(network\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[1;32m      4\u001b[0m                   momentum\u001b[38;5;241m=\u001b[39mmomentum)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m----> 6\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_with_dir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_normal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogistic_regression_results/normal\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/model\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     _, _, _ \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mtest(test_loader_normal, network)\n",
      "File \u001b[0;32m~/Downloads/ML/numbers mnist/train_with_dir.py:14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose)\u001b[0m\n\u001b[1;32m     11\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m network\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m     output \u001b[38;5;241m=\u001b[39m network(data)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 94\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py:134\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torchvision/transforms/functional.py:164\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    163\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[0;32m--> 164\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    167\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    network = logistic_regression.Net(NUM_CLASSES)\n",
    "    optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "    for epoch in range(n_epochs):\n",
    "        _, _ = train_with_dir.train(epoch, train_loader_normal, network, optimizer, f'logistic_regression_results/normal{i}/model{epoch}')\n",
    "        _, _, _ = test.test(test_loader_normal, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e8bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    network = logistic_regression.Net(NUM_CLASSES_REDUCED)\n",
    "    optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "    for epoch in range(n_epochs):\n",
    "        _, _ = train_with_dir.train(epoch, train_loader_reduced, network, optimizer, f'logistic_regression_results/reduced{i}/model{epoch}')\n",
    "        _, _, _ = test.test(test_loader_reduced, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0887ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    network = logistic_regression.Net(NUM_CLASSES_REDUCED)\n",
    "    optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "    for epoch in range(n_epochs): \n",
    "        _, _ = train_with_dir.train(epoch, train_loader_reduced_ratio, network, optimizer, f'logistic_regression_results/reduced_ratio{i}/model{epoch}')\n",
    "        _, _, _ = test.test(test_loader_reduced, network)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "994d3589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0417, Accuracy: 990/2115 (47%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caralee/Downloads/ML/numbers mnist/get_confidence_interval.py:5: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  y_pred = np.asarray(y_pred)\n",
      "/Users/caralee/Downloads/ML/numbers mnist/get_confidence_interval.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_pred = np.asarray(y_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.46808510638297873 [0.448 - 0.491]\n",
      "\n",
      "Test set: Avg. loss: 0.0433, Accuracy: 2105/2115 (100%)\n",
      "\n",
      "Accuracy: 0.9952718676122931 [0.992 - 0.998]\n",
      "\n",
      "Test set: Avg. loss: 0.0059, Accuracy: 2110/2115 (100%)\n",
      "\n",
      "Accuracy: 0.9976359338061466 [0.995 - 1.0]\n",
      "\n",
      "Test set: Avg. loss: 0.0404, Accuracy: 2104/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9947990543735225 [0.991 - 0.998]\n",
      "\n",
      "Test set: Avg. loss: 0.1013, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1280, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1280, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1280, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1280, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1280, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1280, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1280, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1280, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1280, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1280, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1280, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1280, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1280, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1280, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1280, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1280, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0594, Accuracy: 344/2115 (16%)\n",
      "\n",
      "Accuracy: 0.16264775413711585 [0.147 - 0.178]\n",
      "\n",
      "Test set: Avg. loss: 0.0879, Accuracy: 2094/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9900709219858156 [0.986 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.0349, Accuracy: 2093/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9895981087470449 [0.985 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.0231, Accuracy: 2096/2115 (99%)\n",
      "\n",
      "Accuracy: 0.991016548463357 [0.987 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0255, Accuracy: 2098/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9919621749408983 [0.988 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0255, Accuracy: 2098/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9919621749408983 [0.988 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0255, Accuracy: 2098/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9919621749408983 [0.988 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0255, Accuracy: 2098/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9919621749408983 [0.988 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0255, Accuracy: 2098/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9919621749408983 [0.988 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0255, Accuracy: 2098/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9919621749408983 [0.988 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0255, Accuracy: 2098/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9919621749408983 [0.988 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0255, Accuracy: 2098/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9919621749408983 [0.988 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0255, Accuracy: 2098/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9919621749408983 [0.988 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0255, Accuracy: 2098/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9919621749408983 [0.988 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0255, Accuracy: 2098/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9919621749408983 [0.988 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0255, Accuracy: 2098/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9919621749408983 [0.988 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0255, Accuracy: 2098/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9919621749408983 [0.988 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0255, Accuracy: 2098/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9919621749408983 [0.988 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0255, Accuracy: 2098/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9919621749408983 [0.988 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0255, Accuracy: 2098/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9919621749408983 [0.988 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0255, Accuracy: 2098/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9919621749408983 [0.988 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0278, Accuracy: 769/2115 (36%)\n",
      "\n",
      "Accuracy: 0.36359338061465724 [0.344 - 0.385]\n",
      "\n",
      "Test set: Avg. loss: 0.1846, Accuracy: 2072/2115 (98%)\n",
      "\n",
      "Accuracy: 0.9796690307328605 [0.974 - 0.985]\n",
      "\n",
      "Test set: Avg. loss: 0.0198, Accuracy: 2109/2115 (100%)\n",
      "\n",
      "Accuracy: 0.9971631205673759 [0.995 - 0.999]\n",
      "\n",
      "Test set: Avg. loss: 0.0508, Accuracy: 2104/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9947990543735225 [0.991 - 0.998]\n",
      "\n",
      "Test set: Avg. loss: 0.1004, Accuracy: 2095/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9905437352245863 [0.986 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.1319, Accuracy: 2094/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9900709219858156 [0.985 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.1429, Accuracy: 2091/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9886524822695035 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1429, Accuracy: 2091/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9886524822695035 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1429, Accuracy: 2091/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9886524822695035 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1429, Accuracy: 2091/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9886524822695035 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1429, Accuracy: 2091/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9886524822695035 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1429, Accuracy: 2091/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9886524822695035 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1429, Accuracy: 2091/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9886524822695035 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1429, Accuracy: 2091/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9886524822695035 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1429, Accuracy: 2091/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9886524822695035 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1429, Accuracy: 2091/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9886524822695035 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1429, Accuracy: 2091/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9886524822695035 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1429, Accuracy: 2091/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9886524822695035 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1429, Accuracy: 2091/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9886524822695035 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1429, Accuracy: 2091/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9886524822695035 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1429, Accuracy: 2091/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9886524822695035 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0342, Accuracy: 1406/2115 (66%)\n",
      "\n",
      "Accuracy: 0.6647754137115839 [0.645 - 0.684]\n",
      "\n",
      "Test set: Avg. loss: 0.6468, Accuracy: 1997/2115 (94%)\n",
      "\n",
      "Accuracy: 0.9442080378250591 [0.934 - 0.954]\n",
      "\n",
      "Test set: Avg. loss: 0.0425, Accuracy: 2105/2115 (100%)\n",
      "\n",
      "Accuracy: 0.9952718676122931 [0.992 - 0.998]\n",
      "\n",
      "Test set: Avg. loss: 0.0853, Accuracy: 2103/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9943262411347518 [0.991 - 0.997]\n",
      "\n",
      "Test set: Avg. loss: 0.1301, Accuracy: 2095/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9905437352245863 [0.986 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.1517, Accuracy: 2091/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9886524822695035 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1651, Accuracy: 2089/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9877068557919622 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1651, Accuracy: 2089/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9877068557919622 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1651, Accuracy: 2089/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9877068557919622 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1651, Accuracy: 2089/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9877068557919622 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1651, Accuracy: 2089/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9877068557919622 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1651, Accuracy: 2089/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9877068557919622 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1651, Accuracy: 2089/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9877068557919622 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1651, Accuracy: 2089/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9877068557919622 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1651, Accuracy: 2089/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9877068557919622 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1651, Accuracy: 2089/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9877068557919622 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1651, Accuracy: 2089/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9877068557919622 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1651, Accuracy: 2089/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9877068557919622 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1651, Accuracy: 2089/2115 (99%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9877068557919622 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1651, Accuracy: 2089/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9877068557919622 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1651, Accuracy: 2089/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9877068557919622 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.0305, Accuracy: 965/2115 (46%)\n",
      "\n",
      "Accuracy: 0.4562647754137116 [0.434 - 0.478]\n",
      "\n",
      "Test set: Avg. loss: 0.0084, Accuracy: 2111/2115 (100%)\n",
      "\n",
      "Accuracy: 0.9981087470449173 [0.996 - 1.0]\n",
      "\n",
      "Test set: Avg. loss: 0.0017, Accuracy: 2112/2115 (100%)\n",
      "\n",
      "Accuracy: 0.9985815602836879 [0.997 - 1.0]\n",
      "\n",
      "Test set: Avg. loss: 0.0300, Accuracy: 2105/2115 (100%)\n",
      "\n",
      "Accuracy: 0.9952718676122931 [0.992 - 0.998]\n",
      "\n",
      "Test set: Avg. loss: 0.0884, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1151, Accuracy: 2088/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9872340425531915 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1151, Accuracy: 2088/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9872340425531915 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1151, Accuracy: 2088/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9872340425531915 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1151, Accuracy: 2088/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9872340425531915 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1151, Accuracy: 2088/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9872340425531915 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1151, Accuracy: 2088/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9872340425531915 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1151, Accuracy: 2088/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9872340425531915 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1151, Accuracy: 2088/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9872340425531915 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1151, Accuracy: 2088/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9872340425531915 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1151, Accuracy: 2088/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9872340425531915 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1151, Accuracy: 2088/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9872340425531915 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1151, Accuracy: 2088/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9872340425531915 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1151, Accuracy: 2088/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9872340425531915 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1151, Accuracy: 2088/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9872340425531915 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1151, Accuracy: 2088/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9872340425531915 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.1151, Accuracy: 2088/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9872340425531915 [0.983 - 0.992]\n",
      "\n",
      "Test set: Avg. loss: 0.0299, Accuracy: 921/2115 (44%)\n",
      "\n",
      "Accuracy: 0.43546099290780144 [0.415 - 0.456]\n",
      "\n",
      "Test set: Avg. loss: 0.0149, Accuracy: 2110/2115 (100%)\n",
      "\n",
      "Accuracy: 0.9976359338061466 [0.995 - 1.0]\n",
      "\n",
      "Test set: Avg. loss: 0.0076, Accuracy: 2110/2115 (100%)\n",
      "\n",
      "Accuracy: 0.9976359338061466 [0.995 - 1.0]\n",
      "\n",
      "Test set: Avg. loss: 0.0419, Accuracy: 2104/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9947990543735225 [0.991 - 0.998]\n",
      "\n",
      "Test set: Avg. loss: 0.1030, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0999, Accuracy: 2094/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9900709219858156 [0.986 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.1102, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1102, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1102, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1102, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1102, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1102, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1102, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1102, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1102, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1102, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1102, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1102, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1102, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1102, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1102, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.985 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0790, Accuracy: 369/2115 (17%)\n",
      "\n",
      "Accuracy: 0.17446808510638298 [0.157 - 0.191]\n",
      "\n",
      "Test set: Avg. loss: 0.2882, Accuracy: 2053/2115 (97%)\n",
      "\n",
      "Accuracy: 0.9706855791962175 [0.964 - 0.978]\n",
      "\n",
      "Test set: Avg. loss: 0.0197, Accuracy: 2107/2115 (100%)\n",
      "\n",
      "Accuracy: 0.9962174940898345 [0.993 - 0.999]\n",
      "\n",
      "Test set: Avg. loss: 0.0599, Accuracy: 2104/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9947990543735225 [0.991 - 0.998]\n",
      "\n",
      "Test set: Avg. loss: 0.1207, Accuracy: 2093/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9895981087470449 [0.985 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.1202, Accuracy: 2094/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9900709219858156 [0.985 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.1304, Accuracy: 2093/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9895981087470449 [0.985 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.1304, Accuracy: 2093/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9895981087470449 [0.985 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.1304, Accuracy: 2093/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9895981087470449 [0.985 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.1304, Accuracy: 2093/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9895981087470449 [0.985 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.1304, Accuracy: 2093/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9895981087470449 [0.985 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.1304, Accuracy: 2093/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9895981087470449 [0.985 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.1304, Accuracy: 2093/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9895981087470449 [0.985 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.1304, Accuracy: 2093/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9895981087470449 [0.985 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.1304, Accuracy: 2093/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9895981087470449 [0.985 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.1304, Accuracy: 2093/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9895981087470449 [0.985 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.1304, Accuracy: 2093/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9895981087470449 [0.985 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.1304, Accuracy: 2093/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9895981087470449 [0.985 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.1304, Accuracy: 2093/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9895981087470449 [0.985 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.1304, Accuracy: 2093/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9895981087470449 [0.985 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.1304, Accuracy: 2093/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9895981087470449 [0.985 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.0143, Accuracy: 1486/2115 (70%)\n",
      "\n",
      "Accuracy: 0.7026004728132388 [0.684 - 0.721]\n",
      "\n",
      "Test set: Avg. loss: 0.0220, Accuracy: 2110/2115 (100%)\n",
      "\n",
      "Accuracy: 0.9976359338061466 [0.996 - 1.0]\n",
      "\n",
      "Test set: Avg. loss: 0.0086, Accuracy: 2108/2115 (100%)\n",
      "\n",
      "Accuracy: 0.9966903073286052 [0.994 - 0.999]\n",
      "\n",
      "Test set: Avg. loss: 0.0596, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0596, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0596, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0596, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0596, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0596, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0596, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0596, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0596, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0596, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0596, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0596, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0596, Accuracy: 2092/2115 (99%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9891252955082742 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0596, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0596, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0596, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0596, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0596, Accuracy: 2092/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9891252955082742 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.0432, Accuracy: 998/2115 (47%)\n",
      "\n",
      "Accuracy: 0.4718676122931442 [0.452 - 0.495]\n",
      "\n",
      "Test set: Avg. loss: 0.0127, Accuracy: 2111/2115 (100%)\n",
      "\n",
      "Accuracy: 0.9981087470449173 [0.996 - 1.0]\n",
      "\n",
      "Test set: Avg. loss: 0.0031, Accuracy: 2109/2115 (100%)\n",
      "\n",
      "Accuracy: 0.9971631205673759 [0.995 - 0.999]\n",
      "\n",
      "Test set: Avg. loss: 0.0245, Accuracy: 2098/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9919621749408983 [0.988 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0451, Accuracy: 2096/2115 (99%)\n",
      "\n",
      "Accuracy: 0.991016548463357 [0.987 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0451, Accuracy: 2096/2115 (99%)\n",
      "\n",
      "Accuracy: 0.991016548463357 [0.987 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0451, Accuracy: 2096/2115 (99%)\n",
      "\n",
      "Accuracy: 0.991016548463357 [0.987 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0451, Accuracy: 2096/2115 (99%)\n",
      "\n",
      "Accuracy: 0.991016548463357 [0.987 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0451, Accuracy: 2096/2115 (99%)\n",
      "\n",
      "Accuracy: 0.991016548463357 [0.987 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0451, Accuracy: 2096/2115 (99%)\n",
      "\n",
      "Accuracy: 0.991016548463357 [0.987 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0451, Accuracy: 2096/2115 (99%)\n",
      "\n",
      "Accuracy: 0.991016548463357 [0.987 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0451, Accuracy: 2096/2115 (99%)\n",
      "\n",
      "Accuracy: 0.991016548463357 [0.987 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0451, Accuracy: 2096/2115 (99%)\n",
      "\n",
      "Accuracy: 0.991016548463357 [0.987 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0451, Accuracy: 2096/2115 (99%)\n",
      "\n",
      "Accuracy: 0.991016548463357 [0.987 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0451, Accuracy: 2096/2115 (99%)\n",
      "\n",
      "Accuracy: 0.991016548463357 [0.987 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0451, Accuracy: 2096/2115 (99%)\n",
      "\n",
      "Accuracy: 0.991016548463357 [0.987 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0451, Accuracy: 2096/2115 (99%)\n",
      "\n",
      "Accuracy: 0.991016548463357 [0.987 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0451, Accuracy: 2096/2115 (99%)\n",
      "\n",
      "Accuracy: 0.991016548463357 [0.987 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0451, Accuracy: 2096/2115 (99%)\n",
      "\n",
      "Accuracy: 0.991016548463357 [0.987 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0451, Accuracy: 2096/2115 (99%)\n",
      "\n",
      "Accuracy: 0.991016548463357 [0.987 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0451, Accuracy: 2096/2115 (99%)\n",
      "\n",
      "Accuracy: 0.991016548463357 [0.987 - 0.995]\n",
      "\n",
      "Test set: Avg. loss: 0.0225, Accuracy: 913/2115 (43%)\n",
      "\n",
      "Accuracy: 0.43167848699763595 [0.410 - 0.452]\n",
      "\n",
      "Test set: Avg. loss: 0.7483, Accuracy: 1985/2115 (94%)\n",
      "\n",
      "Accuracy: 0.9385342789598109 [0.928 - 0.948]\n",
      "\n",
      "Test set: Avg. loss: 0.0544, Accuracy: 2104/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9947990543735225 [0.991 - 0.998]\n",
      "\n",
      "Test set: Avg. loss: 0.0984, Accuracy: 2101/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9933806146572104 [0.990 - 0.996]\n",
      "\n",
      "Test set: Avg. loss: 0.1376, Accuracy: 2093/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9895981087470449 [0.985 - 0.994]\n",
      "\n",
      "Test set: Avg. loss: 0.1607, Accuracy: 2090/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9881796690307328 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1607, Accuracy: 2090/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9881796690307328 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1607, Accuracy: 2090/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9881796690307328 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1607, Accuracy: 2090/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9881796690307328 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1607, Accuracy: 2090/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9881796690307328 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1607, Accuracy: 2090/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9881796690307328 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1607, Accuracy: 2090/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9881796690307328 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1607, Accuracy: 2090/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9881796690307328 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1607, Accuracy: 2090/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9881796690307328 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1607, Accuracy: 2090/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9881796690307328 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1607, Accuracy: 2090/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9881796690307328 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1607, Accuracy: 2090/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9881796690307328 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1607, Accuracy: 2090/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9881796690307328 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1607, Accuracy: 2090/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9881796690307328 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1607, Accuracy: 2090/2115 (99%)\n",
      "\n",
      "Accuracy: 0.9881796690307328 [0.984 - 0.993]\n",
      "\n",
      "Test set: Avg. loss: 0.1607, Accuracy: 2090/2115 (99%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9881796690307328 [0.984 - 0.993]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1435ef9a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracies = []\n",
    "for i in range(10):\n",
    "    model_accuracies = []\n",
    "    network = logistic_regression.Net(NUM_CLASSES_REDUCED)\n",
    "    _, y_preds, y_true = test.test(test_loader_reduced, network) \n",
    "    _, _, acc = get_confidence_interval.get_confidence_interval(y_preds, y_true)\n",
    "    model_accuracies.append(acc)\n",
    "    for epoch in range(n_epochs):\n",
    "        state_dict = torch.load(f'logistic_regression_results/reduced_ratio{i}/model{epoch}')\n",
    "        network.load_state_dict(state_dict)\n",
    "        _, y_preds, y_true = test.test(test_loader_reduced, network) \n",
    "        _, _, acc = get_confidence_interval.get_confidence_interval(y_preds, y_true)\n",
    "        model_accuracies.append(acc)\n",
    "    accuracies.append(model_accuracies)\n",
    "    \n",
    "for i in range(10):\n",
    "    plt.plot(np.arange(-1, n_epochs), accuracies[i])\n",
    "plt.title(\"Logistic Regression 2 classes 100:1\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3734d3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.1457, Accuracy: 4/2115 (0%)\n",
      "\n",
      "Accuracy: 0.0018912529550827422 [0.000 - 0.00378]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9063/10000 (91%)\n",
      "\n",
      "Accuracy: 0.9063 [0.900 - 0.912]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9123/10000 (91%)\n",
      "\n",
      "Accuracy: 0.9123 [0.906 - 0.918]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9148/10000 (91%)\n",
      "\n",
      "Accuracy: 0.9148 [0.909 - 0.92]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9159/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9159 [0.910 - 0.921]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9172/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9172 [0.912 - 0.923]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9181/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9181 [0.913 - 0.923]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9188/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9188 [0.913 - 0.924]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9189/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9189 [0.914 - 0.924]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9190/10000 (92%)\n",
      "\n",
      "Accuracy: 0.919 [0.914 - 0.924]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9194/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9194 [0.914 - 0.925]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9195/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9195 [0.914 - 0.925]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9197/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9197 [0.914 - 0.925]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9200/10000 (92%)\n",
      "\n",
      "Accuracy: 0.92 [0.915 - 0.925]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9203/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9203 [0.915 - 0.926]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9205/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9205 [0.915 - 0.926]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9205/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9205 [0.915 - 0.926]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9206/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9206 [0.915 - 0.926]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9207/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9207 [0.916 - 0.926]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9207/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9207 [0.915 - 0.926]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9209/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9209 [0.916 - 0.926]\n",
      "\n",
      "Test set: Avg. loss: 0.0861, Accuracy: 59/2115 (3%)\n",
      "\n",
      "Accuracy: 0.027895981087470448 [0.021 - 0.0355]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9064/10000 (91%)\n",
      "\n",
      "Accuracy: 0.9064 [0.900 - 0.912]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9127/10000 (91%)\n",
      "\n",
      "Accuracy: 0.9127 [0.907 - 0.918]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9159/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9159 [0.910 - 0.921]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9176/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9176 [0.912 - 0.923]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9183/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9183 [0.913 - 0.924]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9188/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9188 [0.913 - 0.924]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9184/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9184 [0.913 - 0.924]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9188/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9188 [0.913 - 0.924]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9195/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9195 [0.914 - 0.925]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9200/10000 (92%)\n",
      "\n",
      "Accuracy: 0.92 [0.915 - 0.925]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9199/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9199 [0.915 - 0.925]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9200/10000 (92%)\n",
      "\n",
      "Accuracy: 0.92 [0.915 - 0.925]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9202/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9202 [0.915 - 0.925]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9203/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9203 [0.915 - 0.926]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9208/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9208 [0.915 - 0.926]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9209/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9209 [0.916 - 0.926]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9210/10000 (92%)\n",
      "\n",
      "Accuracy: 0.921 [0.916 - 0.926]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9211/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9211 [0.916 - 0.926]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9214/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9214 [0.916 - 0.927]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9214/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9214 [0.916 - 0.927]\n",
      "\n",
      "Test set: Avg. loss: 0.0861, Accuracy: 150/2115 (7%)\n",
      "\n",
      "Accuracy: 0.07092198581560284 [0.061 - 0.0823]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9071/10000 (91%)\n",
      "\n",
      "Accuracy: 0.9071 [0.901 - 0.913]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9127/10000 (91%)\n",
      "\n",
      "Accuracy: 0.9127 [0.907 - 0.918]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9146/10000 (91%)\n",
      "\n",
      "Accuracy: 0.9146 [0.909 - 0.92]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9164/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9164 [0.911 - 0.922]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9176/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9176 [0.912 - 0.923]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9172/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9172 [0.912 - 0.923]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9181/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9181 [0.913 - 0.923]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9187/10000 (92%)\n",
      "\n",
      "Accuracy: 0.9187 [0.913 - 0.924]\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9191/10000 (92%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "for i in range(10):\n",
    "    model_accuracies = []\n",
    "    network = logistic_regression.Net(NUM_CLASSES)\n",
    "    _, y_preds, y_true = test.test(test_loader_reduced, network) \n",
    "    _, _, acc = get_confidence_interval.get_confidence_interval(y_preds, y_true)\n",
    "    model_accuracies.append(acc)\n",
    "    for epoch in range(n_epochs):\n",
    "        state_dict = torch.load(f'logistic_regression_results/normal{i}/model{epoch}')\n",
    "        network.load_state_dict(state_dict)\n",
    "        _, y_preds, y_true = test.test(test_loader_normal, network) \n",
    "        _, _, acc = get_confidence_interval.get_confidence_interval(y_preds, y_true)\n",
    "        model_accuracies.append(acc)\n",
    "    accuracies.append(model_accuracies)\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    plt.plot(np.arange(-1, n_epochs), accuracies[i])\n",
    "plt.title(\"Logistic Regression 10 classes\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a06e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "for i in range(10):\n",
    "    model_accuracies = []\n",
    "    network = logistic_regression.Net(NUM_CLASSES_REDUCED)\n",
    "    _, y_preds, y_true = test.test(test_loader_reduced, network) \n",
    "    _, _, acc = get_confidence_interval.get_confidence_interval(y_preds, y_true)\n",
    "    model_accuracies.append(acc)\n",
    "    for epoch in range(n_epochs):\n",
    "        state_dict = torch.load(f'logistic_regression_results/reduced{i}/model{epoch}')\n",
    "        network.load_state_dict(state_dict)\n",
    "        _, y_preds, y_true = test.test(test_loader_reduced, network) \n",
    "        _, _, acc = get_confidence_interval.get_confidence_interval(y_preds, y_true)\n",
    "        model_accuracies.append(acc)\n",
    "    accuracies.append(model_accuracies)\n",
    "    \n",
    "for i in range(10):\n",
    "    plt.plot(np.arange(-1, n_epochs), accuracies[i])\n",
    "plt.title(\"Logistic Regression 2 classes\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac114c42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
