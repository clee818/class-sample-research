{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4df9c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  \n",
    "\n",
    "\n",
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops \n",
    "\n",
    "NUM_CLASSES = 10\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "          #   \"mean_40\", \"variance_40\",\n",
    "          #   \"mean_50\", \"variance_50\",\n",
    "             \"cap\", \"normalization\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9edc25c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 3 classes\n",
    "\n",
    "NUM_CLASSES_REDUCED = 3\n",
    "nums = (0, 3, 1)\n",
    "ratio = (200, 20, 1)\n",
    "\n",
    "norm=True\n",
    "\n",
    "if norm:\n",
    "    transform=torchvision.transforms.Compose([torchvision.transforms.Normalize(mean=[134.1855, 122.7346, 118.3749], std=[70.5125, 64.4848, 66.5604])])\n",
    "else:\n",
    "    transform=None\n",
    "\n",
    "    \n",
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "\n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums, transform=transform)\n",
    "targets = ratio_train_CIFAR10.labels \n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED, transform=transform)\n",
    "\n",
    "triplet_train_CIFAR10 = class_sampling.ForTripletLoss(reduced_train_CIFAR10, smote=False, transform=transform, num_classes=3)\n",
    "triplet_ratio_train_CIFAR10 = class_sampling.ForTripletLoss(ratio_train_CIFAR10, smote=False, transform=transform, num_classes=3)\n",
    "triplet_smote_train_CIFAR10 = class_sampling.ForTripletLoss(smote_train_CIFAR10, smote=True, transform=transform, num_classes=3)\n",
    "\n",
    "weight = 1. / class_count\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= max(class_count)\n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss = DataLoader(triplet_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_ratio = DataLoader(triplet_ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_smote = DataLoader(triplet_smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "225b42ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001101137677828471, AUC: 0.48153950000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008184388677279155, AUC: 0.8536186666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006444128354390462, AUC: 0.9132271666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006369576851526896, AUC: 0.9185373333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011027837991714477, AUC: 0.4255000833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007888296842575073, AUC: 0.8538959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006802848180135091, AUC: 0.8962965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006800517241160075, AUC: 0.9124463333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011040879090627035, AUC: 0.442987\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007678199013074239, AUC: 0.8738281666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006366982062657674, AUC: 0.9103801666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005771947900454203, AUC: 0.9220054166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011034004290898642, AUC: 0.5220113333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007123484214146932, AUC: 0.8829556666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006213265458742777, AUC: 0.9172080833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005668141643206279, AUC: 0.9277124999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010979617834091186, AUC: 0.5870384166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007625082929929097, AUC: 0.8707184166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000652503768603007, AUC: 0.9124308333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005518966515858969, AUC: 0.9302519166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011095634698867797, AUC: 0.5292506666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007268446286519369, AUC: 0.8815006666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006475879351298014, AUC: 0.9092468333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006103541652361552, AUC: 0.918965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011051725546518962, AUC: 0.4200318333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007882136901219685, AUC: 0.8619441666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006808364987373352, AUC: 0.9048589166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006117060383160909, AUC: 0.924781\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011070245504379273, AUC: 0.4760095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000718909760316213, AUC: 0.8796826666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000660069485505422, AUC: 0.9081976666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006367768247922262, AUC: 0.9161351666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011035262346267701, AUC: 0.48195566666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007628247539202373, AUC: 0.8785905833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927396655082703, AUC: 0.900569\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007480770945549011, AUC: 0.8913098333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011010510921478272, AUC: 0.500636\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007393390734990438, AUC: 0.8814895000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006523323655128479, AUC: 0.9106549166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006065802971522013, AUC: 0.9243043333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011185652414957682, AUC: 0.46978749999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008411714633305867, AUC: 0.8328979166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007796743512153625, AUC: 0.8569983333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007105754613876342, AUC: 0.8854553333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00109709366162618, AUC: 0.5341400833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007941816051801045, AUC: 0.8495818333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007390896876653036, AUC: 0.8790888333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006584412455558777, AUC: 0.9043703333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011092888116836547, AUC: 0.4348680833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008302397529284159, AUC: 0.8338025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007619841496149699, AUC: 0.8724080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006734261314074199, AUC: 0.8999798333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010955574909845987, AUC: 0.5674233333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008201595346132915, AUC: 0.8417483333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007059560616811117, AUC: 0.8885869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006676798661549886, AUC: 0.9067685000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010971723794937134, AUC: 0.628316\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008112437725067139, AUC: 0.8469965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007004455327987671, AUC: 0.89434\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006247138977050781, AUC: 0.9105003333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994592905044557, AUC: 0.5335838333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008224708040555318, AUC: 0.8411890833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007558638254801432, AUC: 0.8781736666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006885816852251689, AUC: 0.9024890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011088141202926635, AUC: 0.4706688333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008279292583465576, AUC: 0.8437704166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007454203168551127, AUC: 0.8819615000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007090716361999511, AUC: 0.8920193333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011082798639933267, AUC: 0.44204550000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008179512619972229, AUC: 0.8500549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007079941829045614, AUC: 0.8897698333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006597615480422973, AUC: 0.9046443333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011251074075698851, AUC: 0.4107934166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008316404819488525, AUC: 0.841066\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007214332222938537, AUC: 0.8876155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006521119872728983, AUC: 0.9037886666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986471176147462, AUC: 0.5207521666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008360359271367391, AUC: 0.8327512499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000768715222676595, AUC: 0.8625490833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007029150327046712, AUC: 0.8888949166666666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class normal\n",
    "\n",
    "learning_rates = [1e-2, 5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 3, nums, (1, 1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aeea5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7098b880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001102492650349935, AUC: 0.4337535833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028033832708994548, AUC: 0.6646945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025717381636301676, AUC: 0.6802568333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002387967030207316, AUC: 0.6892651666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010961395899454752, AUC: 0.5539086666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024249297777811686, AUC: 0.6684312499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002277838150660197, AUC: 0.6947530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002308895746866862, AUC: 0.7068166666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011040597756703694, AUC: 0.5133956666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002486731767654419, AUC: 0.668329\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023136560916900634, AUC: 0.6831764166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002032056490580241, AUC: 0.6958366666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110069207350413, AUC: 0.5184295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026118515332539875, AUC: 0.6661951666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023768699169158937, AUC: 0.6801688333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002245481808980306, AUC: 0.6852033333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010960990190505982, AUC: 0.5672013333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025847732226053875, AUC: 0.6633676666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022811535199483237, AUC: 0.6769325833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023094260692596434, AUC: 0.6822457499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011023508310317994, AUC: 0.5688680833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025403799215952556, AUC: 0.6601105833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002326909859975179, AUC: 0.6779621666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002284610430399577, AUC: 0.6899411666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011020024220148722, AUC: 0.49279375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002338178078333537, AUC: 0.6673018333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021909658114115395, AUC: 0.6788838333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002237022638320923, AUC: 0.6879068333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094159762064616, AUC: 0.6742128333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023469852606455483, AUC: 0.6684931666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002207674185434977, AUC: 0.6778325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021467512448628745, AUC: 0.6954703333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011133658488591512, AUC: 0.3816805833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002537322123845418, AUC: 0.6561435833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002369732936223348, AUC: 0.6770378333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021483394304911297, AUC: 0.6852068333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010951947768529256, AUC: 0.5664225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024388607343037924, AUC: 0.6693993333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022326231797536213, AUC: 0.6974885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00223704727490743, AUC: 0.7033116666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001091356118520101, AUC: 0.62761775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002040173292160034, AUC: 0.6219205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022304242451985676, AUC: 0.6332745000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002380216916402181, AUC: 0.6396180833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001107655684153239, AUC: 0.38474141666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022205833594004312, AUC: 0.6134000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024893441994984947, AUC: 0.6270658333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002612284580866496, AUC: 0.6345602499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011122562487920124, AUC: 0.42908525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002118132829666138, AUC: 0.61848425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002396357536315918, AUC: 0.6312129166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002522553205490112, AUC: 0.6389041666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100712498029073, AUC: 0.5101249166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020887924830118817, AUC: 0.6171261666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023631734053293865, AUC: 0.6303080833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00253949515024821, AUC: 0.63745775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099520762761434, AUC: 0.48972974999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020869428316752114, AUC: 0.6287610833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002370649496714274, AUC: 0.6408107500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024818964799245198, AUC: 0.64811075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010993789037068685, AUC: 0.49124058333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022473127841949464, AUC: 0.6148156666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002447817802429199, AUC: 0.6283926666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002592841863632202, AUC: 0.637704\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011050006945927937, AUC: 0.5253402500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019942170381546023, AUC: 0.624063\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022978821595509846, AUC: 0.62673475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024611430168151854, AUC: 0.6319806666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011185779968897502, AUC: 0.3662866666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021110915342966714, AUC: 0.6231098333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002345971902211507, AUC: 0.6356253333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002440640131632487, AUC: 0.6421458333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011084505716959635, AUC: 0.37659025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002414268414179484, AUC: 0.5925598333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002585949659347534, AUC: 0.6147188333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002716259400049845, AUC: 0.6274810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011033554077148437, AUC: 0.45658424999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002106731653213501, AUC: 0.6185594166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023506527741750083, AUC: 0.6358556666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002497682015101115, AUC: 0.6427783333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001108721137046814, AUC: 0.45761275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026549111207326255, AUC: 0.6475810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002488568147023519, AUC: 0.6649388333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022735060850779215, AUC: 0.6743026666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011051953633626301, AUC: 0.42576016666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026201767126719155, AUC: 0.6548719166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022784393628438315, AUC: 0.6654743333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002464406172434489, AUC: 0.6767875833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096416195233663, AUC: 0.5504565833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025870129267374677, AUC: 0.6382554166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025457890033721924, AUC: 0.6571811666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002333587646484375, AUC: 0.6695066666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096101999282837, AUC: 0.5617791666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027545117537180583, AUC: 0.6493675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025814154148101807, AUC: 0.6668671666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002497681220372518, AUC: 0.6742775833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001117079496383667, AUC: 0.42911150000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002519168456395467, AUC: 0.6371048333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002577670653661092, AUC: 0.6613165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023378865718841554, AUC: 0.67511125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011048033237457276, AUC: 0.5626856666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025293835798899334, AUC: 0.6512608333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002433168888092041, AUC: 0.6715619166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024334885279337563, AUC: 0.6840011666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011108675400416056, AUC: 0.36799658333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002576249440511068, AUC: 0.6514439166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025568923155466715, AUC: 0.668001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025273990631103516, AUC: 0.6775249166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011024903456370036, AUC: 0.62801\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002614193280537923, AUC: 0.6476620833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002598528782526652, AUC: 0.6650778333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002482144912083944, AUC: 0.67189575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102892557779948, AUC: 0.5917583333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002602562109629313, AUC: 0.6445125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024847689469655355, AUC: 0.66163275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002349287192026774, AUC: 0.67489\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110019318262736, AUC: 0.549006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025797966321309406, AUC: 0.64157075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002530848503112793, AUC: 0.6636798333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002379444440205892, AUC: 0.6769001666666666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class ratio\n",
    "\n",
    "learning_rates =  [1e-2, 1e-3, 5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ec6b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0850938a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010989482005437216, AUC: 0.5574586666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014568779468536377, AUC: 0.7987512499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027695000171661377, AUC: 0.7861481666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033176114559173584, AUC: 0.7812484166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010912991762161255, AUC: 0.5963589166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017322154839833578, AUC: 0.765361\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027109846274058023, AUC: 0.7597248333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033343732357025146, AUC: 0.7680907499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008808612823487, AUC: 0.5308455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016412200133005777, AUC: 0.7837315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028054415384928385, AUC: 0.7703296666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035819047292073567, AUC: 0.7849685000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011016372044881184, AUC: 0.49567524999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014501880804697672, AUC: 0.79916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023616336981455485, AUC: 0.7956493333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003184097448984782, AUC: 0.7928443333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095231294631958, AUC: 0.5783449166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016919046640396119, AUC: 0.7833448333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025914467175801596, AUC: 0.7943026666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002642768859863281, AUC: 0.8087403333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986486673355102, AUC: 0.5176145833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014580437342325847, AUC: 0.7701569999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002820889949798584, AUC: 0.7799118333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036393133799235027, AUC: 0.7857158333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011001242796579997, AUC: 0.5213026666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001639033555984497, AUC: 0.7370420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002714767535527547, AUC: 0.7321203333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036419639587402346, AUC: 0.74222\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011038796504338581, AUC: 0.4549690833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001597296436627706, AUC: 0.7875331666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026737934748331704, AUC: 0.7777233333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003308349370956421, AUC: 0.7956683333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100283145904541, AUC: 0.588877\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011607431570688884, AUC: 0.7964355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019568619330724082, AUC: 0.79623475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003171085755030314, AUC: 0.7770522500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991733074188232, AUC: 0.5572630833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001436136523882548, AUC: 0.7922611666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002246021270751953, AUC: 0.8031480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003004270950953166, AUC: 0.8070894999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011047978401184082, AUC: 0.48873666666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010142487287521363, AUC: 0.746352\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009295734763145447, AUC: 0.7961709166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008795956373214721, AUC: 0.8219855833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011014945904413858, AUC: 0.5559282499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001013463298479716, AUC: 0.7130313333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00095101398229599, AUC: 0.7627391666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009011924266815186, AUC: 0.7952441666666669\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106361945470174, AUC: 0.43234458333333325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010294406414031983, AUC: 0.7234401666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009383618831634522, AUC: 0.7801185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008806079030036926, AUC: 0.8149755833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011031925280888875, AUC: 0.6139911666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000979894717534383, AUC: 0.7455403333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009143596092859904, AUC: 0.7879680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008564819097518921, AUC: 0.8132955000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102788726488749, AUC: 0.4590515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001003290832042694, AUC: 0.7128311666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009441520770390829, AUC: 0.7615456666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009131338596343994, AUC: 0.7730311666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011007806857426961, AUC: 0.4977335833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010093032916386922, AUC: 0.726738\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009371120532353719, AUC: 0.7903825833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008851401805877686, AUC: 0.8162113333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105303923288981, AUC: 0.5849120833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010132359663645426, AUC: 0.7238324166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009407095909118652, AUC: 0.7810070000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008838021755218506, AUC: 0.8200913333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010997193257013956, AUC: 0.5007515833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000994119147459666, AUC: 0.7388348333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009290691415468851, AUC: 0.7784763333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008775129715601603, AUC: 0.8100016666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011012182633082072, AUC: 0.48302700000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010072028636932374, AUC: 0.7620069166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009317566752433777, AUC: 0.7918131666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008764350016911824, AUC: 0.8207295000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001119064966837565, AUC: 0.37957391666666657\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010006605982780456, AUC: 0.763348\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009320196509361267, AUC: 0.7895824166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008877813418706258, AUC: 0.8016588333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008808612823487, AUC: 0.4846025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000883094310760498, AUC: 0.8059379166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016187536319096883, AUC: 0.7769743333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00278557554880778, AUC: 0.7650776666666669\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010935423374176026, AUC: 0.5738760833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010487080415089926, AUC: 0.7926544999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001964112559954325, AUC: 0.8028181666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026551124254862468, AUC: 0.8152443333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010930122534434001, AUC: 0.6262559166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008472077449162801, AUC: 0.8160041666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001395551085472107, AUC: 0.7850604999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022025981744130453, AUC: 0.7840701666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011179414590199788, AUC: 0.35242066666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008765910863876343, AUC: 0.8085844999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001343150774637858, AUC: 0.7917993333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002154683510462443, AUC: 0.7831126666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011044532855351767, AUC: 0.47575416666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009273201624552409, AUC: 0.8059683333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00186006764570872, AUC: 0.7659526666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002393811305363973, AUC: 0.7669024999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101227879524231, AUC: 0.44546983333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011473985910415649, AUC: 0.7761808333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002186598857243856, AUC: 0.7743286666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003095108985900879, AUC: 0.7786075833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992282231648764, AUC: 0.5289455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008166845043500264, AUC: 0.8429258333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014016406933466594, AUC: 0.7950958333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001972997307777405, AUC: 0.7857641666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011013125975926717, AUC: 0.5048068333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009712183475494385, AUC: 0.8051418333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019938271443049114, AUC: 0.7788926666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002541587591171265, AUC: 0.7787780833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011121039787928264, AUC: 0.42407616666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000874048113822937, AUC: 0.8249545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002002699335416158, AUC: 0.7873095000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022958167394002277, AUC: 0.7976748333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010974462032318116, AUC: 0.5333489166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008437161842981975, AUC: 0.8238504999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001465768257776896, AUC: 0.7943851666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018469241460164387, AUC: 0.7940718333333333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class oversampled \n",
    "\n",
    "learning_rates = [1e-2, 1e-3, 5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bd49495",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "801a223f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011113398869832356, AUC: 0.45399366666666663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001039024035135905, AUC: 0.7600510833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014980615377426147, AUC: 0.7576163333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016282843351364137, AUC: 0.7855533333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011053721904754638, AUC: 0.40628075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000999620020389557, AUC: 0.7641230833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00213276473681132, AUC: 0.7200933333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018212167024612427, AUC: 0.7857006666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110055140654246, AUC: 0.5410479166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00124763818581899, AUC: 0.7365446666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001190781593322754, AUC: 0.7589035833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017052281697591146, AUC: 0.7550365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010934751828511557, AUC: 0.582834\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010338213443756104, AUC: 0.7306260833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001256510615348816, AUC: 0.7601573333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015600728988647461, AUC: 0.7772036666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011009525458017986, AUC: 0.5095504166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001012095888455709, AUC: 0.759291\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016606905062993367, AUC: 0.7748548333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001551385482152303, AUC: 0.8017821666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010941548347473144, AUC: 0.5600375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009489623705546061, AUC: 0.7749334166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001336568792661031, AUC: 0.7718415833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002138089736302694, AUC: 0.7530644166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011102606852849324, AUC: 0.35558191666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012823344866434733, AUC: 0.7354369999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015103890895843506, AUC: 0.7758745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002325395663579305, AUC: 0.7750466666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097779671351115, AUC: 0.5878043333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010028896530469259, AUC: 0.7253636666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012898311614990235, AUC: 0.7437476666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014241915146509807, AUC: 0.7635815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106678287188212, AUC: 0.4240346666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001239643414815267, AUC: 0.7400048333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010604337453842163, AUC: 0.7906261666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017612868150075278, AUC: 0.7853321666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011061473687489828, AUC: 0.48844200000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009616493781407674, AUC: 0.7720068333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017050743103027345, AUC: 0.7497554166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001822416067123413, AUC: 0.7868965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011031923294067382, AUC: 0.41072083333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009945464332898459, AUC: 0.6974798333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009716023008028666, AUC: 0.7113190833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009639941652615866, AUC: 0.7240745000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00109904948870341, AUC: 0.5273083333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010117427905400593, AUC: 0.6913511666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000970941424369812, AUC: 0.7096697500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009758197665214539, AUC: 0.7202266666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001108381191889445, AUC: 0.3955576666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010100897153218586, AUC: 0.6944343333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009801236192385355, AUC: 0.7207414999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000994615395863851, AUC: 0.7220246666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988479852676392, AUC: 0.5423835833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009763232072194418, AUC: 0.7182433333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009485452771186829, AUC: 0.7365836666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009338517785072327, AUC: 0.7457319166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001104199767112732, AUC: 0.49273416666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001009968121846517, AUC: 0.6846151666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009896132548650107, AUC: 0.6992801666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010140082637468974, AUC: 0.7116398333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998433430989584, AUC: 0.5313799166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010234491427739462, AUC: 0.6986300000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009773205320040385, AUC: 0.7160968333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009899214506149292, AUC: 0.7209475833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098619023958842, AUC: 0.5116397500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009951512217521667, AUC: 0.6942938333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009669517477353414, AUC: 0.7208125000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009927613139152527, AUC: 0.720976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011067557334899902, AUC: 0.42300125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010168930292129516, AUC: 0.6959984166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009686615268389384, AUC: 0.7232954166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000971798062324524, AUC: 0.7253599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011056735118230184, AUC: 0.4797904166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010057107607523601, AUC: 0.6804016666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000990310509999593, AUC: 0.69979475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009628028074900309, AUC: 0.7240045833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010978631178538004, AUC: 0.5771179166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010060794353485107, AUC: 0.7118674166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009636658430099487, AUC: 0.7323555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009693697690963745, AUC: 0.7328078333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100023587544759, AUC: 0.5092131666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009844680229822796, AUC: 0.7356826666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001056182324886322, AUC: 0.7524725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011689250071843466, AUC: 0.7752487499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106910983721415, AUC: 0.4353023333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009553236961364746, AUC: 0.7417845833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001090216875076294, AUC: 0.7485426666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013120314677556356, AUC: 0.7491558333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099727988243103, AUC: 0.488976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010441067616144817, AUC: 0.7205533333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001166478991508484, AUC: 0.7359143333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010928589900334677, AUC: 0.7567554166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010970599253972372, AUC: 0.6145841666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011241487661997477, AUC: 0.7214166666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012154109875361124, AUC: 0.7490045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107865834236145, AUC: 0.7696853333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011220130920410157, AUC: 0.3805531666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010232747395833334, AUC: 0.7259581666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001148131529490153, AUC: 0.7431843333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001221880038579305, AUC: 0.764319\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010971903800964355, AUC: 0.63350025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010308618148167927, AUC: 0.7222401666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012749990622202556, AUC: 0.7286945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012237332661946614, AUC: 0.7454143333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101090431213379, AUC: 0.49276416666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009749483466148376, AUC: 0.7473329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010557441711425781, AUC: 0.7636331666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010538909037907918, AUC: 0.7770780833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011035247643788656, AUC: 0.46700600000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010697574615478515, AUC: 0.7340816666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011143747170766194, AUC: 0.7581413333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015885307788848877, AUC: 0.7616653333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994923909505207, AUC: 0.5052315833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011150134007136027, AUC: 0.7211219166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001265462358792623, AUC: 0.7439471666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015679079294204713, AUC: 0.7523418333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011034554640452067, AUC: 0.465781\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000986909031867981, AUC: 0.73727175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010056549708048503, AUC: 0.7703507500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001081092913945516, AUC: 0.7771432500000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class SMOTE\n",
    "\n",
    "learning_rates = [1e-2, 1e-3, 5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deb2c447",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64fdff5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011079707543055217, AUC: 0.4224414166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024819238980611164, AUC: 0.6706555833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002305363098780314, AUC: 0.6838636666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002249549627304077, AUC: 0.6926545000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011115634441375732, AUC: 0.44383383333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018068215847015382, AUC: 0.7456852500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023212509950002033, AUC: 0.7392154999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015493378241856893, AUC: 0.8035191666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010960235595703124, AUC: 0.5768070833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017772521575291952, AUC: 0.7280496666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002030446648597717, AUC: 0.7317598333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002225544532140096, AUC: 0.7191306666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011004793246587117, AUC: 0.5102983333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002655739943186442, AUC: 0.6715455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024241018295288086, AUC: 0.6875708333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020734771887461344, AUC: 0.6995684999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010937273104985555, AUC: 0.5866186666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025021345615386962, AUC: 0.6649868333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021567532221476236, AUC: 0.6796695833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002164699633916219, AUC: 0.6854745833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102820873260498, AUC: 0.47283908333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024672954082489013, AUC: 0.667153\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024438456694285075, AUC: 0.6836446666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020616127649943036, AUC: 0.7165444166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094226638476054, AUC: 0.5688254166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026505370140075682, AUC: 0.6677074166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00240738836924235, AUC: 0.6775998333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022335880597432454, AUC: 0.682071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010978671709696451, AUC: 0.5958104166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022194448312123616, AUC: 0.6927335000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002101566791534424, AUC: 0.7338781666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023925371170043946, AUC: 0.730314\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010997230211893718, AUC: 0.5267864166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023940105438232423, AUC: 0.6662441666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002346807559331258, AUC: 0.7045654999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019873799085617064, AUC: 0.7091379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001110329270362854, AUC: 0.3660240833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002104702075322469, AUC: 0.7089835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001959824005762736, AUC: 0.6864575833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001872533122698466, AUC: 0.7095570833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011009675661722818, AUC: 0.5187256666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020721250375111896, AUC: 0.6062379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023756839434305827, AUC: 0.6242713333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002541064977645874, AUC: 0.63143125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011076082388559976, AUC: 0.41983375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020222012599309287, AUC: 0.6294074166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023207520643870037, AUC: 0.6743595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002417462428410848, AUC: 0.6905260000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011179471413294475, AUC: 0.44402650000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013568212191263835, AUC: 0.6903383333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013643276294072468, AUC: 0.7201696666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001355271061261495, AUC: 0.7464018333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011114279826482138, AUC: 0.4271655833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020804568926493325, AUC: 0.6158478333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002302732308705648, AUC: 0.63495125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024565763473510744, AUC: 0.6399930833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101883888244629, AUC: 0.5275499166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020776435534159343, AUC: 0.6093036666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023438007036844888, AUC: 0.6255574166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024835861523946124, AUC: 0.6338859166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021637121836345, AUC: 0.6185741666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002009077310562134, AUC: 0.6403611666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023686267534891764, AUC: 0.66764475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024570186932881674, AUC: 0.6808744166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011052033503850301, AUC: 0.4367945833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021005558172861737, AUC: 0.6255113333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023509362538655597, AUC: 0.6382724166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002499995549519857, AUC: 0.6464299166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021707852681478, AUC: 0.46878899999999996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-2, 1e-3, 5e-3]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f0657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57add3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 class euclidean distance capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-2, 1e-3, 5e-3]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['distance'] = 'euclidean'\n",
    "\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"distance_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bc95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805eafa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3 class cosine distance capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-2, 1e-3, 5e-3]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['distance'] = 'cosine'\n",
    "\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"cosine_distance_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe9f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77723321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3 class triplet loss capped smote\n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-2, 1e-3, 5e-3]\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "loss_caps = [1, 5, 10]\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = loss_cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                _, _ = train.train_triplet_capped_loss(epoch, train_loader_tripletloss_smote, network, optimizer, verbose=False, cap_calc=loss_fns.TripletLoss,loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"triplet_loss_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589562a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd4ae08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251272b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
