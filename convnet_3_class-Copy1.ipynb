{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4df9c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  \n",
    "\n",
    "\n",
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops \n",
    "\n",
    "NUM_CLASSES = 10\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "          #   \"mean_40\", \"variance_40\",\n",
    "          #   \"mean_50\", \"variance_50\",\n",
    "             \"cap\", \"normalization\", \"other\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9edc25c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 3 classes\n",
    "\n",
    "NUM_CLASSES_REDUCED = 3\n",
    "nums = (0, 3, 1)\n",
    "ratio = (200, 20, 1)\n",
    "\n",
    "norm=True\n",
    "\n",
    "if norm:\n",
    "    transform=torchvision.transforms.Compose([torchvision.transforms.Normalize(mean=[134.1855, 122.7346, 118.3749], std=[70.5125, 64.4848, 66.5604])])\n",
    "else:\n",
    "    transform=None\n",
    "\n",
    "    \n",
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "\n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums, transform=transform)\n",
    "targets = ratio_train_CIFAR10.labels \n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED, transform=transform)\n",
    "\n",
    "triplet_train_CIFAR10 = class_sampling.ForTripletLoss(reduced_train_CIFAR10, smote=False, transform=transform, num_classes=3)\n",
    "triplet_ratio_train_CIFAR10 = class_sampling.ForTripletLoss(ratio_train_CIFAR10, smote=False, transform=transform, num_classes=3)\n",
    "triplet_smote_train_CIFAR10 = class_sampling.ForTripletLoss(smote_train_CIFAR10, smote=True, transform=transform, num_classes=3)\n",
    "\n",
    "weight = 1. / class_count\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= max(class_count)\n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss = DataLoader(triplet_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_ratio = DataLoader(triplet_ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_smote = DataLoader(triplet_smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225b42ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3 class normal\n",
    "\n",
    "learning_rates = [1e-2, 5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochsf):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 3, nums, (1, 1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeea5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7098b880",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011114914417266846, AUC: 0.36460475000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023845341205596922, AUC: 0.6524673333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022546345392862955, AUC: 0.6776095833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021248392264048258, AUC: 0.6789875833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010890957514444986, AUC: 0.6553961666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026082831223805747, AUC: 0.6564603333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025372488498687745, AUC: 0.6645089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002328646183013916, AUC: 0.6728794166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011088247299194335, AUC: 0.3817441666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002548003117243449, AUC: 0.6628840833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002347040812174479, AUC: 0.6801826666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024017000993092853, AUC: 0.6815919166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011053735812505087, AUC: 0.47676008333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021743202209472657, AUC: 0.6694651666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024035780429840087, AUC: 0.67927975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002287601153055827, AUC: 0.6911768333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011105512777964273, AUC: 0.4537710833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024255019823710123, AUC: 0.6602790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00244619615872701, AUC: 0.6691070833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002357912302017212, AUC: 0.6678518333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011044793128967285, AUC: 0.5105604166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026529372533162436, AUC: 0.6611834166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002507765293121338, AUC: 0.6739195833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002636707623799642, AUC: 0.6807274166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984182357788087, AUC: 0.5173345833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00264628545443217, AUC: 0.6595385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022256182034810384, AUC: 0.6719530833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002115691582361857, AUC: 0.6757274999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989943345387776, AUC: 0.5011259166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027355337937672933, AUC: 0.6558719166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002476392428080241, AUC: 0.6726106666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023600268363952637, AUC: 0.6768278333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011025760968526204, AUC: 0.42552191666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023835336367289224, AUC: 0.66857325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024136878649393717, AUC: 0.6872316666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023080863157908123, AUC: 0.6927156666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010968177715937296, AUC: 0.5423361666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002490415255228678, AUC: 0.6663253333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022822824319203693, AUC: 0.6766434166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021200869083404543, AUC: 0.6781925000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010987565914789836, AUC: 0.5109765000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021397849718729654, AUC: 0.6143576666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023937728404998778, AUC: 0.6264779166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025934850374857584, AUC: 0.6353635833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010962210893630982, AUC: 0.5468815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002027741233507792, AUC: 0.6175555833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022231519222259523, AUC: 0.6293943333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023514188130696616, AUC: 0.6370340833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100361704826355, AUC: 0.4892809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002287514130274455, AUC: 0.6157618333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024806830088297525, AUC: 0.6311884166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002611160198847453, AUC: 0.6397296666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011075005133946738, AUC: 0.4532289166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002205244302749634, AUC: 0.6109433333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024013428688049315, AUC: 0.6282380000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025176260471343995, AUC: 0.6368599166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011068367958068847, AUC: 0.3566816666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021897602876027426, AUC: 0.5932159166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002451696475346883, AUC: 0.6155333333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002545725425084432, AUC: 0.6272624166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010999751488367717, AUC: 0.5111947499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020940867265065513, AUC: 0.61954425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023363668123881024, AUC: 0.6328976666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024719464778900145, AUC: 0.640209\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100516398747762, AUC: 0.5440201666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020988822778066, AUC: 0.6186764166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023466589450836183, AUC: 0.6286165833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00249346653620402, AUC: 0.6366689166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010951290130615235, AUC: 0.5694688333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022422805627187094, AUC: 0.6073609166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002545572280883789, AUC: 0.6200321666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026336670716603596, AUC: 0.6303713333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098416805267334, AUC: 0.5855186666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00209174911181132, AUC: 0.6160276666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023326919078826905, AUC: 0.630027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025377679665883382, AUC: 0.6363839166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011054413318634033, AUC: 0.47862899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002153881788253784, AUC: 0.6070026666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002315181096394857, AUC: 0.6226469166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002460697650909424, AUC: 0.6328822500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011101826429367066, AUC: 0.5477363333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026010025342305503, AUC: 0.6409613333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025153576532999673, AUC: 0.65616525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024006614685058592, AUC: 0.6659405833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010889729658762615, AUC: 0.6317735833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002673741658528646, AUC: 0.6438288333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002544086774190267, AUC: 0.6609465833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024662026564280194, AUC: 0.6694403333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011057429711023967, AUC: 0.37070891666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026512997150421143, AUC: 0.6436377500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002631504774093628, AUC: 0.6557332499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026166245142618815, AUC: 0.6669566666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011017692486445109, AUC: 0.5049165833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024596280256907145, AUC: 0.6480541666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025252668062845864, AUC: 0.6636315833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024301329453786215, AUC: 0.6712108333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010937167008717855, AUC: 0.5784809166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002661058823267619, AUC: 0.6419309999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002551306168238322, AUC: 0.6576916666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024862375259399414, AUC: 0.6744619166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001104104439417521, AUC: 0.4824792499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002660263140996297, AUC: 0.640744\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002653989553451538, AUC: 0.6594589166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002421207586924235, AUC: 0.66597275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010970288515090942, AUC: 0.60064225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002659346580505371, AUC: 0.64086575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002460325082143148, AUC: 0.654786\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024787142276763915, AUC: 0.66468225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095256487528483, AUC: 0.57242625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026304399967193603, AUC: 0.6404766666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002652269999186198, AUC: 0.66032175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002490153233210246, AUC: 0.6688147500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011030465761820475, AUC: 0.5263896666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026408041318257648, AUC: 0.642167\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025300122102101643, AUC: 0.6629468333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002523405392964681, AUC: 0.6710400833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011044965585072836, AUC: 0.4005821666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002568801959355672, AUC: 0.6445069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025338256359100342, AUC: 0.6592468333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023744911352793375, AUC: 0.6657563333333333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class ratio\n",
    "\n",
    "learning_rates =  [1e-2, 1e-3, 5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ec6b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0850938a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011114914417266846, AUC: 0.36460475000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004324390570322673, AUC: 0.7364573333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037501402695973712, AUC: 0.7574884166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004474962711334228, AUC: 0.79239925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011069943904876709, AUC: 0.37739375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030027631918589275, AUC: 0.7237471666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005124391555786133, AUC: 0.7233443333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005480806827545166, AUC: 0.7494814999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011040428479512532, AUC: 0.5238316666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026677467823028565, AUC: 0.7431178333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030789877573649087, AUC: 0.7665563333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003713517268498739, AUC: 0.7753674999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011082092126210531, AUC: 0.41351783333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037240614891052245, AUC: 0.7545196666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004394479274749756, AUC: 0.7672481666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004347028255462647, AUC: 0.7734269166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106703003247579, AUC: 0.4070911666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026210310459136963, AUC: 0.7629115833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0039685251712799074, AUC: 0.7697971666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004595277627309163, AUC: 0.767683\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001108153025309245, AUC: 0.3999905833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022470784982045492, AUC: 0.7702179166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003133465607961019, AUC: 0.7707361666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0049484554926554365, AUC: 0.765371\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010961705843607584, AUC: 0.56531325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031084604263305663, AUC: 0.72932\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003631563981374105, AUC: 0.7711185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004153175830841065, AUC: 0.7708448333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010985121726989745, AUC: 0.5120645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033005879720052083, AUC: 0.7558905833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004198158264160156, AUC: 0.7633551666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004296850045522054, AUC: 0.7630563333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011044689814249675, AUC: 0.48050491666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028805821736653647, AUC: 0.7561376666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038989100456237793, AUC: 0.7657264166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004408852418263753, AUC: 0.7637334999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011007845004399617, AUC: 0.47587916666666663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003016411225001017, AUC: 0.77174575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004040171146392822, AUC: 0.7790771666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003944351355234782, AUC: 0.7868860833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011097069183985393, AUC: 0.3607334166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019454789559046428, AUC: 0.7370486666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030299894014994304, AUC: 0.7342996666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004138584613800049, AUC: 0.7521163333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011136069695154825, AUC: 0.4649491666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018370604515075683, AUC: 0.75338875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028546384970347086, AUC: 0.7578339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038062037626902263, AUC: 0.7581599999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105426033337911, AUC: 0.5275308333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014596267143885296, AUC: 0.7661285000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003124219497044881, AUC: 0.7716180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003941149473190308, AUC: 0.7777175833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010874457359313966, AUC: 0.6314968333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017514625390370687, AUC: 0.7756105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002999461809794108, AUC: 0.7762269166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035831859906514485, AUC: 0.7834435000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097713589668274, AUC: 0.5584771666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016665372053782145, AUC: 0.7782388333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002889764308929443, AUC: 0.7859846666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033138333956400554, AUC: 0.7981136666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011013548771540325, AUC: 0.4873828333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015104289849599203, AUC: 0.7771413333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003099284887313843, AUC: 0.7623005000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037605947653452554, AUC: 0.788077\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011025866270065307, AUC: 0.4671732500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016522378524144491, AUC: 0.7762235000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003039016882578532, AUC: 0.7727390000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027796326478322347, AUC: 0.7927512499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011211527188618978, AUC: 0.43609574999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016009234189987183, AUC: 0.7540048333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003321781873703003, AUC: 0.7532868333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038763590653737388, AUC: 0.7690661666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001109712322552999, AUC: 0.4256054166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001548742373784383, AUC: 0.7835933333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028251368204752604, AUC: 0.7742723333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037287307580312094, AUC: 0.7819851666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011047581434249878, AUC: 0.5336638333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001437873363494873, AUC: 0.7648771666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027796269257863364, AUC: 0.7473274999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003251352866490682, AUC: 0.747195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099843462308248, AUC: 0.5031244166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000983447293440501, AUC: 0.7540011666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009316765268643697, AUC: 0.7603031666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009038120309511821, AUC: 0.7741791666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010999187231063843, AUC: 0.47526124999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010247697035471598, AUC: 0.716252\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009395894209543864, AUC: 0.7675944166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009158042470614116, AUC: 0.7773415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011077208518981935, AUC: 0.37644199999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010200337966283163, AUC: 0.7103426666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009421564936637879, AUC: 0.7574954166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000915204644203186, AUC: 0.7683401666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010905884504318238, AUC: 0.6186204166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000990890085697174, AUC: 0.7536075833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009268062114715576, AUC: 0.7816728333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008966490427652995, AUC: 0.7833834999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110538911819458, AUC: 0.44230416666666655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010245202779769899, AUC: 0.7361385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009510651628176372, AUC: 0.7567827500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009181015888849894, AUC: 0.7757928333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010952026049296062, AUC: 0.5783590833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010169446468353271, AUC: 0.7366876666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009388421376546224, AUC: 0.7722362500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009153200984001159, AUC: 0.7770106666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006342172622681, AUC: 0.5516030833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009845278660456339, AUC: 0.7520744166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009260993401209514, AUC: 0.7766303333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009036955237388611, AUC: 0.7860978333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011007173856099446, AUC: 0.5188099166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009829029440879822, AUC: 0.7605946666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009153713583946228, AUC: 0.7777763333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009059300422668457, AUC: 0.7845128333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011012860933939615, AUC: 0.4649145833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010173428058624268, AUC: 0.7403071666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009337853988011678, AUC: 0.7675098333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00093434872229894, AUC: 0.7663435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011092154184977213, AUC: 0.3942583333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010150946378707885, AUC: 0.7314218333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009450232187906901, AUC: 0.7625986666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009239435990651448, AUC: 0.7650159999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011078580220540365, AUC: 0.47446458333333336\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010080868005752563, AUC: 0.7724891666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017206243673960368, AUC: 0.7600154166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025581661065419516, AUC: 0.7579385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097543716430664, AUC: 0.5109705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011088798046112061, AUC: 0.7401753333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018518801132837931, AUC: 0.7338064166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002743795871734619, AUC: 0.731045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011036570072174073, AUC: 0.46421399999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008840240041414896, AUC: 0.7970324999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013941014210383097, AUC: 0.7866110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019833201964696246, AUC: 0.7845465000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008837223052978, AUC: 0.4977171666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009412099123001098, AUC: 0.7853393333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015829279820124308, AUC: 0.7723570000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021948086420694986, AUC: 0.7723818333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011010413964589437, AUC: 0.51394675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009324169158935547, AUC: 0.7874215000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017953593333562216, AUC: 0.7668494999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024173832734425862, AUC: 0.7670338333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010929431915283204, AUC: 0.63767925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009704872767130534, AUC: 0.765742\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019546128511428833, AUC: 0.7539439999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002747676134109497, AUC: 0.7575941666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011145447492599487, AUC: 0.46165741666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009553443988164266, AUC: 0.7739273333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001611437956492106, AUC: 0.7479576666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024375863075256346, AUC: 0.7380369999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021378835042318, AUC: 0.53873425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010730412801106771, AUC: 0.7764777500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018152755498886109, AUC: 0.7791993333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027486067612965902, AUC: 0.7782873333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991355975468954, AUC: 0.4919227499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001004781663417816, AUC: 0.7620773333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016514448324839275, AUC: 0.75926925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023767530123392742, AUC: 0.7635225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005579233169555, AUC: 0.4908746666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009588287472724915, AUC: 0.7739688333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015622458457946777, AUC: 0.7492489999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022654112974802654, AUC: 0.7436234166666665\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class oversampled \n",
    "\n",
    "learning_rates = [5e-2, 1e-2, 1e-3, 5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bd49495",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "801a223f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011012120644251506, AUC: 0.5157660833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001253303090731303, AUC: 0.8061060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00180307137966156, AUC: 0.7786304166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020693496068318685, AUC: 0.7883991666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011086985270182292, AUC: 0.44565125000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015388530095418295, AUC: 0.7731013333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001989365021387736, AUC: 0.7840317499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023551328976949056, AUC: 0.7794379166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010967851877212525, AUC: 0.5745733333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001639639377593994, AUC: 0.7576433333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017591052452723185, AUC: 0.7736256666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023112374146779377, AUC: 0.7581950833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002494891484578, AUC: 0.4803038333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014218109846115112, AUC: 0.7767816666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017920361359914144, AUC: 0.7603201666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002506418307622274, AUC: 0.75862225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011049503485361736, AUC: 0.48021725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014112598498662312, AUC: 0.7593684166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001902145783106486, AUC: 0.7781636666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027670453389485677, AUC: 0.75392525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106808344523112, AUC: 0.454666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013035308122634887, AUC: 0.7731513333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002188827991485596, AUC: 0.7705558333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002120443264643351, AUC: 0.7660238333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099803884824117, AUC: 0.49977200000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016421691974004109, AUC: 0.7722381666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018135635058085123, AUC: 0.7667926666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002335370461146037, AUC: 0.7448159166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010977029403050741, AUC: 0.5396588333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015652592579523722, AUC: 0.7653735833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001929542104403178, AUC: 0.77280475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021792489687601727, AUC: 0.7815707499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001108059008916219, AUC: 0.44486525000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013919612963994344, AUC: 0.7734225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001804989218711853, AUC: 0.7789026666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020720606644948326, AUC: 0.7727559166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001104783535003662, AUC: 0.4987771666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098078966140747, AUC: 0.7716941666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001457849899927775, AUC: 0.8023884166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016427493095397948, AUC: 0.8078056666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011019185384114583, AUC: 0.45172300000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010436822573343912, AUC: 0.736511\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012538435856501262, AUC: 0.7564631666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014367674191792805, AUC: 0.7662365833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010970526536305746, AUC: 0.5470054166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001237207571665446, AUC: 0.7254103333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002054531415303548, AUC: 0.7239986666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020512109597524007, AUC: 0.7460268333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097231427828471, AUC: 0.5749736666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001056501825650533, AUC: 0.7505771666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001690821409225464, AUC: 0.7164789166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001415266195933024, AUC: 0.763293\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102907697359721, AUC: 0.5271718333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009345992604891459, AUC: 0.7620933333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014747664133707681, AUC: 0.8023721666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014554672241210936, AUC: 0.8230061666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100229541460673, AUC: 0.5196165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010060510436693828, AUC: 0.76081425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016890269120534262, AUC: 0.7409915833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015492320458094279, AUC: 0.7791948333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010959211190541586, AUC: 0.5640844166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010360184907913209, AUC: 0.7528234166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012657465934753419, AUC: 0.764161\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016044780015945434, AUC: 0.7387245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010979425509770712, AUC: 0.5561055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012147883971532186, AUC: 0.7359966666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015517190297444662, AUC: 0.7607705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017279245456059773, AUC: 0.7684185833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095780611038208, AUC: 0.6577568333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010218689640363056, AUC: 0.7594101666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010960452953974406, AUC: 0.7840351666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016640307903289795, AUC: 0.7826364166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011037744681040445, AUC: 0.45095883333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012176516850789388, AUC: 0.5609355833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011927584012349447, AUC: 0.7690583333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016710100571314494, AUC: 0.784561\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010958511432011921, AUC: 0.5596116666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009781824549039205, AUC: 0.7259526666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014026125272115072, AUC: 0.7609326666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013844541311264038, AUC: 0.780903\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011020680665969848, AUC: 0.5094350833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010055757164955138, AUC: 0.7045060833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009756147861480713, AUC: 0.72436925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009764462113380433, AUC: 0.7343043333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010976256132125854, AUC: 0.5527256666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010024517973264058, AUC: 0.694533\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009781240820884705, AUC: 0.7140665833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009719487031300863, AUC: 0.7265868333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099200169245402, AUC: 0.5543278333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009892687797546386, AUC: 0.7050376666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009547983010609945, AUC: 0.7362915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009518916805585226, AUC: 0.7437065833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010928544600804647, AUC: 0.6360979166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009834163586298625, AUC: 0.7157729166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009556876619656881, AUC: 0.7339911666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009497124552726746, AUC: 0.7484206666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102524717648824, AUC: 0.45531491666666674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009954072038332621, AUC: 0.7283748333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009584515492121378, AUC: 0.7391610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009516260226567586, AUC: 0.7416766666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011051525672276816, AUC: 0.4477594999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001019577383995056, AUC: 0.7081788333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009705155690511067, AUC: 0.7183578333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009593235055605571, AUC: 0.7293769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011037365595499675, AUC: 0.5166208333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010113667647043865, AUC: 0.698139\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009785071810086568, AUC: 0.7217185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009719815055529277, AUC: 0.7402475000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011079127391179402, AUC: 0.41191400000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010055619279543558, AUC: 0.709362\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009706699649492899, AUC: 0.7291680833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009598867694536845, AUC: 0.7349563333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011024837096532186, AUC: 0.4468718333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009982790549596151, AUC: 0.7149139166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009762466351191203, AUC: 0.7226099166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009546750783920289, AUC: 0.7393475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010990931590398152, AUC: 0.5660382500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010203874508539835, AUC: 0.7060394166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009854515194892884, AUC: 0.7155426666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009696878989537557, AUC: 0.7249918333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011034749746322633, AUC: 0.4930429166666666\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0009895188609759012, AUC: 0.7346210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011809653441111248, AUC: 0.7438486666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013460488319396973, AUC: 0.7523574166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011083951393763224, AUC: 0.42329483333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009447627067565918, AUC: 0.7579581666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009011501669883728, AUC: 0.7721625833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009771966139475504, AUC: 0.8049443333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010899279514948526, AUC: 0.6394894166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009647173484166464, AUC: 0.7354416666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010964591900507609, AUC: 0.7503926666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012541187206904093, AUC: 0.7699424166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011026158332824707, AUC: 0.47225925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010245993137359619, AUC: 0.7351810833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010234986543655395, AUC: 0.7520389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010526731808980306, AUC: 0.7591795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011009358962376912, AUC: 0.4875049166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001004116952419281, AUC: 0.74546075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010066910584767658, AUC: 0.7568218333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011317096153895061, AUC: 0.7653755833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101522405942281, AUC: 0.4749663333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009305041035016378, AUC: 0.7516885000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001082132617632548, AUC: 0.7488967500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012077295780181885, AUC: 0.7588745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011083028713862102, AUC: 0.39833583333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010479892094930013, AUC: 0.726082\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010504425962766012, AUC: 0.75870475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012126639684041341, AUC: 0.7507716666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011043804089228312, AUC: 0.4074766666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010150442918141682, AUC: 0.7316553333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010362143516540528, AUC: 0.7608671666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013159387509028117, AUC: 0.7639218333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992871522903442, AUC: 0.5172945833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001013302763303121, AUC: 0.7348711666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010559245745340982, AUC: 0.7470708333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001259264826774597, AUC: 0.7657845833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011163246234258016, AUC: 0.41883325000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009479443828264872, AUC: 0.7565174166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011192746957143147, AUC: 0.7721293333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015033675829569499, AUC: 0.7720348333333332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class SMOTE\n",
    "\n",
    "learning_rates = [5e-2, 1e-2, 1e-3, 5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "deb2c447",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a64fdff5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011079707543055217, AUC: 0.4224414166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024819238980611164, AUC: 0.6706555833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002305363098780314, AUC: 0.6838636666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002249549627304077, AUC: 0.6926545000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011115634441375732, AUC: 0.44383383333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018068215847015382, AUC: 0.7456852500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023212509950002033, AUC: 0.7392154999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015493378241856893, AUC: 0.8035191666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010960235595703124, AUC: 0.5768070833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017772521575291952, AUC: 0.7280496666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002030446648597717, AUC: 0.7317598333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002225544532140096, AUC: 0.7191306666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011004793246587117, AUC: 0.5102983333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002655739943186442, AUC: 0.6715455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024241018295288086, AUC: 0.6875708333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020734771887461344, AUC: 0.6995684999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010937273104985555, AUC: 0.5866186666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025021345615386962, AUC: 0.6649868333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021567532221476236, AUC: 0.6796695833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002164699633916219, AUC: 0.6854745833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102820873260498, AUC: 0.47283908333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024672954082489013, AUC: 0.667153\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024438456694285075, AUC: 0.6836446666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020616127649943036, AUC: 0.7165444166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094226638476054, AUC: 0.5688254166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026505370140075682, AUC: 0.6677074166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00240738836924235, AUC: 0.6775998333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022335880597432454, AUC: 0.682071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010978671709696451, AUC: 0.5958104166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022194448312123616, AUC: 0.6927335000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002101566791534424, AUC: 0.7338781666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023925371170043946, AUC: 0.730314\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010997230211893718, AUC: 0.5267864166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023940105438232423, AUC: 0.6662441666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002346807559331258, AUC: 0.7045654999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019873799085617064, AUC: 0.7091379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001110329270362854, AUC: 0.3660240833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002104702075322469, AUC: 0.7089835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001959824005762736, AUC: 0.6864575833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001872533122698466, AUC: 0.7095570833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011009675661722818, AUC: 0.5187256666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020721250375111896, AUC: 0.6062379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023756839434305827, AUC: 0.6242713333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002541064977645874, AUC: 0.63143125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011076082388559976, AUC: 0.41983375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020222012599309287, AUC: 0.6294074166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023207520643870037, AUC: 0.6743595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002417462428410848, AUC: 0.6905260000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011179471413294475, AUC: 0.44402650000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013568212191263835, AUC: 0.6903383333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013643276294072468, AUC: 0.7201696666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001355271061261495, AUC: 0.7464018333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011114279826482138, AUC: 0.4271655833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020804568926493325, AUC: 0.6158478333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002302732308705648, AUC: 0.63495125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024565763473510744, AUC: 0.6399930833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101883888244629, AUC: 0.5275499166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020776435534159343, AUC: 0.6093036666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023438007036844888, AUC: 0.6255574166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024835861523946124, AUC: 0.6338859166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021637121836345, AUC: 0.6185741666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002009077310562134, AUC: 0.6403611666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023686267534891764, AUC: 0.66764475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024570186932881674, AUC: 0.6808744166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011052033503850301, AUC: 0.4367945833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021005558172861737, AUC: 0.6255113333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023509362538655597, AUC: 0.6382724166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002499995549519857, AUC: 0.6464299166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021707852681478, AUC: 0.46878899999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00220019261042277, AUC: 0.6068617499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002459147294362386, AUC: 0.6274133333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025924087365468344, AUC: 0.6359256666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011013393799463908, AUC: 0.5305079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013590420484542846, AUC: 0.6783560833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013645085096359252, AUC: 0.7183881666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014338650306065877, AUC: 0.7315265000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011121948957443237, AUC: 0.4090535833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023055950005849203, AUC: 0.62341475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024989202817281087, AUC: 0.6361519166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025223532517751057, AUC: 0.6455465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011048592726389567, AUC: 0.4177319166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025481193860371907, AUC: 0.6484254166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024674687385559083, AUC: 0.6651425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024357466697692873, AUC: 0.6713125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011073541641235351, AUC: 0.43439625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025950984160105386, AUC: 0.6409186666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00265375812848409, AUC: 0.65872775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002514628251393636, AUC: 0.6681988333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011076154311498007, AUC: 0.5551947500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022441479365030922, AUC: 0.6971596666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022008349895477294, AUC: 0.7016465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002065916140874227, AUC: 0.6919836666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011046329736709595, AUC: 0.41931783333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026869331200917563, AUC: 0.6524774999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025786888599395753, AUC: 0.6713168333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024820197423299152, AUC: 0.6819194999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011026749610900878, AUC: 0.5098351666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027338616847991945, AUC: 0.6478489166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002631261905034383, AUC: 0.6607544166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025313848654429116, AUC: 0.6689055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011076873143513997, AUC: 0.44860174999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026160202026367188, AUC: 0.6505420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026627834637959797, AUC: 0.6636318333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002465544621149699, AUC: 0.6722847499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105912446975708, AUC: 0.44809116666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024165361722310384, AUC: 0.6961640833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020986950397491456, AUC: 0.6857074166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019524118900299072, AUC: 0.6852285833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005394061406454, AUC: 0.49269275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002697857062021891, AUC: 0.6385236666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025773534774780273, AUC: 0.6588340833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002471854289372762, AUC: 0.6727894166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011079903443654377, AUC: 0.43869183333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025158895651499432, AUC: 0.6504198333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002521126906077067, AUC: 0.66575775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022981462478637695, AUC: 0.7020506666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098792274792989, AUC: 0.5084715833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002566495656967163, AUC: 0.6594516666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023025930722554524, AUC: 0.7074231666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021977332433064777, AUC: 0.7066765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011064178148905437, AUC: 0.4041901666666667\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0008978073398272197, AUC: 0.7981495000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010476662516593932, AUC: 0.797174\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013957345088322958, AUC: 0.801761\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011011792421340943, AUC: 0.5008129166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010919816493988037, AUC: 0.7615899166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017090630531311035, AUC: 0.75741025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017677673896153768, AUC: 0.7701735833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011115765571594238, AUC: 0.4346426666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011828872362772624, AUC: 0.7415978333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012650069793065388, AUC: 0.7911315833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015466095209121704, AUC: 0.7957371666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096881588300069, AUC: 0.5596155833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011262359221776326, AUC: 0.7633\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018325774272282918, AUC: 0.7689121666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017438907623291016, AUC: 0.7971378333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011114460229873658, AUC: 0.4237485833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012267327308654785, AUC: 0.71922525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015028331677118937, AUC: 0.7402779166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015713134209314982, AUC: 0.7578658333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010921560128529866, AUC: 0.6199454999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012324892679850261, AUC: 0.7233569166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001209743618965149, AUC: 0.7630251666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001463202993075053, AUC: 0.7786341666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011094931761423746, AUC: 0.3925545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010498208204905192, AUC: 0.7461782499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013926180203755697, AUC: 0.7727065000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016756459871927897, AUC: 0.7822736666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011045343081156412, AUC: 0.5020824166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010106836358706156, AUC: 0.7523293333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012493394215901694, AUC: 0.7634164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015255335966746011, AUC: 0.78138175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011037658055623371, AUC: 0.45513633333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001259947101275126, AUC: 0.7325080833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013927234411239625, AUC: 0.7789625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014783886273701985, AUC: 0.8053306666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989155371983845, AUC: 0.5090311666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011903045972188313, AUC: 0.7370009166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011771143277486166, AUC: 0.7879765000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017296486695607502, AUC: 0.7904987499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992594162623087, AUC: 0.5278233333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009940093557039896, AUC: 0.6945128333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009657553434371948, AUC: 0.7195705833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009802146951357524, AUC: 0.7263654166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096669832865397, AUC: 0.6075720833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001032851497332255, AUC: 0.7264590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009843558669090271, AUC: 0.7145568333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009739188353220622, AUC: 0.725774\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011045145988464356, AUC: 0.3714389166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010056789914766947, AUC: 0.693098\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009946979482968648, AUC: 0.7099201666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009966118733088175, AUC: 0.7193093333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011034783919652302, AUC: 0.5589384166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009846031665802003, AUC: 0.6988330833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009612971941630045, AUC: 0.7240057499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009637996355692546, AUC: 0.7308990833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989969968795777, AUC: 0.5054628333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009815845489501954, AUC: 0.7026663333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009770113428433736, AUC: 0.7166691666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001000847041606903, AUC: 0.7217943333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011000647147496542, AUC: 0.5001725833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010088207324345907, AUC: 0.7015523333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009667608340581258, AUC: 0.7267001666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009430569211641948, AUC: 0.7397771666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998655557632447, AUC: 0.47744099999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009838627179463704, AUC: 0.7025991666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009565305511156718, AUC: 0.7255278333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009549003640810648, AUC: 0.7324212500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105409860610962, AUC: 0.44774575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001001656452814738, AUC: 0.6875285833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009738151431083679, AUC: 0.7117248333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009515709479649861, AUC: 0.7313041666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011092275778452555, AUC: 0.42017716666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000999273439248403, AUC: 0.6986846666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009803808530171713, AUC: 0.723987\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009712782502174377, AUC: 0.7371078333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002099911371866, AUC: 0.4877943333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010111721754074096, AUC: 0.6899492500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000988524337609609, AUC: 0.7154061666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009806044101715089, AUC: 0.7299138333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095471421877543, AUC: 0.55029225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009719979961713155, AUC: 0.7315994166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000976609210173289, AUC: 0.7509495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011375132004419963, AUC: 0.7662175000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994465748469034, AUC: 0.48439916666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001061615268389384, AUC: 0.7240506666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012210078239440919, AUC: 0.7371387500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015706359545389812, AUC: 0.7466465000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003560225168864, AUC: 0.4753065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010294776360193888, AUC: 0.7180058333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011562711795171101, AUC: 0.7532751666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001341441512107849, AUC: 0.7613789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011035508314768472, AUC: 0.4413095833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001077295462290446, AUC: 0.7351731666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001011013646920522, AUC: 0.7633433333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013661166429519652, AUC: 0.7625381666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011045393546422323, AUC: 0.46448399999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009626933336257935, AUC: 0.7466321666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013255577882130942, AUC: 0.7482018333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001673708995183309, AUC: 0.7612776666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099991281827291, AUC: 0.5080065833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009914895296096802, AUC: 0.7380545833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010950936476389567, AUC: 0.7510445833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010407268206278483, AUC: 0.7815078333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010982731978098552, AUC: 0.5526868333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008987191915512085, AUC: 0.7940515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009225935141245524, AUC: 0.7729006666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011297925710678101, AUC: 0.7763786666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011019532680511474, AUC: 0.46340766666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001046321153640747, AUC: 0.7276085000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001444495439529419, AUC: 0.7264068333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011280192136764527, AUC: 0.7771543333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011034169991811116, AUC: 0.5026128333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012736262877782185, AUC: 0.7118106666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012326940298080444, AUC: 0.75678\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014776597420374552, AUC: 0.7702973333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096749226252238, AUC: 0.5954095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088329792022705, AUC: 0.7272213333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009565931757291158, AUC: 0.7696033333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014221105575561524, AUC: 0.7676381666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011046780745188394, AUC: 0.39627016666666665\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010901000102361044, AUC: 0.7558843333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001592508633931478, AUC: 0.7648861666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015164965391159058, AUC: 0.793024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011064399878184, AUC: 0.44293325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011655902862548828, AUC: 0.7469121666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001499106486638387, AUC: 0.7580291666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018441102107365926, AUC: 0.7539993333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011045443614323934, AUC: 0.46125925000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011498111089070639, AUC: 0.7564623333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015798531770706177, AUC: 0.7602367499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002093439976374308, AUC: 0.7589875833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103798508644104, AUC: 0.4563561666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010767038265864055, AUC: 0.7555844999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013137869834899901, AUC: 0.7909311666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018448962370554607, AUC: 0.8021533333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011111820141474407, AUC: 0.39343100000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010647828578948975, AUC: 0.7487516666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010543740590413411, AUC: 0.7829160833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015670624574025472, AUC: 0.78262625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989521741867066, AUC: 0.5096501666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010397976636886597, AUC: 0.6715216666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012110722064971923, AUC: 0.7950606666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001315303881963094, AUC: 0.8034713333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988940795262656, AUC: 0.54246775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010396425724029542, AUC: 0.77369625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013023833433787027, AUC: 0.7920919166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001647231618563334, AUC: 0.7933439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011101274490356446, AUC: 0.4800931666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012190799315770467, AUC: 0.7243924166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014194357792536418, AUC: 0.7684516666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018088542620340983, AUC: 0.7991959166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011079596281051635, AUC: 0.47734308333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010078697800636291, AUC: 0.7623546666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019584929545720417, AUC: 0.7404785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002114712397257487, AUC: 0.7761678333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099352796872457, AUC: 0.5210364166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000985157032807668, AUC: 0.7524676666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013552032709121704, AUC: 0.7752496666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020298471848169963, AUC: 0.7630496666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011084710756937663, AUC: 0.40398716666666673\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010389591852823894, AUC: 0.7078804999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00101042640209198, AUC: 0.7339591666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001005630652109782, AUC: 0.7341568333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011061173677444458, AUC: 0.42995625000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010182913541793824, AUC: 0.6911618333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009861480991045634, AUC: 0.7041939166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009900185267130534, AUC: 0.7167741666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010904707511266072, AUC: 0.6296754999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009889025092124938, AUC: 0.7117481666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009789374073346457, AUC: 0.7166211666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010026154716809592, AUC: 0.7136285833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011012648344039917, AUC: 0.5568328333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000994162897268931, AUC: 0.69452975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009807746410369874, AUC: 0.713223\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010047656496365864, AUC: 0.7179258333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011070818106333415, AUC: 0.5687063333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001006205121676127, AUC: 0.6988258333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000978074550628662, AUC: 0.7124868333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001018133560816447, AUC: 0.7152780833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010985445976257324, AUC: 0.5230313333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009950926502545674, AUC: 0.6956164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009819940725962321, AUC: 0.7155480833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009712765018145244, AUC: 0.7215335833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010997783740361532, AUC: 0.498168\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009731910427411397, AUC: 0.721351\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009465526739756266, AUC: 0.7449560833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009440785646438599, AUC: 0.7537543333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100550929705302, AUC: 0.48551108333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000999036133289337, AUC: 0.6902816666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009760162234306335, AUC: 0.7158144999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009940849741299947, AUC: 0.7182773333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102190891901652, AUC: 0.497121\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010059087276458741, AUC: 0.6950874166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009829614957173665, AUC: 0.7152500833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009939456383387247, AUC: 0.7259315000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011133894920349122, AUC: 0.43749166666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009896463751792907, AUC: 0.6964640833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009717114965120951, AUC: 0.7129487499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009850355784098307, AUC: 0.7198729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011116003195444743, AUC: 0.4375711666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010845239957173666, AUC: 0.7239355833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001000300625960032, AUC: 0.7716781666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010804837544759114, AUC: 0.7892138333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005011796951295, AUC: 0.5969188333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010175198117891948, AUC: 0.7381701666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009666255712509155, AUC: 0.7659948333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013635136683781942, AUC: 0.763916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096467932065328, AUC: 0.5623629166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010189871191978454, AUC: 0.7297233333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010409197807312012, AUC: 0.7609380833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107648233572642, AUC: 0.786237\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100347916285197, AUC: 0.5323065833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001034481922785441, AUC: 0.7378368333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010677272478739422, AUC: 0.781034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011011224587758382, AUC: 0.8040006666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011178616285324097, AUC: 0.420619\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010660743713378905, AUC: 0.7216486666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009368961652119954, AUC: 0.7537175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014726648330688477, AUC: 0.7627055833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001109140396118164, AUC: 0.4925566666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010664844512939453, AUC: 0.735557\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010442968606948853, AUC: 0.7712745000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013896885712941487, AUC: 0.76267225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011015418370564779, AUC: 0.5088568333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010164373715718588, AUC: 0.7339413333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010537364880243937, AUC: 0.7531733333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012184868653615315, AUC: 0.7592548333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097795605659485, AUC: 0.5675945833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009861572980880736, AUC: 0.7270125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013601859013239544, AUC: 0.7332570833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001307805856068929, AUC: 0.7573036666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105371634165446, AUC: 0.49668891666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000987179716428121, AUC: 0.7165998333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012264320850372316, AUC: 0.7397115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012034877141316731, AUC: 0.7726480833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00109502907594045, AUC: 0.572469\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010664153893788656, AUC: 0.73090825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001129280169804891, AUC: 0.7496519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017589335441589356, AUC: 0.7420976666666667\n",
      "\n",
      "[['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.01, 0.507028475, 0.005550550233304238, 0.6883744416666667, 0.0007813762927709038, 0.7008225166666667, 0.0005489203807511092, 0.7147971916666667, 0.001084670799979236, 1, True], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.48010207500000013, 0.0040245759886089584, 0.6325640333333333, 0.0007813134463016689, 0.656717975, 0.001237946620091735, 0.6682541083333333, 0.0016138611797778496, 1, True], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.4673024999999999, 0.001925358944370834, 0.6581931416666668, 0.00040011063935423635, 0.673894225, 0.0002918169134722911, 0.6821349916666667, 0.0001776262204992362, 1, True], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.01, 0.48017598333333333, 0.004692025142696941, 0.7475235999999998, 0.0004790775363775008, 0.7720993083333332, 0.0002686783432514601, 0.7860794416666667, 0.00019925992750145826, 5, True], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.49045664166666664, 0.0041226773691917335, 0.6995883249999999, 0.00010520508650756941, 0.71880685, 3.2524694837222436e-05, 0.7294666416666666, 3.6434124062569385e-05, 5, True], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.5037914916666667, 0.002123346875735349, 0.7354207833333335, 0.0004703117509461113, 0.7529644, 0.00017828695719694404, 0.7671035166666667, 9.118455736916647e-05, 5, True], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.01, 0.46808404166666673, 0.0021841574194295136, 0.7448027666666666, 0.0007383391036969455, 0.7728331749999999, 0.00028070084277701386, 0.782601925, 0.0003116596469659035, 10, True], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.5030481166666666, 0.004345725503407218, 0.700294675, 9.148313168812509e-05, 0.7185001583333332, 0.00012684131736729194, 0.723713325, 0.00013200010181868067, 10, True], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.5187944499999999, 0.0030958091478530563, 0.7295333499999999, 4.683173186777771e-05, 0.7580431, 0.00020244076459972408, 0.7700049583333334, 0.0002994984176447915, 10, True]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-2, 1e-3, 5e-3]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm, None] \n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56f0657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57add3d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011004855235417683, AUC: 0.4887625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011042307217915854, AUC: 0.7546737499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001469256083170573, AUC: 0.7447474166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018616555134455363, AUC: 0.7519853333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011009955008824666, AUC: 0.4859675833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014954797426859538, AUC: 0.7307878333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014571794668833415, AUC: 0.7434764166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001918233076731364, AUC: 0.7510033333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010990285078684488, AUC: 0.5017333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015441822210947673, AUC: 0.7289679166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013621874650319417, AUC: 0.7503736666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020592970053354897, AUC: 0.7446693333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001104023019472758, AUC: 0.4327043333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011127008597056072, AUC: 0.7686645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016269298394521077, AUC: 0.7490771666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014620014031728108, AUC: 0.7722405833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001109751025835673, AUC: 0.41226466666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013858009179433188, AUC: 0.7245768333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013513893286387125, AUC: 0.7421195833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016335872014363606, AUC: 0.7550279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100349466005961, AUC: 0.48139916666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001224625587463379, AUC: 0.7251274166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014618058204650878, AUC: 0.7639809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001893748124440511, AUC: 0.7608105833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998533964157104, AUC: 0.5124565833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012189514636993409, AUC: 0.7386929166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00120151948928833, AUC: 0.7562131666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017067712942759195, AUC: 0.7634075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010969582398732503, AUC: 0.538173\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011792327960332234, AUC: 0.7508575000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016109437545140585, AUC: 0.75193575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017545213301976522, AUC: 0.7468036666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105100949605306, AUC: 0.5000758333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001394121487935384, AUC: 0.7358794166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012873896360397338, AUC: 0.7424176666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002090839942296346, AUC: 0.75268925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010947660207748413, AUC: 0.553981\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001437082529067993, AUC: 0.7315373333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013612343072891236, AUC: 0.7496693333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001404009222984314, AUC: 0.7582930000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098819613456726, AUC: 0.5065030833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011336627006530763, AUC: 0.69624475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011035274664560953, AUC: 0.7289848333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001082752227783203, AUC: 0.7467866666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010977459748586018, AUC: 0.5505764166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011157012383143108, AUC: 0.6855154166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010927118460337321, AUC: 0.7211614166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010983180602391561, AUC: 0.7388749166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989580949147541, AUC: 0.5287226666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001136611819267273, AUC: 0.6970389166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011052985588709513, AUC: 0.7183966666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011363418102264403, AUC: 0.7306098333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010997610092163086, AUC: 0.5101794166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011257091760635376, AUC: 0.6869568333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011254167556762696, AUC: 0.7219326666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010943048397699991, AUC: 0.7476164166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096621036529541, AUC: 0.5347249166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011145275036493938, AUC: 0.7228910833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010782265663146972, AUC: 0.7450464166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010909222761789957, AUC: 0.7538976666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010995964606602987, AUC: 0.5132687499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011139294306437173, AUC: 0.7132966666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010838580926259358, AUC: 0.7403186666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001140423615773519, AUC: 0.7461186666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984042088190715, AUC: 0.5455530833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011301365296045938, AUC: 0.704956\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010922157764434815, AUC: 0.7356759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010945091247558593, AUC: 0.7518811666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010974519650141397, AUC: 0.5811269166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011277774572372437, AUC: 0.6995382499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011200769344965617, AUC: 0.7218839166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011261459191640219, AUC: 0.7390232499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011095584630966186, AUC: 0.42576891666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011234860420227051, AUC: 0.7081191666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010852831602096557, AUC: 0.7371175000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001119327465693156, AUC: 0.7424775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986959139506023, AUC: 0.5085265833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011195060014724731, AUC: 0.7145661666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011048330068588257, AUC: 0.7293056666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011280235052108765, AUC: 0.7391818333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010918916463851928, AUC: 0.6364757499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013359930912653604, AUC: 0.7268925833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001345164696375529, AUC: 0.7378811666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015003972450892131, AUC: 0.7446510833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101840615272522, AUC: 0.49139299999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011844180425008139, AUC: 0.7531694999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001401252547899882, AUC: 0.7349535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014045570691426596, AUC: 0.7383793333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010917581717173259, AUC: 0.6260662499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001304520845413208, AUC: 0.7264386666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001295343279838562, AUC: 0.735718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010601287682851155, AUC: 0.7226601666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011114172538121542, AUC: 0.3950299166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012072943449020386, AUC: 0.7412561666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001505006194114685, AUC: 0.7308335833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002036314606666565, AUC: 0.7183768333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010921948750813803, AUC: 0.6299536666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011139898697535198, AUC: 0.7404611666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012745067675908406, AUC: 0.7435571666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016304620504379273, AUC: 0.7429684166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010962167183558146, AUC: 0.5731191666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001125450571378072, AUC: 0.7569435833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015077069600423177, AUC: 0.7386705833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014020786682764688, AUC: 0.7601788333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011072311798731487, AUC: 0.42304625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010492402315139772, AUC: 0.7703495833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013651695251464843, AUC: 0.7397073333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013542430400848389, AUC: 0.7496026666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986221234003703, AUC: 0.5332675833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011064138015111288, AUC: 0.733161\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001267626961072286, AUC: 0.7363205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012699822584788004, AUC: 0.7474246666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011010087331136068, AUC: 0.5428624166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011095962127049765, AUC: 0.7421406666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012405165831247966, AUC: 0.7388935000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00154677943388621, AUC: 0.7376058333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011017957925796508, AUC: 0.46182016666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013037713368733723, AUC: 0.7322810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013881571292877197, AUC: 0.7311537500000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015433359940846762, AUC: 0.73665975\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.00109506889184316, AUC: 0.6075227500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010930527051289877, AUC: 0.7969098333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00121285879611969, AUC: 0.7962791666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015341360569000244, AUC: 0.7968086666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110688320795695, AUC: 0.39512858333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010997189680735271, AUC: 0.7778755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013350268205006917, AUC: 0.77851625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011274513800938924, AUC: 0.81341375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011012434562047322, AUC: 0.49900916666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001257138729095459, AUC: 0.7524218333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002043050686518351, AUC: 0.7320953333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026825573444366453, AUC: 0.7337874166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010923410654067993, AUC: 0.6525029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009580798546473185, AUC: 0.7926976666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015643794139226278, AUC: 0.7747434999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015132335424423219, AUC: 0.79968725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011098385254542033, AUC: 0.4015779166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001084994395573934, AUC: 0.7721173333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015012899239857992, AUC: 0.7486565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017386534611384074, AUC: 0.7702032500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098043163617452, AUC: 0.533933\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011203059752782187, AUC: 0.7818624999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012514605522155762, AUC: 0.7917668333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013795143763224284, AUC: 0.791999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010959664980570475, AUC: 0.5668985000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00145314892133077, AUC: 0.757642\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011314831574757893, AUC: 0.7822493333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015677810112635295, AUC: 0.7832585000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011059436400731404, AUC: 0.3984905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013966322739919026, AUC: 0.7307428333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001782632311185201, AUC: 0.7407533333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017486469745635986, AUC: 0.7711287499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011188734372456868, AUC: 0.419768\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011598172187805176, AUC: 0.7888086666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013642297983169555, AUC: 0.7982881666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019693562587102253, AUC: 0.7966835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010957918961842855, AUC: 0.55242275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012303212881088258, AUC: 0.7539231666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013039887348810831, AUC: 0.7951523333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013862740198771159, AUC: 0.815563\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010953092177708945, AUC: 0.6404583333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009861884315808614, AUC: 0.7094814166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00097804460922877, AUC: 0.7470353333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001000538984934489, AUC: 0.7563279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010915393034617107, AUC: 0.6032895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001000980297724406, AUC: 0.6951053333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010108818213144938, AUC: 0.7165991666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010097415447235106, AUC: 0.7320798333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011058648029963175, AUC: 0.3926294166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010212364196777344, AUC: 0.6951484166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010278218189875286, AUC: 0.7308895833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001052735169728597, AUC: 0.7477645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011016406218210856, AUC: 0.4643534166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001031091054280599, AUC: 0.6915429166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001040674090385437, AUC: 0.7243474999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010715934832890828, AUC: 0.7400508333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101608196894328, AUC: 0.5494846666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010480308532714844, AUC: 0.70314025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010486969947814942, AUC: 0.7253285000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010532618761062622, AUC: 0.7350355000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010982596874237062, AUC: 0.5231133333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001039299170176188, AUC: 0.7073885000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010384729305903118, AUC: 0.7258065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010636101961135865, AUC: 0.7394919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010999561150868734, AUC: 0.5147990833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010455213387807211, AUC: 0.7027111666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010325393279393513, AUC: 0.7284436666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009992482860883077, AUC: 0.7512784166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102043390274048, AUC: 0.46415566666666663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000999273419380188, AUC: 0.6942214999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010055386622746785, AUC: 0.7411754166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010026804804801942, AUC: 0.7553766666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011024383703867595, AUC: 0.4691273333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010185602903366089, AUC: 0.6940140833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001030768116315206, AUC: 0.7317110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010726646184921265, AUC: 0.7374634166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011024768352508544, AUC: 0.46273383333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010103808442751567, AUC: 0.6928208333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001022193173567454, AUC: 0.7269661666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010319610039393108, AUC: 0.7471425833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011040633122126262, AUC: 0.48861574999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001084409515062968, AUC: 0.7446874166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011547755400339763, AUC: 0.7705745833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013452511628468831, AUC: 0.7643196666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011084726254145304, AUC: 0.40680758333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009926408131917318, AUC: 0.7801149166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011829959551493327, AUC: 0.7703308333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013285505771636963, AUC: 0.7772196666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010999813874562581, AUC: 0.4784380833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009671451250712077, AUC: 0.7715393333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011262832880020142, AUC: 0.7781401666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011721332867940268, AUC: 0.7816708333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098308563232422, AUC: 0.60804125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011307073831558227, AUC: 0.7352096666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001225026528040568, AUC: 0.746285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015529809792836507, AUC: 0.75701375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010934036572774251, AUC: 0.5863502500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010170689225196838, AUC: 0.7685586666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012939701080322266, AUC: 0.7643324166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012338793675104778, AUC: 0.7774453333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011004685958226521, AUC: 0.4946271666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011737916072209677, AUC: 0.7453371666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013495200475056965, AUC: 0.7713799166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015454222361246746, AUC: 0.7744378333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011088493665059407, AUC: 0.42956875000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010903794765472412, AUC: 0.7601739166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012612099647521973, AUC: 0.7723468333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017190736929575603, AUC: 0.7535865833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011022999286651612, AUC: 0.46452258333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001025501569112142, AUC: 0.7681906666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011542065540949503, AUC: 0.7616131666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013855027357737224, AUC: 0.7757744999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011009726921717327, AUC: 0.45751366666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011024720668792725, AUC: 0.7465085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009692980845769246, AUC: 0.7718578333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014292685985565186, AUC: 0.75875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011026564439137777, AUC: 0.4574010833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011313136816024781, AUC: 0.7420356666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011586749951044719, AUC: 0.7638674999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011045318841934205, AUC: 0.7781661666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010964237451553344, AUC: 0.5580166666666666\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010996352036794026, AUC: 0.7698611666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018259684642155966, AUC: 0.7553820833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001500830332438151, AUC: 0.7931090833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00109957222143809, AUC: 0.5198979166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001127657175064087, AUC: 0.75847275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012819421291351317, AUC: 0.7650260000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013352120717366536, AUC: 0.7699018333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011067168315251669, AUC: 0.44954541666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012927870750427245, AUC: 0.7444374166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011801127195358277, AUC: 0.7940649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015621389547983806, AUC: 0.7898746666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011187699635823568, AUC: 0.4121849166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011955602169036864, AUC: 0.770215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012250461181004843, AUC: 0.802267\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017819425264994303, AUC: 0.7973338333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010961958964665732, AUC: 0.5457154166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005438963572183, AUC: 0.7572135000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012234842379887899, AUC: 0.7690803333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011455086469650269, AUC: 0.7881431666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011067813237508137, AUC: 0.3803885833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011677629550298056, AUC: 0.7619088333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014500645001729328, AUC: 0.7645486666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001860355059305827, AUC: 0.7638666666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011117841402689616, AUC: 0.4633545833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099915345509847, AUC: 0.7573715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001427885929743449, AUC: 0.7878805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014267033735911052, AUC: 0.7818525833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106459140777588, AUC: 0.41029958333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010455037355422974, AUC: 0.7735473333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016185382604598998, AUC: 0.7632205000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016153693596522014, AUC: 0.7897609166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010995869636535646, AUC: 0.5424655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001349367658297221, AUC: 0.7474436666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014319247007369995, AUC: 0.7798839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002240425109863281, AUC: 0.7748928333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001112542986869812, AUC: 0.39676633333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001428845485051473, AUC: 0.7254413333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001521015445391337, AUC: 0.7650594999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016000083287556967, AUC: 0.7887598333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010965664784113565, AUC: 0.5540881666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010067923466364542, AUC: 0.7155252499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009594269792238871, AUC: 0.7264059999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009900189042091369, AUC: 0.7335655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011067347129185994, AUC: 0.4409669166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010062939127286276, AUC: 0.6824180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010033489863077799, AUC: 0.7116429166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010325006246566773, AUC: 0.7199965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008529265721639, AUC: 0.48820725000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010037936568260194, AUC: 0.7161085000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009837868809700013, AUC: 0.7212610000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010075665712356568, AUC: 0.7258711666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011031516393025715, AUC: 0.5129689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009884562095006308, AUC: 0.6943776666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009829114079475403, AUC: 0.72011225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009787832895914715, AUC: 0.7366175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011079587936401367, AUC: 0.442594\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982944965362549, AUC: 0.7137113333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009438162247339885, AUC: 0.7512716666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009215635061264038, AUC: 0.7633853333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011136085192362468, AUC: 0.409268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010027713179588317, AUC: 0.7005760833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009915505051612853, AUC: 0.7259881666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010038012663523357, AUC: 0.7276538333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094768722852071, AUC: 0.5596040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000996839423974355, AUC: 0.697042\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010020377039909362, AUC: 0.720401\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009891287883122763, AUC: 0.7314843333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010974709987640382, AUC: 0.5351565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000994803229967753, AUC: 0.6923281666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009822248617808024, AUC: 0.71804525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000992626468340556, AUC: 0.7318358333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008121172587076, AUC: 0.4931770833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010070029695828755, AUC: 0.6928299166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009795227845509848, AUC: 0.71896975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000989958345890045, AUC: 0.7314066666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011039932568868002, AUC: 0.45952300000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000989994724591573, AUC: 0.7174366666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000979889710744222, AUC: 0.7403578333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009953999916712444, AUC: 0.7433108333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011043513218561808, AUC: 0.41522525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009718645413716634, AUC: 0.7270281666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011164419253667196, AUC: 0.7660351666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011509071191151937, AUC: 0.7759428333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001104005455970764, AUC: 0.41259700000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010233759880065919, AUC: 0.7295719166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00105691126982371, AUC: 0.7594516666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001448151707649231, AUC: 0.7491384166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011149702469507854, AUC: 0.4148265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009852914611498515, AUC: 0.755124\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00093146679798762, AUC: 0.7856808333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016483883460362751, AUC: 0.7272283333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010919492244720458, AUC: 0.5823371666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010667858521143595, AUC: 0.7523311666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011237112681070963, AUC: 0.7698888333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015951190789540608, AUC: 0.7473676666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103950023651123, AUC: 0.4217269166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013290177583694458, AUC: 0.7162369166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00134693710009257, AUC: 0.7619587500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014634109338124593, AUC: 0.7654395833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011031290690104167, AUC: 0.48289116666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010358746846516928, AUC: 0.7330788333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012607671817143758, AUC: 0.7561255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011030386686325074, AUC: 0.7893501666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011073987086613972, AUC: 0.41945241666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010171599785486858, AUC: 0.7517405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011877509355545043, AUC: 0.7741628333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012733789285024008, AUC: 0.7773958333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011084714730580647, AUC: 0.4261519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010705119768778483, AUC: 0.7449903333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001441202163696289, AUC: 0.7424363333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001385249892870585, AUC: 0.7750850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010996285676956176, AUC: 0.5633390833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011788311004638672, AUC: 0.7243786666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014172059694925944, AUC: 0.73077975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013685308694839477, AUC: 0.7604694166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003657976786296, AUC: 0.4996061666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011321732997894287, AUC: 0.7504623333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001021358887354533, AUC: 0.7870571666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011578699747721354, AUC: 0.7811336666666667\n",
      "\n",
      "[['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.01, 0.49075180000000007, 0.0016569387512752772, 0.7389765416666666, 0.0001900326219322916, 0.7494011166666666, 4.250724220027743e-05, 0.7556930583333333, 6.120316620701434e-05, 1, True], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.520495075, 0.0014949891056506256, 0.702912325, 0.00013216531674507, 0.7299823750000001, 7.609654498368012e-05, 0.7436467916666666, 4.400255806423629e-05, 1, True], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.5313034166666666, 0.00683699558109722, 0.7423093916666667, 0.00017967271548479153, 0.7367689083333333, 1.3544144057569141e-05, 0.7398507583333334, 0.00013724905611034747, 1, True], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.01, 0.5027254166666667, 0.008054628501159723, 0.7705001333333333, 0.0004059115129433333, 0.773850075, 0.0005441768836367385, 0.7872533083333334, 0.0005268431168917374, 5, True], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.5084144583333334, 0.004931703042892011, 0.6985574416666667, 3.8066961930625436e-05, 0.7298302833333334, 6.810911244055561e-05, 0.7442011749999999, 6.59123590714582e-05, 5, True], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.48718861666666663, 0.003663513749928056, 0.7562355916666667, 0.00021051987886868117, 0.767072825, 6.943509721034767e-05, 0.7698384333333332, 9.601010035249923e-05, 5, True], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.01, 0.4678634916666667, 0.004193781311493682, 0.7565912499999999, 0.00018837299881666693, 0.7746413583333334, 0.00021616243245284659, 0.7837495416666667, 0.00010593222269479213, 10, True], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.4895553916666667, 0.0023628100871486827, 0.7022353583333334, 0.00014026730110701353, 0.7254455833333334, 0.00012491965382916645, 0.7345127499999999, 0.00012747877522361047, 10, True], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.4638153666666668, 0.003813952248840557, 0.7384942833333332, 0.00017639505210444427, 0.7633576833333333, 0.0002795107722566666, 0.7648550916666668, 0.00032380325691867877, 10, True]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class euclidean distance capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-2, 1e-3, 5e-3]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['distance'] = 'euclidean'\n",
    "\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"distance_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15bc95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "805eafa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011052825848261514, AUC: 0.43978183333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001242885112762451, AUC: 0.7556419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017786058187484741, AUC: 0.7451270833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001355748494466146, AUC: 0.7903454166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010999015967051188, AUC: 0.5015735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001183523694674174, AUC: 0.7529743333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017083600759506227, AUC: 0.7684294999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020539085070292156, AUC: 0.7834125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010926770766576132, AUC: 0.5972151666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012481575806935629, AUC: 0.7396061666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001280996561050415, AUC: 0.774436\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013489604791005452, AUC: 0.8045683333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011085643370946248, AUC: 0.41532358333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001295187473297119, AUC: 0.7403531666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014481489260991414, AUC: 0.7683135833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001503763198852539, AUC: 0.8040346666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011077126661936441, AUC: 0.3876288333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014019229412078858, AUC: 0.7298151666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012458863655726115, AUC: 0.7880338333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018081894715627034, AUC: 0.7905300000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011023714542388915, AUC: 0.44327941666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011233824491500854, AUC: 0.7764533333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013599769274393718, AUC: 0.7757853333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001630991260210673, AUC: 0.7814448333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011145607630411783, AUC: 0.32745641666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010820915301640829, AUC: 0.7825736666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012192320426305135, AUC: 0.7953826666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016848710775375366, AUC: 0.8065705833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010936137040456137, AUC: 0.5703022500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009436519344647726, AUC: 0.7783104999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012579033374786378, AUC: 0.7965208333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016022408405939738, AUC: 0.7867226666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011033010482788086, AUC: 0.4292242499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010503697395324706, AUC: 0.7746530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016495122909545897, AUC: 0.7660344166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019463297128677367, AUC: 0.7708870000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011015632549921672, AUC: 0.4795859166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013584723075230917, AUC: 0.7521388333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016266434987386067, AUC: 0.7868365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015543033281962078, AUC: 0.8185890833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011108194192250569, AUC: 0.3419486666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010033171375592549, AUC: 0.6896026666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009849712053934733, AUC: 0.72248\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010011465152104696, AUC: 0.7318060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010970837672551472, AUC: 0.5532395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009985850652058919, AUC: 0.6955717499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009926936427752176, AUC: 0.7176396666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009789584875106812, AUC: 0.7361153333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011035648981730143, AUC: 0.42958316666666674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010014365712801615, AUC: 0.6950899166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001002144197622935, AUC: 0.7168530833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010018961628278096, AUC: 0.7311090833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010966077248255413, AUC: 0.5706053333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009843558271725973, AUC: 0.6987758333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000980299413204193, AUC: 0.7181961666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009855879147847493, AUC: 0.7292071666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099665880203247, AUC: 0.49452908333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009799832304318746, AUC: 0.7109610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009829253951708476, AUC: 0.7233472499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010060605804125468, AUC: 0.7308336666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011001358032226563, AUC: 0.49598933333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009760411183039348, AUC: 0.7241001666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009547644257545472, AUC: 0.7409946666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009742421507835388, AUC: 0.7328423333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010971740484237672, AUC: 0.5320221666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009834888378779094, AUC: 0.7105694166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009741238554318746, AUC: 0.728897\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010064294139544168, AUC: 0.7299821666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010978577534357706, AUC: 0.6019133333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001010177175203959, AUC: 0.6874166666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000999732792377472, AUC: 0.7099293333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010325587193171183, AUC: 0.7148534166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094303846359253, AUC: 0.5705435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009925708373387655, AUC: 0.6994970833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009863321582476298, AUC: 0.7201563333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010269964138666788, AUC: 0.7253165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994608402252197, AUC: 0.49848108333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000989067792892456, AUC: 0.7082652499999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009848726987838744, AUC: 0.7217981666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001016507347424825, AUC: 0.7257485833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011030149459838867, AUC: 0.45802425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00105222221215566, AUC: 0.7335145000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011837135553359986, AUC: 0.7562842499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001032602588335673, AUC: 0.7859023333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010944536129633585, AUC: 0.5927108333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010121003786722818, AUC: 0.6790400833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012255022128423055, AUC: 0.7485508333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010648034016291301, AUC: 0.7785492499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011124239762624105, AUC: 0.39449208333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010099299947420755, AUC: 0.7372508333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005038817723592, AUC: 0.76264625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001221539815266927, AUC: 0.7828745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100527286529541, AUC: 0.5038714999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009799331227938334, AUC: 0.7291404166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00094316432873408, AUC: 0.7760418333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018191998799641927, AUC: 0.7438628333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994138717651367, AUC: 0.52235325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012390422423680623, AUC: 0.7235440833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010458950599034626, AUC: 0.7724103333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012372745275497437, AUC: 0.7888243333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100235939025879, AUC: 0.47842308333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009931529959042867, AUC: 0.7360367500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00150886603196462, AUC: 0.7333489999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001342439651489258, AUC: 0.7685581666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010941846370697021, AUC: 0.5773158333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009656792084376017, AUC: 0.7361185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009749161005020142, AUC: 0.757888\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014002594550450644, AUC: 0.7639665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011123396158218384, AUC: 0.4910478333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011187382141749063, AUC: 0.723629\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00129604438940684, AUC: 0.7470101666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011723470687866212, AUC: 0.7781036666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102103273073832, AUC: 0.5101065833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001005542576313019, AUC: 0.7405304999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001044869025548299, AUC: 0.7692404166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017746853033701578, AUC: 0.7518397499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008099714914959, AUC: 0.5085564166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010576653480529785, AUC: 0.7403221666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014580024878184002, AUC: 0.7394916666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016340305010477702, AUC: 0.7730399166666665\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001102006713549296, AUC: 0.5132036666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012708915074666342, AUC: 0.7639296666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015375802119572957, AUC: 0.7768043333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002539357344309489, AUC: 0.7442037499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011142092148462932, AUC: 0.3598644166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010708965063095093, AUC: 0.77946075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001908510684967041, AUC: 0.7509295000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016529592672983805, AUC: 0.79660725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011188224951426187, AUC: 0.3916071666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012840594053268432, AUC: 0.7236514166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012736543416976929, AUC: 0.7729545833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002117136081059774, AUC: 0.7873815000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011116609573364258, AUC: 0.357033\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010108190377553304, AUC: 0.7509784166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012108296553293863, AUC: 0.7767719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001450315276781718, AUC: 0.7914846666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011061280171076457, AUC: 0.4922578333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010047587553660075, AUC: 0.769164\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013816887140274048, AUC: 0.7790140833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016818987131118775, AUC: 0.79546\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992931922276816, AUC: 0.49916625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011477706829706828, AUC: 0.7628538333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019601945479710897, AUC: 0.7763200833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002211273511250814, AUC: 0.7834740833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011038367748260499, AUC: 0.41970124999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012042486667633056, AUC: 0.7399732499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011269136667251586, AUC: 0.7799583333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018957820336023966, AUC: 0.7809191666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010996092557907105, AUC: 0.5076415833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010746674140294393, AUC: 0.7665055000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014257461229960123, AUC: 0.7873306666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001272143284479777, AUC: 0.8101783333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011019491751988728, AUC: 0.4687748333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011048280398050943, AUC: 0.7717258333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001278264323870341, AUC: 0.7874515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015009965896606446, AUC: 0.8082316666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010943660736083985, AUC: 0.6048100833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010948184728622437, AUC: 0.7534705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010304831266403198, AUC: 0.80318075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001120049794514974, AUC: 0.8049934999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010892634391784669, AUC: 0.6238854166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009899799823760986, AUC: 0.7010975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009848402341206869, AUC: 0.7140080833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009783457517623902, AUC: 0.7287761666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010961312850316365, AUC: 0.5681430833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000979781448841095, AUC: 0.7110272499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009761015176773071, AUC: 0.7241073333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009822571277618409, AUC: 0.7305580833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010993290742238363, AUC: 0.49248458333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010197827021280925, AUC: 0.693164\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009717961351076762, AUC: 0.7150595000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009892191092173259, AUC: 0.7171325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00109678049882253, AUC: 0.5567589166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000997121493021647, AUC: 0.6950178333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009780041774113974, AUC: 0.7164443333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009903105894724527, AUC: 0.7235989166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010975958506266275, AUC: 0.52984825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000988736351331075, AUC: 0.6955344999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009852591554323832, AUC: 0.7098253333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009989083607991537, AUC: 0.7179411666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011011060078938801, AUC: 0.47580808333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009996693332990012, AUC: 0.6979223333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009856516520182293, AUC: 0.7132318333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000999704122543335, AUC: 0.7189101666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011030961275100709, AUC: 0.62071825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009902743697166442, AUC: 0.7095595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009602400461832683, AUC: 0.7332179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009596516688664754, AUC: 0.7373553333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010972580512364706, AUC: 0.5385525000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009918743968009949, AUC: 0.7113572500000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009952367345492045, AUC: 0.7165809166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010061280330022175, AUC: 0.7193573333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010995231866836547, AUC: 0.48535325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000980647027492523, AUC: 0.706823\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009634583592414856, AUC: 0.7360128333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982534686724345, AUC: 0.7365725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096468210220337, AUC: 0.5642393333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000983853022257487, AUC: 0.712351\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000967814286549886, AUC: 0.730292\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001010051707426707, AUC: 0.7342801666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010963884592056275, AUC: 0.5534795833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010277713537216186, AUC: 0.7370356666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012712493340174358, AUC: 0.7509178333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001284215529759725, AUC: 0.7760003333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010941277742385865, AUC: 0.5934653333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010443700949350993, AUC: 0.7331920833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001042675018310547, AUC: 0.7509549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001183596650759379, AUC: 0.7665721666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011013166904449462, AUC: 0.5063045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009572117924690247, AUC: 0.7558829166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001197301189104716, AUC: 0.7744365833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014734627803166707, AUC: 0.7803225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101922074953715, AUC: 0.4716498333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000951429824034373, AUC: 0.7340738333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001137385368347168, AUC: 0.7487791666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015652597745259603, AUC: 0.751424\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011025995016098022, AUC: 0.4636045833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011023643414179484, AUC: 0.7282416666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010836116870244345, AUC: 0.7575198333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015277689695358276, AUC: 0.7550169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011035694678624471, AUC: 0.44808183333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010065564115842183, AUC: 0.7502305833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009970664978027344, AUC: 0.7872975000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012350488901138306, AUC: 0.7897646666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010914289156595865, AUC: 0.6476768333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001074254552523295, AUC: 0.7358363333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011178109645843507, AUC: 0.7672175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011115504503250123, AUC: 0.7735415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994761387507121, AUC: 0.5082473333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010498007933298747, AUC: 0.7432927500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012039331992467245, AUC: 0.7681236666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012377911408742268, AUC: 0.7824813333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103779395421346, AUC: 0.4356425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001020856738090515, AUC: 0.7293618333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009965181350708007, AUC: 0.7491035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009932926694552105, AUC: 0.786505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011035188833872476, AUC: 0.5002131666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010656888087590536, AUC: 0.7392074166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010090548793474834, AUC: 0.7769553333333331\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012523607015609741, AUC: 0.7811840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095573623975118, AUC: 0.5769076666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001190720001856486, AUC: 0.7571106666666667\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001682835817337036, AUC: 0.7700859166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016216527223587036, AUC: 0.8116925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011027799050013225, AUC: 0.46017141666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009993523557980854, AUC: 0.7918486666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013525794744491577, AUC: 0.7881920833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015977139075597127, AUC: 0.8012561666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100353757540385, AUC: 0.47812058333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011181455055872599, AUC: 0.7524395833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015768436988194783, AUC: 0.7707788333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001684269905090332, AUC: 0.8039378333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010977291266123454, AUC: 0.5273618333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012737759749094646, AUC: 0.7475231666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001591060439745585, AUC: 0.7671234166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019627333879470824, AUC: 0.7713725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011134448846181234, AUC: 0.35818750000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001361437201499939, AUC: 0.7452843333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017297926743825276, AUC: 0.7570045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021948784987131753, AUC: 0.7524543333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992012023925781, AUC: 0.51481375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001275275429089864, AUC: 0.74888025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015842588742574057, AUC: 0.7755140000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017796413898468018, AUC: 0.7967448333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986533164978028, AUC: 0.5251801666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011307824850082398, AUC: 0.7625631666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015177246332168579, AUC: 0.7633570000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011920704046885172, AUC: 0.8018175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010993882417678832, AUC: 0.49319108333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00128338094552358, AUC: 0.7359430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018545222679773967, AUC: 0.7471930000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001524183948834737, AUC: 0.8040221666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011053677002588907, AUC: 0.38088941666666676\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014618106285730998, AUC: 0.7298687500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016418736775716146, AUC: 0.7568385000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018790031274159749, AUC: 0.7944025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011033515930175782, AUC: 0.4883530833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012057714462280274, AUC: 0.7622263333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013016608556111653, AUC: 0.7870888333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016563831170399983, AUC: 0.7989648333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011013116041819254, AUC: 0.49133975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010110619068145752, AUC: 0.7324885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009568597873051961, AUC: 0.7396402499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009771706660588583, AUC: 0.7409061666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011158833106358847, AUC: 0.36710058333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010048614343007405, AUC: 0.7264472500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009443788528442383, AUC: 0.7469942499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009457928935686748, AUC: 0.7489013333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011042243639628093, AUC: 0.5048244999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000983566423257192, AUC: 0.701836\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009656203587849935, AUC: 0.7191089166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009712597131729126, AUC: 0.7234695833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011041393280029296, AUC: 0.4540325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010098834037780761, AUC: 0.7173815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009562164346377055, AUC: 0.73596875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000958151658376058, AUC: 0.7382905000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010937712987263998, AUC: 0.5809078333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009975797533988953, AUC: 0.7011835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009747327168782552, AUC: 0.7245771666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009806480606396994, AUC: 0.7382973333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101788600285848, AUC: 0.44576499999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009888164003690083, AUC: 0.6972745833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009654398560523987, AUC: 0.722465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010301814874013266, AUC: 0.7206955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110673721631368, AUC: 0.44465724999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009793102343877156, AUC: 0.7027516666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009551057815551758, AUC: 0.72633775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009896982312202454, AUC: 0.7317795833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002353032430012, AUC: 0.4934808333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009941839377085367, AUC: 0.7027701666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009796630144119263, AUC: 0.7223933333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009789421955744425, AUC: 0.7291474999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011032837231953938, AUC: 0.4235675833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009952846964200338, AUC: 0.7023316666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000962993303934733, AUC: 0.7324005833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009806098540623982, AUC: 0.7361134166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011010559797286986, AUC: 0.5137915833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009895623723665873, AUC: 0.69841075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009702318906784058, AUC: 0.7203309166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009630068143208822, AUC: 0.7288178333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011041100025177002, AUC: 0.4476700833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001025624096393585, AUC: 0.7443106666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011343144178390502, AUC: 0.7549480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011133350133895874, AUC: 0.7800176666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011041619777679443, AUC: 0.3866853333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011449777285257976, AUC: 0.7210405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011483944654464722, AUC: 0.7664501666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013678685029347739, AUC: 0.7762005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010950481494267782, AUC: 0.600333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001033435583114624, AUC: 0.7393013333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011688275337219237, AUC: 0.7573256666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001280771811803182, AUC: 0.7758483333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010996214946111043, AUC: 0.49606500000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011098840236663818, AUC: 0.7282593333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014085972309112549, AUC: 0.7406581666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017645678917566935, AUC: 0.7515601666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011140156189600626, AUC: 0.4158268333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010116553703943887, AUC: 0.7442073333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010211445887883504, AUC: 0.7734190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013178801933924358, AUC: 0.776474\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010879511833190919, AUC: 0.6545525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009831160306930542, AUC: 0.7396068333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010708336035410563, AUC: 0.7659816666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012179024219512939, AUC: 0.7825603333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010950992107391358, AUC: 0.587346\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010979522466659545, AUC: 0.7438468333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013305134773254396, AUC: 0.7644817499999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015312257607777914, AUC: 0.774778\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006502310434978, AUC: 0.5078773333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010491578181584676, AUC: 0.7292878333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013517614205678305, AUC: 0.7487765000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016176618734995525, AUC: 0.7512013333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100757400194804, AUC: 0.5296706666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011655190785725912, AUC: 0.720366\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010746469895044962, AUC: 0.7642760000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017612395286560059, AUC: 0.73286675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010967158873875936, AUC: 0.5402444166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009925466378529868, AUC: 0.7525466666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012996623118718465, AUC: 0.7625176666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017931011120478312, AUC: 0.767113\n",
      "\n",
      "[['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.01, 0.4591371166666667, 0.00595359750880167, 0.7582520166666666, 0.00031428937253583273, 0.7764899749999999, 0.00022279699736256928, 0.7937105083333333, 0.0001856483329881249, 1, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.5088855166666667, 0.005340524332167775, 0.701984975, 0.00011535018669173595, 0.7220291666666666, 6.195161896250061e-05, 0.728781425, 3.058712175201377e-05, 1, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.5036901666666667, 0.002856646368004166, 0.7279126833333334, 0.0002992594824552793, 0.7562912749999999, 0.0001815152384681249, 0.771552125, 0.0001948329910364584, 1, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.01, 0.46140600833333345, 0.005563300994183958, 0.7581713166666667, 0.00024789757636361153, 0.7790715833333333, 0.00015678708116805457, 0.7902933916666667, 0.000326837682745902, 5, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.5455791666666666, 0.0024371105269472212, 0.7033854166666667, 5.2347625627777996e-05, 0.7208780166666668, 7.801890974833345e-05, 0.7264482333333333, 5.7948991446944645e-05, 5, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.5128365500000001, 0.0040830064120988855, 0.7386355083333334, 7.088034587284703e-05, 0.7631305916666667, 0.0001676221813561805, 0.7742812499999999, 0.00014970399305694513, 5, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.01, 0.48031764999999993, 0.00401881512389555, 0.7533687916666667, 0.00026341453906979093, 0.7683176083333334, 0.000154036539470902, 0.7936665166666668, 0.000289527392324723, 10, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.4719467416666666, 0.003062864302536738, 0.7082875583333333, 0.00014056473178479235, 0.7290216916666667, 7.831205259590164e-05, 0.733641875, 6.530355852256975e-05, 10, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.5166271166666666, 0.006405915376032223, 0.7362773333333333, 0.00010681110807777761, 0.7598834583333333, 8.347758637673639e-05, 0.7668620083333334, 0.00023776672112840158, 10, True]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class cosine distance capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-2, 1e-3, 5e-3]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['distance'] = 'cosine'\n",
    "\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"cosine_distance_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abe9f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77723321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001108316699663798, AUC: 0.40338908333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010837951103846231, AUC: 0.7395576666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013000988165537517, AUC: 0.7524381666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015997310082117716, AUC: 0.772925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011064944664637248, AUC: 0.41710908333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012484875917434693, AUC: 0.7461605000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010611064831415812, AUC: 0.7906749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017651242812474568, AUC: 0.7832611666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011017123063405355, AUC: 0.5216805833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013332428534825642, AUC: 0.7182235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014227912425994872, AUC: 0.7462981666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014862947066624959, AUC: 0.7949613333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011074897845586142, AUC: 0.4555526666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013799664974212646, AUC: 0.722566\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015054897467295329, AUC: 0.7591636666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015086971918741863, AUC: 0.7907500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011106066703796386, AUC: 0.39748416666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009962827563285827, AUC: 0.6837430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016348551114400227, AUC: 0.7555446666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016455980936686198, AUC: 0.8010544999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099474827448527, AUC: 0.4837579166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012109869718551637, AUC: 0.7279065833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013948575655619304, AUC: 0.7551441666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013081971406936645, AUC: 0.7756515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106783628463745, AUC: 0.38075725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010790023803710937, AUC: 0.7392400833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001598769982655843, AUC: 0.7416505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018386648098627726, AUC: 0.7535893333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011023524602254231, AUC: 0.46798066666666677\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110946253935496, AUC: 0.7509043333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013033068577448527, AUC: 0.78103\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010643877585728963, AUC: 0.797689\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001113852818806966, AUC: 0.4341163333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001278955578804016, AUC: 0.7264231666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001447514017422994, AUC: 0.7727108333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011898645162582398, AUC: 0.8041308333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011077947616577148, AUC: 0.509829\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011931478182474772, AUC: 0.7292121666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011129018068313599, AUC: 0.7702851666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014262359142303467, AUC: 0.8040346666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010983148018519084, AUC: 0.5353438333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001176230510075887, AUC: 0.7148219166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00131669811407725, AUC: 0.7221225833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001371519962946574, AUC: 0.7569379166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098140319188436, AUC: 0.5093491666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001127615173657735, AUC: 0.735289\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012508739233016968, AUC: 0.7514921666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012201573053995769, AUC: 0.7711183333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011016160647074381, AUC: 0.5379974166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011521583398183186, AUC: 0.7157040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013505146106084188, AUC: 0.721601\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010180414120356243, AUC: 0.780211\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011086788177490235, AUC: 0.43450191666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010646405617396038, AUC: 0.7206655833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011438413858413695, AUC: 0.7358983333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012199196418126424, AUC: 0.7528596666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011032557884852092, AUC: 0.4933260833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010830989678700764, AUC: 0.7232893333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011844847202301025, AUC: 0.7304441666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014599141677220661, AUC: 0.730903\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011009581486384073, AUC: 0.5023568333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010910247166951497, AUC: 0.7239515000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010342297554016113, AUC: 0.6845008333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010408371686935424, AUC: 0.7088203333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011074060996373494, AUC: 0.49688808333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010752207040786743, AUC: 0.7341996666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011757938861846923, AUC: 0.7444383333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00117725400129954, AUC: 0.7670661666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103461464246114, AUC: 0.45454750000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011236743927001953, AUC: 0.725577\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011706708669662477, AUC: 0.74766675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013995300928751628, AUC: 0.7634123333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011049946546554566, AUC: 0.4698711666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001350115458170573, AUC: 0.6937524166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012235039075215658, AUC: 0.7255956666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001253804842631022, AUC: 0.743948\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011078846057256063, AUC: 0.5084611666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011552693049112956, AUC: 0.7196049166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001338384707768758, AUC: 0.7246473333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013134447733561198, AUC: 0.7493005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095426360766093, AUC: 0.6009685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000994011143843333, AUC: 0.6966093333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010252301692962647, AUC: 0.7109865000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001069219986597697, AUC: 0.7147478333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010990165869394937, AUC: 0.5063510833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010170281132062277, AUC: 0.6823075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001057655652364095, AUC: 0.6960731666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010598207314809164, AUC: 0.7150521666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011025707721710205, AUC: 0.570955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001021635870138804, AUC: 0.686658\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001033881942431132, AUC: 0.7068651666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010207864443461101, AUC: 0.7195855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105343222618103, AUC: 0.4715689166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010212722619374593, AUC: 0.6733506666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010349076986312866, AUC: 0.7026155833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010303428570429484, AUC: 0.7163876666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102108637491862, AUC: 0.5144847499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010048419237136841, AUC: 0.6973254166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001030346910158793, AUC: 0.7108606666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010546918710072836, AUC: 0.7165604166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011121177275975546, AUC: 0.40138925000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010189816951751709, AUC: 0.6717723333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001041333278020223, AUC: 0.69434225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001037049969037374, AUC: 0.7124031666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011049208243687947, AUC: 0.57879725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010155560970306397, AUC: 0.7082393333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009921395778656006, AUC: 0.7107231666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010642049312591553, AUC: 0.7148636666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011017905076344807, AUC: 0.47757083333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001027794321378072, AUC: 0.6910831666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010477624336878459, AUC: 0.7116131666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010587366819381713, AUC: 0.7238308333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010956641435623169, AUC: 0.5643155833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000998046855131785, AUC: 0.6926420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010323768059412638, AUC: 0.7054631666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010360782146453858, AUC: 0.7221325833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105961004892985, AUC: 0.4140705833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000999930481115977, AUC: 0.6879784999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001022699197133382, AUC: 0.7005969999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010173312425613404, AUC: 0.7155046666666666\n",
      "\n",
      "[['triplet_loss_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.01, 0.44716567500000004, 0.0021190166701228474, 0.7283937, 0.0003200536216224993, 0.7624940333333333, 0.00022141187159333242, 0.7878047333333333, 0.00024235439805111143, 3, True, None], ['triplet_loss_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.49426431666666665, 0.0009846462980747223, 0.7206855333333333, 0.00012230243758222162, 0.7288407166666667, 0.0003254861248961114, 0.752457725, 0.00039233178733062555, 3, True, None], ['triplet_loss_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.510047175, 0.004323477655268682, 0.688796625, 0.00011075687120729117, 0.7050139833333334, 3.6871346106666825e-05, 0.71710685, 1.1693965031666895e-05, 3, True, None]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class triplet loss capped smote\n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-2, 5e-3, 1e-3]\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "loss_caps = [3] \n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = loss_cap\n",
    "    \n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['print_capped'] = False\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                loss_fn_args['print_capped'] = False\n",
    "                _, _ = train.train_triplet_capped_loss(epoch, train_loader_tripletloss_smote, network, optimizer, verbose=False, cap_calc=loss_fns.TripletLoss,loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"triplet_loss_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm, None]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "589562a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd4ae08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251272b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
