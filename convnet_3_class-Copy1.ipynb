{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4df9c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  \n",
    "\n",
    "\n",
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops \n",
    "\n",
    "NUM_CLASSES = 10\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "          #   \"mean_40\", \"variance_40\",\n",
    "          #   \"mean_50\", \"variance_50\",\n",
    "             \"cap\", \"normalization\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9edc25c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 3 classes\n",
    "\n",
    "NUM_CLASSES_REDUCED = 3\n",
    "nums = (0, 3, 1)\n",
    "ratio = (200, 20, 1)\n",
    "\n",
    "norm=True\n",
    "\n",
    "if norm:\n",
    "    transform=torchvision.transforms.Compose([torchvision.transforms.Normalize(mean=[134.1855, 122.7346, 118.3749], std=[70.5125, 64.4848, 66.5604])])\n",
    "else:\n",
    "    transform=None\n",
    "\n",
    "    \n",
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "\n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums, transform=transform)\n",
    "targets = ratio_train_CIFAR10.labels \n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED, transform=transform)\n",
    "\n",
    "triplet_train_CIFAR10 = class_sampling.ForTripletLoss(reduced_train_CIFAR10, smote=False, transform=transform, num_classes=3)\n",
    "triplet_ratio_train_CIFAR10 = class_sampling.ForTripletLoss(ratio_train_CIFAR10, smote=False, transform=transform, num_classes=3)\n",
    "triplet_smote_train_CIFAR10 = class_sampling.ForTripletLoss(smote_train_CIFAR10, smote=True, transform=transform, num_classes=3)\n",
    "\n",
    "weight = 1. / class_count\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= max(class_count)\n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss = DataLoader(triplet_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_ratio = DataLoader(triplet_ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_smote = DataLoader(triplet_smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225b42ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3 class normal\n",
    "\n",
    "learning_rates = [1e-2, 5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 3, nums, (1, 1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeea5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7098b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  3 class ratio\n",
    "\n",
    "learning_rates =  [1e-2, 1e-3, 5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ec6b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0850938a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010989482005437216, AUC: 0.5574586666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014568779468536377, AUC: 0.7987512499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027695000171661377, AUC: 0.7861481666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033176114559173584, AUC: 0.7812484166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010912991762161255, AUC: 0.5963589166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017322154839833578, AUC: 0.765361\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027109846274058023, AUC: 0.7597248333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033343732357025146, AUC: 0.7680907499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008808612823487, AUC: 0.5308455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016412200133005777, AUC: 0.7837315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028054415384928385, AUC: 0.7703296666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035819047292073567, AUC: 0.7849685000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011016372044881184, AUC: 0.49567524999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014501880804697672, AUC: 0.79916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023616336981455485, AUC: 0.7956493333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003184097448984782, AUC: 0.7928443333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095231294631958, AUC: 0.5783449166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016919046640396119, AUC: 0.7833448333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025914467175801596, AUC: 0.7943026666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002642768859863281, AUC: 0.8087403333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986486673355102, AUC: 0.5176145833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014580437342325847, AUC: 0.7701569999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002820889949798584, AUC: 0.7799118333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036393133799235027, AUC: 0.7857158333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011001242796579997, AUC: 0.5213026666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001639033555984497, AUC: 0.7370420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002714767535527547, AUC: 0.7321203333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036419639587402346, AUC: 0.74222\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011038796504338581, AUC: 0.4549690833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001597296436627706, AUC: 0.7875331666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026737934748331704, AUC: 0.7777233333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003308349370956421, AUC: 0.7956683333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100283145904541, AUC: 0.588877\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011607431570688884, AUC: 0.7964355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019568619330724082, AUC: 0.79623475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003171085755030314, AUC: 0.7770522500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991733074188232, AUC: 0.5572630833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001436136523882548, AUC: 0.7922611666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002246021270751953, AUC: 0.8031480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003004270950953166, AUC: 0.8070894999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011047978401184082, AUC: 0.48873666666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010142487287521363, AUC: 0.746352\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009295734763145447, AUC: 0.7961709166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008795956373214721, AUC: 0.8219855833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011014945904413858, AUC: 0.5559282499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001013463298479716, AUC: 0.7130313333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00095101398229599, AUC: 0.7627391666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009011924266815186, AUC: 0.7952441666666669\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106361945470174, AUC: 0.43234458333333325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010294406414031983, AUC: 0.7234401666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009383618831634522, AUC: 0.7801185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008806079030036926, AUC: 0.8149755833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011031925280888875, AUC: 0.6139911666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000979894717534383, AUC: 0.7455403333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009143596092859904, AUC: 0.7879680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008564819097518921, AUC: 0.8132955000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102788726488749, AUC: 0.4590515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001003290832042694, AUC: 0.7128311666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009441520770390829, AUC: 0.7615456666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009131338596343994, AUC: 0.7730311666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011007806857426961, AUC: 0.4977335833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010093032916386922, AUC: 0.726738\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009371120532353719, AUC: 0.7903825833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008851401805877686, AUC: 0.8162113333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105303923288981, AUC: 0.5849120833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010132359663645426, AUC: 0.7238324166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009407095909118652, AUC: 0.7810070000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008838021755218506, AUC: 0.8200913333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010997193257013956, AUC: 0.5007515833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000994119147459666, AUC: 0.7388348333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009290691415468851, AUC: 0.7784763333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008775129715601603, AUC: 0.8100016666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011012182633082072, AUC: 0.48302700000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010072028636932374, AUC: 0.7620069166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009317566752433777, AUC: 0.7918131666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008764350016911824, AUC: 0.8207295000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001119064966837565, AUC: 0.37957391666666657\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010006605982780456, AUC: 0.763348\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009320196509361267, AUC: 0.7895824166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008877813418706258, AUC: 0.8016588333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008808612823487, AUC: 0.4846025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000883094310760498, AUC: 0.8059379166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016187536319096883, AUC: 0.7769743333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00278557554880778, AUC: 0.7650776666666669\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010935423374176026, AUC: 0.5738760833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010487080415089926, AUC: 0.7926544999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001964112559954325, AUC: 0.8028181666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026551124254862468, AUC: 0.8152443333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010930122534434001, AUC: 0.6262559166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008472077449162801, AUC: 0.8160041666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001395551085472107, AUC: 0.7850604999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022025981744130453, AUC: 0.7840701666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011179414590199788, AUC: 0.35242066666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008765910863876343, AUC: 0.8085844999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001343150774637858, AUC: 0.7917993333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002154683510462443, AUC: 0.7831126666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011044532855351767, AUC: 0.47575416666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009273201624552409, AUC: 0.8059683333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00186006764570872, AUC: 0.7659526666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002393811305363973, AUC: 0.7669024999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101227879524231, AUC: 0.44546983333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011473985910415649, AUC: 0.7761808333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002186598857243856, AUC: 0.7743286666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003095108985900879, AUC: 0.7786075833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992282231648764, AUC: 0.5289455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008166845043500264, AUC: 0.8429258333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014016406933466594, AUC: 0.7950958333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001972997307777405, AUC: 0.7857641666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011013125975926717, AUC: 0.5048068333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009712183475494385, AUC: 0.8051418333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019938271443049114, AUC: 0.7788926666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002541587591171265, AUC: 0.7787780833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011121039787928264, AUC: 0.42407616666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000874048113822937, AUC: 0.8249545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002002699335416158, AUC: 0.7873095000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022958167394002277, AUC: 0.7976748333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010974462032318116, AUC: 0.5333489166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008437161842981975, AUC: 0.8238504999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001465768257776896, AUC: 0.7943851666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018469241460164387, AUC: 0.7940718333333333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class oversampled \n",
    "\n",
    "learning_rates = [1e-2, 1e-3, 5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bd49495",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "801a223f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011113398869832356, AUC: 0.45399366666666663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001039024035135905, AUC: 0.7600510833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014980615377426147, AUC: 0.7576163333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016282843351364137, AUC: 0.7855533333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011053721904754638, AUC: 0.40628075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000999620020389557, AUC: 0.7641230833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00213276473681132, AUC: 0.7200933333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018212167024612427, AUC: 0.7857006666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110055140654246, AUC: 0.5410479166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00124763818581899, AUC: 0.7365446666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001190781593322754, AUC: 0.7589035833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017052281697591146, AUC: 0.7550365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010934751828511557, AUC: 0.582834\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010338213443756104, AUC: 0.7306260833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001256510615348816, AUC: 0.7601573333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015600728988647461, AUC: 0.7772036666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011009525458017986, AUC: 0.5095504166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001012095888455709, AUC: 0.759291\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016606905062993367, AUC: 0.7748548333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001551385482152303, AUC: 0.8017821666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010941548347473144, AUC: 0.5600375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009489623705546061, AUC: 0.7749334166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001336568792661031, AUC: 0.7718415833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002138089736302694, AUC: 0.7530644166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011102606852849324, AUC: 0.35558191666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012823344866434733, AUC: 0.7354369999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015103890895843506, AUC: 0.7758745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002325395663579305, AUC: 0.7750466666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097779671351115, AUC: 0.5878043333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010028896530469259, AUC: 0.7253636666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012898311614990235, AUC: 0.7437476666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014241915146509807, AUC: 0.7635815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106678287188212, AUC: 0.4240346666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001239643414815267, AUC: 0.7400048333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010604337453842163, AUC: 0.7906261666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017612868150075278, AUC: 0.7853321666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011061473687489828, AUC: 0.48844200000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009616493781407674, AUC: 0.7720068333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017050743103027345, AUC: 0.7497554166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001822416067123413, AUC: 0.7868965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011031923294067382, AUC: 0.41072083333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009945464332898459, AUC: 0.6974798333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009716023008028666, AUC: 0.7113190833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009639941652615866, AUC: 0.7240745000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00109904948870341, AUC: 0.5273083333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010117427905400593, AUC: 0.6913511666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000970941424369812, AUC: 0.7096697500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009758197665214539, AUC: 0.7202266666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001108381191889445, AUC: 0.3955576666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010100897153218586, AUC: 0.6944343333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009801236192385355, AUC: 0.7207414999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000994615395863851, AUC: 0.7220246666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988479852676392, AUC: 0.5423835833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009763232072194418, AUC: 0.7182433333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009485452771186829, AUC: 0.7365836666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009338517785072327, AUC: 0.7457319166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001104199767112732, AUC: 0.49273416666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001009968121846517, AUC: 0.6846151666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009896132548650107, AUC: 0.6992801666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010140082637468974, AUC: 0.7116398333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998433430989584, AUC: 0.5313799166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010234491427739462, AUC: 0.6986300000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009773205320040385, AUC: 0.7160968333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009899214506149292, AUC: 0.7209475833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098619023958842, AUC: 0.5116397500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009951512217521667, AUC: 0.6942938333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009669517477353414, AUC: 0.7208125000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009927613139152527, AUC: 0.720976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011067557334899902, AUC: 0.42300125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010168930292129516, AUC: 0.6959984166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009686615268389384, AUC: 0.7232954166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000971798062324524, AUC: 0.7253599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011056735118230184, AUC: 0.4797904166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010057107607523601, AUC: 0.6804016666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000990310509999593, AUC: 0.69979475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009628028074900309, AUC: 0.7240045833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010978631178538004, AUC: 0.5771179166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010060794353485107, AUC: 0.7118674166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009636658430099487, AUC: 0.7323555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009693697690963745, AUC: 0.7328078333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100023587544759, AUC: 0.5092131666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009844680229822796, AUC: 0.7356826666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001056182324886322, AUC: 0.7524725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011689250071843466, AUC: 0.7752487499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106910983721415, AUC: 0.4353023333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009553236961364746, AUC: 0.7417845833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001090216875076294, AUC: 0.7485426666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013120314677556356, AUC: 0.7491558333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099727988243103, AUC: 0.488976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010441067616144817, AUC: 0.7205533333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001166478991508484, AUC: 0.7359143333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010928589900334677, AUC: 0.7567554166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010970599253972372, AUC: 0.6145841666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011241487661997477, AUC: 0.7214166666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012154109875361124, AUC: 0.7490045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107865834236145, AUC: 0.7696853333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011220130920410157, AUC: 0.3805531666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010232747395833334, AUC: 0.7259581666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001148131529490153, AUC: 0.7431843333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001221880038579305, AUC: 0.764319\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010971903800964355, AUC: 0.63350025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010308618148167927, AUC: 0.7222401666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012749990622202556, AUC: 0.7286945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012237332661946614, AUC: 0.7454143333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101090431213379, AUC: 0.49276416666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009749483466148376, AUC: 0.7473329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010557441711425781, AUC: 0.7636331666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010538909037907918, AUC: 0.7770780833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011035247643788656, AUC: 0.46700600000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010697574615478515, AUC: 0.7340816666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011143747170766194, AUC: 0.7581413333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015885307788848877, AUC: 0.7616653333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994923909505207, AUC: 0.5052315833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011150134007136027, AUC: 0.7211219166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001265462358792623, AUC: 0.7439471666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015679079294204713, AUC: 0.7523418333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011034554640452067, AUC: 0.465781\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000986909031867981, AUC: 0.73727175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010056549708048503, AUC: 0.7703507500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001081092913945516, AUC: 0.7771432500000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class SMOTE\n",
    "\n",
    "learning_rates = [1e-2, 1e-3, 5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deb2c447",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a64fdff5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011079707543055217, AUC: 0.4224414166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024819238980611164, AUC: 0.6706555833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002305363098780314, AUC: 0.6838636666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002249549627304077, AUC: 0.6926545000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011115634441375732, AUC: 0.44383383333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018068215847015382, AUC: 0.7456852500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023212509950002033, AUC: 0.7392154999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015493378241856893, AUC: 0.8035191666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010960235595703124, AUC: 0.5768070833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017772521575291952, AUC: 0.7280496666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002030446648597717, AUC: 0.7317598333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002225544532140096, AUC: 0.7191306666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011004793246587117, AUC: 0.5102983333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002655739943186442, AUC: 0.6715455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024241018295288086, AUC: 0.6875708333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020734771887461344, AUC: 0.6995684999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010937273104985555, AUC: 0.5866186666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025021345615386962, AUC: 0.6649868333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021567532221476236, AUC: 0.6796695833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002164699633916219, AUC: 0.6854745833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102820873260498, AUC: 0.47283908333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024672954082489013, AUC: 0.667153\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024438456694285075, AUC: 0.6836446666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020616127649943036, AUC: 0.7165444166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094226638476054, AUC: 0.5688254166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026505370140075682, AUC: 0.6677074166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00240738836924235, AUC: 0.6775998333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022335880597432454, AUC: 0.682071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010978671709696451, AUC: 0.5958104166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022194448312123616, AUC: 0.6927335000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002101566791534424, AUC: 0.7338781666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023925371170043946, AUC: 0.730314\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010997230211893718, AUC: 0.5267864166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023940105438232423, AUC: 0.6662441666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002346807559331258, AUC: 0.7045654999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019873799085617064, AUC: 0.7091379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001110329270362854, AUC: 0.3660240833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002104702075322469, AUC: 0.7089835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001959824005762736, AUC: 0.6864575833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001872533122698466, AUC: 0.7095570833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011009675661722818, AUC: 0.5187256666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020721250375111896, AUC: 0.6062379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023756839434305827, AUC: 0.6242713333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002541064977645874, AUC: 0.63143125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011076082388559976, AUC: 0.41983375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020222012599309287, AUC: 0.6294074166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023207520643870037, AUC: 0.6743595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002417462428410848, AUC: 0.6905260000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011179471413294475, AUC: 0.44402650000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013568212191263835, AUC: 0.6903383333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013643276294072468, AUC: 0.7201696666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001355271061261495, AUC: 0.7464018333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011114279826482138, AUC: 0.4271655833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020804568926493325, AUC: 0.6158478333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002302732308705648, AUC: 0.63495125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024565763473510744, AUC: 0.6399930833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101883888244629, AUC: 0.5275499166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020776435534159343, AUC: 0.6093036666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023438007036844888, AUC: 0.6255574166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024835861523946124, AUC: 0.6338859166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021637121836345, AUC: 0.6185741666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002009077310562134, AUC: 0.6403611666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023686267534891764, AUC: 0.66764475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024570186932881674, AUC: 0.6808744166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011052033503850301, AUC: 0.4367945833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021005558172861737, AUC: 0.6255113333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023509362538655597, AUC: 0.6382724166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002499995549519857, AUC: 0.6464299166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021707852681478, AUC: 0.46878899999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00220019261042277, AUC: 0.6068617499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002459147294362386, AUC: 0.6274133333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025924087365468344, AUC: 0.6359256666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011013393799463908, AUC: 0.5305079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013590420484542846, AUC: 0.6783560833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013645085096359252, AUC: 0.7183881666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014338650306065877, AUC: 0.7315265000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011121948957443237, AUC: 0.4090535833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023055950005849203, AUC: 0.62341475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024989202817281087, AUC: 0.6361519166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025223532517751057, AUC: 0.6455465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011048592726389567, AUC: 0.4177319166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025481193860371907, AUC: 0.6484254166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024674687385559083, AUC: 0.6651425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024357466697692873, AUC: 0.6713125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011073541641235351, AUC: 0.43439625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025950984160105386, AUC: 0.6409186666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00265375812848409, AUC: 0.65872775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002514628251393636, AUC: 0.6681988333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011076154311498007, AUC: 0.5551947500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022441479365030922, AUC: 0.6971596666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022008349895477294, AUC: 0.7016465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002065916140874227, AUC: 0.6919836666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011046329736709595, AUC: 0.41931783333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026869331200917563, AUC: 0.6524774999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025786888599395753, AUC: 0.6713168333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024820197423299152, AUC: 0.6819194999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011026749610900878, AUC: 0.5098351666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027338616847991945, AUC: 0.6478489166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002631261905034383, AUC: 0.6607544166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025313848654429116, AUC: 0.6689055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011076873143513997, AUC: 0.44860174999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026160202026367188, AUC: 0.6505420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026627834637959797, AUC: 0.6636318333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002465544621149699, AUC: 0.6722847499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105912446975708, AUC: 0.44809116666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024165361722310384, AUC: 0.6961640833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020986950397491456, AUC: 0.6857074166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019524118900299072, AUC: 0.6852285833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005394061406454, AUC: 0.49269275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002697857062021891, AUC: 0.6385236666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025773534774780273, AUC: 0.6588340833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002471854289372762, AUC: 0.6727894166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011079903443654377, AUC: 0.43869183333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025158895651499432, AUC: 0.6504198333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002521126906077067, AUC: 0.66575775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022981462478637695, AUC: 0.7020506666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098792274792989, AUC: 0.5084715833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002566495656967163, AUC: 0.6594516666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023025930722554524, AUC: 0.7074231666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021977332433064777, AUC: 0.7066765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011064178148905437, AUC: 0.4041901666666667\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0008978073398272197, AUC: 0.7981495000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010476662516593932, AUC: 0.797174\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013957345088322958, AUC: 0.801761\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011011792421340943, AUC: 0.5008129166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010919816493988037, AUC: 0.7615899166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017090630531311035, AUC: 0.75741025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017677673896153768, AUC: 0.7701735833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011115765571594238, AUC: 0.4346426666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011828872362772624, AUC: 0.7415978333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012650069793065388, AUC: 0.7911315833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015466095209121704, AUC: 0.7957371666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096881588300069, AUC: 0.5596155833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011262359221776326, AUC: 0.7633\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018325774272282918, AUC: 0.7689121666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017438907623291016, AUC: 0.7971378333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011114460229873658, AUC: 0.4237485833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012267327308654785, AUC: 0.71922525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015028331677118937, AUC: 0.7402779166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015713134209314982, AUC: 0.7578658333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010921560128529866, AUC: 0.6199454999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012324892679850261, AUC: 0.7233569166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001209743618965149, AUC: 0.7630251666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001463202993075053, AUC: 0.7786341666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011094931761423746, AUC: 0.3925545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010498208204905192, AUC: 0.7461782499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013926180203755697, AUC: 0.7727065000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016756459871927897, AUC: 0.7822736666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011045343081156412, AUC: 0.5020824166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010106836358706156, AUC: 0.7523293333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012493394215901694, AUC: 0.7634164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015255335966746011, AUC: 0.78138175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011037658055623371, AUC: 0.45513633333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001259947101275126, AUC: 0.7325080833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013927234411239625, AUC: 0.7789625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014783886273701985, AUC: 0.8053306666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989155371983845, AUC: 0.5090311666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011903045972188313, AUC: 0.7370009166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011771143277486166, AUC: 0.7879765000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017296486695607502, AUC: 0.7904987499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992594162623087, AUC: 0.5278233333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009940093557039896, AUC: 0.6945128333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009657553434371948, AUC: 0.7195705833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009802146951357524, AUC: 0.7263654166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096669832865397, AUC: 0.6075720833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001032851497332255, AUC: 0.7264590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009843558669090271, AUC: 0.7145568333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009739188353220622, AUC: 0.725774\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011045145988464356, AUC: 0.3714389166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010056789914766947, AUC: 0.693098\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009946979482968648, AUC: 0.7099201666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009966118733088175, AUC: 0.7193093333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011034783919652302, AUC: 0.5589384166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009846031665802003, AUC: 0.6988330833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009612971941630045, AUC: 0.7240057499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009637996355692546, AUC: 0.7308990833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989969968795777, AUC: 0.5054628333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009815845489501954, AUC: 0.7026663333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009770113428433736, AUC: 0.7166691666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001000847041606903, AUC: 0.7217943333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011000647147496542, AUC: 0.5001725833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010088207324345907, AUC: 0.7015523333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009667608340581258, AUC: 0.7267001666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009430569211641948, AUC: 0.7397771666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998655557632447, AUC: 0.47744099999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009838627179463704, AUC: 0.7025991666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009565305511156718, AUC: 0.7255278333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009549003640810648, AUC: 0.7324212500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105409860610962, AUC: 0.44774575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001001656452814738, AUC: 0.6875285833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009738151431083679, AUC: 0.7117248333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009515709479649861, AUC: 0.7313041666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011092275778452555, AUC: 0.42017716666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000999273439248403, AUC: 0.6986846666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009803808530171713, AUC: 0.723987\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009712782502174377, AUC: 0.7371078333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002099911371866, AUC: 0.4877943333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010111721754074096, AUC: 0.6899492500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000988524337609609, AUC: 0.7154061666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009806044101715089, AUC: 0.7299138333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095471421877543, AUC: 0.55029225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009719979961713155, AUC: 0.7315994166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000976609210173289, AUC: 0.7509495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011375132004419963, AUC: 0.7662175000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994465748469034, AUC: 0.48439916666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001061615268389384, AUC: 0.7240506666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012210078239440919, AUC: 0.7371387500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015706359545389812, AUC: 0.7466465000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003560225168864, AUC: 0.4753065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010294776360193888, AUC: 0.7180058333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011562711795171101, AUC: 0.7532751666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001341441512107849, AUC: 0.7613789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011035508314768472, AUC: 0.4413095833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001077295462290446, AUC: 0.7351731666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001011013646920522, AUC: 0.7633433333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013661166429519652, AUC: 0.7625381666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011045393546422323, AUC: 0.46448399999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009626933336257935, AUC: 0.7466321666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013255577882130942, AUC: 0.7482018333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001673708995183309, AUC: 0.7612776666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099991281827291, AUC: 0.5080065833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009914895296096802, AUC: 0.7380545833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010950936476389567, AUC: 0.7510445833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010407268206278483, AUC: 0.7815078333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010982731978098552, AUC: 0.5526868333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008987191915512085, AUC: 0.7940515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009225935141245524, AUC: 0.7729006666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011297925710678101, AUC: 0.7763786666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011019532680511474, AUC: 0.46340766666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001046321153640747, AUC: 0.7276085000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001444495439529419, AUC: 0.7264068333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011280192136764527, AUC: 0.7771543333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011034169991811116, AUC: 0.5026128333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012736262877782185, AUC: 0.7118106666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012326940298080444, AUC: 0.75678\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014776597420374552, AUC: 0.7702973333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096749226252238, AUC: 0.5954095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088329792022705, AUC: 0.7272213333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009565931757291158, AUC: 0.7696033333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014221105575561524, AUC: 0.7676381666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011046780745188394, AUC: 0.39627016666666665\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010901000102361044, AUC: 0.7558843333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001592508633931478, AUC: 0.7648861666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015164965391159058, AUC: 0.793024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011064399878184, AUC: 0.44293325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011655902862548828, AUC: 0.7469121666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001499106486638387, AUC: 0.7580291666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018441102107365926, AUC: 0.7539993333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011045443614323934, AUC: 0.46125925000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011498111089070639, AUC: 0.7564623333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015798531770706177, AUC: 0.7602367499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002093439976374308, AUC: 0.7589875833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103798508644104, AUC: 0.4563561666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010767038265864055, AUC: 0.7555844999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013137869834899901, AUC: 0.7909311666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018448962370554607, AUC: 0.8021533333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011111820141474407, AUC: 0.39343100000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010647828578948975, AUC: 0.7487516666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010543740590413411, AUC: 0.7829160833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015670624574025472, AUC: 0.78262625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989521741867066, AUC: 0.5096501666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010397976636886597, AUC: 0.6715216666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012110722064971923, AUC: 0.7950606666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001315303881963094, AUC: 0.8034713333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988940795262656, AUC: 0.54246775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010396425724029542, AUC: 0.77369625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013023833433787027, AUC: 0.7920919166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001647231618563334, AUC: 0.7933439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011101274490356446, AUC: 0.4800931666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012190799315770467, AUC: 0.7243924166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014194357792536418, AUC: 0.7684516666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018088542620340983, AUC: 0.7991959166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011079596281051635, AUC: 0.47734308333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010078697800636291, AUC: 0.7623546666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019584929545720417, AUC: 0.7404785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002114712397257487, AUC: 0.7761678333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099352796872457, AUC: 0.5210364166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000985157032807668, AUC: 0.7524676666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013552032709121704, AUC: 0.7752496666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020298471848169963, AUC: 0.7630496666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011084710756937663, AUC: 0.40398716666666673\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010389591852823894, AUC: 0.7078804999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00101042640209198, AUC: 0.7339591666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001005630652109782, AUC: 0.7341568333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011061173677444458, AUC: 0.42995625000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010182913541793824, AUC: 0.6911618333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009861480991045634, AUC: 0.7041939166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009900185267130534, AUC: 0.7167741666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010904707511266072, AUC: 0.6296754999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009889025092124938, AUC: 0.7117481666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009789374073346457, AUC: 0.7166211666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010026154716809592, AUC: 0.7136285833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011012648344039917, AUC: 0.5568328333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000994162897268931, AUC: 0.69452975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009807746410369874, AUC: 0.713223\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010047656496365864, AUC: 0.7179258333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011070818106333415, AUC: 0.5687063333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001006205121676127, AUC: 0.6988258333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000978074550628662, AUC: 0.7124868333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001018133560816447, AUC: 0.7152780833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010985445976257324, AUC: 0.5230313333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009950926502545674, AUC: 0.6956164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009819940725962321, AUC: 0.7155480833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009712765018145244, AUC: 0.7215335833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010997783740361532, AUC: 0.498168\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009731910427411397, AUC: 0.721351\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009465526739756266, AUC: 0.7449560833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009440785646438599, AUC: 0.7537543333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100550929705302, AUC: 0.48551108333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000999036133289337, AUC: 0.6902816666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009760162234306335, AUC: 0.7158144999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009940849741299947, AUC: 0.7182773333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102190891901652, AUC: 0.497121\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010059087276458741, AUC: 0.6950874166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009829614957173665, AUC: 0.7152500833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009939456383387247, AUC: 0.7259315000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011133894920349122, AUC: 0.43749166666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009896463751792907, AUC: 0.6964640833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009717114965120951, AUC: 0.7129487499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009850355784098307, AUC: 0.7198729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011116003195444743, AUC: 0.4375711666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010845239957173666, AUC: 0.7239355833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001000300625960032, AUC: 0.7716781666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010804837544759114, AUC: 0.7892138333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005011796951295, AUC: 0.5969188333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010175198117891948, AUC: 0.7381701666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009666255712509155, AUC: 0.7659948333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013635136683781942, AUC: 0.763916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096467932065328, AUC: 0.5623629166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010189871191978454, AUC: 0.7297233333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010409197807312012, AUC: 0.7609380833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107648233572642, AUC: 0.786237\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100347916285197, AUC: 0.5323065833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001034481922785441, AUC: 0.7378368333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010677272478739422, AUC: 0.781034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011011224587758382, AUC: 0.8040006666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011178616285324097, AUC: 0.420619\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010660743713378905, AUC: 0.7216486666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009368961652119954, AUC: 0.7537175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014726648330688477, AUC: 0.7627055833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001109140396118164, AUC: 0.4925566666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010664844512939453, AUC: 0.735557\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010442968606948853, AUC: 0.7712745000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013896885712941487, AUC: 0.76267225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011015418370564779, AUC: 0.5088568333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010164373715718588, AUC: 0.7339413333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010537364880243937, AUC: 0.7531733333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012184868653615315, AUC: 0.7592548333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097795605659485, AUC: 0.5675945833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009861572980880736, AUC: 0.7270125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013601859013239544, AUC: 0.7332570833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001307805856068929, AUC: 0.7573036666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105371634165446, AUC: 0.49668891666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000987179716428121, AUC: 0.7165998333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012264320850372316, AUC: 0.7397115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012034877141316731, AUC: 0.7726480833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00109502907594045, AUC: 0.572469\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010664153893788656, AUC: 0.73090825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001129280169804891, AUC: 0.7496519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017589335441589356, AUC: 0.7420976666666667\n",
      "\n",
      "[['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.01, 0.507028475, 0.005550550233304238, 0.6883744416666667, 0.0007813762927709038, 0.7008225166666667, 0.0005489203807511092, 0.7147971916666667, 0.001084670799979236, 1, True], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.48010207500000013, 0.0040245759886089584, 0.6325640333333333, 0.0007813134463016689, 0.656717975, 0.001237946620091735, 0.6682541083333333, 0.0016138611797778496, 1, True], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.4673024999999999, 0.001925358944370834, 0.6581931416666668, 0.00040011063935423635, 0.673894225, 0.0002918169134722911, 0.6821349916666667, 0.0001776262204992362, 1, True], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.01, 0.48017598333333333, 0.004692025142696941, 0.7475235999999998, 0.0004790775363775008, 0.7720993083333332, 0.0002686783432514601, 0.7860794416666667, 0.00019925992750145826, 5, True], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.49045664166666664, 0.0041226773691917335, 0.6995883249999999, 0.00010520508650756941, 0.71880685, 3.2524694837222436e-05, 0.7294666416666666, 3.6434124062569385e-05, 5, True], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.5037914916666667, 0.002123346875735349, 0.7354207833333335, 0.0004703117509461113, 0.7529644, 0.00017828695719694404, 0.7671035166666667, 9.118455736916647e-05, 5, True], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.01, 0.46808404166666673, 0.0021841574194295136, 0.7448027666666666, 0.0007383391036969455, 0.7728331749999999, 0.00028070084277701386, 0.782601925, 0.0003116596469659035, 10, True], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.5030481166666666, 0.004345725503407218, 0.700294675, 9.148313168812509e-05, 0.7185001583333332, 0.00012684131736729194, 0.723713325, 0.00013200010181868067, 10, True], ['capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.5187944499999999, 0.0030958091478530563, 0.7295333499999999, 4.683173186777771e-05, 0.7580431, 0.00020244076459972408, 0.7700049583333334, 0.0002994984176447915, 10, True]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-2, 1e-3, 5e-3]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56f0657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57add3d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011004855235417683, AUC: 0.4887625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011042307217915854, AUC: 0.7546737499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001469256083170573, AUC: 0.7447474166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018616555134455363, AUC: 0.7519853333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011009955008824666, AUC: 0.4859675833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014954797426859538, AUC: 0.7307878333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014571794668833415, AUC: 0.7434764166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001918233076731364, AUC: 0.7510033333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010990285078684488, AUC: 0.5017333333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015441822210947673, AUC: 0.7289679166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013621874650319417, AUC: 0.7503736666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020592970053354897, AUC: 0.7446693333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001104023019472758, AUC: 0.4327043333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011127008597056072, AUC: 0.7686645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016269298394521077, AUC: 0.7490771666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014620014031728108, AUC: 0.7722405833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001109751025835673, AUC: 0.41226466666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013858009179433188, AUC: 0.7245768333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013513893286387125, AUC: 0.7421195833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016335872014363606, AUC: 0.7550279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100349466005961, AUC: 0.48139916666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001224625587463379, AUC: 0.7251274166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014618058204650878, AUC: 0.7639809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001893748124440511, AUC: 0.7608105833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998533964157104, AUC: 0.5124565833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012189514636993409, AUC: 0.7386929166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00120151948928833, AUC: 0.7562131666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017067712942759195, AUC: 0.7634075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010969582398732503, AUC: 0.538173\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011792327960332234, AUC: 0.7508575000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016109437545140585, AUC: 0.75193575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017545213301976522, AUC: 0.7468036666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105100949605306, AUC: 0.5000758333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001394121487935384, AUC: 0.7358794166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012873896360397338, AUC: 0.7424176666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002090839942296346, AUC: 0.75268925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010947660207748413, AUC: 0.553981\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001437082529067993, AUC: 0.7315373333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013612343072891236, AUC: 0.7496693333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001404009222984314, AUC: 0.7582930000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098819613456726, AUC: 0.5065030833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011336627006530763, AUC: 0.69624475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011035274664560953, AUC: 0.7289848333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001082752227783203, AUC: 0.7467866666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010977459748586018, AUC: 0.5505764166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011157012383143108, AUC: 0.6855154166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010927118460337321, AUC: 0.7211614166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010983180602391561, AUC: 0.7388749166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989580949147541, AUC: 0.5287226666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001136611819267273, AUC: 0.6970389166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011052985588709513, AUC: 0.7183966666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011363418102264403, AUC: 0.7306098333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010997610092163086, AUC: 0.5101794166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011257091760635376, AUC: 0.6869568333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011254167556762696, AUC: 0.7219326666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010943048397699991, AUC: 0.7476164166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096621036529541, AUC: 0.5347249166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011145275036493938, AUC: 0.7228910833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010782265663146972, AUC: 0.7450464166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010909222761789957, AUC: 0.7538976666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010995964606602987, AUC: 0.5132687499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011139294306437173, AUC: 0.7132966666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010838580926259358, AUC: 0.7403186666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001140423615773519, AUC: 0.7461186666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984042088190715, AUC: 0.5455530833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011301365296045938, AUC: 0.704956\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010922157764434815, AUC: 0.7356759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010945091247558593, AUC: 0.7518811666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010974519650141397, AUC: 0.5811269166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011277774572372437, AUC: 0.6995382499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011200769344965617, AUC: 0.7218839166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011261459191640219, AUC: 0.7390232499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011095584630966186, AUC: 0.42576891666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011234860420227051, AUC: 0.7081191666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010852831602096557, AUC: 0.7371175000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001119327465693156, AUC: 0.7424775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986959139506023, AUC: 0.5085265833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011195060014724731, AUC: 0.7145661666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011048330068588257, AUC: 0.7293056666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011280235052108765, AUC: 0.7391818333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010918916463851928, AUC: 0.6364757499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013359930912653604, AUC: 0.7268925833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001345164696375529, AUC: 0.7378811666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015003972450892131, AUC: 0.7446510833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101840615272522, AUC: 0.49139299999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011844180425008139, AUC: 0.7531694999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001401252547899882, AUC: 0.7349535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014045570691426596, AUC: 0.7383793333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010917581717173259, AUC: 0.6260662499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001304520845413208, AUC: 0.7264386666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001295343279838562, AUC: 0.735718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010601287682851155, AUC: 0.7226601666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011114172538121542, AUC: 0.3950299166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012072943449020386, AUC: 0.7412561666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001505006194114685, AUC: 0.7308335833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002036314606666565, AUC: 0.7183768333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010921948750813803, AUC: 0.6299536666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011139898697535198, AUC: 0.7404611666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012745067675908406, AUC: 0.7435571666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016304620504379273, AUC: 0.7429684166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010962167183558146, AUC: 0.5731191666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001125450571378072, AUC: 0.7569435833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015077069600423177, AUC: 0.7386705833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014020786682764688, AUC: 0.7601788333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011072311798731487, AUC: 0.42304625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010492402315139772, AUC: 0.7703495833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013651695251464843, AUC: 0.7397073333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013542430400848389, AUC: 0.7496026666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986221234003703, AUC: 0.5332675833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011064138015111288, AUC: 0.733161\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001267626961072286, AUC: 0.7363205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012699822584788004, AUC: 0.7474246666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011010087331136068, AUC: 0.5428624166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011095962127049765, AUC: 0.7421406666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012405165831247966, AUC: 0.7388935000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00154677943388621, AUC: 0.7376058333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011017957925796508, AUC: 0.46182016666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013037713368733723, AUC: 0.7322810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013881571292877197, AUC: 0.7311537500000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015433359940846762, AUC: 0.73665975\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.00109506889184316, AUC: 0.6075227500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010930527051289877, AUC: 0.7969098333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00121285879611969, AUC: 0.7962791666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015341360569000244, AUC: 0.7968086666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110688320795695, AUC: 0.39512858333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010997189680735271, AUC: 0.7778755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013350268205006917, AUC: 0.77851625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011274513800938924, AUC: 0.81341375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011012434562047322, AUC: 0.49900916666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001257138729095459, AUC: 0.7524218333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002043050686518351, AUC: 0.7320953333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026825573444366453, AUC: 0.7337874166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010923410654067993, AUC: 0.6525029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009580798546473185, AUC: 0.7926976666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015643794139226278, AUC: 0.7747434999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015132335424423219, AUC: 0.79968725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011098385254542033, AUC: 0.4015779166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001084994395573934, AUC: 0.7721173333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015012899239857992, AUC: 0.7486565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017386534611384074, AUC: 0.7702032500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098043163617452, AUC: 0.533933\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011203059752782187, AUC: 0.7818624999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012514605522155762, AUC: 0.7917668333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013795143763224284, AUC: 0.791999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010959664980570475, AUC: 0.5668985000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00145314892133077, AUC: 0.757642\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011314831574757893, AUC: 0.7822493333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015677810112635295, AUC: 0.7832585000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011059436400731404, AUC: 0.3984905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013966322739919026, AUC: 0.7307428333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001782632311185201, AUC: 0.7407533333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017486469745635986, AUC: 0.7711287499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011188734372456868, AUC: 0.419768\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011598172187805176, AUC: 0.7888086666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013642297983169555, AUC: 0.7982881666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019693562587102253, AUC: 0.7966835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010957918961842855, AUC: 0.55242275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012303212881088258, AUC: 0.7539231666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013039887348810831, AUC: 0.7951523333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013862740198771159, AUC: 0.815563\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010953092177708945, AUC: 0.6404583333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009861884315808614, AUC: 0.7094814166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00097804460922877, AUC: 0.7470353333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001000538984934489, AUC: 0.7563279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010915393034617107, AUC: 0.6032895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001000980297724406, AUC: 0.6951053333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010108818213144938, AUC: 0.7165991666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010097415447235106, AUC: 0.7320798333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011058648029963175, AUC: 0.3926294166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010212364196777344, AUC: 0.6951484166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010278218189875286, AUC: 0.7308895833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001052735169728597, AUC: 0.7477645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011016406218210856, AUC: 0.4643534166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001031091054280599, AUC: 0.6915429166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001040674090385437, AUC: 0.7243474999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010715934832890828, AUC: 0.7400508333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101608196894328, AUC: 0.5494846666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010480308532714844, AUC: 0.70314025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010486969947814942, AUC: 0.7253285000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010532618761062622, AUC: 0.7350355000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010982596874237062, AUC: 0.5231133333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001039299170176188, AUC: 0.7073885000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010384729305903118, AUC: 0.7258065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010636101961135865, AUC: 0.7394919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010999561150868734, AUC: 0.5147990833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010455213387807211, AUC: 0.7027111666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010325393279393513, AUC: 0.7284436666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009992482860883077, AUC: 0.7512784166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102043390274048, AUC: 0.46415566666666663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000999273419380188, AUC: 0.6942214999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010055386622746785, AUC: 0.7411754166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010026804804801942, AUC: 0.7553766666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011024383703867595, AUC: 0.4691273333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010185602903366089, AUC: 0.6940140833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001030768116315206, AUC: 0.7317110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010726646184921265, AUC: 0.7374634166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011024768352508544, AUC: 0.46273383333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010103808442751567, AUC: 0.6928208333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001022193173567454, AUC: 0.7269661666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010319610039393108, AUC: 0.7471425833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011040633122126262, AUC: 0.48861574999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001084409515062968, AUC: 0.7446874166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011547755400339763, AUC: 0.7705745833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013452511628468831, AUC: 0.7643196666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011084726254145304, AUC: 0.40680758333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009926408131917318, AUC: 0.7801149166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011829959551493327, AUC: 0.7703308333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013285505771636963, AUC: 0.7772196666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010999813874562581, AUC: 0.4784380833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009671451250712077, AUC: 0.7715393333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011262832880020142, AUC: 0.7781401666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011721332867940268, AUC: 0.7816708333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098308563232422, AUC: 0.60804125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011307073831558227, AUC: 0.7352096666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001225026528040568, AUC: 0.746285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015529809792836507, AUC: 0.75701375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010934036572774251, AUC: 0.5863502500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010170689225196838, AUC: 0.7685586666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012939701080322266, AUC: 0.7643324166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012338793675104778, AUC: 0.7774453333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011004685958226521, AUC: 0.4946271666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011737916072209677, AUC: 0.7453371666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013495200475056965, AUC: 0.7713799166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015454222361246746, AUC: 0.7744378333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011088493665059407, AUC: 0.42956875000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010903794765472412, AUC: 0.7601739166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012612099647521973, AUC: 0.7723468333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017190736929575603, AUC: 0.7535865833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011022999286651612, AUC: 0.46452258333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001025501569112142, AUC: 0.7681906666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011542065540949503, AUC: 0.7616131666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013855027357737224, AUC: 0.7757744999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011009726921717327, AUC: 0.45751366666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011024720668792725, AUC: 0.7465085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009692980845769246, AUC: 0.7718578333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014292685985565186, AUC: 0.75875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011026564439137777, AUC: 0.4574010833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011313136816024781, AUC: 0.7420356666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011586749951044719, AUC: 0.7638674999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011045318841934205, AUC: 0.7781661666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010964237451553344, AUC: 0.5580166666666666\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010996352036794026, AUC: 0.7698611666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018259684642155966, AUC: 0.7553820833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001500830332438151, AUC: 0.7931090833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00109957222143809, AUC: 0.5198979166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001127657175064087, AUC: 0.75847275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012819421291351317, AUC: 0.7650260000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013352120717366536, AUC: 0.7699018333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011067168315251669, AUC: 0.44954541666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012927870750427245, AUC: 0.7444374166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011801127195358277, AUC: 0.7940649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015621389547983806, AUC: 0.7898746666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011187699635823568, AUC: 0.4121849166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011955602169036864, AUC: 0.770215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012250461181004843, AUC: 0.802267\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017819425264994303, AUC: 0.7973338333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010961958964665732, AUC: 0.5457154166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005438963572183, AUC: 0.7572135000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012234842379887899, AUC: 0.7690803333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011455086469650269, AUC: 0.7881431666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011067813237508137, AUC: 0.3803885833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011677629550298056, AUC: 0.7619088333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014500645001729328, AUC: 0.7645486666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001860355059305827, AUC: 0.7638666666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011117841402689616, AUC: 0.4633545833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099915345509847, AUC: 0.7573715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001427885929743449, AUC: 0.7878805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014267033735911052, AUC: 0.7818525833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106459140777588, AUC: 0.41029958333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010455037355422974, AUC: 0.7735473333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016185382604598998, AUC: 0.7632205000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016153693596522014, AUC: 0.7897609166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010995869636535646, AUC: 0.5424655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001349367658297221, AUC: 0.7474436666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014319247007369995, AUC: 0.7798839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002240425109863281, AUC: 0.7748928333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001112542986869812, AUC: 0.39676633333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001428845485051473, AUC: 0.7254413333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001521015445391337, AUC: 0.7650594999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016000083287556967, AUC: 0.7887598333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010965664784113565, AUC: 0.5540881666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010067923466364542, AUC: 0.7155252499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009594269792238871, AUC: 0.7264059999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009900189042091369, AUC: 0.7335655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011067347129185994, AUC: 0.4409669166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010062939127286276, AUC: 0.6824180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010033489863077799, AUC: 0.7116429166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010325006246566773, AUC: 0.7199965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008529265721639, AUC: 0.48820725000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010037936568260194, AUC: 0.7161085000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009837868809700013, AUC: 0.7212610000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010075665712356568, AUC: 0.7258711666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011031516393025715, AUC: 0.5129689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009884562095006308, AUC: 0.6943776666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009829114079475403, AUC: 0.72011225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009787832895914715, AUC: 0.7366175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011079587936401367, AUC: 0.442594\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982944965362549, AUC: 0.7137113333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009438162247339885, AUC: 0.7512716666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009215635061264038, AUC: 0.7633853333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011136085192362468, AUC: 0.409268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010027713179588317, AUC: 0.7005760833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009915505051612853, AUC: 0.7259881666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010038012663523357, AUC: 0.7276538333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094768722852071, AUC: 0.5596040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000996839423974355, AUC: 0.697042\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010020377039909362, AUC: 0.720401\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009891287883122763, AUC: 0.7314843333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010974709987640382, AUC: 0.5351565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000994803229967753, AUC: 0.6923281666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009822248617808024, AUC: 0.71804525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000992626468340556, AUC: 0.7318358333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008121172587076, AUC: 0.4931770833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010070029695828755, AUC: 0.6928299166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009795227845509848, AUC: 0.71896975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000989958345890045, AUC: 0.7314066666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011039932568868002, AUC: 0.45952300000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000989994724591573, AUC: 0.7174366666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000979889710744222, AUC: 0.7403578333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009953999916712444, AUC: 0.7433108333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011043513218561808, AUC: 0.41522525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009718645413716634, AUC: 0.7270281666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011164419253667196, AUC: 0.7660351666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011509071191151937, AUC: 0.7759428333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001104005455970764, AUC: 0.41259700000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010233759880065919, AUC: 0.7295719166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00105691126982371, AUC: 0.7594516666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001448151707649231, AUC: 0.7491384166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011149702469507854, AUC: 0.4148265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009852914611498515, AUC: 0.755124\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00093146679798762, AUC: 0.7856808333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016483883460362751, AUC: 0.7272283333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010919492244720458, AUC: 0.5823371666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010667858521143595, AUC: 0.7523311666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011237112681070963, AUC: 0.7698888333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015951190789540608, AUC: 0.7473676666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103950023651123, AUC: 0.4217269166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013290177583694458, AUC: 0.7162369166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00134693710009257, AUC: 0.7619587500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014634109338124593, AUC: 0.7654395833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011031290690104167, AUC: 0.48289116666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010358746846516928, AUC: 0.7330788333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012607671817143758, AUC: 0.7561255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011030386686325074, AUC: 0.7893501666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011073987086613972, AUC: 0.41945241666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010171599785486858, AUC: 0.7517405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011877509355545043, AUC: 0.7741628333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012733789285024008, AUC: 0.7773958333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011084714730580647, AUC: 0.4261519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010705119768778483, AUC: 0.7449903333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001441202163696289, AUC: 0.7424363333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001385249892870585, AUC: 0.7750850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010996285676956176, AUC: 0.5633390833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011788311004638672, AUC: 0.7243786666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014172059694925944, AUC: 0.73077975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013685308694839477, AUC: 0.7604694166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003657976786296, AUC: 0.4996061666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011321732997894287, AUC: 0.7504623333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001021358887354533, AUC: 0.7870571666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011578699747721354, AUC: 0.7811336666666667\n",
      "\n",
      "[['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.01, 0.49075180000000007, 0.0016569387512752772, 0.7389765416666666, 0.0001900326219322916, 0.7494011166666666, 4.250724220027743e-05, 0.7556930583333333, 6.120316620701434e-05, 1, True], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.520495075, 0.0014949891056506256, 0.702912325, 0.00013216531674507, 0.7299823750000001, 7.609654498368012e-05, 0.7436467916666666, 4.400255806423629e-05, 1, True], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.5313034166666666, 0.00683699558109722, 0.7423093916666667, 0.00017967271548479153, 0.7367689083333333, 1.3544144057569141e-05, 0.7398507583333334, 0.00013724905611034747, 1, True], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.01, 0.5027254166666667, 0.008054628501159723, 0.7705001333333333, 0.0004059115129433333, 0.773850075, 0.0005441768836367385, 0.7872533083333334, 0.0005268431168917374, 5, True], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.5084144583333334, 0.004931703042892011, 0.6985574416666667, 3.8066961930625436e-05, 0.7298302833333334, 6.810911244055561e-05, 0.7442011749999999, 6.59123590714582e-05, 5, True], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.48718861666666663, 0.003663513749928056, 0.7562355916666667, 0.00021051987886868117, 0.767072825, 6.943509721034767e-05, 0.7698384333333332, 9.601010035249923e-05, 5, True], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.01, 0.4678634916666667, 0.004193781311493682, 0.7565912499999999, 0.00018837299881666693, 0.7746413583333334, 0.00021616243245284659, 0.7837495416666667, 0.00010593222269479213, 10, True], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.4895553916666667, 0.0023628100871486827, 0.7022353583333334, 0.00014026730110701353, 0.7254455833333334, 0.00012491965382916645, 0.7345127499999999, 0.00012747877522361047, 10, True], ['distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.4638153666666668, 0.003813952248840557, 0.7384942833333332, 0.00017639505210444427, 0.7633576833333333, 0.0002795107722566666, 0.7648550916666668, 0.00032380325691867877, 10, True]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class euclidean distance capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-2, 1e-3, 5e-3]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['distance'] = 'euclidean'\n",
    "\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"distance_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15bc95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "805eafa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011052825848261514, AUC: 0.43978183333333337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001242885112762451, AUC: 0.7556419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017786058187484741, AUC: 0.7451270833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001355748494466146, AUC: 0.7903454166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010999015967051188, AUC: 0.5015735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001183523694674174, AUC: 0.7529743333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017083600759506227, AUC: 0.7684294999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020539085070292156, AUC: 0.7834125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010926770766576132, AUC: 0.5972151666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012481575806935629, AUC: 0.7396061666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001280996561050415, AUC: 0.774436\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013489604791005452, AUC: 0.8045683333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011085643370946248, AUC: 0.41532358333333336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001295187473297119, AUC: 0.7403531666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014481489260991414, AUC: 0.7683135833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001503763198852539, AUC: 0.8040346666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011077126661936441, AUC: 0.3876288333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014019229412078858, AUC: 0.7298151666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012458863655726115, AUC: 0.7880338333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018081894715627034, AUC: 0.7905300000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011023714542388915, AUC: 0.44327941666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011233824491500854, AUC: 0.7764533333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013599769274393718, AUC: 0.7757853333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001630991260210673, AUC: 0.7814448333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011145607630411783, AUC: 0.32745641666666664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010820915301640829, AUC: 0.7825736666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012192320426305135, AUC: 0.7953826666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016848710775375366, AUC: 0.8065705833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010936137040456137, AUC: 0.5703022500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009436519344647726, AUC: 0.7783104999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012579033374786378, AUC: 0.7965208333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016022408405939738, AUC: 0.7867226666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011033010482788086, AUC: 0.4292242499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010503697395324706, AUC: 0.7746530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016495122909545897, AUC: 0.7660344166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019463297128677367, AUC: 0.7708870000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011015632549921672, AUC: 0.4795859166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013584723075230917, AUC: 0.7521388333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016266434987386067, AUC: 0.7868365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015543033281962078, AUC: 0.8185890833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011108194192250569, AUC: 0.3419486666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010033171375592549, AUC: 0.6896026666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009849712053934733, AUC: 0.72248\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010011465152104696, AUC: 0.7318060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010970837672551472, AUC: 0.5532395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009985850652058919, AUC: 0.6955717499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009926936427752176, AUC: 0.7176396666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009789584875106812, AUC: 0.7361153333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011035648981730143, AUC: 0.42958316666666674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010014365712801615, AUC: 0.6950899166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001002144197622935, AUC: 0.7168530833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010018961628278096, AUC: 0.7311090833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010966077248255413, AUC: 0.5706053333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009843558271725973, AUC: 0.6987758333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000980299413204193, AUC: 0.7181961666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009855879147847493, AUC: 0.7292071666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099665880203247, AUC: 0.49452908333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009799832304318746, AUC: 0.7109610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009829253951708476, AUC: 0.7233472499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010060605804125468, AUC: 0.7308336666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011001358032226563, AUC: 0.49598933333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009760411183039348, AUC: 0.7241001666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009547644257545472, AUC: 0.7409946666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009742421507835388, AUC: 0.7328423333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010971740484237672, AUC: 0.5320221666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009834888378779094, AUC: 0.7105694166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009741238554318746, AUC: 0.728897\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010064294139544168, AUC: 0.7299821666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010978577534357706, AUC: 0.6019133333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001010177175203959, AUC: 0.6874166666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000999732792377472, AUC: 0.7099293333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010325587193171183, AUC: 0.7148534166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094303846359253, AUC: 0.5705435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009925708373387655, AUC: 0.6994970833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009863321582476298, AUC: 0.7201563333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010269964138666788, AUC: 0.7253165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994608402252197, AUC: 0.49848108333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000989067792892456, AUC: 0.7082652499999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009848726987838744, AUC: 0.7217981666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001016507347424825, AUC: 0.7257485833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011030149459838867, AUC: 0.45802425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00105222221215566, AUC: 0.7335145000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011837135553359986, AUC: 0.7562842499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001032602588335673, AUC: 0.7859023333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010944536129633585, AUC: 0.5927108333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010121003786722818, AUC: 0.6790400833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012255022128423055, AUC: 0.7485508333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010648034016291301, AUC: 0.7785492499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011124239762624105, AUC: 0.39449208333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010099299947420755, AUC: 0.7372508333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005038817723592, AUC: 0.76264625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001221539815266927, AUC: 0.7828745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100527286529541, AUC: 0.5038714999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009799331227938334, AUC: 0.7291404166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00094316432873408, AUC: 0.7760418333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018191998799641927, AUC: 0.7438628333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994138717651367, AUC: 0.52235325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012390422423680623, AUC: 0.7235440833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010458950599034626, AUC: 0.7724103333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012372745275497437, AUC: 0.7888243333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100235939025879, AUC: 0.47842308333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009931529959042867, AUC: 0.7360367500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00150886603196462, AUC: 0.7333489999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001342439651489258, AUC: 0.7685581666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010941846370697021, AUC: 0.5773158333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009656792084376017, AUC: 0.7361185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009749161005020142, AUC: 0.757888\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014002594550450644, AUC: 0.7639665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011123396158218384, AUC: 0.4910478333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011187382141749063, AUC: 0.723629\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00129604438940684, AUC: 0.7470101666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011723470687866212, AUC: 0.7781036666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102103273073832, AUC: 0.5101065833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001005542576313019, AUC: 0.7405304999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001044869025548299, AUC: 0.7692404166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017746853033701578, AUC: 0.7518397499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008099714914959, AUC: 0.5085564166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010576653480529785, AUC: 0.7403221666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014580024878184002, AUC: 0.7394916666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016340305010477702, AUC: 0.7730399166666665\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001102006713549296, AUC: 0.5132036666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012708915074666342, AUC: 0.7639296666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015375802119572957, AUC: 0.7768043333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002539357344309489, AUC: 0.7442037499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011142092148462932, AUC: 0.3598644166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010708965063095093, AUC: 0.77946075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001908510684967041, AUC: 0.7509295000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016529592672983805, AUC: 0.79660725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011188224951426187, AUC: 0.3916071666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012840594053268432, AUC: 0.7236514166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012736543416976929, AUC: 0.7729545833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002117136081059774, AUC: 0.7873815000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011116609573364258, AUC: 0.357033\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010108190377553304, AUC: 0.7509784166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012108296553293863, AUC: 0.7767719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001450315276781718, AUC: 0.7914846666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011061280171076457, AUC: 0.4922578333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010047587553660075, AUC: 0.769164\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013816887140274048, AUC: 0.7790140833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016818987131118775, AUC: 0.79546\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992931922276816, AUC: 0.49916625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011477706829706828, AUC: 0.7628538333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019601945479710897, AUC: 0.7763200833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002211273511250814, AUC: 0.7834740833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011038367748260499, AUC: 0.41970124999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012042486667633056, AUC: 0.7399732499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011269136667251586, AUC: 0.7799583333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018957820336023966, AUC: 0.7809191666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010996092557907105, AUC: 0.5076415833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010746674140294393, AUC: 0.7665055000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014257461229960123, AUC: 0.7873306666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001272143284479777, AUC: 0.8101783333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011019491751988728, AUC: 0.4687748333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011048280398050943, AUC: 0.7717258333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001278264323870341, AUC: 0.7874515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015009965896606446, AUC: 0.8082316666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010943660736083985, AUC: 0.6048100833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010948184728622437, AUC: 0.7534705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010304831266403198, AUC: 0.80318075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001120049794514974, AUC: 0.8049934999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010892634391784669, AUC: 0.6238854166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009899799823760986, AUC: 0.7010975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009848402341206869, AUC: 0.7140080833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009783457517623902, AUC: 0.7287761666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010961312850316365, AUC: 0.5681430833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000979781448841095, AUC: 0.7110272499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009761015176773071, AUC: 0.7241073333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009822571277618409, AUC: 0.7305580833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010993290742238363, AUC: 0.49248458333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010197827021280925, AUC: 0.693164\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009717961351076762, AUC: 0.7150595000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009892191092173259, AUC: 0.7171325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00109678049882253, AUC: 0.5567589166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000997121493021647, AUC: 0.6950178333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009780041774113974, AUC: 0.7164443333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009903105894724527, AUC: 0.7235989166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010975958506266275, AUC: 0.52984825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000988736351331075, AUC: 0.6955344999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009852591554323832, AUC: 0.7098253333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009989083607991537, AUC: 0.7179411666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011011060078938801, AUC: 0.47580808333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009996693332990012, AUC: 0.6979223333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009856516520182293, AUC: 0.7132318333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000999704122543335, AUC: 0.7189101666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011030961275100709, AUC: 0.62071825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009902743697166442, AUC: 0.7095595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009602400461832683, AUC: 0.7332179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009596516688664754, AUC: 0.7373553333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010972580512364706, AUC: 0.5385525000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009918743968009949, AUC: 0.7113572500000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009952367345492045, AUC: 0.7165809166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010061280330022175, AUC: 0.7193573333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010995231866836547, AUC: 0.48535325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000980647027492523, AUC: 0.706823\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009634583592414856, AUC: 0.7360128333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982534686724345, AUC: 0.7365725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096468210220337, AUC: 0.5642393333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000983853022257487, AUC: 0.712351\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000967814286549886, AUC: 0.730292\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001010051707426707, AUC: 0.7342801666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010963884592056275, AUC: 0.5534795833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010277713537216186, AUC: 0.7370356666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012712493340174358, AUC: 0.7509178333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001284215529759725, AUC: 0.7760003333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010941277742385865, AUC: 0.5934653333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010443700949350993, AUC: 0.7331920833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001042675018310547, AUC: 0.7509549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001183596650759379, AUC: 0.7665721666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011013166904449462, AUC: 0.5063045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009572117924690247, AUC: 0.7558829166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001197301189104716, AUC: 0.7744365833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014734627803166707, AUC: 0.7803225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101922074953715, AUC: 0.4716498333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000951429824034373, AUC: 0.7340738333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001137385368347168, AUC: 0.7487791666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015652597745259603, AUC: 0.751424\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011025995016098022, AUC: 0.4636045833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011023643414179484, AUC: 0.7282416666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010836116870244345, AUC: 0.7575198333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015277689695358276, AUC: 0.7550169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011035694678624471, AUC: 0.44808183333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010065564115842183, AUC: 0.7502305833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009970664978027344, AUC: 0.7872975000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012350488901138306, AUC: 0.7897646666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010914289156595865, AUC: 0.6476768333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001074254552523295, AUC: 0.7358363333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011178109645843507, AUC: 0.7672175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011115504503250123, AUC: 0.7735415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010994761387507121, AUC: 0.5082473333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010498007933298747, AUC: 0.7432927500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012039331992467245, AUC: 0.7681236666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012377911408742268, AUC: 0.7824813333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103779395421346, AUC: 0.4356425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001020856738090515, AUC: 0.7293618333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009965181350708007, AUC: 0.7491035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009932926694552105, AUC: 0.786505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011035188833872476, AUC: 0.5002131666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010656888087590536, AUC: 0.7392074166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010090548793474834, AUC: 0.7769553333333331\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012523607015609741, AUC: 0.7811840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095573623975118, AUC: 0.5769076666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001190720001856486, AUC: 0.7571106666666667\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001682835817337036, AUC: 0.7700859166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016216527223587036, AUC: 0.8116925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011027799050013225, AUC: 0.46017141666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009993523557980854, AUC: 0.7918486666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013525794744491577, AUC: 0.7881920833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015977139075597127, AUC: 0.8012561666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100353757540385, AUC: 0.47812058333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011181455055872599, AUC: 0.7524395833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015768436988194783, AUC: 0.7707788333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001684269905090332, AUC: 0.8039378333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010977291266123454, AUC: 0.5273618333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012737759749094646, AUC: 0.7475231666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001591060439745585, AUC: 0.7671234166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019627333879470824, AUC: 0.7713725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011134448846181234, AUC: 0.35818750000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001361437201499939, AUC: 0.7452843333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017297926743825276, AUC: 0.7570045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021948784987131753, AUC: 0.7524543333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992012023925781, AUC: 0.51481375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001275275429089864, AUC: 0.74888025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015842588742574057, AUC: 0.7755140000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017796413898468018, AUC: 0.7967448333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986533164978028, AUC: 0.5251801666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011307824850082398, AUC: 0.7625631666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015177246332168579, AUC: 0.7633570000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011920704046885172, AUC: 0.8018175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010993882417678832, AUC: 0.49319108333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00128338094552358, AUC: 0.7359430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018545222679773967, AUC: 0.7471930000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001524183948834737, AUC: 0.8040221666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011053677002588907, AUC: 0.38088941666666676\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014618106285730998, AUC: 0.7298687500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016418736775716146, AUC: 0.7568385000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018790031274159749, AUC: 0.7944025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011033515930175782, AUC: 0.4883530833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012057714462280274, AUC: 0.7622263333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013016608556111653, AUC: 0.7870888333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016563831170399983, AUC: 0.7989648333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011013116041819254, AUC: 0.49133975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010110619068145752, AUC: 0.7324885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009568597873051961, AUC: 0.7396402499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009771706660588583, AUC: 0.7409061666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011158833106358847, AUC: 0.36710058333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010048614343007405, AUC: 0.7264472500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009443788528442383, AUC: 0.7469942499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009457928935686748, AUC: 0.7489013333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011042243639628093, AUC: 0.5048244999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000983566423257192, AUC: 0.701836\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009656203587849935, AUC: 0.7191089166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009712597131729126, AUC: 0.7234695833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011041393280029296, AUC: 0.4540325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010098834037780761, AUC: 0.7173815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009562164346377055, AUC: 0.73596875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000958151658376058, AUC: 0.7382905000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010937712987263998, AUC: 0.5809078333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009975797533988953, AUC: 0.7011835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009747327168782552, AUC: 0.7245771666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009806480606396994, AUC: 0.7382973333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101788600285848, AUC: 0.44576499999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009888164003690083, AUC: 0.6972745833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009654398560523987, AUC: 0.722465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010301814874013266, AUC: 0.7206955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110673721631368, AUC: 0.44465724999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009793102343877156, AUC: 0.7027516666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009551057815551758, AUC: 0.72633775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009896982312202454, AUC: 0.7317795833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002353032430012, AUC: 0.4934808333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009941839377085367, AUC: 0.7027701666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009796630144119263, AUC: 0.7223933333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009789421955744425, AUC: 0.7291474999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011032837231953938, AUC: 0.4235675833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009952846964200338, AUC: 0.7023316666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000962993303934733, AUC: 0.7324005833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009806098540623982, AUC: 0.7361134166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011010559797286986, AUC: 0.5137915833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009895623723665873, AUC: 0.69841075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009702318906784058, AUC: 0.7203309166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009630068143208822, AUC: 0.7288178333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011041100025177002, AUC: 0.4476700833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001025624096393585, AUC: 0.7443106666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011343144178390502, AUC: 0.7549480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011133350133895874, AUC: 0.7800176666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011041619777679443, AUC: 0.3866853333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011449777285257976, AUC: 0.7210405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011483944654464722, AUC: 0.7664501666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013678685029347739, AUC: 0.7762005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010950481494267782, AUC: 0.600333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001033435583114624, AUC: 0.7393013333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011688275337219237, AUC: 0.7573256666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001280771811803182, AUC: 0.7758483333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010996214946111043, AUC: 0.49606500000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011098840236663818, AUC: 0.7282593333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014085972309112549, AUC: 0.7406581666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017645678917566935, AUC: 0.7515601666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011140156189600626, AUC: 0.4158268333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010116553703943887, AUC: 0.7442073333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010211445887883504, AUC: 0.7734190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013178801933924358, AUC: 0.776474\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010879511833190919, AUC: 0.6545525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009831160306930542, AUC: 0.7396068333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010708336035410563, AUC: 0.7659816666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012179024219512939, AUC: 0.7825603333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010950992107391358, AUC: 0.587346\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010979522466659545, AUC: 0.7438468333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013305134773254396, AUC: 0.7644817499999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015312257607777914, AUC: 0.774778\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006502310434978, AUC: 0.5078773333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010491578181584676, AUC: 0.7292878333333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013517614205678305, AUC: 0.7487765000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016176618734995525, AUC: 0.7512013333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100757400194804, AUC: 0.5296706666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011655190785725912, AUC: 0.720366\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010746469895044962, AUC: 0.7642760000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017612395286560059, AUC: 0.73286675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010967158873875936, AUC: 0.5402444166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009925466378529868, AUC: 0.7525466666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012996623118718465, AUC: 0.7625176666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017931011120478312, AUC: 0.767113\n",
      "\n",
      "[['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.01, 0.4591371166666667, 0.00595359750880167, 0.7582520166666666, 0.00031428937253583273, 0.7764899749999999, 0.00022279699736256928, 0.7937105083333333, 0.0001856483329881249, 1, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.5088855166666667, 0.005340524332167775, 0.701984975, 0.00011535018669173595, 0.7220291666666666, 6.195161896250061e-05, 0.728781425, 3.058712175201377e-05, 1, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.5036901666666667, 0.002856646368004166, 0.7279126833333334, 0.0002992594824552793, 0.7562912749999999, 0.0001815152384681249, 0.771552125, 0.0001948329910364584, 1, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.01, 0.46140600833333345, 0.005563300994183958, 0.7581713166666667, 0.00024789757636361153, 0.7790715833333333, 0.00015678708116805457, 0.7902933916666667, 0.000326837682745902, 5, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.5455791666666666, 0.0024371105269472212, 0.7033854166666667, 5.2347625627777996e-05, 0.7208780166666668, 7.801890974833345e-05, 0.7264482333333333, 5.7948991446944645e-05, 5, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.5128365500000001, 0.0040830064120988855, 0.7386355083333334, 7.088034587284703e-05, 0.7631305916666667, 0.0001676221813561805, 0.7742812499999999, 0.00014970399305694513, 5, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.01, 0.48031764999999993, 0.00401881512389555, 0.7533687916666667, 0.00026341453906979093, 0.7683176083333334, 0.000154036539470902, 0.7936665166666668, 0.000289527392324723, 10, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.001, 0.4719467416666666, 0.003062864302536738, 0.7082875583333333, 0.00014056473178479235, 0.7290216916666667, 7.831205259590164e-05, 0.733641875, 6.530355852256975e-05, 10, True], ['cosine_distance_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.005, 0.5166271166666666, 0.006405915376032223, 0.7362773333333333, 0.00010681110807777761, 0.7598834583333333, 8.347758637673639e-05, 0.7668620083333334, 0.00023776672112840158, 10, True]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class cosine distance capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-2, 1e-3, 5e-3]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['distance'] = 'cosine'\n",
    "\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"cosine_distance_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abe9f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77723321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011002787351608277, AUC: 0.5535941666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013811708688735963, AUC: 0.8021806666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022490737438201906, AUC: 0.7867922499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026706024010976154, AUC: 0.7802557499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011052557229995728, AUC: 0.4228015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001886900782585144, AUC: 0.7912025000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018693280220031738, AUC: 0.7861763333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002670501152674357, AUC: 0.7802293333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011140325864156088, AUC: 0.3403848333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014941403865814209, AUC: 0.8026481666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018344403505325317, AUC: 0.79061975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002138315518697103, AUC: 0.8051755000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011004663705825805, AUC: 0.5112873333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018857736190160115, AUC: 0.7956465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002059891661008199, AUC: 0.7814070833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003114476998647054, AUC: 0.7428803333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010936484734217326, AUC: 0.5868754166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016188606818517048, AUC: 0.7946828333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023695968786875406, AUC: 0.8020391666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025649107297261555, AUC: 0.8003500833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010947204033533733, AUC: 0.5843878333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017162395715713502, AUC: 0.7738083333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022282013098398846, AUC: 0.7961503333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026281104087829588, AUC: 0.7943905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010976907809575399, AUC: 0.5402590833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001764705220858256, AUC: 0.77339175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019605228503545126, AUC: 0.8005628333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024525395234425863, AUC: 0.7863826666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010965106884638469, AUC: 0.5781491666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011351037820180258, AUC: 0.8086471666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018727817138036091, AUC: 0.8061373333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027613320350646973, AUC: 0.7941148333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010957347949345907, AUC: 0.5677858333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013622175455093384, AUC: 0.7705605000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002083128809928894, AUC: 0.7932509166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025074694951375326, AUC: 0.781327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011037404139836629, AUC: 0.4179565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014335426886876424, AUC: 0.7925838333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002125535170237223, AUC: 0.7728895833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002784558057785034, AUC: 0.7595515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001091559370358785, AUC: 0.6172539166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013655627965927124, AUC: 0.7988387499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019727245569229127, AUC: 0.7838934999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002194498062133789, AUC: 0.7923365833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991420348485312, AUC: 0.5131314166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001631360650062561, AUC: 0.79860275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023010902404785156, AUC: 0.7818447499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002659001032511393, AUC: 0.7904125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011034923791885375, AUC: 0.42525491666666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016084826389948527, AUC: 0.7966300000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002003541668256124, AUC: 0.7963814999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024935291608174644, AUC: 0.7860875833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096120516459147, AUC: 0.5866459166666665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014700205326080322, AUC: 0.8227628333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002139884869257609, AUC: 0.8096954166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002567270914713542, AUC: 0.8119672499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010943733056386312, AUC: 0.5975117499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017803054650624593, AUC: 0.78764475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022513546148935954, AUC: 0.7779980000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026842278639475506, AUC: 0.7941709166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010955755710601806, AUC: 0.6152059166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014845548073450724, AUC: 0.7876823333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025251168409983317, AUC: 0.7821544166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002747184673945109, AUC: 0.7880129166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010956888596216838, AUC: 0.5500964166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015696738560994467, AUC: 0.7940733333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017898446321487427, AUC: 0.8001685833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026626942952473957, AUC: 0.7783858333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011142232418060302, AUC: 0.34397725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015465458631515503, AUC: 0.7750846666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018062354326248168, AUC: 0.7933370000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002465574661890666, AUC: 0.7959345833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011204453309377035, AUC: 0.37144\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014546320835749308, AUC: 0.7796935833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002542213757832845, AUC: 0.7739533333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024199875990549725, AUC: 0.7938263333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011010787884394328, AUC: 0.46566308333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001927318572998047, AUC: 0.78678925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023125343322753906, AUC: 0.7907994166666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028102544943491617, AUC: 0.7690454166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011041489442189534, AUC: 0.5575845833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017359646161397297, AUC: 0.8072088333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001990538676579793, AUC: 0.8096151666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002651673396428426, AUC: 0.798825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010955560207366944, AUC: 0.5826096666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016221673091252645, AUC: 0.7812164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00231536602973938, AUC: 0.7762130833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017120191256205241, AUC: 0.79382225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011025813817977905, AUC: 0.50698875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017700879176457724, AUC: 0.7683534999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020599591334660846, AUC: 0.7866186666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027080301443735758, AUC: 0.7772543333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011018489996592203, AUC: 0.448115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014406781593958536, AUC: 0.7920159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024520308176676432, AUC: 0.7703140833333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026363066832224527, AUC: 0.7707413333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011099278926849365, AUC: 0.4146441666666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016558313369750978, AUC: 0.7817351666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002131727894147237, AUC: 0.7850569166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024334941705067953, AUC: 0.7899563333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011000746488571166, AUC: 0.5263705000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017618296543757122, AUC: 0.7653325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018831220070521036, AUC: 0.8012522500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002161686340967814, AUC: 0.7871710000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010997104247411093, AUC: 0.5418834166666667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013661898374557496, AUC: 0.8201225833333332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018932850360870361, AUC: 0.8004571666666668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002459901571273804, AUC: 0.8026918333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011023834546407063, AUC: 0.44456483333333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019432127475738526, AUC: 0.7649884166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002017125407854716, AUC: 0.7834930833333335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022735071976979575, AUC: 0.7784574166666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011087087790171305, AUC: 0.39612091666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015401715834935507, AUC: 0.7950588333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024303363958994547, AUC: 0.7802288333333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021912121772766115, AUC: 0.7893650833333333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010972108443578084, AUC: 0.5565408333333334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014957450230916342, AUC: 0.789931\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016299136479695638, AUC: 0.8094021666666666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00201155694325765, AUC: 0.7960963333333333\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['triplet_loss_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.05, 0.5103481666666667, 0.006708828846331946, 0.7905352250000001, 0.00016310292161395815, 0.7916025583333333, 9.315403885840298e-05, 0.78246575, 0.0003232075641888893, 1, True], ['triplet_loss_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.05, 0.5086180583333333, 0.009361293969013956, 0.792780225, 0.00015587035809451445, 0.7890225916666666, 0.00011011485702006879, 0.7900179916666668, 0.00011486821998256862, 5, True], ['triplet_loss_capped_smote', 3, (0, 3, 1), (200, 20, 1), 0.05, 0.4975422666666667, 0.0039616862222150015, 0.7865963333333332, 0.00029654184769027756, 0.7902651416666666, 0.00017477588351118005, 0.7884380916666667, 9.41330314631246e-05, 10, True]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class triplet loss capped smote\n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2]\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "loss_caps = [1, 5, 10] \n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = loss_cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                _, _ = train.train_triplet_capped_loss(epoch, train_loader_tripletloss_smote, network, optimizer, verbose=False, cap_calc=loss_fns.TripletLoss,loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"triplet_loss_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "589562a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd4ae08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251272b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
