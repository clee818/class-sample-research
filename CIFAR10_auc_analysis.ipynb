{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "783b8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "242f3056",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (class_sampling.py, line 223)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:3505\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 2\u001b[0;36m\n\u001b[0;31m    import class_sampling\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Downloads/class-sample-research/class_sampling.py:223\u001b[0;36m\u001b[0m\n\u001b[0;31m    pos_images = self.images[self.labels==anchor_label && self.smote_labels==False]\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb51a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "n_epochs = 50\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "NUM_CLASSES_REDUCED = 2\n",
    "nums = (0, 1)\n",
    "ratio = (10, 1)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba6047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name, num_classes, classes used, ratio, learning rate, mean 10, variance 10, mean 20, variance 20, ... 50\n",
    "\n",
    "# mean, variance every 10 epochs - average of 10 models \n",
    "# name, num_classes, classes used, ratio, learning rate, mean 10, variance 10, mean 20, variance 20, ... 50\n",
    "# name: normal/ratio/oversampled/undersampled/weighted\n",
    "\n",
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "            \"mean_40\", \"variance_40\",\n",
    "            \"mean_50\", \"variance_50\",\n",
    "            \"cap\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d80dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor() ]))  \n",
    "\n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()]))\n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "    \n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums)\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED)\n",
    "triplet_train_CIFAR10 = class_sampling.ForTripletLoss(smote_train_CIFAR10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbde3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ratio_train_CIFAR10.labels \n",
    "\n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "print(class_count)\n",
    "\n",
    "weight = 1. / class_count\n",
    "\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= class_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a65e877",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.999 \n",
    "\n",
    "exp = np.empty_like(targets)\n",
    "for i, count in enumerate(class_count):\n",
    "    exp[targets==i] = count\n",
    "effective_weights = (1 - beta) / ( 1 - (beta ** torch.from_numpy(exp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25bab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(effective_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss = DataLoader(triplet_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f8c2d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(test_loader_reduced.dataset.images.shape) # tupe instead of torch.Size() like the others "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "943f017c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(example_data[i]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mint())\n\u001b[0;32m----> 8\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGround Truth: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[43mexample_targets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m))\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mxticks([])\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39myticks([])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = enumerate(train_loader_smote) # enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i].reshape(32, 32, 3).int())\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e241471e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.024605756759643556, AUC: 0.6469975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014931785106658936, AUC: 0.8215375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.051244140625, AUC: 0.75086\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12066669464111328, AUC: 0.5902445000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06057313537597656, AUC: 0.7290795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016814103603363037, AUC: 0.8081860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020942140579223632, AUC: 0.459742\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.13868421936035155, AUC: 0.504016\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012890518665313721, AUC: 0.8556889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04611336135864258, AUC: 0.6312180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020766493797302246, AUC: 0.858468\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021261009216308593, AUC: 0.8546545000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04852569198608398, AUC: 0.59049\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05658713340759278, AUC: 0.7340209999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013382947444915772, AUC: 0.8688965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01964276695251465, AUC: 0.779631\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 17\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     19\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:15\u001b[0m, in \u001b[0;36mtrain_sigmoid\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args, smote)\u001b[0m\n\u001b[1;32m     12\u001b[0m loss_fn\u001b[38;5;241m=\u001b[39mloss_fn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_fn_args)\n\u001b[1;32m     14\u001b[0m network\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m     output \u001b[38;5;241m=\u001b[39m network(data)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/class_sampling.py:28\u001b[0m, in \u001b[0;36mReduce.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index): \n\u001b[1;32m     27\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[index]\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnums\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     29\u001b[0m         label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SIGMOID 2 CLASS normal AUC saving  \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [2e-5, 3e-5, 5e-5, 7e-5, 9e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa634095",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(learning_rate_aucs.shape)\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "print(auc_mean.shape)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "print(auc_variance.shape)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n",
    "\n",
    "print(rows)\n",
    "\n",
    "# pd.DataFrame(rows, columns = col_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9e1e7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SIGMOID 2 CLASS ratio\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836305f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SIGMOID 2 CLASS oversampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8753923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIGMOID 2 CLASS undersampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21ae5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIGMOID 2 CLASS over+undersampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_sampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"both_sampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863fbdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIGMOID 2 CLASS weighted \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['pos_weight'] = torch.tensor([weight[1]])\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c1a77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2 CLASS Focal Loss \n",
    "# no weights (yet)\n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 1e-4, 5e-4, 1e-5, 5e-5, 1e-6, 5e-7]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['reduction'] = 'mean'\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SigmoidFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e93f7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.029068840980529784, AUC: 0.6637670000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 7.40823779296875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5127647399902344, AUC: 0.505503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.67704736328125, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11136227798461915, AUC: 0.6836469999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.840927978515625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04511044883728027, AUC: 0.4715375000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6244243774414062, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 10.89170458984375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1552860107421875, AUC: 0.6288684999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.21777433776855468, AUC: 0.5584169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7307884216308593, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.051012250900268556, AUC: 0.485054\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6317517700195312, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6158645629882813, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.6241728515625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.29889247131347657, AUC: 0.5330250000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.22756174468994142, AUC: 0.580836\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014183250427246094, AUC: 0.5658275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3671016845703126, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.835934326171875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5530687866210937, AUC: 0.49999949999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.14467527770996094, AUC: 0.626852\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8766797180175782, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03294654655456543, AUC: 0.699768\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 8.26989013671875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6926331176757812, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 8.60008740234375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.23150346374511718, AUC: 0.570564\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7629148559570313, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037239631652832034, AUC: 0.4314665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.08380029296875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.23565668487548827, AUC: 0.535421\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4310329895019531, AUC: 0.5149975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.6823560791015626, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.17602841186523438, AUC: 0.6111179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04455807304382324, AUC: 0.653585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7080382690429687, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.32638525390625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.044556396484375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1634937744140625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.4505847778320313, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05705178451538086, AUC: 0.4819724999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2507387390136719, AUC: 0.5420750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08383403015136719, AUC: 0.7163825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.31526837158203125, AUC: 0.5179914999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.21568181610107423, AUC: 0.5878694999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5081837158203124, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021881378173828125, AUC: 0.425256\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 15.8883916015625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.36298307800292967, AUC: 0.5159914999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7297033996582031, AUC: 0.49999949999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5309517669677735, AUC: 0.501499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.25771785736083985, AUC: 0.542991\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023244968414306642, AUC: 0.5695129999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 8.65422802734375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.46364114379882815, AUC: 0.509496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.75985205078125, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2729025421142578, AUC: 0.5449955000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6669471740722657, AUC: 0.5005000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023708465576171876, AUC: 0.4792465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1861210327148437, AUC: 0.514992\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.5497086181640625, AUC: 0.5130054999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.4842750854492188, AUC: 0.5305395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9969316101074219, AUC: 0.5639715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1799517822265626, AUC: 0.503499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030244620323181152, AUC: 0.622506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2768734741210936, AUC: 0.528023\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3030728759765626, AUC: 0.5064985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 12.76247265625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4705571899414063, AUC: 0.653333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8656889038085938, AUC: 0.600461\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014232007026672364, AUC: 0.5252275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.93646240234375, AUC: 0.502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1360843505859375, AUC: 0.55997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.330618896484375, AUC: 0.5019994999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3521423950195313, AUC: 0.533504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 7.44705712890625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018488920211791993, AUC: 0.45004999999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 9.451263671875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2754279174804688, AUC: 0.5110025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 65.476384765625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 7.83490380859375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8296143798828125, AUC: 0.5930765000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06718689346313476, AUC: 0.4950045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 12.27019287109375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.8590966796875, AUC: 0.5159824999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3092099609375, AUC: 0.5380054999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 26.0144794921875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.474029541015625, AUC: 0.5500105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03564090538024902, AUC: 0.44011500000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.425338134765625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3491432495117188, AUC: 0.523496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5467410278320313, AUC: 0.512\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7456589965820313, AUC: 0.5194925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6692831420898437, AUC: 0.631085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029167010307312013, AUC: 0.6661250000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.1048450927734375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 8.96268408203125, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 7.012041259765625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.47164541625976564, AUC: 0.6648185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0834772338867187, AUC: 0.5034989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011895322799682617, AUC: 0.501235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 14.19181494140625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1349903564453125, AUC: 0.5050024999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.23386669921875, AUC: 0.5010014999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2699061889648438, AUC: 0.5380155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.3418662109375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012370940685272216, AUC: 0.497099\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 11.3038603515625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1110340576171875, AUC: 0.5315275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5396260070800781, AUC: 0.6441784999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8162196044921874, AUC: 0.57157\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9914517517089844, AUC: 0.570458\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04257650375366211, AUC: 0.6364299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 33.915423828125, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5594153442382812, AUC: 0.6204689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 6.9320029296875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 6.97012841796875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6636133422851562, AUC: 0.507496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06253123474121093, AUC: 0.4969985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013069580078125, AUC: 0.806475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.045728286743164065, AUC: 0.6980830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024334691047668458, AUC: 0.8015329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05328927421569824, AUC: 0.6603014999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02997529697418213, AUC: 0.7785314999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02852574348449707, AUC: 0.5758955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01227988338470459, AUC: 0.770404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02341863536834717, AUC: 0.7958529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03702316665649414, AUC: 0.7673304999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.13597105407714843, AUC: 0.502995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05044404602050781, AUC: 0.6703129999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.032517844200134274, AUC: 0.7116395000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037573554992675784, AUC: 0.74861\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0649498119354248, AUC: 0.585882\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10367212295532227, AUC: 0.50549\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015233963966369628, AUC: 0.8029640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02108296012878418, AUC: 0.8065770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03047437858581543, AUC: 0.3955679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1792476501464844, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03789205169677735, AUC: 0.7459100000000001\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.029435909271240235, AUC: 0.774112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.043125858306884764, AUC: 0.7159804999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.32170054626464845, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01369522523880005, AUC: 0.516219\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12513492584228517, AUC: 0.502995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.058791873931884765, AUC: 0.6153120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.13666724395751953, AUC: 0.5024949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.059439298629760745, AUC: 0.608271\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021141726493835448, AUC: 0.803672\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03345775032043457, AUC: 0.45481399999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02317213726043701, AUC: 0.7973180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010115370273590088, AUC: 0.789774\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5971633605957031, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06598271942138671, AUC: 0.5879685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021370010375976564, AUC: 0.803662\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02171405029296875, AUC: 0.4277695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8716934814453124, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.031814878463745117, AUC: 0.784575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.059524940490722655, AUC: 0.6449545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02659313678741455, AUC: 0.792167\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.18326407623291016, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012795899868011475, AUC: 0.6302495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2935269775390625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.059789186477661134, AUC: 0.6299480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028318270683288575, AUC: 0.7962215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.13109254455566408, AUC: 0.5039954999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018757739067077638, AUC: 0.7855725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04424954223632813, AUC: 0.45935050000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03177303504943848, AUC: 0.7699885000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05475870513916015, AUC: 0.6495084999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02387538433074951, AUC: 0.7886839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11229401779174805, AUC: 0.503498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024900227546691894, AUC: 0.7818875000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06370569038391113, AUC: 0.488517\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09176447677612305, AUC: 0.5179505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09444039916992188, AUC: 0.514984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.16351358032226562, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.033499616622924805, AUC: 0.7812749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021050064086914063, AUC: 0.7986139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04566512298583984, AUC: 0.6602320000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022629907608032226, AUC: 0.7671935000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03387494850158691, AUC: 0.7645229999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021666088104248048, AUC: 0.7748255000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019359353065490723, AUC: 0.7890695000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01465141487121582, AUC: 0.7805175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.035382822036743165, AUC: 0.42008049999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020867056846618652, AUC: 0.7455769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020987175941467286, AUC: 0.7642829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03368626022338867, AUC: 0.755595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014256035804748535, AUC: 0.7612475000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018050233840942384, AUC: 0.7828060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.038242820739746095, AUC: 0.417755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013154576778411866, AUC: 0.6968559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021867493629455566, AUC: 0.746386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02142896366119385, AUC: 0.769296\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030088013648986816, AUC: 0.770766\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026858837127685546, AUC: 0.77849\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04998508071899414, AUC: 0.48951300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.035694150924682616, AUC: 0.7551125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03812778854370117, AUC: 0.741732\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025139434814453125, AUC: 0.7893365000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04647598075866699, AUC: 0.6920055000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026918153762817382, AUC: 0.7919365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021142802238464355, AUC: 0.43594600000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037452198028564455, AUC: 0.726645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01845305061340332, AUC: 0.7505999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0436752872467041, AUC: 0.688782\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021592983245849608, AUC: 0.766801\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015693858146667482, AUC: 0.7866755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08102265548706054, AUC: 0.555934\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03371454811096192, AUC: 0.7362390000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015308978080749512, AUC: 0.7772350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.036178647994995115, AUC: 0.7496354999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037542150497436526, AUC: 0.738165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017550529479980467, AUC: 0.7809955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03659624862670898, AUC: 0.7115265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07324045944213867, AUC: 0.5335629999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04494003105163574, AUC: 0.693006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011330324649810791, AUC: 0.7610049999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01781573486328125, AUC: 0.7741\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028444123268127442, AUC: 0.788974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05396267890930176, AUC: 0.47698349999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021234413146972657, AUC: 0.7203875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02448776626586914, AUC: 0.739504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03163255023956299, AUC: 0.7638425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018987480163574218, AUC: 0.763025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016019587993621826, AUC: 0.7630425000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09086427688598633, AUC: 0.4999985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030430325508117674, AUC: 0.7635689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03314482021331787, AUC: 0.775753\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01623390769958496, AUC: 0.7814039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.17052671813964843, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026249880790710448, AUC: 0.7865340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.057639955520629886, AUC: 0.4829995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019724475860595704, AUC: 0.7583285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029424031257629393, AUC: 0.764791\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06511692428588867, AUC: 0.57164\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.046658580780029296, AUC: 0.676185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015715662479400636, AUC: 0.776805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 class weighted focal loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 5e-5, 1e-6, 5e-7]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['reduction'] = 'mean'\n",
    "loss_fn_args['weight']=torch.tensor([weight[1]])\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SigmoidFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted_focal_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5], None]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d82bd4c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.02157968044281006, AUC: 0.6867034999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04202408409118653, AUC: 0.5679595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01122785186767578, AUC: 0.7952355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009371079921722413, AUC: 0.8330525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01840770721435547, AUC: 0.6837489999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.051747398376464845, AUC: 0.6998095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023658714294433593, AUC: 0.39837999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.036989910125732425, AUC: 0.5953179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019817763328552247, AUC: 0.8361535000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06258223724365235, AUC: 0.6625259999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023065343856811523, AUC: 0.8184055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1660221939086914, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04466410827636719, AUC: 0.5848144999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019423234939575196, AUC: 0.8312700000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015033366680145264, AUC: 0.7412275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07601138687133789, AUC: 0.626493\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03341779327392578, AUC: 0.596404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.031233074188232422, AUC: 0.8006485000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04384331321716309, AUC: 0.4436590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024861087799072267, AUC: 0.6537995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.046678565979003904, AUC: 0.7248254999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024052400588989258, AUC: 0.6468825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028643247604370117, AUC: 0.6151334999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01757850933074951, AUC: 0.6956695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04979433631896973, AUC: 0.46103200000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06888628387451172, AUC: 0.511482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06603289413452149, AUC: 0.5159940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017977940559387208, AUC: 0.8436810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026384526252746583, AUC: 0.8148645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02604616641998291, AUC: 0.8142515000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011570464134216309, AUC: 0.651253\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0302976016998291, AUC: 0.793967\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03339131450653076, AUC: 0.779576\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08760675811767578, AUC: 0.6006465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013762806415557861, AUC: 0.7453695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04165582847595215, AUC: 0.7497680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03422253608703613, AUC: 0.6372905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011200609683990479, AUC: 0.786203\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013259670734405517, AUC: 0.7621070000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07646782684326171, AUC: 0.6232249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08534825134277343, AUC: 0.50501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013772080898284913, AUC: 0.8494510000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04881747436523438, AUC: 0.4903955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07178605270385742, AUC: 0.5119545000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010169760227203369, AUC: 0.8342529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.057615945816040036, AUC: 0.5229745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021845489501953124, AUC: 0.6660035000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025428202629089354, AUC: 0.6450505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029022104263305665, AUC: 0.636784\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012839582443237305, AUC: 0.8401609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.035134546279907225, AUC: 0.7806424999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07618846893310546, AUC: 0.6249600000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02815249443054199, AUC: 0.810743\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013517923831939697, AUC: 0.7479985000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0384180850982666, AUC: 0.49347649999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020593582153320314, AUC: 0.681716\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013721189498901366, AUC: 0.7572204999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010134970664978028, AUC: 0.8449795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011493403434753418, AUC: 0.7720244999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.032214184761047364, AUC: 0.791414\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01917431926727295, AUC: 0.4311704999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0049877619743347165, AUC: 0.6959265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034903968572616577, AUC: 0.7380015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032030287981033327, AUC: 0.782322\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002368504047393799, AUC: 0.7648355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034267951250076295, AUC: 0.7303225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018824248313903807, AUC: 0.6324909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005648926973342895, AUC: 0.6614215000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003485495567321777, AUC: 0.7123385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027559136152267454, AUC: 0.7661294999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034918612241744995, AUC: 0.7872545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004706173181533813, AUC: 0.7962935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017050434112548828, AUC: 0.49219050000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004432138681411743, AUC: 0.712433\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027856410741806032, AUC: 0.7559829999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026375510692596437, AUC: 0.7826645\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 18\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCappedBCELoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     20\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:15\u001b[0m, in \u001b[0;36mtrain_sigmoid\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args, smote)\u001b[0m\n\u001b[1;32m     12\u001b[0m loss_fn\u001b[38;5;241m=\u001b[39mloss_fn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_fn_args)\n\u001b[1;32m     14\u001b[0m network\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m     output \u001b[38;5;241m=\u001b[39m network(data)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:624\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/sampler.py:254\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m    253\u001b[0m idx_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 254\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler:\n\u001b[1;32m    255\u001b[0m     batch[idx_in_batch] \u001b[38;5;241m=\u001b[39m idx\n\u001b[1;32m    256\u001b[0m     idx_in_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2 Class SMOTE \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, smote=True)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5], None]\n",
    "    rows.append(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c104725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.024605756759643556, AUC: 0.6469975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025179288864135743, AUC: 0.8371780000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02434657955169678, AUC: 0.835716\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02363310146331787, AUC: 0.8414855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10839350509643554, AUC: 0.659127\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09762242507934571, AUC: 0.6706274999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020942140579223632, AUC: 0.459742\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1355465545654297, AUC: 0.5949840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02055882549285889, AUC: 0.8423615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09752316284179688, AUC: 0.677659\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.21606256866455079, AUC: 0.534526\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07548645401000977, AUC: 0.7280519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04852569198608398, AUC: 0.59049\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07702235794067383, AUC: 0.70361\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.056303215026855466, AUC: 0.757323\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024388296127319336, AUC: 0.8475915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1351273193359375, AUC: 0.6125345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02426826286315918, AUC: 0.8391955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04567551612854004, AUC: 0.42372100000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021844879150390625, AUC: 0.8452089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.15755242919921875, AUC: 0.594211\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02564561080932617, AUC: 0.7665905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.16403722381591798, AUC: 0.586513\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.15925967407226563, AUC: 0.50303\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04762502288818359, AUC: 0.46799950000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07201957702636719, AUC: 0.7144025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.20579266357421874, AUC: 0.541999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021843817710876463, AUC: 0.80515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04780360984802246, AUC: 0.7804405000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06569469833374024, AUC: 0.7446665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011635943412780762, AUC: 0.6585844999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025336673736572266, AUC: 0.8463850000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08384462356567383, AUC: 0.701065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03053032398223877, AUC: 0.7387155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04437282562255859, AUC: 0.6659534999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02383935832977295, AUC: 0.7875655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.033952896118164065, AUC: 0.6635895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0714105453491211, AUC: 0.710832\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11627243423461914, AUC: 0.638752\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07767597961425782, AUC: 0.7131179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07439785766601563, AUC: 0.7270249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.14591839599609374, AUC: 0.6040005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05262186050415039, AUC: 0.481568\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5353020324707031, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07195192337036133, AUC: 0.712343\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030319454193115235, AUC: 0.8304864999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10676837539672851, AUC: 0.660942\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03905027389526367, AUC: 0.8087685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02853425407409668, AUC: 0.668543\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1438208541870117, AUC: 0.597591\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02215341567993164, AUC: 0.8342040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08355887985229492, AUC: 0.697349\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03838635063171387, AUC: 0.6938234999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.14161627960205078, AUC: 0.5964740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03971935272216797, AUC: 0.48994299999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.21324475860595704, AUC: 0.538004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08977023315429687, AUC: 0.6869124999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06815893173217774, AUC: 0.7321685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.13515896606445313, AUC: 0.6135320000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027368723869323732, AUC: 0.8383\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02168908214569092, AUC: 0.40815900000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7118953552246093, AUC: 0.5055360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.16413160705566407, AUC: 0.768388\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8713588562011719, AUC: 0.533516\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.4866256103515627, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.36950436401367187, AUC: 0.662127\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019410190582275392, AUC: 0.6326890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.507010498046875, AUC: 0.5985325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2604044189453125, AUC: 0.7024805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6360225830078124, AUC: 0.5724699999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.26627224731445315, AUC: 0.7165484999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4498340148925781, AUC: 0.6375120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021570839881896973, AUC: 0.4500539999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.21782393646240233, AUC: 0.663478\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.35367152404785157, AUC: 0.67454\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.13910415649414062, AUC: 0.7893785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3046870269775391, AUC: 0.683879\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1294459228515625, AUC: 0.7956529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026163442611694336, AUC: 0.598169\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11998067092895508, AUC: 0.8076009999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.139654052734375, AUC: 0.792771\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.33744512939453125, AUC: 0.6832635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1594681930541992, AUC: 0.7754969999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7848233947753906, AUC: 0.5530235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013832940101623534, AUC: 0.5365035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3122037353515625, AUC: 0.6771575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9247701721191406, AUC: 0.5380275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8439567565917969, AUC: 0.543031\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.24412014770507812, AUC: 0.719065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1482080078125, AUC: 0.502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014129616737365723, AUC: 0.58012\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5122101287841797, AUC: 0.594967\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.2206864013671876, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.20906797790527343, AUC: 0.7500195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12957549285888673, AUC: 0.782365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.38416929626464846, AUC: 0.6579505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0784141502380371, AUC: 0.4989995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8751229248046875, AUC: 0.541489\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3939179382324219, AUC: 0.648374\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11242463684082031, AUC: 0.7988075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.13731287002563478, AUC: 0.7910470000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.019956024169922, AUC: 0.5285245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03155163383483887, AUC: 0.41963549999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09464305114746094, AUC: 0.8124355000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.45861904907226564, AUC: 0.6151564999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2604384155273438, AUC: 0.5119984999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3402468872070312, AUC: 0.5030075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.17398699951171875, AUC: 0.769775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03000270175933838, AUC: 0.6101475000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.47744068908691406, AUC: 0.61776\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.24420303344726563, AUC: 0.611242\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11918070602416993, AUC: 0.808749\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1540202331542969, AUC: 0.785581\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.33969056701660155, AUC: 0.690059\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012502269268035889, AUC: 0.39657299999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10123884582519531, AUC: 0.7984405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.4405374145507812, AUC: 0.5035040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12729612731933593, AUC: 0.8136150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6572356872558593, AUC: 0.5774625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.20340540313720704, AUC: 0.7524865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01677943420410156, AUC: 0.4385135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.32286299133300783, AUC: 0.740126\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8304380493164063, AUC: 0.626211\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 7.539842529296875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6540796813964844, AUC: 0.679933\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2846470336914062, AUC: 0.5784865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03469704437255859, AUC: 0.6751760000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6005957336425781, AUC: 0.67605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6877423706054688, AUC: 0.536996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8135179748535156, AUC: 0.6484475000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.5810975341796873, AUC: 0.5114985000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.23670665740966798, AUC: 0.7914699999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07548549270629883, AUC: 0.49999600000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9473751831054688, AUC: 0.6161675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3511250610351562, AUC: 0.766025\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 5.902949951171875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.317109619140625, AUC: 0.711518\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.88033056640625, AUC: 0.5329925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027087728500366212, AUC: 0.4486684999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.20748065948486327, AUC: 0.8004370000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 5.94118408203125, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2501363754272461, AUC: 0.7552719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7738500366210936, AUC: 0.5050095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6469757690429687, AUC: 0.546493\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02875083065032959, AUC: 0.6469410000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.8680609130859374, AUC: 0.5050044999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.240472900390625, AUC: 0.564488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.546728515625, AUC: 0.6959295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.96422998046875, AUC: 0.506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3515675048828124, AUC: 0.564991\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027386792182922365, AUC: 0.402786\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.22187775421142578, AUC: 0.8089434999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 7.72029150390625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.03884619140625, AUC: 0.5224955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.22419603729248047, AUC: 0.7838130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6470921936035157, AUC: 0.668246\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010502933979034424, AUC: 0.5167915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.4802308349609374, AUC: 0.503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7123376770019532, AUC: 0.6457135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2548774566650391, AUC: 0.7996595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7591460876464844, AUC: 0.6561174999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5658612365722656, AUC: 0.7075185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.035588520050048825, AUC: 0.4444045000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.911457275390625, AUC: 0.5049994999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9061611328125, AUC: 0.6104775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 6.758236328125, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8027864379882812, AUC: 0.6472384999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7568324584960937, AUC: 0.535994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016968350410461425, AUC: 0.48580999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6499987182617187, AUC: 0.6514975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.31323606872558596, AUC: 0.7741020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2337339630126953, AUC: 0.804635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4132541198730469, AUC: 0.7344324999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0742858276367186, AUC: 0.5924775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014771098613739014, AUC: 0.6540375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5234847412109375, AUC: 0.5889165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.24887393188476561, AUC: 0.8120405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.32650563049316406, AUC: 0.7679935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.28965292358398437, AUC: 0.787155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5341734619140625, AUC: 0.7174794999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04766560173034668, AUC: 0.491496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12179278564453125, AUC: 0.629709\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03179255962371826, AUC: 0.833862\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07888339996337891, AUC: 0.7130585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025963253021240234, AUC: 0.8363229999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.14133121490478515, AUC: 0.613722\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0652132625579834, AUC: 0.496502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.24275657653808594, AUC: 0.527494\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025513718605041504, AUC: 0.8422375000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02313667583465576, AUC: 0.8161569999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02332502746582031, AUC: 0.8130105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026839704513549806, AUC: 0.833851\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03302265167236328, AUC: 0.6779765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7806407775878906, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05085219192504883, AUC: 0.7723029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030515407562255858, AUC: 0.8349335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03906522178649902, AUC: 0.6995804999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07689196014404297, AUC: 0.563876\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02510342025756836, AUC: 0.437044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06754397201538086, AUC: 0.559377\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1361353759765625, AUC: 0.516514\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06033833503723145, AUC: 0.753186\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04730165863037109, AUC: 0.789325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029788878440856932, AUC: 0.744283\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.035276716232299804, AUC: 0.469998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.41685516357421876, AUC: 0.5020005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.22369606018066407, AUC: 0.5034890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.052074607849121095, AUC: 0.77213\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04571922874450684, AUC: 0.799995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03362308883666992, AUC: 0.8258709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.056951194763183596, AUC: 0.489482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021256649017333984, AUC: 0.8503500000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.15024951934814454, AUC: 0.5070515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.048031063079833985, AUC: 0.7866145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09270640182495117, AUC: 0.69157\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024611557006835937, AUC: 0.8299225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03241660690307617, AUC: 0.4092784999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08692391967773437, AUC: 0.685621\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.20444451141357423, AUC: 0.545518\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02258029556274414, AUC: 0.829826\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030508114814758302, AUC: 0.834625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.035315021514892575, AUC: 0.8176709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07690467071533204, AUC: 0.5481245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10806746292114258, AUC: 0.53749\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.28184548950195315, AUC: 0.5170005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03241751766204834, AUC: 0.8276085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.038991371154785154, AUC: 0.8130459999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.063033052444458, AUC: 0.7495315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04281192970275879, AUC: 0.6854589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05607395362854004, AUC: 0.7660775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1398150863647461, AUC: 0.60632\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04507774925231934, AUC: 0.7991220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06661328887939454, AUC: 0.7453764999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.14808567810058593, AUC: 0.6075155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037264318466186525, AUC: 0.46985200000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8560708923339844, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3593461151123047, AUC: 0.5095089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3415921630859375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.16886688232421876, AUC: 0.578013\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0856920166015625, AUC: 0.7076659999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08677769088745117, AUC: 0.4940065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.524771484375, AUC: 0.502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.4359254760742188, AUC: 0.509007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.22437976837158202, AUC: 0.7415579999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.86549853515625, AUC: 0.503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8165594482421875, AUC: 0.5604685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.060067832946777346, AUC: 0.4920295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11370589447021484, AUC: 0.812105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.8936149291992188, AUC: 0.5015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5725805969238281, AUC: 0.5979685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.17687178802490233, AUC: 0.765643\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8421575927734375, AUC: 0.5034905000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05857536888122559, AUC: 0.5912029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.21991211700439453, AUC: 0.726844\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7184452819824219, AUC: 0.5509775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12517907333374023, AUC: 0.799998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12812535095214844, AUC: 0.806449\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.4093076171875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.053520713806152344, AUC: 0.598757\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09150889587402344, AUC: 0.8195680000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.749743896484375, AUC: 0.5694720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5496188659667969, AUC: 0.6024579999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10036811065673829, AUC: 0.800808\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7252168579101562, AUC: 0.5549715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04632254409790039, AUC: 0.47904500000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09188681411743164, AUC: 0.8184775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.17493067169189452, AUC: 0.7349910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.16230355072021485, AUC: 0.7796204999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.45636672973632814, AUC: 0.6280505000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.25493095397949217, AUC: 0.7280185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06196899604797363, AUC: 0.5754319999999999\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.1286066246032715, AUC: 0.7922980000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.20144508361816407, AUC: 0.753472\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3547326507568359, AUC: 0.6704270000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.17192459869384766, AUC: 0.6935100000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2671983184814453, AUC: 0.7166830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03369696426391602, AUC: 0.4749055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9279878234863281, AUC: 0.5025000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.14519380187988282, AUC: 0.7908790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.23044691467285155, AUC: 0.729785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1999288101196289, AUC: 0.7627674999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.970044189453125, AUC: 0.536488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012355063438415527, AUC: 0.5283694999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4322799072265625, AUC: 0.6235390000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.5117054443359375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1270251159667969, AUC: 0.8010585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.27561334228515627, AUC: 0.7031959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5317534484863281, AUC: 0.62191\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06091798973083496, AUC: 0.472333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1454264678955078, AUC: 0.72858\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.29357073974609377, AUC: 0.6858485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.568765869140625, AUC: 0.5025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7559880065917969, AUC: 0.552532\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9428932800292968, AUC: 0.5320265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01493039083480835, AUC: 0.610955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.18540736389160156, AUC: 0.7526585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7727302551269531, AUC: 0.5060159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11379108810424805, AUC: 0.7783519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4267183990478516, AUC: 0.548373\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2190026550292969, AUC: 0.7534719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022432169914245605, AUC: 0.474771\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1471082763671876, AUC: 0.577488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.21682059478759766, AUC: 0.809588\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0461236572265626, AUC: 0.5269975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5534541625976562, AUC: 0.6900055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.730552734375, AUC: 0.502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.033266233444213866, AUC: 0.4479485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9593914184570314, AUC: 0.528997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.612742919921875, AUC: 0.5721674999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3108630828857422, AUC: 0.7695934999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.49952120971679687, AUC: 0.7213944999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0925827026367188, AUC: 0.5245215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014838393211364747, AUC: 0.5802109999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5649544677734375, AUC: 0.5479965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.371788330078125, AUC: 0.7471064999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3052445678710938, AUC: 0.5744805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7614789733886719, AUC: 0.5638825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 5.60783056640625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016110142707824706, AUC: 0.3819345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8130072326660156, AUC: 0.637212\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.6412208251953126, AUC: 0.5010025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6914915466308593, AUC: 0.6657049999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2928884887695313, AUC: 0.779858\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.38631808471679685, AUC: 0.6670115000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05945938301086426, AUC: 0.527283\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.91984375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.34815016174316404, AUC: 0.6831829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.602117431640625, AUC: 0.511499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.25826593780517576, AUC: 0.7998909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2814027099609375, AUC: 0.7855305000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019826248168945314, AUC: 0.5040629999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.41085708618164063, AUC: 0.6318245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7957232055664063, AUC: 0.529998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 7.77699560546875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.589827392578125, AUC: 0.5464835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.40407107543945314, AUC: 0.74606\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04158819770812988, AUC: 0.447053\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.855871337890625, AUC: 0.6218345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2312274627685547, AUC: 0.8041659999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7751330871582032, AUC: 0.646602\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5791262817382813, AUC: 0.6994569999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6503917236328125, AUC: 0.6672374999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01996380615234375, AUC: 0.5282315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.31718687438964843, AUC: 0.7620254999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7346712036132812, AUC: 0.6513515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.8516929931640624, AUC: 0.5355305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5023161163330078, AUC: 0.7139005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.23856129455566405, AUC: 0.7935574999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015119621753692627, AUC: 0.43264749999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.21567926025390624, AUC: 0.7585759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3199058837890625, AUC: 0.7596389999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5525273132324219, AUC: 0.6957359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4653106536865234, AUC: 0.718407\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.6981964111328125, AUC: 0.503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016432628631591797, AUC: 0.5491675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3096212158203125, AUC: 0.763663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0422048950195313, AUC: 0.6089685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.32952415466308593, AUC: 0.767576\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.27862828063964845, AUC: 0.7894300000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8241738891601562, AUC: 0.644713\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.061681797027587894, AUC: 0.501498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.16891622161865236, AUC: 0.50502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07572759246826172, AUC: 0.729264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1376971206665039, AUC: 0.632663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028448664665222168, AUC: 0.834448\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.039431276321411135, AUC: 0.81682\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028273516654968262, AUC: 0.4200259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0464138240814209, AUC: 0.7884890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.053531871795654294, AUC: 0.6454219999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.047731767654418944, AUC: 0.796112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04672444915771484, AUC: 0.7974745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03794847679138184, AUC: 0.702747\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013139777183532715, AUC: 0.5278945000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04011567497253418, AUC: 0.6713525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024282111167907714, AUC: 0.7927095000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11519441223144532, AUC: 0.6587535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10112374877929688, AUC: 0.6880915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02719546318054199, AUC: 0.786764\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028171226501464842, AUC: 0.589321\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019885467529296874, AUC: 0.8409705000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2386512451171875, AUC: 0.529994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09417508697509766, AUC: 0.6861659999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2581971740722656, AUC: 0.5345279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02435831546783447, AUC: 0.8296454999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0447327823638916, AUC: 0.447717\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03122106647491455, AUC: 0.7180205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028618261337280272, AUC: 0.83763\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08402849578857421, AUC: 0.711807\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3329420928955078, AUC: 0.511011\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.061671113967895506, AUC: 0.7484695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015912240028381347, AUC: 0.458171\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.14205084228515624, AUC: 0.509007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12029990005493164, AUC: 0.6289725000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11918328094482422, AUC: 0.6522245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08123017501831055, AUC: 0.5629070000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8020512084960938, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.040173543930053714, AUC: 0.6218115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2262888946533203, AUC: 0.5365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.053062129974365234, AUC: 0.7757219999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023890092849731447, AUC: 0.8173234999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03459514808654785, AUC: 0.8194119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02959327507019043, AUC: 0.769845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03015384769439697, AUC: 0.5652145000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1397670669555664, AUC: 0.62961\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.043082130432128905, AUC: 0.8004490000000001\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.17418785095214845, AUC: 0.5809785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025650479316711425, AUC: 0.8188964999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030268095016479493, AUC: 0.8380839999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04620059967041015, AUC: 0.6466655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04102304458618164, AUC: 0.8002800000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.15011007690429687, AUC: 0.602543\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.051661659240722656, AUC: 0.7813510000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0623509578704834, AUC: 0.768413\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07724285888671875, AUC: 0.7274444999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028779868125915528, AUC: 0.4497709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12466008377075195, AUC: 0.5160245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0765390396118164, AUC: 0.7234185000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05918923377990723, AUC: 0.7649835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.30781658935546874, AUC: 0.512494\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0477901382446289, AUC: 0.6517535000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0776613655090332, AUC: 0.5743699999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.22653282928466797, AUC: 0.7245050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1082965698242186, AUC: 0.521495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9479389038085938, AUC: 0.504009\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.14052549743652343, AUC: 0.7898985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1736219177246094, AUC: 0.7018605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019238964080810547, AUC: 0.6725130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3836293029785156, AUC: 0.6485474999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.19272383880615235, AUC: 0.7537529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.30819569396972657, AUC: 0.6973739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.18438612365722656, AUC: 0.7678289999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.5068006591796874, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037819238662719724, AUC: 0.653976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1478861999511719, AUC: 0.7748855000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7905582275390626, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11764218521118164, AUC: 0.8150935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3044246826171877, AUC: 0.5025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.578552001953125, AUC: 0.520466\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026900861740112306, AUC: 0.605938\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1863357391357422, AUC: 0.7455140000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3666446380615234, AUC: 0.6788834999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5103203735351562, AUC: 0.505505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4816048278808594, AUC: 0.633405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3748487091064453, AUC: 0.6633100000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06898881912231446, AUC: 0.4960075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1259915351867676, AUC: 0.8014965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1163682632446289, AUC: 0.8095085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.18743700408935546, AUC: 0.754312\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2456814575195312, AUC: 0.513\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.0578743896484375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021334126472473146, AUC: 0.6697805000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08962821960449219, AUC: 0.810983\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1175961303710937, AUC: 0.5199984999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.27909632873535156, AUC: 0.5910415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.31726100158691406, AUC: 0.6794449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.23472393798828126, AUC: 0.6390750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03674860000610351, AUC: 0.45491250000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.984388427734375, AUC: 0.501502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1469718017578123, AUC: 0.5015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.889529052734375, AUC: 0.5015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0822232666015625, AUC: 0.5035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3309053649902344, AUC: 0.6742604999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02513908100128174, AUC: 0.375421\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9252318725585937, AUC: 0.5015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6208449096679688, AUC: 0.584973\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7188799133300782, AUC: 0.5619774999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.16184851837158204, AUC: 0.7792135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11522916793823242, AUC: 0.7921784999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07500678253173829, AUC: 0.49002900000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.13281446838378907, AUC: 0.7842170000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.46021456909179687, AUC: 0.635688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.26872454833984377, AUC: 0.704798\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1441991424560547, AUC: 0.8028205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2519802780151367, AUC: 0.726781\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014500118732452393, AUC: 0.505349\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.013287353515625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11814860916137696, AUC: 0.7920365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1860421142578125, AUC: 0.7682919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1669052963256836, AUC: 0.7760835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.915853271484375, AUC: 0.5500415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012062180519104003, AUC: 0.553423\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.28491165161132814, AUC: 0.727159\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2758274536132812, AUC: 0.571985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.693962890625, AUC: 0.6680269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.4049366455078125, AUC: 0.5634864999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8995792846679688, AUC: 0.6321895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.047478702545166014, AUC: 0.491501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9144408264160157, AUC: 0.6125489999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5298472900390625, AUC: 0.6929765000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5943058166503906, AUC: 0.6810205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5606892700195313, AUC: 0.7094245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3700945129394531, AUC: 0.7529999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013193941593170166, AUC: 0.5095629999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7753860778808593, AUC: 0.6376989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4259226379394531, AUC: 0.6399885000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.931800537109375, AUC: 0.6089745000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1966277465820312, AUC: 0.5944864999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5275560913085937, AUC: 0.7086680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011519768238067628, AUC: 0.569655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0061216125488281, AUC: 0.5114755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 6.80871630859375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.4286658935546876, AUC: 0.5155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7071849670410156, AUC: 0.6753965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3498892364501953, AUC: 0.7620419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0696839370727539, AUC: 0.49749600000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6718251037597657, AUC: 0.669172\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.286220703125, AUC: 0.7927590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.11403662109375, AUC: 0.5055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.573477783203125, AUC: 0.6986209999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2686205520629883, AUC: 0.789472\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011481070518493652, AUC: 0.5658030000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9114639587402343, AUC: 0.607162\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7311967163085937, AUC: 0.658047\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.21313037872314453, AUC: 0.801722\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.442586669921875, AUC: 0.5654955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 7.171505126953125, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037826580047607423, AUC: 0.44227299999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1590983276367188, AUC: 0.5789889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.36026075744628905, AUC: 0.750185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0688609619140625, AUC: 0.594979\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2543186569213867, AUC: 0.7627805000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.26192349243164065, AUC: 0.751412\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.040367496490478515, AUC: 0.47796700000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.36619474792480466, AUC: 0.6608269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1105289306640624, AUC: 0.5884805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1253308715820312, AUC: 0.5809715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.33549305725097656, AUC: 0.7718409999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.23328031158447265, AUC: 0.786688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.054044065475463865, AUC: 0.4649435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.868732177734375, AUC: 0.529998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.22981356048583984, AUC: 0.7823330000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2188019714355469, AUC: 0.8023434999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6531478881835937, AUC: 0.6880525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8246365051269531, AUC: 0.6517259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012683309078216553, AUC: 0.6922759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2371470947265624, AUC: 0.5064995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4735169982910156, AUC: 0.717428\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2994371337890627, AUC: 0.5269959999999999\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.3535929260253906, AUC: 0.6831195000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.31580372619628905, AUC: 0.775344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 Class SMOTE with capped loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 5e-4, 1e-3]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_sigmoid(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args, smote=True)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "for i in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[i][0]\n",
    "    auc_variance = cap_aucs[i][1]\n",
    "    cap = caps[i]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3],\n",
    "                auc_mean[i][4], auc_variance[i][4],\n",
    "                auc_mean[i][5], auc_variance[i][5], cap]\n",
    "        rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b7ce280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.02157968044281006, AUC: 0.6867034999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013757909774780273, AUC: 0.7648969999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013857486248016358, AUC: 0.800988\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017318458557128907, AUC: 0.8124695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009805487632751466, AUC: 0.8018260000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014841811656951904, AUC: 0.8085429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023658714294433593, AUC: 0.39837999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026063546180725098, AUC: 0.7488279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01888744068145752, AUC: 0.765158\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019569753646850586, AUC: 0.7862880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008232975006103515, AUC: 0.7549595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011351889610290527, AUC: 0.7745679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04466410827636719, AUC: 0.5848144999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02735144519805908, AUC: 0.7608940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014998260974884034, AUC: 0.775344\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019471473693847656, AUC: 0.8011909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012362385272979736, AUC: 0.793639\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012634827136993407, AUC: 0.79663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04384331321716309, AUC: 0.4436590000000001\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 20\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     22\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:17\u001b[0m, in \u001b[0;36mtrain_sigmoid\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args, smote)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m smote:\n\u001b[1;32m     19\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mfloat(), (target[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat(), target[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/models.py:63\u001b[0m, in \u001b[0;36mSigmoidLogisticRegression.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     62\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 63\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2 class effective # of samples \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['pos_weight'] = effective_weights\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"effective_samples\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a2fba63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0008116806149482727, AUC: 0.6054329999999999\n",
      "\n",
      "Train Epoch: 0 [0/10000 (0%)]\tLoss: 7542.551270\n",
      "Train Epoch: 0 [640/10000 (6%)]\tLoss: 1.000539\n",
      "Train Epoch: 0 [1280/10000 (13%)]\tLoss: 0.998103\n",
      "Train Epoch: 0 [1920/10000 (19%)]\tLoss: 1.001201\n",
      "Train Epoch: 0 [2560/10000 (25%)]\tLoss: 1.004069\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 18\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid_with_triplet_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_tripletloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     20\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:108\u001b[0m, in \u001b[0;36mtrain_sigmoid_with_triplet_loss\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn_args)\u001b[0m\n\u001b[1;32m    105\u001b[0m loss_fn\u001b[38;5;241m=\u001b[39mloss_fns\u001b[38;5;241m.\u001b[39mTripletLoss(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_fn_args)\n\u001b[1;32m    107\u001b[0m network\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (anchor_data, pos_data, neg_data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m    109\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    110\u001b[0m     _, anchor_embeds \u001b[38;5;241m=\u001b[39m network(anchor_data\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2 class triplet loss w/ SMOTE --> CONVNET, change files \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_with_triplet_loss(epoch, train_loader_tripletloss, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote_triplet_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5d929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES_REDUCED = 3\n",
    "nums = (0, 3, 1)\n",
    "ratio = (200, 20, 1)\n",
    "\n",
    "\n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums)\n",
    "targets = ratio_train_CIFAR10.labels \n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED)\n",
    "\n",
    "\n",
    "weight = 1. / class_count\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= max(class_count)\n",
    "\n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49887493",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Softmax 3 class normal\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 3, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4386e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 3 class ratio\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e15c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Softmax 3 class oversampled\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f9856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 3 class undersampled\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f39e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 3 class both sampled\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_sampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"both_sampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e12075c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Softmax 3 class weighted\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['weight'] = torch.from_numpy(class_weights).float()\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba2dbda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Softmax 3 class focal loss \n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 5e-4, 1e-5, 5e-5, 1e-6, 5e-7]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['reduction'] = 'mean'\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SoftmaxFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b8a3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3 class SMOTE \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(3, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, smote=True)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "    \n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5], 'n/a']\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b20715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3 Class SMOTE with capped loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args, smote=True)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "for i in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[i][0]\n",
    "    auc_variance = cap_aucs[i][1]\n",
    "    cap = caps[i]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", 3, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3],\n",
    "                auc_mean[i][4], auc_variance[i][4],\n",
    "                auc_mean[i][5], auc_variance[i][5], cap]\n",
    "        rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86c3a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/auc_analysis_other_methods.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names)\n",
    "\n",
    "df = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15c14544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>num_classes</th>\n",
       "      <th>classes_used</th>\n",
       "      <th>ratio</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>mean_0</th>\n",
       "      <th>variance_0</th>\n",
       "      <th>mean_10</th>\n",
       "      <th>variance_10</th>\n",
       "      <th>mean_20</th>\n",
       "      <th>variance_20</th>\n",
       "      <th>mean_30</th>\n",
       "      <th>variance_30</th>\n",
       "      <th>mean_40</th>\n",
       "      <th>variance_40</th>\n",
       "      <th>mean_50</th>\n",
       "      <th>variance_50</th>\n",
       "      <th>cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>capped_smote</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.555118</td>\n",
       "      <td>0.008864</td>\n",
       "      <td>0.688820</td>\n",
       "      <td>0.014818</td>\n",
       "      <td>0.714489</td>\n",
       "      <td>0.009811</td>\n",
       "      <td>0.765031</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.653442</td>\n",
       "      <td>0.004531</td>\n",
       "      <td>0.712068</td>\n",
       "      <td>0.011892</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>capped_smote</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.513105</td>\n",
       "      <td>0.007350</td>\n",
       "      <td>0.661740</td>\n",
       "      <td>0.011216</td>\n",
       "      <td>0.635448</td>\n",
       "      <td>0.009486</td>\n",
       "      <td>0.680485</td>\n",
       "      <td>0.014542</td>\n",
       "      <td>0.683445</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>0.654911</td>\n",
       "      <td>0.009386</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>capped_smote</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.521312</td>\n",
       "      <td>0.009090</td>\n",
       "      <td>0.639514</td>\n",
       "      <td>0.012413</td>\n",
       "      <td>0.633605</td>\n",
       "      <td>0.011966</td>\n",
       "      <td>0.649443</td>\n",
       "      <td>0.015718</td>\n",
       "      <td>0.652272</td>\n",
       "      <td>0.010946</td>\n",
       "      <td>0.623615</td>\n",
       "      <td>0.007430</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>capped_smote</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.517521</td>\n",
       "      <td>0.007951</td>\n",
       "      <td>0.605812</td>\n",
       "      <td>0.013855</td>\n",
       "      <td>0.615380</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>0.763264</td>\n",
       "      <td>0.009034</td>\n",
       "      <td>0.760086</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>0.729391</td>\n",
       "      <td>0.009485</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>capped_smote</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.531704</td>\n",
       "      <td>0.002876</td>\n",
       "      <td>0.707857</td>\n",
       "      <td>0.013717</td>\n",
       "      <td>0.610216</td>\n",
       "      <td>0.012469</td>\n",
       "      <td>0.700373</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.676433</td>\n",
       "      <td>0.011296</td>\n",
       "      <td>0.600753</td>\n",
       "      <td>0.008559</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>capped_smote</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.487331</td>\n",
       "      <td>0.003343</td>\n",
       "      <td>0.632962</td>\n",
       "      <td>0.008845</td>\n",
       "      <td>0.666717</td>\n",
       "      <td>0.011334</td>\n",
       "      <td>0.619372</td>\n",
       "      <td>0.009671</td>\n",
       "      <td>0.702271</td>\n",
       "      <td>0.006749</td>\n",
       "      <td>0.633363</td>\n",
       "      <td>0.012781</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>capped_smote</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.522809</td>\n",
       "      <td>0.005756</td>\n",
       "      <td>0.651527</td>\n",
       "      <td>0.015544</td>\n",
       "      <td>0.706612</td>\n",
       "      <td>0.009075</td>\n",
       "      <td>0.708236</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.684768</td>\n",
       "      <td>0.017553</td>\n",
       "      <td>0.737157</td>\n",
       "      <td>0.009356</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>capped_smote</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.549830</td>\n",
       "      <td>0.009192</td>\n",
       "      <td>0.679315</td>\n",
       "      <td>0.015516</td>\n",
       "      <td>0.629834</td>\n",
       "      <td>0.013540</td>\n",
       "      <td>0.640390</td>\n",
       "      <td>0.013192</td>\n",
       "      <td>0.674770</td>\n",
       "      <td>0.014640</td>\n",
       "      <td>0.626847</td>\n",
       "      <td>0.009595</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>capped_smote</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.526490</td>\n",
       "      <td>0.004721</td>\n",
       "      <td>0.604153</td>\n",
       "      <td>0.004803</td>\n",
       "      <td>0.669418</td>\n",
       "      <td>0.008309</td>\n",
       "      <td>0.628603</td>\n",
       "      <td>0.010651</td>\n",
       "      <td>0.671270</td>\n",
       "      <td>0.004983</td>\n",
       "      <td>0.711054</td>\n",
       "      <td>0.007628</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name  num_classes classes_used    ratio  learning_rate    mean_0   \n",
       "0  capped_smote            2       (0, 1)  (10, 1)         0.0001  0.555118  \\\n",
       "1  capped_smote            2       (0, 1)  (10, 1)         0.0005  0.513105   \n",
       "2  capped_smote            2       (0, 1)  (10, 1)         0.0010  0.521312   \n",
       "3  capped_smote            2       (0, 1)  (10, 1)         0.0001  0.517521   \n",
       "4  capped_smote            2       (0, 1)  (10, 1)         0.0005  0.531704   \n",
       "5  capped_smote            2       (0, 1)  (10, 1)         0.0010  0.487331   \n",
       "6  capped_smote            2       (0, 1)  (10, 1)         0.0001  0.522809   \n",
       "7  capped_smote            2       (0, 1)  (10, 1)         0.0005  0.549830   \n",
       "8  capped_smote            2       (0, 1)  (10, 1)         0.0010  0.526490   \n",
       "\n",
       "   variance_0   mean_10  variance_10   mean_20  variance_20   mean_30   \n",
       "0    0.008864  0.688820     0.014818  0.714489     0.009811  0.765031  \\\n",
       "1    0.007350  0.661740     0.011216  0.635448     0.009486  0.680485   \n",
       "2    0.009090  0.639514     0.012413  0.633605     0.011966  0.649443   \n",
       "3    0.007951  0.605812     0.013855  0.615380     0.018370  0.763264   \n",
       "4    0.002876  0.707857     0.013717  0.610216     0.012469  0.700373   \n",
       "5    0.003343  0.632962     0.008845  0.666717     0.011334  0.619372   \n",
       "6    0.005756  0.651527     0.015544  0.706612     0.009075  0.708236   \n",
       "7    0.009192  0.679315     0.015516  0.629834     0.013540  0.640390   \n",
       "8    0.004721  0.604153     0.004803  0.669418     0.008309  0.628603   \n",
       "\n",
       "   variance_30   mean_40  variance_40   mean_50  variance_50  cap  \n",
       "0     0.003524  0.653442     0.004531  0.712068     0.011892    1  \n",
       "1     0.014542  0.683445     0.012008  0.654911     0.009386    1  \n",
       "2     0.015718  0.652272     0.010946  0.623615     0.007430    1  \n",
       "3     0.009034  0.760086     0.006114  0.729391     0.009485    5  \n",
       "4     0.009511  0.676433     0.011296  0.600753     0.008559    5  \n",
       "5     0.009671  0.702271     0.006749  0.633363     0.012781    5  \n",
       "6     0.005618  0.684768     0.017553  0.737157     0.009356   10  \n",
       "7     0.013192  0.674770     0.014640  0.626847     0.009595   10  \n",
       "8     0.010651  0.671270     0.004983  0.711054     0.007628   10  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02884ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('results/auc_analysis_other_methods.csv', index=False)\n",
    "# df.to_csv('results/auc_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f80feb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
