{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "783b8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "242f3056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdb51a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "n_epochs = 50\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "NUM_CLASSES_REDUCED = 2\n",
    "nums = (0, 1)\n",
    "ratio = (100, 1)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ba6047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name, num_classes, classes used, ratio, learning rate, mean 10, variance 10, mean 20, variance 20, ... 50\n",
    "\n",
    "# mean, variance every 10 epochs - average of 10 models \n",
    "# name, num_classes, classes used, ratio, learning rate, mean 10, variance 10, mean 20, variance 20, ... 50\n",
    "# name: normal/ratio/oversampled/undersampled/weighted\n",
    "\n",
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "            \"mean_40\", \"variance_40\",\n",
    "            \"mean_50\", \"variance_50\",\n",
    "            \"cap\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05d80dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor() ]))  \n",
    "\n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()]))\n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "    \n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums)\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dbde3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000   50]\n"
     ]
    }
   ],
   "source": [
    "targets = ratio_train_CIFAR10.labels \n",
    "\n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "print(class_count)\n",
    "\n",
    "weight = 1. / class_count\n",
    "\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= class_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a65e877",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.999 \n",
    "\n",
    "exp = np.empty_like(targets)\n",
    "for i, count in enumerate(class_count):\n",
    "    exp[targets==i] = count\n",
    "effective_weights = (1 - beta) / ( 1 - (beta ** torch.from_numpy(exp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c25bab27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0010, 0.0010, 0.0010,  ..., 0.0205, 0.0205, 0.0205])\n"
     ]
    }
   ],
   "source": [
    "print(effective_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8ca20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f8c2d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(test_loader_reduced.dataset.images.shape) # tupe instead of torch.Size() like the others "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943f017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(train_loader_smote) # enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i].reshape(32, 32, 3).int())\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ee92cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011436396837234497, AUC: 0.457495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005883850753307342, AUC: 0.8319540000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005516811013221741, AUC: 0.8421819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006083952784538269, AUC: 0.8229979999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007547683715820313, AUC: 0.716763\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005385654270648957, AUC: 0.863451\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009431315064430236, AUC: 0.46823899999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005839791893959045, AUC: 0.8426415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005741381347179413, AUC: 0.8439435000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000624245285987854, AUC: 0.8317945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006380014121532441, AUC: 0.828502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005466452538967132, AUC: 0.86643\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008816532790660858, AUC: 0.533277\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000598762571811676, AUC: 0.8466360000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005307139754295349, AUC: 0.88264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000682082325220108, AUC: 0.782151\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006003609895706177, AUC: 0.8469580000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004681074172258377, AUC: 0.8902039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013176203966140747, AUC: 0.622134\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931164264678955, AUC: 0.5019944999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930808126926422, AUC: 0.5000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693071037530899, AUC: 0.4990045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931046843528748, AUC: 0.499502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931604146957397, AUC: 0.4960125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00811676025390625, AUC: 0.384132\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005941659808158874, AUC: 0.8493525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006095007956027985, AUC: 0.843075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005208902657032013, AUC: 0.8731615000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006543502807617187, AUC: 0.8257585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007129051983356476, AUC: 0.8101889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00858149242401123, AUC: 0.640699\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005778900682926178, AUC: 0.8573890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005516550540924073, AUC: 0.85483\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005538095235824585, AUC: 0.8520030000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007228837609291077, AUC: 0.7377840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988179385662078, AUC: 0.7855670000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021530042886734008, AUC: 0.61821\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005566418170928955, AUC: 0.8662079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005456461906433106, AUC: 0.8685029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005584597289562225, AUC: 0.8545130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000519916296005249, AUC: 0.860048\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008367151916027069, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00150935697555542, AUC: 0.450461\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006117416918277741, AUC: 0.7922425000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006196804344654084, AUC: 0.786186\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005850068926811218, AUC: 0.8200765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007139301896095276, AUC: 0.7625169999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005188652873039245, AUC: 0.850921\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007176195085048675, AUC: 0.6283110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005750634670257568, AUC: 0.8405255000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005882067084312439, AUC: 0.8268409999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006036419868469239, AUC: 0.8295039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005292186737060547, AUC: 0.8660649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005613011121749878, AUC: 0.8534575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001766030192375183, AUC: 0.556632\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005584144592285156, AUC: 0.84721\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005392710566520691, AUC: 0.8610250000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005679791569709778, AUC: 0.8528519999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007390778064727783, AUC: 0.7616590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005363746881484985, AUC: 0.8789179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023898414373397826, AUC: 0.376991\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006406008899211883, AUC: 0.7653845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006220220625400544, AUC: 0.7937134999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006240826845169068, AUC: 0.7999375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006160204112529755, AUC: 0.8107385000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006136663258075714, AUC: 0.8109339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018302631974220277, AUC: 0.6587560000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006143629550933837, AUC: 0.83047\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006070818305015564, AUC: 0.842505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005955512821674346, AUC: 0.850957\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006013936400413513, AUC: 0.8480690000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005974387526512146, AUC: 0.8510104999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012365342378616332, AUC: 0.616908\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006389518976211548, AUC: 0.7839240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000618268758058548, AUC: 0.8201339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000616977721452713, AUC: 0.8205360000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006034552156925202, AUC: 0.835796\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005994971394538879, AUC: 0.839007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036650893688201904, AUC: 0.45205049999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006254703998565674, AUC: 0.8122019999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006200121939182281, AUC: 0.8194270000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006148791313171386, AUC: 0.8246574999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006103866696357727, AUC: 0.8336769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006066541075706482, AUC: 0.833571\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034440916776657103, AUC: 0.474306\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006534666419029236, AUC: 0.7836995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006277360618114472, AUC: 0.8301010000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006291105151176453, AUC: 0.8231120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006198726892471313, AUC: 0.841566\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006303223669528961, AUC: 0.8260255000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011995865106582643, AUC: 0.448773\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006115688681602478, AUC: 0.8329979999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005996610820293427, AUC: 0.8355510000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006038209795951843, AUC: 0.825926\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006004272699356079, AUC: 0.8311420000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000589173287153244, AUC: 0.8353970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002155637264251709, AUC: 0.561711\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006544679999351501, AUC: 0.727513\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006256233453750611, AUC: 0.8006680000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000623337060213089, AUC: 0.8041255000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006019582748413086, AUC: 0.8267415000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006123208105564118, AUC: 0.815537\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017152709364891053, AUC: 0.44340399999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006141621768474579, AUC: 0.8059360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006037168502807617, AUC: 0.830253\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005960087478160858, AUC: 0.8443205000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005997392237186432, AUC: 0.8380350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005813049972057343, AUC: 0.8541949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006253282546997071, AUC: 0.39935350000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006216481626033783, AUC: 0.830678\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006118220984935761, AUC: 0.8385255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005928091108798981, AUC: 0.857809\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005962712466716767, AUC: 0.8477005000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005834138393402099, AUC: 0.858955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016776196360588074, AUC: 0.328026\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006232531368732452, AUC: 0.7903374999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006071761548519135, AUC: 0.8226705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005910335779190063, AUC: 0.8412339999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006030176877975464, AUC: 0.8287789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000600478857755661, AUC: 0.830577\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# distance + capped loss\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 1e-4]\n",
    "\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args, smote=True)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e241471e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.024605756759643556, AUC: 0.6469975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014931785106658936, AUC: 0.8215375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.051244140625, AUC: 0.75086\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12066669464111328, AUC: 0.5902445000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06057313537597656, AUC: 0.7290795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016814103603363037, AUC: 0.8081860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020942140579223632, AUC: 0.459742\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.13868421936035155, AUC: 0.504016\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012890518665313721, AUC: 0.8556889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04611336135864258, AUC: 0.6312180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020766493797302246, AUC: 0.858468\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021261009216308593, AUC: 0.8546545000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04852569198608398, AUC: 0.59049\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05658713340759278, AUC: 0.7340209999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013382947444915772, AUC: 0.8688965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01964276695251465, AUC: 0.779631\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 17\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     19\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:15\u001b[0m, in \u001b[0;36mtrain_sigmoid\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args, smote)\u001b[0m\n\u001b[1;32m     12\u001b[0m loss_fn\u001b[38;5;241m=\u001b[39mloss_fn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_fn_args)\n\u001b[1;32m     14\u001b[0m network\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m     output \u001b[38;5;241m=\u001b[39m network(data)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/class_sampling.py:28\u001b[0m, in \u001b[0;36mReduce.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index): \n\u001b[1;32m     27\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[index]\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnums\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     29\u001b[0m         label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SIGMOID 2 CLASS normal AUC saving  \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [2e-5, 3e-5, 5e-5, 7e-5, 9e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa634095",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(learning_rate_aucs.shape)\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "print(auc_mean.shape)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "print(auc_variance.shape)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n",
    "\n",
    "print(rows)\n",
    "\n",
    "# pd.DataFrame(rows, columns = col_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9e1e7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SIGMOID 2 CLASS ratio\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836305f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SIGMOID 2 CLASS oversampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8753923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIGMOID 2 CLASS undersampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21ae5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIGMOID 2 CLASS over+undersampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_sampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"both_sampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863fbdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIGMOID 2 CLASS weighted \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['pos_weight'] = torch.tensor([weight[1]])\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c1a77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2 CLASS Focal Loss \n",
    "# no weights (yet)\n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 1e-4, 5e-4, 1e-5, 5e-5, 1e-6, 5e-7]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['reduction'] = 'mean'\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SigmoidFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e93f7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.029068840980529784, AUC: 0.6637670000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 7.40823779296875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5127647399902344, AUC: 0.505503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.67704736328125, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11136227798461915, AUC: 0.6836469999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.840927978515625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04511044883728027, AUC: 0.4715375000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6244243774414062, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 10.89170458984375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1552860107421875, AUC: 0.6288684999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.21777433776855468, AUC: 0.5584169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7307884216308593, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.051012250900268556, AUC: 0.485054\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6317517700195312, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6158645629882813, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.6241728515625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.29889247131347657, AUC: 0.5330250000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.22756174468994142, AUC: 0.580836\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014183250427246094, AUC: 0.5658275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3671016845703126, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.835934326171875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5530687866210937, AUC: 0.49999949999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.14467527770996094, AUC: 0.626852\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8766797180175782, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03294654655456543, AUC: 0.699768\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 8.26989013671875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6926331176757812, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 8.60008740234375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.23150346374511718, AUC: 0.570564\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7629148559570313, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037239631652832034, AUC: 0.4314665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.08380029296875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.23565668487548827, AUC: 0.535421\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4310329895019531, AUC: 0.5149975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.6823560791015626, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.17602841186523438, AUC: 0.6111179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04455807304382324, AUC: 0.653585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7080382690429687, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.32638525390625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.044556396484375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1634937744140625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.4505847778320313, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05705178451538086, AUC: 0.4819724999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2507387390136719, AUC: 0.5420750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08383403015136719, AUC: 0.7163825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.31526837158203125, AUC: 0.5179914999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.21568181610107423, AUC: 0.5878694999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5081837158203124, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021881378173828125, AUC: 0.425256\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 15.8883916015625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.36298307800292967, AUC: 0.5159914999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7297033996582031, AUC: 0.49999949999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5309517669677735, AUC: 0.501499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.25771785736083985, AUC: 0.542991\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023244968414306642, AUC: 0.5695129999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 8.65422802734375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.46364114379882815, AUC: 0.509496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.75985205078125, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2729025421142578, AUC: 0.5449955000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6669471740722657, AUC: 0.5005000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023708465576171876, AUC: 0.4792465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1861210327148437, AUC: 0.514992\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.5497086181640625, AUC: 0.5130054999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.4842750854492188, AUC: 0.5305395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9969316101074219, AUC: 0.5639715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1799517822265626, AUC: 0.503499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030244620323181152, AUC: 0.622506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2768734741210936, AUC: 0.528023\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3030728759765626, AUC: 0.5064985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 12.76247265625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4705571899414063, AUC: 0.653333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8656889038085938, AUC: 0.600461\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014232007026672364, AUC: 0.5252275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.93646240234375, AUC: 0.502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1360843505859375, AUC: 0.55997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.330618896484375, AUC: 0.5019994999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3521423950195313, AUC: 0.533504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 7.44705712890625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018488920211791993, AUC: 0.45004999999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 9.451263671875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2754279174804688, AUC: 0.5110025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 65.476384765625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 7.83490380859375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8296143798828125, AUC: 0.5930765000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06718689346313476, AUC: 0.4950045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 12.27019287109375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.8590966796875, AUC: 0.5159824999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3092099609375, AUC: 0.5380054999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 26.0144794921875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.474029541015625, AUC: 0.5500105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03564090538024902, AUC: 0.44011500000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.425338134765625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3491432495117188, AUC: 0.523496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5467410278320313, AUC: 0.512\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7456589965820313, AUC: 0.5194925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6692831420898437, AUC: 0.631085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029167010307312013, AUC: 0.6661250000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.1048450927734375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 8.96268408203125, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 7.012041259765625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.47164541625976564, AUC: 0.6648185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0834772338867187, AUC: 0.5034989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011895322799682617, AUC: 0.501235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 14.19181494140625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1349903564453125, AUC: 0.5050024999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.23386669921875, AUC: 0.5010014999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2699061889648438, AUC: 0.5380155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.3418662109375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012370940685272216, AUC: 0.497099\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 11.3038603515625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1110340576171875, AUC: 0.5315275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5396260070800781, AUC: 0.6441784999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8162196044921874, AUC: 0.57157\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9914517517089844, AUC: 0.570458\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04257650375366211, AUC: 0.6364299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 33.915423828125, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5594153442382812, AUC: 0.6204689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 6.9320029296875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 6.97012841796875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6636133422851562, AUC: 0.507496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06253123474121093, AUC: 0.4969985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013069580078125, AUC: 0.806475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.045728286743164065, AUC: 0.6980830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024334691047668458, AUC: 0.8015329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05328927421569824, AUC: 0.6603014999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02997529697418213, AUC: 0.7785314999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02852574348449707, AUC: 0.5758955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01227988338470459, AUC: 0.770404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02341863536834717, AUC: 0.7958529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03702316665649414, AUC: 0.7673304999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.13597105407714843, AUC: 0.502995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05044404602050781, AUC: 0.6703129999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.032517844200134274, AUC: 0.7116395000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037573554992675784, AUC: 0.74861\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0649498119354248, AUC: 0.585882\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10367212295532227, AUC: 0.50549\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015233963966369628, AUC: 0.8029640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02108296012878418, AUC: 0.8065770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03047437858581543, AUC: 0.3955679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1792476501464844, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03789205169677735, AUC: 0.7459100000000001\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.029435909271240235, AUC: 0.774112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.043125858306884764, AUC: 0.7159804999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.32170054626464845, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01369522523880005, AUC: 0.516219\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12513492584228517, AUC: 0.502995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.058791873931884765, AUC: 0.6153120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.13666724395751953, AUC: 0.5024949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.059439298629760745, AUC: 0.608271\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021141726493835448, AUC: 0.803672\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03345775032043457, AUC: 0.45481399999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02317213726043701, AUC: 0.7973180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010115370273590088, AUC: 0.789774\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5971633605957031, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06598271942138671, AUC: 0.5879685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021370010375976564, AUC: 0.803662\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02171405029296875, AUC: 0.4277695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8716934814453124, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.031814878463745117, AUC: 0.784575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.059524940490722655, AUC: 0.6449545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02659313678741455, AUC: 0.792167\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.18326407623291016, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012795899868011475, AUC: 0.6302495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2935269775390625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.059789186477661134, AUC: 0.6299480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028318270683288575, AUC: 0.7962215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.13109254455566408, AUC: 0.5039954999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018757739067077638, AUC: 0.7855725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04424954223632813, AUC: 0.45935050000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03177303504943848, AUC: 0.7699885000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05475870513916015, AUC: 0.6495084999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02387538433074951, AUC: 0.7886839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11229401779174805, AUC: 0.503498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024900227546691894, AUC: 0.7818875000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06370569038391113, AUC: 0.488517\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09176447677612305, AUC: 0.5179505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09444039916992188, AUC: 0.514984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.16351358032226562, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.033499616622924805, AUC: 0.7812749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021050064086914063, AUC: 0.7986139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04566512298583984, AUC: 0.6602320000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022629907608032226, AUC: 0.7671935000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03387494850158691, AUC: 0.7645229999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021666088104248048, AUC: 0.7748255000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019359353065490723, AUC: 0.7890695000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01465141487121582, AUC: 0.7805175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.035382822036743165, AUC: 0.42008049999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020867056846618652, AUC: 0.7455769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020987175941467286, AUC: 0.7642829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03368626022338867, AUC: 0.755595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014256035804748535, AUC: 0.7612475000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018050233840942384, AUC: 0.7828060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.038242820739746095, AUC: 0.417755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013154576778411866, AUC: 0.6968559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021867493629455566, AUC: 0.746386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02142896366119385, AUC: 0.769296\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030088013648986816, AUC: 0.770766\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026858837127685546, AUC: 0.77849\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04998508071899414, AUC: 0.48951300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.035694150924682616, AUC: 0.7551125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03812778854370117, AUC: 0.741732\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025139434814453125, AUC: 0.7893365000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04647598075866699, AUC: 0.6920055000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026918153762817382, AUC: 0.7919365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021142802238464355, AUC: 0.43594600000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037452198028564455, AUC: 0.726645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01845305061340332, AUC: 0.7505999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0436752872467041, AUC: 0.688782\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021592983245849608, AUC: 0.766801\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015693858146667482, AUC: 0.7866755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08102265548706054, AUC: 0.555934\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03371454811096192, AUC: 0.7362390000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015308978080749512, AUC: 0.7772350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.036178647994995115, AUC: 0.7496354999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037542150497436526, AUC: 0.738165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017550529479980467, AUC: 0.7809955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03659624862670898, AUC: 0.7115265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07324045944213867, AUC: 0.5335629999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04494003105163574, AUC: 0.693006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011330324649810791, AUC: 0.7610049999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01781573486328125, AUC: 0.7741\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028444123268127442, AUC: 0.788974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05396267890930176, AUC: 0.47698349999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021234413146972657, AUC: 0.7203875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02448776626586914, AUC: 0.739504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03163255023956299, AUC: 0.7638425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018987480163574218, AUC: 0.763025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016019587993621826, AUC: 0.7630425000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09086427688598633, AUC: 0.4999985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030430325508117674, AUC: 0.7635689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03314482021331787, AUC: 0.775753\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01623390769958496, AUC: 0.7814039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.17052671813964843, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026249880790710448, AUC: 0.7865340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.057639955520629886, AUC: 0.4829995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019724475860595704, AUC: 0.7583285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029424031257629393, AUC: 0.764791\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06511692428588867, AUC: 0.57164\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.046658580780029296, AUC: 0.676185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015715662479400636, AUC: 0.776805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 class weighted focal loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 5e-5, 1e-6, 5e-7]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['reduction'] = 'mean'\n",
    "loss_fn_args['weight']=torch.tensor([weight[1]])\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SigmoidFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted_focal_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5], None]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d82bd4c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.02157968044281006, AUC: 0.6867034999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04202408409118653, AUC: 0.5679595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01122785186767578, AUC: 0.7952355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009371079921722413, AUC: 0.8330525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01840770721435547, AUC: 0.6837489999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.051747398376464845, AUC: 0.6998095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023658714294433593, AUC: 0.39837999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.036989910125732425, AUC: 0.5953179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019817763328552247, AUC: 0.8361535000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06258223724365235, AUC: 0.6625259999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023065343856811523, AUC: 0.8184055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1660221939086914, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04466410827636719, AUC: 0.5848144999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019423234939575196, AUC: 0.8312700000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015033366680145264, AUC: 0.7412275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07601138687133789, AUC: 0.626493\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03341779327392578, AUC: 0.596404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.031233074188232422, AUC: 0.8006485000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04384331321716309, AUC: 0.4436590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024861087799072267, AUC: 0.6537995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.046678565979003904, AUC: 0.7248254999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024052400588989258, AUC: 0.6468825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028643247604370117, AUC: 0.6151334999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01757850933074951, AUC: 0.6956695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04979433631896973, AUC: 0.46103200000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06888628387451172, AUC: 0.511482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06603289413452149, AUC: 0.5159940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017977940559387208, AUC: 0.8436810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026384526252746583, AUC: 0.8148645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02604616641998291, AUC: 0.8142515000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011570464134216309, AUC: 0.651253\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0302976016998291, AUC: 0.793967\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03339131450653076, AUC: 0.779576\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08760675811767578, AUC: 0.6006465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013762806415557861, AUC: 0.7453695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04165582847595215, AUC: 0.7497680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03422253608703613, AUC: 0.6372905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011200609683990479, AUC: 0.786203\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013259670734405517, AUC: 0.7621070000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07646782684326171, AUC: 0.6232249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08534825134277343, AUC: 0.50501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013772080898284913, AUC: 0.8494510000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04881747436523438, AUC: 0.4903955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07178605270385742, AUC: 0.5119545000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010169760227203369, AUC: 0.8342529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.057615945816040036, AUC: 0.5229745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021845489501953124, AUC: 0.6660035000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025428202629089354, AUC: 0.6450505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029022104263305665, AUC: 0.636784\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012839582443237305, AUC: 0.8401609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.035134546279907225, AUC: 0.7806424999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07618846893310546, AUC: 0.6249600000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02815249443054199, AUC: 0.810743\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013517923831939697, AUC: 0.7479985000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0384180850982666, AUC: 0.49347649999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020593582153320314, AUC: 0.681716\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013721189498901366, AUC: 0.7572204999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010134970664978028, AUC: 0.8449795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011493403434753418, AUC: 0.7720244999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.032214184761047364, AUC: 0.791414\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01917431926727295, AUC: 0.4311704999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0049877619743347165, AUC: 0.6959265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034903968572616577, AUC: 0.7380015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032030287981033327, AUC: 0.782322\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002368504047393799, AUC: 0.7648355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034267951250076295, AUC: 0.7303225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018824248313903807, AUC: 0.6324909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005648926973342895, AUC: 0.6614215000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003485495567321777, AUC: 0.7123385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027559136152267454, AUC: 0.7661294999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034918612241744995, AUC: 0.7872545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004706173181533813, AUC: 0.7962935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017050434112548828, AUC: 0.49219050000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004432138681411743, AUC: 0.712433\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027856410741806032, AUC: 0.7559829999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026375510692596437, AUC: 0.7826645\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 18\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCappedBCELoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     20\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:15\u001b[0m, in \u001b[0;36mtrain_sigmoid\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args, smote)\u001b[0m\n\u001b[1;32m     12\u001b[0m loss_fn\u001b[38;5;241m=\u001b[39mloss_fn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_fn_args)\n\u001b[1;32m     14\u001b[0m network\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m     output \u001b[38;5;241m=\u001b[39m network(data)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:624\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/sampler.py:254\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m    253\u001b[0m idx_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 254\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler:\n\u001b[1;32m    255\u001b[0m     batch[idx_in_batch] \u001b[38;5;241m=\u001b[39m idx\n\u001b[1;32m    256\u001b[0m     idx_in_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2 Class SMOTE \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, smote=True)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5], None]\n",
    "    rows.append(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c104725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.043651641845703124, AUC: 0.4405345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05309476089477539, AUC: 0.7162425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9890640869140624, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11034839630126952, AUC: 0.5455470000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07372441864013672, AUC: 0.642726\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04674614906311035, AUC: 0.736783\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05178194808959961, AUC: 0.479507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.053079261779785156, AUC: 0.702415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.13817947387695312, AUC: 0.5174839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04859622192382813, AUC: 0.7293620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05824649047851563, AUC: 0.69985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04465114784240723, AUC: 0.7492264999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011324538707733154, AUC: 0.628558\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07766576766967774, AUC: 0.6113135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10414237594604492, AUC: 0.563539\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4346322937011719, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.17372201538085938, AUC: 0.505496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08989744186401367, AUC: 0.6116750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03226158237457275, AUC: 0.67492\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07547613906860351, AUC: 0.6075305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1678723678588867, AUC: 0.5059980000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.032904546737670896, AUC: 0.7806745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05441599082946778, AUC: 0.698959\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.15263349151611327, AUC: 0.512993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04180476760864258, AUC: 0.4799785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.30637785339355467, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.13590684509277343, AUC: 0.5300465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08892767333984375, AUC: 0.5967605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0542543830871582, AUC: 0.7102849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02560442638397217, AUC: 0.6097275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03419033432006836, AUC: 0.584747\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.37172531127929687, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8596504821777343, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05839499473571777, AUC: 0.6955464999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10681319046020508, AUC: 0.554117\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.33663468933105467, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037840221405029295, AUC: 0.4950024999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07808576202392578, AUC: 0.603731\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0404064884185791, AUC: 0.75734\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07703141784667969, AUC: 0.617969\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06799485397338867, AUC: 0.6645235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023998952865600585, AUC: 0.815809\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03943236351013184, AUC: 0.47848149999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03659125900268555, AUC: 0.7754630000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3182792510986328, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10621945190429688, AUC: 0.55105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05837744140625, AUC: 0.7016915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013953322410583497, AUC: 0.8104215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015256577491760254, AUC: 0.608114\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08964618301391601, AUC: 0.568206\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0352486457824707, AUC: 0.7843910000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12435921859741211, AUC: 0.5439655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.39695411682128906, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6044379577636719, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.034485397338867185, AUC: 0.480641\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04207136726379394, AUC: 0.752563\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.052544401168823245, AUC: 0.7059515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0822454719543457, AUC: 0.6181705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07724650955200195, AUC: 0.635029\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.19141546630859374, AUC: 0.5020015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03171080112457275, AUC: 0.6483865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.164287841796875, AUC: 0.6223515000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9247216796875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.36183277893066407, AUC: 0.5340745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2987734375, AUC: 0.5748825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2822566528320313, AUC: 0.596116\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013178233146667481, AUC: 0.4593625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 5.15568896484375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.17391566467285155, AUC: 0.643077\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.23959645843505858, AUC: 0.5958665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2183768539428711, AUC: 0.6416434999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4971228790283203, AUC: 0.514508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017585429191589355, AUC: 0.6277945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5593604125976563, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.20820986938476563, AUC: 0.6213954999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9806484375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2345706558227539, AUC: 0.617679\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2772058868408203, AUC: 0.581548\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07616076278686523, AUC: 0.499501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08911953353881837, AUC: 0.7439525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 Class SMOTE with capped loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 5e-4, 1e-3]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_sigmoid(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args, smote=True)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "for i in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[i][0]\n",
    "    auc_variance = cap_aucs[i][1]\n",
    "    cap = caps[i]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3],\n",
    "                auc_mean[i][4], auc_variance[i][4],\n",
    "                auc_mean[i][5], auc_variance[i][5], cap]\n",
    "        rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b7ce280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.02157968044281006, AUC: 0.6867034999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013757909774780273, AUC: 0.7648969999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013857486248016358, AUC: 0.800988\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017318458557128907, AUC: 0.8124695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009805487632751466, AUC: 0.8018260000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014841811656951904, AUC: 0.8085429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023658714294433593, AUC: 0.39837999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026063546180725098, AUC: 0.7488279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01888744068145752, AUC: 0.765158\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019569753646850586, AUC: 0.7862880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008232975006103515, AUC: 0.7549595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011351889610290527, AUC: 0.7745679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04466410827636719, AUC: 0.5848144999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02735144519805908, AUC: 0.7608940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014998260974884034, AUC: 0.775344\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019471473693847656, AUC: 0.8011909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012362385272979736, AUC: 0.793639\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012634827136993407, AUC: 0.79663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04384331321716309, AUC: 0.4436590000000001\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 20\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     22\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:17\u001b[0m, in \u001b[0;36mtrain_sigmoid\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args, smote)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m smote:\n\u001b[1;32m     19\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mfloat(), (target[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat(), target[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/models.py:63\u001b[0m, in \u001b[0;36mSigmoidLogisticRegression.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     62\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 63\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2 class effective # of samples \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['pos_weight'] = effective_weights\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"effective_samples\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5d929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES_REDUCED = 3\n",
    "nums = (0, 3, 1)\n",
    "ratio = (200, 20, 1)\n",
    "\n",
    "\n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums)\n",
    "targets = ratio_train_CIFAR10.labels \n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED)\n",
    "\n",
    "\n",
    "weight = 1. / class_count\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= max(class_count)\n",
    "\n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49887493",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Softmax 3 class normal\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 3, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4386e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 3 class ratio\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e15c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Softmax 3 class oversampled\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f9856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 3 class undersampled\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f39e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 3 class both sampled\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_sampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"both_sampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e12075c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Softmax 3 class weighted\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['weight'] = torch.from_numpy(class_weights).float()\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba2dbda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Softmax 3 class focal loss \n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 5e-4, 1e-5, 5e-5, 1e-6, 5e-7]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['reduction'] = 'mean'\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SoftmaxFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b8a3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3 class SMOTE \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(3, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, smote=True)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "    \n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5], 'n/a']\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b20715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3 Class SMOTE with capped loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args, smote=True)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "for i in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[i][0]\n",
    "    auc_variance = cap_aucs[i][1]\n",
    "    cap = caps[i]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", 3, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3],\n",
    "                auc_mean[i][4], auc_variance[i][4],\n",
    "                auc_mean[i][5], auc_variance[i][5], cap]\n",
    "        rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86c3a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/auc_analysis_other_methods.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names)\n",
    "\n",
    "df = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15c14544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>num_classes</th>\n",
       "      <th>classes_used</th>\n",
       "      <th>ratio</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>mean_0</th>\n",
       "      <th>variance_0</th>\n",
       "      <th>mean_10</th>\n",
       "      <th>variance_10</th>\n",
       "      <th>mean_20</th>\n",
       "      <th>variance_20</th>\n",
       "      <th>mean_30</th>\n",
       "      <th>variance_30</th>\n",
       "      <th>mean_40</th>\n",
       "      <th>variance_40</th>\n",
       "      <th>mean_50</th>\n",
       "      <th>variance_50</th>\n",
       "      <th>cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weighted_focal_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(100, 1)</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.544775</td>\n",
       "      <td>0.009107</td>\n",
       "      <td>0.504158</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.528279</td>\n",
       "      <td>0.004045</td>\n",
       "      <td>0.516186</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.560687</td>\n",
       "      <td>0.003222</td>\n",
       "      <td>0.523594</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weighted_focal_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(100, 1)</td>\n",
       "      <td>5.000000e-05</td>\n",
       "      <td>0.531304</td>\n",
       "      <td>0.005860</td>\n",
       "      <td>0.504502</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.528695</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.522772</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.554471</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>0.545959</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weighted_focal_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(100, 1)</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.515702</td>\n",
       "      <td>0.008527</td>\n",
       "      <td>0.641324</td>\n",
       "      <td>0.019075</td>\n",
       "      <td>0.680983</td>\n",
       "      <td>0.008478</td>\n",
       "      <td>0.658132</td>\n",
       "      <td>0.017960</td>\n",
       "      <td>0.645942</td>\n",
       "      <td>0.013512</td>\n",
       "      <td>0.722783</td>\n",
       "      <td>0.013886</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_focal_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(100, 1)</td>\n",
       "      <td>5.000000e-07</td>\n",
       "      <td>0.515097</td>\n",
       "      <td>0.008945</td>\n",
       "      <td>0.720347</td>\n",
       "      <td>0.004308</td>\n",
       "      <td>0.751781</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.740536</td>\n",
       "      <td>0.003853</td>\n",
       "      <td>0.723086</td>\n",
       "      <td>0.006758</td>\n",
       "      <td>0.781678</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  num_classes classes_used     ratio  learning_rate   \n",
       "0  weighted_focal_loss            2       (0, 1)  (100, 1)   1.000000e-05  \\\n",
       "1  weighted_focal_loss            2       (0, 1)  (100, 1)   5.000000e-05   \n",
       "2  weighted_focal_loss            2       (0, 1)  (100, 1)   1.000000e-06   \n",
       "3  weighted_focal_loss            2       (0, 1)  (100, 1)   5.000000e-07   \n",
       "\n",
       "     mean_0  variance_0   mean_10  variance_10   mean_20  variance_20   \n",
       "0  0.544775    0.009107  0.504158     0.000160  0.528279     0.004045  \\\n",
       "1  0.531304    0.005860  0.504502     0.000081  0.528695     0.001203   \n",
       "2  0.515702    0.008527  0.641324     0.019075  0.680983     0.008478   \n",
       "3  0.515097    0.008945  0.720347     0.004308  0.751781     0.000545   \n",
       "\n",
       "    mean_30  variance_30   mean_40  variance_40   mean_50  variance_50   cap  \n",
       "0  0.516186     0.001454  0.560687     0.003222  0.523594     0.001516  None  \n",
       "1  0.522772     0.001813  0.554471     0.003317  0.545959     0.002236  None  \n",
       "2  0.658132     0.017960  0.645942     0.013512  0.722783     0.013886  None  \n",
       "3  0.740536     0.003853  0.723086     0.006758  0.781678     0.000059  None  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02884ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('results/auc_analysis_other_methods.csv', index=False)\n",
    "# df.to_csv('results/auc_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04e783a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([16, 128])\n"
     ]
    }
   ],
   "source": [
    "model = models.ConvNetWithEmbeddings(2)\n",
    "\n",
    "train_loader = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "    output, embed = model(data)\n",
    "    print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f80feb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
