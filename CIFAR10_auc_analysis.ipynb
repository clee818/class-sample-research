{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "783b8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "242f3056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fdb51a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "n_epochs = 50\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "NUM_CLASSES_REDUCED = 2\n",
    "nums = (0, 1)\n",
    "ratio = (100, 1)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1ba6047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name, num_classes, classes used, ratio, learning rate, mean 10, variance 10, mean 20, variance 20, ... 50\n",
    "\n",
    "# mean, variance every 10 epochs - average of 10 models \n",
    "# name, num_classes, classes used, ratio, learning rate, mean 10, variance 10, mean 20, variance 20, ... 50\n",
    "# name: normal/ratio/oversampled/undersampled/weighted\n",
    "\n",
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "            \"mean_40\", \"variance_40\",\n",
    "            \"mean_50\", \"variance_50\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "05d80dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor() ]))  \n",
    "\n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()]))\n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "    \n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums)\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fd6f17c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = smote_train_CIFAR10.images\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(examples[i].reshape(32, 32, 3).int())\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dbde3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000   50]\n"
     ]
    }
   ],
   "source": [
    "targets = ratio_train_CIFAR10.labels \n",
    "\n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "print(class_count)\n",
    "\n",
    "weight = 1. / class_count\n",
    "\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= class_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a65e877",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.999 \n",
    "\n",
    "exp = np.empty_like(targets)\n",
    "for i, count in enumerate(class_count):\n",
    "    exp[targets==i] = count\n",
    "effective_weights = (1 - beta) / ( 1 - (beta ** torch.from_numpy(exp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c25bab27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0010, 0.0010, 0.0010,  ..., 0.0205, 0.0205, 0.0205])\n"
     ]
    }
   ],
   "source": [
    "print(effective_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8ca20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f8c2d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(test_loader_reduced.dataset.images.shape) # tupe instead of torch.Size() like the others "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "943f017c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5050 is out of bounds for dimension 0 with size 64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[0;32m----> 7\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mexample_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5050\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mint())\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGround Truth: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(example_targets[i]))\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mxticks([])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5050 is out of bounds for dimension 0 with size 64"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = enumerate(train_loader_smote) # enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i].reshape(32, 32, 3).int())\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e241471e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.024605756759643556, AUC: 0.6469975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014931785106658936, AUC: 0.8215375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.051244140625, AUC: 0.75086\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12066669464111328, AUC: 0.5902445000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06057313537597656, AUC: 0.7290795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016814103603363037, AUC: 0.8081860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020942140579223632, AUC: 0.459742\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.13868421936035155, AUC: 0.504016\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012890518665313721, AUC: 0.8556889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04611336135864258, AUC: 0.6312180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020766493797302246, AUC: 0.858468\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021261009216308593, AUC: 0.8546545000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04852569198608398, AUC: 0.59049\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05658713340759278, AUC: 0.7340209999999999\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 17\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     19\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/ml/train.py:15\u001b[0m, in \u001b[0;36mtrain_sigmoid\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     12\u001b[0m loss_fn\u001b[38;5;241m=\u001b[39mloss_fn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_fn_args)\n\u001b[1;32m     14\u001b[0m network\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m     output \u001b[38;5;241m=\u001b[39m network(data)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Downloads/ml/class_sampling.py:27\u001b[0m, in \u001b[0;36mReduce.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index): \n\u001b[0;32m---> 27\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[index]\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnums[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m     29\u001b[0m         label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SIGMOID 2 CLASS normal AUC saving  \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [2e-5, 3e-5, 5e-5, 7e-5, 9e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa634095",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(learning_rate_aucs.shape)\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "print(auc_mean.shape)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "print(auc_variance.shape)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n",
    "\n",
    "print(rows)\n",
    "\n",
    "# pd.DataFrame(rows, columns = col_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f9e1e7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.03197626209259033, AUC: 0.4619719999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014868484020233155, AUC: 0.731327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02078729820251465, AUC: 0.765631\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01243867301940918, AUC: 0.7592875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008404238224029541, AUC: 0.7387854999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016378780364990235, AUC: 0.7749185000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.062348251342773435, AUC: 0.491014\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02947155952453613, AUC: 0.772812\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011246074676513671, AUC: 0.791176\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013778327465057373, AUC: 0.8004434999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01350959300994873, AUC: 0.803744\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 17\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     19\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/ml/train.py:15\u001b[0m, in \u001b[0;36mtrain_sigmoid\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     12\u001b[0m loss_fn\u001b[38;5;241m=\u001b[39mloss_fn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_fn_args)\n\u001b[1;32m     14\u001b[0m network\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m     output \u001b[38;5;241m=\u001b[39m network(data)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Downloads/ml/class_sampling.py:167\u001b[0m, in \u001b[0;36mRatio.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index): \n\u001b[1;32m    166\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[index]\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m--> 167\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (image, label)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SIGMOID 2 CLASS ratio\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836305f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SIGMOID 2 CLASS oversampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8753923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIGMOID 2 CLASS undersampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21ae5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIGMOID 2 CLASS over+undersampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_sampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"both_sampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863fbdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIGMOID 2 CLASS weighted \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['pos_weight'] = torch.tensor([weight[1]])\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d3c1a77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.02157968044281006, AUC: 0.6867034999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.25539259338378906, AUC: 0.52\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5048945159912109, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9401114196777344, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.21155704498291017, AUC: 0.589535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3151288604736328, AUC: 0.540535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023658714294433593, AUC: 0.39837999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.180468505859375, AUC: 0.571037\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08399776077270507, AUC: 0.6958759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.16746481323242188, AUC: 0.6145299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1457880859375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.28829020690917967, AUC: 0.55198\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04466410827636719, AUC: 0.5848144999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 7.139232666015625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5686558227539062, AUC: 0.502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10955883407592773, AUC: 0.690185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3668731231689453, AUC: 0.51\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.21694229888916017, AUC: 0.605065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04384331321716309, AUC: 0.4436590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 6.674289306640625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5732989807128906, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.35412017822265623, AUC: 0.5199904999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 5.267047119140625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 5.77702734375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04979433631896973, AUC: 0.46103200000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4999161834716797, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.17462056732177733, AUC: 0.5937695000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8829172058105469, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.26676353454589846, AUC: 0.538065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.45693614196777343, AUC: 0.5095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011570464134216309, AUC: 0.651253\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 6.46621826171875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.22045648193359374, AUC: 0.5550010000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8684164733886719, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5115269927978515, AUC: 0.523\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2674881591796875, AUC: 0.555928\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03422253608703613, AUC: 0.6372905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7166910095214843, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5778602905273438, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5673576049804687, AUC: 0.5035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3228272247314453, AUC: 0.54854\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.38013385009765627, AUC: 0.518513\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04881747436523438, AUC: 0.4903955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.4386279296875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.95120458984375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6089764099121093, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5350477294921875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2230844268798828, AUC: 0.5779289999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029022104263305665, AUC: 0.636784\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8526256103515625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.18715541076660155, AUC: 0.577512\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.485755126953125, AUC: 0.5080034999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.40101708984375, AUC: 0.516507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3384692687988281, AUC: 0.5249875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0384180850982666, AUC: 0.49347649999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.898302978515625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.34407749938964843, AUC: 0.5190125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6712857971191406, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.023772216796875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07982427597045899, AUC: 0.718971\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01917431926727295, AUC: 0.4311704999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06394366264343261, AUC: 0.597337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02672089385986328, AUC: 0.7809320000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03797200393676758, AUC: 0.7667835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.21341192626953126, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026108357429504396, AUC: 0.811767\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018824248313903807, AUC: 0.6324909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.032958127975463866, AUC: 0.790215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015796066761016844, AUC: 0.8293785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01650595235824585, AUC: 0.8322134999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02554157543182373, AUC: 0.8161615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020271424293518067, AUC: 0.8187519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017050434112548828, AUC: 0.49219050000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.042409296035766604, AUC: 0.7225140000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018463167190551757, AUC: 0.82488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03595870971679688, AUC: 0.766802\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.16017689514160155, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.23060936737060547, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025902076721191405, AUC: 0.6785265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04266296195983887, AUC: 0.7197250000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0675365219116211, AUC: 0.5851014999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05974405097961426, AUC: 0.6191085000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037364004135131836, AUC: 0.764741\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023921048164367676, AUC: 0.8215160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013246778964996337, AUC: 0.5174624999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10579840087890625, AUC: 0.5069859999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016768999099731445, AUC: 0.814265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.20792122650146486, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04499868965148926, AUC: 0.7135345000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11444670867919922, AUC: 0.5090115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013060497760772704, AUC: 0.651512\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006536483526229858, AUC: 0.7694719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.031229813575744628, AUC: 0.792281\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09571591567993164, AUC: 0.520515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.37175712585449217, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022185397148132325, AUC: 0.813931\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07371361541748046, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0417993221282959, AUC: 0.730525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8859598693847657, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02058237075805664, AUC: 0.8202115000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03661252975463867, AUC: 0.7612064999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02450685405731201, AUC: 0.8211345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024945231437683107, AUC: 0.4672625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03207170867919922, AUC: 0.7840894999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025537230491638183, AUC: 0.8156735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07227820205688476, AUC: 0.5596899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6227947998046875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.039818994522094725, AUC: 0.751931\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02865324306488037, AUC: 0.5881215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05282268524169922, AUC: 0.6508169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015923991203308105, AUC: 0.8118874999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01767426776885986, AUC: 0.814754\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03425726509094238, AUC: 0.7871714999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03756260108947754, AUC: 0.762037\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013817548274993896, AUC: 0.3908135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023602903366088866, AUC: 0.788279\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037950952529907225, AUC: 0.7616850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06033032989501953, AUC: 0.609188\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.038427330017089846, AUC: 0.7559429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020651951789855957, AUC: 0.824409\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013077984809875489, AUC: 0.6445015000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1644978790283203, AUC: 0.5565370000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1452919387817383, AUC: 0.559053\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07454119491577148, AUC: 0.6815455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.14986153411865236, AUC: 0.5932885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.16636363220214845, AUC: 0.5450205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03605061340332031, AUC: 0.5307915000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.26954620361328124, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.15859034729003907, AUC: 0.570615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03984218025207519, AUC: 0.7536349999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2179149169921875, AUC: 0.5245150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.45292144775390625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07314898300170898, AUC: 0.499006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.15558218383789063, AUC: 0.563613\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.062054754257202145, AUC: 0.6998915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12731245040893555, AUC: 0.574013\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.13057307052612305, AUC: 0.582543\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.15351804351806642, AUC: 0.541571\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022195216178894042, AUC: 0.616943\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.8143094482421875, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2145692749023438, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0932828369140624, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.19784551239013673, AUC: 0.5225339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.14437006378173828, AUC: 0.5754865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024299232482910157, AUC: 0.6556175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.14648637390136718, AUC: 0.5325259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08377293014526367, AUC: 0.63031\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0941864013671876, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.36805392456054686, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12643811416625977, AUC: 0.5844545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019841660499572755, AUC: 0.461005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2676147155761719, AUC: 0.5045000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05666688346862793, AUC: 0.7131529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5234431915283203, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06799781799316407, AUC: 0.6953485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.15751282501220704, AUC: 0.5510320000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016441553592681884, AUC: 0.41137499999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.14145905303955078, AUC: 0.5565300000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.17243612670898437, AUC: 0.541498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.21695858001708984, AUC: 0.5170155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8317302551269531, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4252542724609375, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03080390739440918, AUC: 0.4539889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12082997512817382, AUC: 0.5700709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.70410400390625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11734649276733398, AUC: 0.596419\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1246383056640625, AUC: 0.5831245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8111062316894532, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01945985412597656, AUC: 0.401874\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.08673828125, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.13975074768066406, AUC: 0.5484910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5639797973632813, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08519174957275391, AUC: 0.661869\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06048710250854492, AUC: 0.7179125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015312074661254883, AUC: 0.6002565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.21765155029296876, AUC: 0.502501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.4818853759765624, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.14056514739990233, AUC: 0.5704885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2438016128540039, AUC: 0.515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2303832473754883, AUC: 0.5195190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04828184509277344, AUC: 0.486481\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019664740562438963, AUC: 0.73464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024153520584106445, AUC: 0.7543195000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01708263301849365, AUC: 0.745822\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019327674865722656, AUC: 0.7566375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018810784339904785, AUC: 0.7614160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.062263938903808595, AUC: 0.49799550000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022366440773010254, AUC: 0.704919\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02402254295349121, AUC: 0.7454759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017587182998657226, AUC: 0.748756\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013711828231811524, AUC: 0.7445995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01640038776397705, AUC: 0.759173\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.035586790084838865, AUC: 0.647975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02681404685974121, AUC: 0.6868825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01868080711364746, AUC: 0.709267\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020062341690063476, AUC: 0.7427769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020080883979797362, AUC: 0.7520140000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013655925273895263, AUC: 0.753067\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028565074920654295, AUC: 0.43635849999999987\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023308874130249024, AUC: 0.683874\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02036716938018799, AUC: 0.7166025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01870932388305664, AUC: 0.7344825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018960381507873537, AUC: 0.7473665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019660449028015135, AUC: 0.75893\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04257582855224609, AUC: 0.46773650000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026304771423339843, AUC: 0.709223\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023380974769592285, AUC: 0.7368055000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01930994415283203, AUC: 0.739588\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016168478965759278, AUC: 0.741579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017130768775939942, AUC: 0.754022\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.061005592346191406, AUC: 0.49299049999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03263898658752441, AUC: 0.729666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021347292900085447, AUC: 0.7455245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017007975578308104, AUC: 0.749401\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016244997024536134, AUC: 0.762113\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02138670253753662, AUC: 0.7736144999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027727532386779784, AUC: 0.44274450000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028199255943298338, AUC: 0.727125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022661017417907714, AUC: 0.7333000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020907702445983885, AUC: 0.723196\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019970261573791503, AUC: 0.720645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017632474899291994, AUC: 0.7137914999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07662530517578126, AUC: 0.539671\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02723433208465576, AUC: 0.6982119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020282000541687013, AUC: 0.7434095000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017604975700378417, AUC: 0.7595580000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016794427871704102, AUC: 0.7652975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01858513927459717, AUC: 0.7698149999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0431246280670166, AUC: 0.6561009999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023245326995849608, AUC: 0.739042\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020164746284484864, AUC: 0.7486330000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018526297569274902, AUC: 0.7576130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01929311466217041, AUC: 0.7649119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016531875133514406, AUC: 0.7619815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04020754814147949, AUC: 0.4491095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02732034397125244, AUC: 0.678517\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024720409393310546, AUC: 0.7032355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021228827476501465, AUC: 0.7136279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016927027702331544, AUC: 0.7159905000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0165041823387146, AUC: 0.7262190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08669427871704101, AUC: 0.49800400000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03619421577453613, AUC: 0.726752\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.22554965209960937, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010378730297088624, AUC: 0.7920619999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013839887142181397, AUC: 0.8096425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018072003364562987, AUC: 0.8217379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04958260726928711, AUC: 0.5020074999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026429426193237304, AUC: 0.8037620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01572334861755371, AUC: 0.8204845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012298777103424072, AUC: 0.8140269999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012926404953002929, AUC: 0.8158045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014190206527709961, AUC: 0.8182965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.055263671875, AUC: 0.6254205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05108163642883301, AUC: 0.6463725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04077780151367188, AUC: 0.7288500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01849630069732666, AUC: 0.7956599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.031226165771484377, AUC: 0.7782359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01784758186340332, AUC: 0.8005755000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.051190927505493165, AUC: 0.5851744999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025047727584838867, AUC: 0.7562625000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009792949199676514, AUC: 0.7565344999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02256877040863037, AUC: 0.8072010000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01578116226196289, AUC: 0.798843\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.060469085693359376, AUC: 0.595791\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03852338981628418, AUC: 0.5131654999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017012497901916505, AUC: 0.8104645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03234713554382324, AUC: 0.7883169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018919898986816407, AUC: 0.809771\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.041002771377563474, AUC: 0.740261\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0222384672164917, AUC: 0.8273369999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0658036994934082, AUC: 0.5360860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022249459266662597, AUC: 0.7320845\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.07702728271484376, AUC: 0.5258685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017853473663330077, AUC: 0.7562289999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023167854309082032, AUC: 0.7897390000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015082174301147461, AUC: 0.7945310000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03468166732788086, AUC: 0.4495929999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03586968994140625, AUC: 0.7529619999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12154244232177734, AUC: 0.502993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0179623441696167, AUC: 0.8119695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07455818176269531, AUC: 0.539934\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.040379995346069335, AUC: 0.746824\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01720845317840576, AUC: 0.39403199999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004933303594589234, AUC: 0.7571275000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01837593936920166, AUC: 0.811898\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0162269926071167, AUC: 0.8133875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011354846954345703, AUC: 0.7839635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015536299705505371, AUC: 0.7945515000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05087214469909668, AUC: 0.4924375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01999150276184082, AUC: 0.7526215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02838347339630127, AUC: 0.779287\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04276674842834473, AUC: 0.708228\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01594639253616333, AUC: 0.784044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028088376998901366, AUC: 0.8049080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015332425594329834, AUC: 0.5502675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020160316467285156, AUC: 0.7586115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020061100959777832, AUC: 0.7891115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0677602882385254, AUC: 0.5586925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03751943588256836, AUC: 0.7568575000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04617692947387695, AUC: 0.698534\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021256857872009278, AUC: 0.602456\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.034163526535034176, AUC: 0.666518\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03125203800201416, AUC: 0.6671625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0285546236038208, AUC: 0.670907\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027462430000305176, AUC: 0.676821\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02610208225250244, AUC: 0.67965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027020174980163573, AUC: 0.470211\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03656805038452148, AUC: 0.6773534999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03503935432434082, AUC: 0.6851709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03316427612304688, AUC: 0.6882039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.032158936500549316, AUC: 0.6936305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030808701515197753, AUC: 0.695336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018543965339660646, AUC: 0.4351165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03761576461791992, AUC: 0.551236\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.036040533065795896, AUC: 0.555151\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03398856735229492, AUC: 0.5579350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03187570285797119, AUC: 0.563605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030934672355651855, AUC: 0.570464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015882391452789305, AUC: 0.394959\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04358355331420898, AUC: 0.588808\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04094113540649414, AUC: 0.603248\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.038891704559326175, AUC: 0.612947\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.036773681640625, AUC: 0.618608\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03470506286621094, AUC: 0.6250629999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06080618858337403, AUC: 0.5909125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05174780082702637, AUC: 0.608705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0454811897277832, AUC: 0.6038505000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04241244316101074, AUC: 0.603521\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04061722183227539, AUC: 0.607298\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037416719436645506, AUC: 0.609356\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027196504592895507, AUC: 0.44928250000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.031529644012451175, AUC: 0.6661225000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.031120644569396973, AUC: 0.6689065000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030208450317382812, AUC: 0.6684235000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030269147872924805, AUC: 0.6706565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029465352058410645, AUC: 0.6703945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.040741342544555664, AUC: 0.43096400000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04370228576660156, AUC: 0.552652\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03916903495788574, AUC: 0.559628\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.035649085998535154, AUC: 0.5706409999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03320571136474609, AUC: 0.5799435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030959056854248046, AUC: 0.586592\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01812809944152832, AUC: 0.5815745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03697983360290527, AUC: 0.6645875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03374356842041016, AUC: 0.676562\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03154918098449707, AUC: 0.6893415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029897703170776366, AUC: 0.698615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028113112449645997, AUC: 0.705009\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016122743129730225, AUC: 0.430056\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037218162536621094, AUC: 0.617641\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03572398567199707, AUC: 0.6207449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03445857238769531, AUC: 0.6244045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03384239196777344, AUC: 0.6309975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03296655654907227, AUC: 0.63666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019150092124938966, AUC: 0.46735350000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0353601131439209, AUC: 0.684298\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.032575225830078124, AUC: 0.6879935000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030277375221252442, AUC: 0.691949\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02903926944732666, AUC: 0.6927960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02849730110168457, AUC: 0.6972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06895138549804687, AUC: 0.4975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.035508403778076174, AUC: 0.6714709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03421870994567871, AUC: 0.6757470000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03298958015441895, AUC: 0.6807399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.031710214614868164, AUC: 0.683014\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03084628963470459, AUC: 0.6862925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029704508781433107, AUC: 0.41243900000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037920333862304687, AUC: 0.5896585000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.036034725189208985, AUC: 0.5837955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.034953550338745115, AUC: 0.581408\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03417902183532715, AUC: 0.581195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03273401069641113, AUC: 0.5820669999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01452540683746338, AUC: 0.5210604999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04289643669128418, AUC: 0.6526635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0405926513671875, AUC: 0.6591210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0389146671295166, AUC: 0.666656\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037127058029174806, AUC: 0.673585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03557005310058594, AUC: 0.6808240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02534897804260254, AUC: 0.5979389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.041577566146850584, AUC: 0.6170255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04043461990356445, AUC: 0.6198089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03863982582092285, AUC: 0.6234894999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03776037979125976, AUC: 0.6260815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03733997535705567, AUC: 0.6271570000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03807023239135742, AUC: 0.44092549999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04465500831604004, AUC: 0.6257510000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04400471496582031, AUC: 0.629121\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.043099584579467776, AUC: 0.6365900000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0420089054107666, AUC: 0.6397425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04070076560974121, AUC: 0.642273\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019316554069519043, AUC: 0.4194485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.032718019485473634, AUC: 0.6369705000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.031531097412109375, AUC: 0.6363475000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030677413940429686, AUC: 0.638163\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029842398643493653, AUC: 0.6396765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029010675430297853, AUC: 0.6409665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0369910774230957, AUC: 0.656821\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.044789060592651364, AUC: 0.640809\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04339824676513672, AUC: 0.6424529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.041227453231811526, AUC: 0.6558775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.039967279434204105, AUC: 0.660845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03858086967468262, AUC: 0.6718720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027193774223327638, AUC: 0.6144540000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04144111824035644, AUC: 0.634252\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.04019078254699707, AUC: 0.6350349999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03835585212707519, AUC: 0.6323344999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03742955017089844, AUC: 0.6358940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03620248031616211, AUC: 0.6359174999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04511733245849609, AUC: 0.6412525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04101388359069824, AUC: 0.647138\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03769660568237305, AUC: 0.6524810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03643505859375, AUC: 0.6563595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03591868019104004, AUC: 0.6615504999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03551282501220703, AUC: 0.6665095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0275010986328125, AUC: 0.45815349999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02960848045349121, AUC: 0.6919795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028941520690917967, AUC: 0.691463\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027770015716552736, AUC: 0.691391\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026958831787109373, AUC: 0.691355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02633824825286865, AUC: 0.6931224999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS Focal Loss \n",
    "# no weights (yet)\n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 1e-4, 5e-4, 1e-5, 5e-5, 1e-6, 5e-7]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['reduction'] = 'mean'\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SigmoidFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d82bd4c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.02460575580596924, AUC: 0.6469985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026045684814453126, AUC: 0.6504340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01497733449935913, AUC: 0.7351255000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01858936882019043, AUC: 0.6978739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020224379539489745, AUC: 0.688307\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0205801362991333, AUC: 0.8344085000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02094214153289795, AUC: 0.459742\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01317272424697876, AUC: 0.838303\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010728169441223144, AUC: 0.7882945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020182360649108888, AUC: 0.835347\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025778563499450683, AUC: 0.8216855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009684601306915282, AUC: 0.8063224999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.048525693893432614, AUC: 0.59049\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01905726909637451, AUC: 0.700017\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05620224761962891, AUC: 0.6844739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010672938346862794, AUC: 0.8569535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06479360008239746, AUC: 0.6547025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015689518451690674, AUC: 0.719343\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.045675514221191406, AUC: 0.42372100000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02393719482421875, AUC: 0.662747\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01323098611831665, AUC: 0.7472814999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0277181396484375, AUC: 0.6320495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.14380500793457032, AUC: 0.5219735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009276706695556641, AUC: 0.799947\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.047625019073486326, AUC: 0.46799950000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013994871616363525, AUC: 0.7536835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.047595016479492186, AUC: 0.7234815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00948693561553955, AUC: 0.8488655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013809308528900147, AUC: 0.7494479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01912715530395508, AUC: 0.8392715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011635944366455079, AUC: 0.6585844999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00977953815460205, AUC: 0.8500194999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016805845260620116, AUC: 0.8467254999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014944967746734619, AUC: 0.7368565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01112067413330078, AUC: 0.8560995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013534265041351318, AUC: 0.8478170000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.033952896118164065, AUC: 0.6635895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011667448043823242, AUC: 0.7845165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011251608848571777, AUC: 0.836004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023802247047424317, AUC: 0.8238255000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.032673380851745606, AUC: 0.7871395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009664185523986816, AUC: 0.8409234999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05262186050415039, AUC: 0.481568\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009229084968566895, AUC: 0.8386899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04529050827026367, AUC: 0.5545599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03551731300354004, AUC: 0.581961\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04170477294921875, AUC: 0.7482599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011635917663574219, AUC: 0.8624040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028534255027770995, AUC: 0.6685420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012684363842010498, AUC: 0.7701125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019048283576965332, AUC: 0.838249\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013684340000152588, AUC: 0.852438\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026593607902526857, AUC: 0.6313005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021295233726501463, AUC: 0.6722469999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03971935653686524, AUC: 0.48994299999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029297701835632323, AUC: 0.7977389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01709589195251465, AUC: 0.7240249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009470664978027343, AUC: 0.8059645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008829710483551025, AUC: 0.8448215000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03850510597229004, AUC: 0.7598825000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021689083099365233, AUC: 0.40815900000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036171747446060182, AUC: 0.7294295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002789658784866333, AUC: 0.7562645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006705563306808471, AUC: 0.8034685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002294803500175476, AUC: 0.7653245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023117780685424803, AUC: 0.7961575000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019410189628601075, AUC: 0.6326890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035070074796676636, AUC: 0.7418155000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002709715008735657, AUC: 0.7710779999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029238629341125488, AUC: 0.7925794999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002601914644241333, AUC: 0.8013170000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002004059135913849, AUC: 0.793437\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021570838928222657, AUC: 0.4500539999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038390761613845826, AUC: 0.7186425000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002710166692733765, AUC: 0.759787\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002366536259651184, AUC: 0.7826285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030399146080017088, AUC: 0.7468119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020015442371368406, AUC: 0.7729424999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026163442611694336, AUC: 0.598169\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003723966121673584, AUC: 0.7415775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004123410701751709, AUC: 0.725478\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032816864252090453, AUC: 0.80263\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003371599793434143, AUC: 0.8095065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022097939252853395, AUC: 0.7728505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013832940578460693, AUC: 0.5365035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003996200442314148, AUC: 0.724019\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003783612847328186, AUC: 0.7580545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027546544075012207, AUC: 0.7571185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002732579827308655, AUC: 0.773823\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002290425181388855, AUC: 0.7629465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014129615783691407, AUC: 0.58012\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037061218023300172, AUC: 0.7491145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003274141550064087, AUC: 0.7717464999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003306826949119568, AUC: 0.786337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024954878091812136, AUC: 0.7616815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025930172204971314, AUC: 0.7530690000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0784141502380371, AUC: 0.4989995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003657300353050232, AUC: 0.745934\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004346909999847412, AUC: 0.725414\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004435632228851318, AUC: 0.720575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005315084934234619, AUC: 0.7080084999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002882662773132324, AUC: 0.794905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.031551632881164554, AUC: 0.41963549999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003932761192321777, AUC: 0.7258560000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008207703113555908, AUC: 0.6447305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026868257522583007, AUC: 0.7999705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025619206428527832, AUC: 0.7706895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006696071147918701, AUC: 0.8254414999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030002702713012697, AUC: 0.6101485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003975639700889587, AUC: 0.7043944999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029886085987091064, AUC: 0.7500635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00426514744758606, AUC: 0.7103364999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002848837614059448, AUC: 0.743649\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002241510272026062, AUC: 0.7715785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012502269268035889, AUC: 0.39657300000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036311042308807373, AUC: 0.725877\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037828220129013062, AUC: 0.777557\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002627422332763672, AUC: 0.7741475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030362781286239625, AUC: 0.742334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023697584867477415, AUC: 0.7903674999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016779435157775878, AUC: 0.4385135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004205324411392212, AUC: 0.708003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034001466035842895, AUC: 0.7331590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00450618314743042, AUC: 0.7065535000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002967706322669983, AUC: 0.7659115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028136472702026366, AUC: 0.7416699999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03469704437255859, AUC: 0.6751760000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004474036693572998, AUC: 0.704461\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035074015855789185, AUC: 0.7396024999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003127364754676819, AUC: 0.7457860000000001\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0027709816694259644, AUC: 0.758486\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002616809606552124, AUC: 0.758127\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07548549270629883, AUC: 0.49999600000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003938430190086365, AUC: 0.725591\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003295870542526245, AUC: 0.747362\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003393442630767822, AUC: 0.7736390000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002863102316856384, AUC: 0.7522115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029881786108016967, AUC: 0.744436\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027087728500366212, AUC: 0.44866949999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005449873447418213, AUC: 0.715775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003962276101112366, AUC: 0.7225360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035350985527038575, AUC: 0.7281434999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036542681455612185, AUC: 0.7106265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00295078182220459, AUC: 0.736918\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02875083065032959, AUC: 0.6469420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004768420934677124, AUC: 0.7050924999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004370874404907227, AUC: 0.7465605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003729865908622742, AUC: 0.721682\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033865134716033933, AUC: 0.7270075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003265783667564392, AUC: 0.751769\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027386793136596678, AUC: 0.402786\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004652531147003174, AUC: 0.7059365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003400458335876465, AUC: 0.7568835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030872774124145507, AUC: 0.7766165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036897428035736085, AUC: 0.73661\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025734113454818726, AUC: 0.762696\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010502933502197265, AUC: 0.5167915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0039894931316375734, AUC: 0.6994715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004123176336288452, AUC: 0.7080225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002917725086212158, AUC: 0.7462495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026790194511413573, AUC: 0.765906\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002509024500846863, AUC: 0.7626219999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0355885181427002, AUC: 0.4444045000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004488845348358154, AUC: 0.72858\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003673151135444641, AUC: 0.727474\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034767075777053834, AUC: 0.7730755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003188016653060913, AUC: 0.735541\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024708343744277955, AUC: 0.7610045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01696835231781006, AUC: 0.4858095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004084589004516602, AUC: 0.7340905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003293622612953186, AUC: 0.751184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029312856197357177, AUC: 0.763375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028176342248916624, AUC: 0.7623124999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031250697374343873, AUC: 0.754351\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014771099090576173, AUC: 0.6540375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0051419010162353515, AUC: 0.692688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003937243700027466, AUC: 0.723954\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035572255849838255, AUC: 0.750578\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031023619174957275, AUC: 0.7383285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029360930919647217, AUC: 0.7645299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04766559982299805, AUC: 0.491496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00662899374961853, AUC: 0.6411165000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0058597569465637205, AUC: 0.6654405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005425869941711426, AUC: 0.6848345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005059855222702026, AUC: 0.6937945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004795619487762451, AUC: 0.6962945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06521325874328614, AUC: 0.496502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005880830764770507, AUC: 0.6449625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005021943807601929, AUC: 0.6853739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004584294557571411, AUC: 0.7033535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004236081600189209, AUC: 0.7210295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004052584171295166, AUC: 0.726509\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03302265167236328, AUC: 0.6779765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008056613206863404, AUC: 0.6163064999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006708371639251709, AUC: 0.6541275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005907225847244263, AUC: 0.6725015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0053672902584075925, AUC: 0.6867084999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005008548259735107, AUC: 0.6959120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02510342025756836, AUC: 0.437044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00819556450843811, AUC: 0.6189515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006749381065368652, AUC: 0.66348\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005857798099517822, AUC: 0.689215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005305838346481323, AUC: 0.7012705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004908576250076294, AUC: 0.712486\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03527671432495117, AUC: 0.469998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006740056037902832, AUC: 0.6412144999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005747252225875855, AUC: 0.673505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005135333299636841, AUC: 0.6923315000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004730107069015503, AUC: 0.7044555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004343687772750854, AUC: 0.7225235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05695119094848633, AUC: 0.489482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006755156755447388, AUC: 0.6555235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006006895303726196, AUC: 0.6889259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005550761222839356, AUC: 0.707958\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005271525382995606, AUC: 0.7151035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004970269441604614, AUC: 0.7275585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03241660690307617, AUC: 0.4092784999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00679035210609436, AUC: 0.5966905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005556597948074341, AUC: 0.6502185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004986517429351807, AUC: 0.668771\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004536568164825439, AUC: 0.687265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004238332867622376, AUC: 0.6975185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07690467071533204, AUC: 0.5481245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007189604520797729, AUC: 0.6263525000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005837963342666626, AUC: 0.671399\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005167591333389282, AUC: 0.690239\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004737916231155395, AUC: 0.7095885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004452943086624145, AUC: 0.717289\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04281192970275879, AUC: 0.6854589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005376289606094361, AUC: 0.69286\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004935477256774902, AUC: 0.6974404999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0046190981864929195, AUC: 0.701398\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004347802639007569, AUC: 0.710831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004165374994277954, AUC: 0.724592\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037264318466186525, AUC: 0.46985200000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007001612424850464, AUC: 0.6255970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005889024257659912, AUC: 0.65871\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005182584524154663, AUC: 0.6880655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004839876413345337, AUC: 0.709438\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004508289813995361, AUC: 0.7110885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08677769088745117, AUC: 0.4940065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012643664360046387, AUC: 0.481959\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011174339771270752, AUC: 0.487456\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010117609024047852, AUC: 0.49497100000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009379085540771484, AUC: 0.5045115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008873297214508056, AUC: 0.5156135000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06006782913208008, AUC: 0.4920295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012692314624786377, AUC: 0.47502900000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011228451728820801, AUC: 0.497037\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009975229263305664, AUC: 0.5201425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008923868179321288, AUC: 0.5453844999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008074443817138672, AUC: 0.5665915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05857536888122559, AUC: 0.5912029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011898754596710205, AUC: 0.48258699999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011052810192108154, AUC: 0.4948835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0103179292678833, AUC: 0.5095025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009677209377288819, AUC: 0.5218625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00911921215057373, AUC: 0.5348634999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05352071189880371, AUC: 0.598757\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012488711833953857, AUC: 0.424217\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01170066499710083, AUC: 0.44102600000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011027633190155029, AUC: 0.45559550000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010451643466949464, AUC: 0.469486\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.009970030784606933, AUC: 0.48227600000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04632254409790039, AUC: 0.47904500000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012526666164398194, AUC: 0.582388\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010988277435302734, AUC: 0.594391\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009710521221160888, AUC: 0.6040715000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008653043270111084, AUC: 0.617047\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007799698352813721, AUC: 0.630919\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06196899604797363, AUC: 0.5754319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0074221229553222655, AUC: 0.6066335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006804392099380493, AUC: 0.6222065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006396243810653687, AUC: 0.6348889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0061254088878631595, AUC: 0.644071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005921830415725708, AUC: 0.6511915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03369696807861328, AUC: 0.4749055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00892920446395874, AUC: 0.5917525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008496898174285889, AUC: 0.6023225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008138007640838623, AUC: 0.6102875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007827154636383057, AUC: 0.6199839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007557638168334961, AUC: 0.6271185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012355064392089844, AUC: 0.5283694999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007983857154846192, AUC: 0.603625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007602840900421143, AUC: 0.614123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007299638271331787, AUC: 0.621059\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007053744077682495, AUC: 0.6290285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006850150108337403, AUC: 0.635749\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.060917991638183594, AUC: 0.472333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0157938551902771, AUC: 0.4101605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01407738208770752, AUC: 0.430356\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01261007308959961, AUC: 0.45122450000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011387672901153565, AUC: 0.471987\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010373607158660888, AUC: 0.49368550000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014930389881134033, AUC: 0.610955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01193277883529663, AUC: 0.5730755000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011529846668243408, AUC: 0.5762555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011174687385559082, AUC: 0.5780865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010852728366851807, AUC: 0.579813\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010556648254394532, AUC: 0.5834025\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 Class SMOTE \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c104725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Class SMOTE with capped loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['smote_labels'] =  \n",
    "loss_fn_args['loss_cap'] = 1e-3\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5b7ce280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.021182982444763184, AUC: 0.672261\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02040205478668213, AUC: 0.784783\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008496701717376709, AUC: 0.7658635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010849761486053466, AUC: 0.7865255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014077957153320313, AUC: 0.8000940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01160053539276123, AUC: 0.7966500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01782694339752197, AUC: 0.5028665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023290175437927246, AUC: 0.7743450000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020170438766479493, AUC: 0.7914720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0160395770072937, AUC: 0.780689\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0165785551071167, AUC: 0.7878360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015160138607025147, AUC: 0.78366\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05102617263793945, AUC: 0.5758955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02155221652984619, AUC: 0.765247\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008215628385543823, AUC: 0.7573829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010915471076965331, AUC: 0.7642705000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013713106155395508, AUC: 0.7941985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017176090240478517, AUC: 0.80308\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.035037864685058595, AUC: 0.46588599999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018102649688720704, AUC: 0.7682129999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012711784839630126, AUC: 0.7860095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027279739379882813, AUC: 0.8116869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019777530670166014, AUC: 0.8119015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015880674362182618, AUC: 0.804704\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0447655029296875, AUC: 0.473485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020135549545288085, AUC: 0.7631255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013973820209503173, AUC: 0.777613\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015478405952453613, AUC: 0.7848799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018580872535705565, AUC: 0.797552\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01238748550415039, AUC: 0.781376\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016568662643432618, AUC: 0.5951505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016172502994537354, AUC: 0.764655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016868967056274414, AUC: 0.802041\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027323065757751466, AUC: 0.805863\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016519526481628417, AUC: 0.800784\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011068009376525878, AUC: 0.789434\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.032380596160888675, AUC: 0.6301640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019904308319091797, AUC: 0.7465435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020391210556030274, AUC: 0.779659\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013272620677947999, AUC: 0.779018\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.031978708267211914, AUC: 0.7883974999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013876071453094482, AUC: 0.7823005000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05520010185241699, AUC: 0.481528\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025602606773376464, AUC: 0.7477785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020315832138061523, AUC: 0.7755639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01479287052154541, AUC: 0.7655255000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020112116813659667, AUC: 0.7873415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017014772415161132, AUC: 0.7848020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029715638160705566, AUC: 0.6657489999999999\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [104]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mtrain_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[0;32m---> 22\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m \u001b[43mmetric_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauc_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m         model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     24\u001b[0m aucs\u001b[38;5;241m.\u001b[39mappend(model_aucs)\n",
      "File \u001b[0;32m~/Downloads/ML/numbers mnist/metric_utils.py:16\u001b[0m, in \u001b[0;36mauc_sigmoid\u001b[0;34m(test_loader, network)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauc_sigmoid\u001b[39m(test_loader, network):\n\u001b[1;32m     15\u001b[0m     test_losses, y_preds, y_true \u001b[38;5;241m=\u001b[39m inference\u001b[38;5;241m.\u001b[39mrun_inference_sigmoid(test_loader, network)\n\u001b[0;32m---> 16\u001b[0m     network_auc \u001b[38;5;241m=\u001b[39m \u001b[43mauc\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTest set: Avg. loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnetwork_auc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)   \n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m test_losses, network_auc\n",
      "File \u001b[0;32m~/Downloads/ML/numbers mnist/metric_utils.py:55\u001b[0m, in \u001b[0;36mauc\u001b[0;34m(y_pred, y_true)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauc\u001b[39m(y_pred, y_true):\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:572\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    570\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true)\n\u001b[1;32m    571\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m label_binarize(y_true, classes\u001b[38;5;241m=\u001b[39mlabels)[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_average_binary_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_binary_roc_auc_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_fpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_fpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multilabel-indicator\u001b[39;00m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[1;32m    581\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[38;5;241m=\u001b[39mmax_fpr),\n\u001b[1;32m    582\u001b[0m         y_true,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    585\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    586\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_base.py:75\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m     78\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:344\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_true)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    340\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not defined in that case.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    342\u001b[0m     )\n\u001b[0;32m--> 344\u001b[0m fpr, tpr, _ \u001b[38;5;241m=\u001b[39m \u001b[43mroc_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_fpr \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m auc(fpr, tpr)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:992\u001b[0m, in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mroc_curve\u001b[39m(\n\u001b[1;32m    905\u001b[0m     y_true, y_score, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    906\u001b[0m ):\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;124;03m    array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 992\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:772\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    769\u001b[0m y_true \u001b[38;5;241m=\u001b[39m y_true \u001b[38;5;241m==\u001b[39m pos_label\n\u001b[1;32m    771\u001b[0m \u001b[38;5;66;03m# sort scores and corresponding truth values\u001b[39;00m\n\u001b[0;32m--> 772\u001b[0m desc_score_indices \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmergesort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    773\u001b[0m y_score \u001b[38;5;241m=\u001b[39m y_score[desc_score_indices]\n\u001b[1;32m    774\u001b[0m y_true \u001b[38;5;241m=\u001b[39m y_true[desc_score_indices]\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:1114\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argsort_dispatcher)\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margsort\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;124;03m    Returns the indices that would sort an array.\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \n\u001b[1;32m   1113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margsort\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2 class effective # of samples \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['pos_weight'] = effective_weights\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SigmoidLogisticRegression(2, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"effective_samples\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a5d929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES_REDUCED = 3\n",
    "nums = (0, 3, 1)\n",
    "ratio = (200, 20, 1)\n",
    "\n",
    "\n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums)\n",
    "targets = ratio_train_CIFAR10.labels \n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10)\n",
    "\n",
    "\n",
    "weight = 1. / class_count\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= max(class_count)\n",
    "\n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49887493",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.029016796747843424, AUC: 0.512664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12362478892008463, AUC: 0.5425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0237501106262207, AUC: 0.6890000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028362667719523112, AUC: 0.661\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027204696655273437, AUC: 0.68075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.057541033426920574, AUC: 0.55625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.033389368693033855, AUC: 0.5614167499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07811309560139974, AUC: 0.52025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04119581731160482, AUC: 0.598\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05999888737996419, AUC: 0.5587500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.061533856709798176, AUC: 0.6505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.039205940246582034, AUC: 0.7025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10757938893636068, AUC: 0.54775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.052771939595540364, AUC: 0.65775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06673320007324218, AUC: 0.62325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07701803588867187, AUC: 0.5222500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02959756088256836, AUC: 0.74325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015836956024169922, AUC: 0.76525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.045948589324951175, AUC: 0.52168575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04256668345133464, AUC: 0.5929487499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01912473487854004, AUC: 0.781\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04891455078125, AUC: 0.57745775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08464574432373047, AUC: 0.51474175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017022288004557292, AUC: 0.75075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08520065307617188, AUC: 0.5011925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016303669611612955, AUC: 0.77875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03158966128031413, AUC: 0.6487499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026286234537760418, AUC: 0.7505000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.032823240280151364, AUC: 0.72075\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mtrain_softmax(epoch, train_loader_reduced, network, optimizer, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[0;32m---> 18\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m \u001b[43mmetric_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauc_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m         model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     20\u001b[0m aucs\u001b[38;5;241m.\u001b[39mappend(model_aucs)\n",
      "File \u001b[0;32m~/Downloads/ml/metric_utils.py:24\u001b[0m, in \u001b[0;36mauc_softmax\u001b[0;34m(test_loader, network, average)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauc_softmax\u001b[39m(test_loader, network, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 24\u001b[0m     test_losses, y_preds, y_true \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_inference_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m average: \n\u001b[1;32m     27\u001b[0m         network_auc \u001b[38;5;241m=\u001b[39m auc(np\u001b[38;5;241m.\u001b[39masarray(y_preds), np\u001b[38;5;241m.\u001b[39masarray(y_true))\n",
      "File \u001b[0;32m~/Downloads/ml/inference.py:42\u001b[0m, in \u001b[0;36mrun_inference_softmax\u001b[0;34m(dataloader, network)\u001b[0m\n\u001b[1;32m     39\u001b[0m loss_fn\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     43\u001b[0m         output \u001b[38;5;241m=\u001b[39m network(data)\n\u001b[1;32m     44\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze(), target)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Downloads/ml/class_sampling.py:27\u001b[0m, in \u001b[0;36mReduce.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index): \n\u001b[0;32m---> 27\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[index]\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnums[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m     29\u001b[0m         label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Softmax 3 class normal\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 3, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4386e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 3 class ratio\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21e15c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.01764530849456787, AUC: 0.5389999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.035015865325927736, AUC: 0.6815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05701504135131836, AUC: 0.611\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0557035026550293, AUC: 0.6060000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06728232192993164, AUC: 0.5825\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 16\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_oversampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     18\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_softmax(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/ML/numbers mnist/train.py:39\u001b[0m, in \u001b[0;36mtrain_softmax\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     35\u001b[0m network\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     37\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m loss_fn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_fn_args)\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     41\u001b[0m     output \u001b[38;5;241m=\u001b[39m network(data)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Downloads/ML/numbers mnist/class_sampling.py:166\u001b[0m, in \u001b[0;36mRatio.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    164\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[index]\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    165\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[index]\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (image, label)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Softmax 3 class oversampled\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f9856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 3 class undersampled\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f39e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 3 class both sampled\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_sampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"both_sampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e12075c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Softmax 3 class weighted\n",
    "\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['weight'] = torch.from_numpy(class_weights).float()\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ba2dbda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.021192817052205402, AUC: 0.4998545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 46.84014453125, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 8.155067708333334, AUC: 0.50975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 25.784237630208334, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 21.013444010416666, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2271815592447917, AUC: 0.647511\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02458227729797363, AUC: 0.4927750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2777367350260416, AUC: 0.53232075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 20.985467447916665, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.506778076171875, AUC: 0.643527\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.30812060546875, AUC: 0.5106235000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.1348290201822917, AUC: 0.6364495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08998995463053386, AUC: 0.59325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.030626708984375, AUC: 0.6094999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 12.6286962890625, AUC: 0.50175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0780894165039063, AUC: 0.6932820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.491607259114583, AUC: 0.5820805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2718911539713542, AUC: 0.69899825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.14895314534505208, AUC: 0.49148499999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 8.128333821614584, AUC: 0.5157499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 12.437986979166666, AUC: 0.5025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.4756564127604166, AUC: 0.717633\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.6250325520833333, AUC: 0.53164825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.445370849609375, AUC: 0.7216492499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.24434510294596354, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.501394775390625, AUC: 0.528\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0723311767578125, AUC: 0.6817025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 5.111925618489583, AUC: 0.5551429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9751982218424479, AUC: 0.628667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5948148600260417, AUC: 0.6967500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1107143783569336, AUC: 0.50225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 5.00222900390625, AUC: 0.543005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2906629638671876, AUC: 0.6914534999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9380587565104166, AUC: 0.67525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.5187647298177085, AUC: 0.6625745000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 5.467151529947917, AUC: 0.545837\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07171204376220704, AUC: 0.52047225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 27.494795572916665, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 7.193567545572916, AUC: 0.5185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0869657389322918, AUC: 0.642\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.423319498697917, AUC: 0.5915157499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7117761637369793, AUC: 0.6792792500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04207762145996094, AUC: 0.500425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 8.4980703125, AUC: 0.51\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 15.701685546875, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 6.037508463541666, AUC: 0.5488085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.8077643636067708, AUC: 0.70114725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.922751220703125, AUC: 0.603\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04097941716512044, AUC: 0.47083375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0450133666992187, AUC: 0.7240815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 11.096056966145833, AUC: 0.50625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.051478108723958, AUC: 0.5827595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 5.817074381510417, AUC: 0.550567\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 10.4897646484375, AUC: 0.504752\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09930953979492188, AUC: 0.5034959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 34.450868489583335, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1388672688802084, AUC: 0.6732635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.4114529622395833, AUC: 0.7160985000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 6.293673014322916, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 36.87110026041667, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.037793074289957686, AUC: 0.48462250000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3540452880859375, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3145724792480469, AUC: 0.6198985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.36261680094401044, AUC: 0.5792499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12749126434326172, AUC: 0.70626425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.21676692199707032, AUC: 0.67283\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08127295939127605, AUC: 0.5062575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.27919673665364586, AUC: 0.6422499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.15902405802408853, AUC: 0.7385577499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1142738037109375, AUC: 0.6705000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8422886149088542, AUC: 0.5247910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.15665141805013022, AUC: 0.691113\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05450002415974935, AUC: 0.39338324999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3671424153645833, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9676664021809895, AUC: 0.5015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7806502888997395, AUC: 0.51100225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10764247131347657, AUC: 0.6764999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.577374267578125, AUC: 0.5395497499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026185598373413086, AUC: 0.46217299999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9290351969401042, AUC: 0.5075019999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.090196573893229, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2045401916503906, AUC: 0.6520429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.28356629435221353, AUC: 0.6530874999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.19728182983398437, AUC: 0.6718775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08445564270019532, AUC: 0.54144775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11325121561686198, AUC: 0.6522437499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.26078597513834634, AUC: 0.6247704999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.38926666259765624, AUC: 0.575021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.4670145670572916, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6912386067708334, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.044293448130289716, AUC: 0.48104850000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9828107096354166, AUC: 0.5150015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.23005826314290365, AUC: 0.6325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.40874073282877604, AUC: 0.5984750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1948823954264323, AUC: 0.69143025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6905738118489584, AUC: 0.52899975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05573761113484701, AUC: 0.46658999999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12172350056966146, AUC: 0.70915025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9143028767903646, AUC: 0.506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2429325256347656, AUC: 0.41602999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.47329645792643227, AUC: 0.56684825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.23848624674479166, AUC: 0.6725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.047076778411865236, AUC: 0.5899632499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1496147715250651, AUC: 0.6945275000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1879471842447917, AUC: 0.50325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5524504597981771, AUC: 0.534508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0095260416666667, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.40493123372395834, AUC: 0.5045000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.13502986653645832, AUC: 0.48951325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1057191162109375, AUC: 0.6347499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.24802846272786458, AUC: 0.6550279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9816376546223958, AUC: 0.50225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7135444742838541, AUC: 0.5289999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.099995361328125, AUC: 0.507254\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.15307223510742188, AUC: 0.484\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.13480514526367188, AUC: 0.5724184999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10733706665039063, AUC: 0.7054652500000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3115762736002604, AUC: 0.6078539999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.43698333740234374, AUC: 0.5918785000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5397029622395834, AUC: 0.549044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04595521672566732, AUC: 0.44006325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5078662821451823, AUC: 0.6986425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 9.074680013020833, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.6093087565104165, AUC: 0.5605904999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6071248372395834, AUC: 0.636235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.6676982421875, AUC: 0.5727829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.038479740142822266, AUC: 0.51309325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 7.242147786458333, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5386299235026042, AUC: 0.598693\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.268547607421875, AUC: 0.5926210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7823256632486979, AUC: 0.6958070000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 7.18076171875, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07918470764160156, AUC: 0.5110365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.112271240234375, AUC: 0.5082614999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2092791341145834, AUC: 0.579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 13.943452473958333, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6215241902669271, AUC: 0.6501015\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 4.013204427083333, AUC: 0.5222725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05618905766805013, AUC: 0.5266975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 11.043680338541666, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5850803629557292, AUC: 0.72115925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.296892008463542, AUC: 0.519765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 22.896383463541667, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 14.719406901041667, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021946542739868163, AUC: 0.47858825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 6.2800416666666665, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7451414184570313, AUC: 0.630284\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.704031005859375, AUC: 0.6459999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0209950561523438, AUC: 0.675238\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.766576822916667, AUC: 0.51050025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07846341705322266, AUC: 0.49882400000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5466991780598958, AUC: 0.61453425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.214802490234375, AUC: 0.5685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1017816569010415, AUC: 0.5665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1978457845052084, AUC: 0.55625975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5299435221354166, AUC: 0.637226\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02142019271850586, AUC: 0.42497175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6787865193684895, AUC: 0.684\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1497478841145834, AUC: 0.56500975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 7.1270146484375, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9789656168619791, AUC: 0.68747475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8073821614583333, AUC: 0.6910972500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.038619196573893226, AUC: 0.45053675000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3151239013671876, AUC: 0.6217499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6895452677408854, AUC: 0.70154025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.399286173502604, AUC: 0.62475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 12.360550130208333, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 11.9848828125, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023271150588989258, AUC: 0.5157235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5367782084147136, AUC: 0.7042499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2816129557291667, AUC: 0.51275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 6.105809733072917, AUC: 0.50175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6971309407552083, AUC: 0.70025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6666297403971354, AUC: 0.60975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02211925252278646, AUC: 0.519517\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.79904541015625, AUC: 0.50225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.4444411214192707, AUC: 0.60635725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.633792724609375, AUC: 0.53902\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5110081787109375, AUC: 0.6256384999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9236122233072916, AUC: 0.612698\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023204755147298178, AUC: 0.5108715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.19174552408854167, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024724646886189777, AUC: 0.6657200000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04297781880696615, AUC: 0.58022375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012708748817443848, AUC: 0.6713555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07528960164388021, AUC: 0.526262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04589464823404948, AUC: 0.49674900000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025040302276611328, AUC: 0.634513\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02539968744913737, AUC: 0.530715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.040034079233805336, AUC: 0.5128455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01222857920328776, AUC: 0.6650167499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.20083318583170573, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03554845682779948, AUC: 0.465748\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.049210609436035155, AUC: 0.565192\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10917167409261068, AUC: 0.5035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04838481903076172, AUC: 0.56150425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08158755747477213, AUC: 0.519015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05467021179199219, AUC: 0.553011\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.049264046986897785, AUC: 0.493\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2686650899251302, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.059089239756266274, AUC: 0.5320260000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03451568857828776, AUC: 0.6367197499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.048834644317626956, AUC: 0.638976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.043664618174235025, AUC: 0.58484825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016033717473347982, AUC: 0.43723749999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04618967819213867, AUC: 0.5772295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06294453811645508, AUC: 0.52579725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05149462509155273, AUC: 0.5654514999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10370457712809245, AUC: 0.50674775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02021983273824056, AUC: 0.67594425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05548513921101888, AUC: 0.53823325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04170655568440755, AUC: 0.5758119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05594320297241211, AUC: 0.5772595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04975010426839193, AUC: 0.56142925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.030662909189860026, AUC: 0.6174682499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12916983032226562, AUC: 0.5025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06925718943277995, AUC: 0.5377292499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0831977310180664, AUC: 0.52817875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021723331451416014, AUC: 0.6825915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2696484171549479, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0403823127746582, AUC: 0.5844175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018070940017700196, AUC: 0.7021872499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028865896224975585, AUC: 0.514883\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08007303873697917, AUC: 0.51824625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04815326817830404, AUC: 0.54977275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06066883850097656, AUC: 0.540522\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09531778462727865, AUC: 0.50875425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08533423868815104, AUC: 0.5137545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0608027712504069, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0309768435160319, AUC: 0.6131365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024249454498291016, AUC: 0.656868\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.19622225952148437, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1307867431640625, AUC: 0.503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01603282356262207, AUC: 0.68774925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09843619537353515, AUC: 0.4673675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028253854115804035, AUC: 0.610027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.031853862762451175, AUC: 0.66392375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01618173821767171, AUC: 0.6829864999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.049027599334716794, AUC: 0.5820690000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0231590576171875, AUC: 0.6596869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01901871109008789, AUC: 0.493285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.13884280395507811, AUC: 0.61325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.056182828267415365, AUC: 0.6777699999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.372990966796875, AUC: 0.5190049999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0619918696085612, AUC: 0.7104395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2408195292154948, AUC: 0.58095275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0560741704305013, AUC: 0.52186625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.36178614298502604, AUC: 0.5120164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.25597318522135415, AUC: 0.5585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.15160066731770833, AUC: 0.687235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1885712178548177, AUC: 0.6253925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.15891397603352864, AUC: 0.6120365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03065041478474935, AUC: 0.4962785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2990591328938802, AUC: 0.5380014999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05851965840657552, AUC: 0.70025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.57766064453125, AUC: 0.50075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3895089619954427, AUC: 0.50725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.22356112162272135, AUC: 0.580412\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.054496622721354165, AUC: 0.5608812500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06462470499674479, AUC: 0.699218\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0086091715494792, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.20711615498860678, AUC: 0.574322\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05779354476928711, AUC: 0.6971995000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05228090031941732, AUC: 0.66305825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023991107940673828, AUC: 0.4058225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1390980224609375, AUC: 0.6576065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2740536295572917, AUC: 0.57502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1612003224690755, AUC: 0.6364955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8085103352864583, AUC: 0.50075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06486537679036458, AUC: 0.607\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06012518946329753, AUC: 0.49234425000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.26969673665364585, AUC: 0.524542\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7061895548502605, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04650724156697591, AUC: 0.663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.26628185017903644, AUC: 0.552902\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 1.4937379557291666, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022666733423868816, AUC: 0.44328199999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.29379188028971354, AUC: 0.51675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.17841273498535157, AUC: 0.626\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3687196756998698, AUC: 0.517506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06367534001668294, AUC: 0.6262449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7151102701822917, AUC: 0.50125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01522603702545166, AUC: 0.5071755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.042757946014404295, AUC: 0.6859999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2339578603108724, AUC: 0.56085725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04630814615885417, AUC: 0.69365275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.279176350911458, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08832268524169921, AUC: 0.56188075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020447699228922527, AUC: 0.38424650000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.15732760111490884, AUC: 0.614\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.620976786295573, AUC: 0.50375025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8289298095703125, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5019064229329427, AUC: 0.50875025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.864135009765625, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017902876536051434, AUC: 0.4233085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05976816813151042, AUC: 0.7221515000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2535540669759115, AUC: 0.54161275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.19201712036132812, AUC: 0.6250979999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6766991170247396, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08720833587646484, AUC: 0.68613975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015638343175252277, AUC: 0.5344152499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014036879221598308, AUC: 0.6265547499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011168757438659667, AUC: 0.6350210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02093135452270508, AUC: 0.5706567499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01101641050974528, AUC: 0.6285465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011041500091552735, AUC: 0.624365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2094457244873047, AUC: 0.4975015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021312422434488932, AUC: 0.5853875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011772420247395833, AUC: 0.6210829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017282217025756837, AUC: 0.5760584999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0178712100982666, AUC: 0.56595875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015986121813456218, AUC: 0.5726155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08195469919840495, AUC: 0.48123400000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02264773368835449, AUC: 0.5734899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014171643257141113, AUC: 0.6100380000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01573676331837972, AUC: 0.58838625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008954978942871094, AUC: 0.641441\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009617403348286946, AUC: 0.6277894999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1578870137532552, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0182611935933431, AUC: 0.5947790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016056677182515463, AUC: 0.6035012499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01451526705423991, AUC: 0.6098105000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008255361557006836, AUC: 0.6429285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014341296831766765, AUC: 0.60886925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03144806289672852, AUC: 0.5429999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02484320386250814, AUC: 0.5746875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013818256696065267, AUC: 0.6259172500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011178727467854819, AUC: 0.6312617500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014461545944213868, AUC: 0.598824\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011173666954040528, AUC: 0.61941325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.036930343627929685, AUC: 0.46921525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020720506668090822, AUC: 0.58401775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015029392878214518, AUC: 0.601217\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015457411766052247, AUC: 0.587885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009928135871887208, AUC: 0.6310457500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018628583272298178, AUC: 0.560102\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11387761688232421, AUC: 0.480605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019767464955647785, AUC: 0.599165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015446702639261881, AUC: 0.607254\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013970800399780273, AUC: 0.606818\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01081461238861084, AUC: 0.627263\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007858184019724528, AUC: 0.6457495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01625444825490316, AUC: 0.54892125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024303959528605142, AUC: 0.5643009999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014902332623799641, AUC: 0.603325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012632748285929363, AUC: 0.6151735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019350898106892903, AUC: 0.5663725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01168148136138916, AUC: 0.61633775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04129976908365885, AUC: 0.552367\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013825926780700683, AUC: 0.6241745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01494932492574056, AUC: 0.6031564999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014214518547058106, AUC: 0.6006184999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00850426165262858, AUC: 0.64504575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017528592427571616, AUC: 0.569079\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09178609720865885, AUC: 0.4188355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019297480901082357, AUC: 0.5928770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008665713946024577, AUC: 0.61268325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013338961919148762, AUC: 0.6097017499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014235228538513183, AUC: 0.5939697500000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010566569646199545, AUC: 0.6157425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08782530212402344, AUC: 0.55475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023004559199015298, AUC: 0.5802319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01810068639119466, AUC: 0.5905790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013578881581624349, AUC: 0.6171125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015122029940287272, AUC: 0.598087\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013019805590311686, AUC: 0.611756\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07794708251953125, AUC: 0.4769845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018196477890014648, AUC: 0.5947562500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015144651412963867, AUC: 0.6020562500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016369089762369792, AUC: 0.5888487499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013561383247375488, AUC: 0.60565225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015086743354797363, AUC: 0.588318\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05759501393636068, AUC: 0.57164775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01693520927429199, AUC: 0.597472\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01986793327331543, AUC: 0.5814362500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014717295328776041, AUC: 0.6032620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014207910537719727, AUC: 0.603251\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013990990320841471, AUC: 0.60523625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02636221122741699, AUC: 0.48592175000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02529040273030599, AUC: 0.574794\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017443474451700847, AUC: 0.59534975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014492831230163573, AUC: 0.6088575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014318856557210286, AUC: 0.6123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013520306269327799, AUC: 0.6147665000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06384498850504557, AUC: 0.501996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020091037114461262, AUC: 0.58645825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017143254597981772, AUC: 0.5948720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014573551813761394, AUC: 0.603695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020435789744059244, AUC: 0.566723\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013951380411783855, AUC: 0.60176575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08199847666422526, AUC: 0.43327875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019167744318644205, AUC: 0.59508375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019938642501831055, AUC: 0.5791149999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012370995203653971, AUC: 0.6221445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016518167813618978, AUC: 0.58597\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013525412877400717, AUC: 0.60237875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09063881174723307, AUC: 0.5235000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014975472132364909, AUC: 0.6037795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014541528701782226, AUC: 0.5996205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012209501584370931, AUC: 0.6104235000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01162519931793213, AUC: 0.612835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012676394780476889, AUC: 0.5999424999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08323400115966798, AUC: 0.48067950000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022062887191772462, AUC: 0.57482675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01934793472290039, AUC: 0.5830749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01823687489827474, AUC: 0.5819939999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02035928535461426, AUC: 0.57279975\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.013784931182861329, AUC: 0.6046385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07315342712402344, AUC: 0.4541855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015412540117899576, AUC: 0.6071667500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016331341425577798, AUC: 0.6053122500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017100783665974936, AUC: 0.598764\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013548744519551596, AUC: 0.60973675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014062112808227539, AUC: 0.59762775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08766093953450521, AUC: 0.5285445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02332563845316569, AUC: 0.5734159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020291112899780275, AUC: 0.58406425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012171136220296225, AUC: 0.619581\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013312102635701498, AUC: 0.6095742500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01814821434020996, AUC: 0.585245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Softmax 3 class focal loss \n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 5e-4, 1e-5, 5e-5, 1e-6, 5e-7]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['reduction'] = 'mean'\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(NUM_CLASSES_REDUCED, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SoftmaxFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b1b8a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.07237802886962891, AUC: 0.5750774999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05649503326416016, AUC: 0.664074\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06258813603719075, AUC: 0.6244999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03302341016133626, AUC: 0.7087694999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03082382583618164, AUC: 0.6707339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.055621556599934896, AUC: 0.6383584999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1472303721110026, AUC: 0.4675225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11613861083984375, AUC: 0.4162235000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05096723047892253, AUC: 0.6523117500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05036033884684245, AUC: 0.7254320000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0955191650390625, AUC: 0.59844\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02813099733988444, AUC: 0.73565575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05136255137125651, AUC: 0.4657285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04113990656534831, AUC: 0.74175375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.040553648630778, AUC: 0.54446825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04734367879231771, AUC: 0.7135630000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1302681579589844, AUC: 0.6496804999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.11105381520589193, AUC: 0.5859787499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.17031331380208334, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.040066683451334635, AUC: 0.5751687500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10966723887125651, AUC: 0.6960455000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10328226470947266, AUC: 0.601564\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027716858545939128, AUC: 0.7319885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.032469216028849286, AUC: 0.6023955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028677059809366863, AUC: 0.465213\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1996075236002604, AUC: 0.60557325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.043339293162027996, AUC: 0.7217215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.082706423441569, AUC: 0.7233499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03427162806193034, AUC: 0.7210535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023427788416544595, AUC: 0.67669275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1074490966796875, AUC: 0.52006225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1241040776570638, AUC: 0.659049\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019890445073445638, AUC: 0.71488525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1593479461669922, AUC: 0.5692\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05912483469645182, AUC: 0.626932\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05294607035319011, AUC: 0.53850875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05954407246907552, AUC: 0.5636585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07350287882486979, AUC: 0.66034425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026789944966634116, AUC: 0.71169\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.029737329483032225, AUC: 0.7252314999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03351861190795898, AUC: 0.7342517500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024240870793660482, AUC: 0.7219507500000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07110960388183593, AUC: 0.44936899999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04148624674479166, AUC: 0.538595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08633831787109375, AUC: 0.722021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02872608248392741, AUC: 0.696828\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.028842461268107095, AUC: 0.72050975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024289936701456707, AUC: 0.698985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.054243563334147135, AUC: 0.5047849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06781107076009114, AUC: 0.48578800000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05305246988932292, AUC: 0.5435114999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04443963114420573, AUC: 0.56322775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03245640563964844, AUC: 0.7368465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03479707845052083, AUC: 0.7224375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.14239719645182292, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020818394978841148, AUC: 0.6965855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06370979181925455, AUC: 0.53133875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.12192725372314453, AUC: 0.5556595000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04685245895385742, AUC: 0.72565325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03350884183247884, AUC: 0.5957835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08177729034423828, AUC: 0.5610875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01230893103281657, AUC: 0.6835485000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0074126429557800294, AUC: 0.675367\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0055433923403422035, AUC: 0.6680765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004872041702270508, AUC: 0.6761962499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007485327243804932, AUC: 0.7011879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09512309010823568, AUC: 0.4149850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013890148162841797, AUC: 0.54616325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008038125038146972, AUC: 0.6821405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005401466210683187, AUC: 0.671276\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006130077362060547, AUC: 0.6215679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006220274130503337, AUC: 0.7038117500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0578023198445638, AUC: 0.5812280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014325414975484212, AUC: 0.6123565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008279226620992024, AUC: 0.64153475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007587937196095785, AUC: 0.6800554999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006035931428273519, AUC: 0.672609\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005817000230153402, AUC: 0.7008665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09527760060628256, AUC: 0.4073859999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00756710958480835, AUC: 0.6802745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006716414292653402, AUC: 0.6836025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007824865023295085, AUC: 0.6695805000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007885607083638509, AUC: 0.7100675000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005781257152557373, AUC: 0.69934275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10232300059000651, AUC: 0.46593100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010444589932759603, AUC: 0.64101725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013625558853149413, AUC: 0.6868502500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006343451340993246, AUC: 0.6786177499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006276561260223389, AUC: 0.674194\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006248976866404216, AUC: 0.7039545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03181565284729004, AUC: 0.4795995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015106796582539876, AUC: 0.603986\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008673968315124512, AUC: 0.676716\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006306803226470948, AUC: 0.7124360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011152089754740397, AUC: 0.6658075000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005162662506103516, AUC: 0.7073170000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08281566111246745, AUC: 0.5119999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008998674392700195, AUC: 0.6799040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007396859010060628, AUC: 0.620689\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006178381443023681, AUC: 0.65778125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01020684560139974, AUC: 0.6843239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005586979389190674, AUC: 0.67558325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013922775268554687, AUC: 0.5145685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00966865317026774, AUC: 0.694841\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009225232760111491, AUC: 0.69591325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005038081169128418, AUC: 0.68121825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006131968816121419, AUC: 0.7109285000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00788427464167277, AUC: 0.70518425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.034872767130533854, AUC: 0.5271015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009854930877685547, AUC: 0.63625225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012530004501342774, AUC: 0.545984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00825352923075358, AUC: 0.6880619999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011729859034220377, AUC: 0.6922482499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012271151224772136, AUC: 0.51995025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04983663558959961, AUC: 0.500996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012939532597859701, AUC: 0.6790642500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0063826540311177574, AUC: 0.6908155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0062926309903462724, AUC: 0.6200367499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009281212170918783, AUC: 0.6918610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008354426383972168, AUC: 0.67692625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1519547373453776, AUC: 0.46725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010201784133911132, AUC: 0.64767\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010786798159281413, AUC: 0.6749449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008084708054860433, AUC: 0.66146825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007387070337931315, AUC: 0.6802062499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006433963775634766, AUC: 0.65844875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09670623779296875, AUC: 0.49323175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011899009704589843, AUC: 0.6921315000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01019191869099935, AUC: 0.6928384999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00961746597290039, AUC: 0.69916625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006200355211893717, AUC: 0.6905479999999999\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.007336761792500814, AUC: 0.6777925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.027241098403930665, AUC: 0.47561549999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011156509399414063, AUC: 0.6355157499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008739068984985351, AUC: 0.6953795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011028128623962402, AUC: 0.667353\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007935586452484131, AUC: 0.6926224999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006317375183105468, AUC: 0.7053652499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06409891764322917, AUC: 0.559998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011135673522949219, AUC: 0.683471\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009395796775817871, AUC: 0.6666819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011101839065551757, AUC: 0.66575925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009117230415344239, AUC: 0.6930274999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0062185343106587725, AUC: 0.65587025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0923882598876953, AUC: 0.45663925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011025364875793458, AUC: 0.6666175000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011470144589742024, AUC: 0.6873765000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009700278282165527, AUC: 0.6779777499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009441937446594238, AUC: 0.6810215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007708474477132162, AUC: 0.6847205000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07140410614013672, AUC: 0.40655625000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011601394971211751, AUC: 0.6800862500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010070629437764485, AUC: 0.683892\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008701374689737956, AUC: 0.69955525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011382681528727213, AUC: 0.64686925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006152069091796875, AUC: 0.6739145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06865576426188151, AUC: 0.51805075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012079203923543294, AUC: 0.649752\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00977096430460612, AUC: 0.6877834999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009612272580464681, AUC: 0.71495125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006845881938934326, AUC: 0.70822275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007947663148244221, AUC: 0.6495615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.10474769337972005, AUC: 0.4704310000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013109733581542968, AUC: 0.683579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008331416447957356, AUC: 0.673098\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007375285784403483, AUC: 0.6752085000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007149174849192301, AUC: 0.6679489999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006987385114034017, AUC: 0.6923555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06735273742675782, AUC: 0.53631875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013694303194681803, AUC: 0.6681587499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01213077481587728, AUC: 0.7012085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008637311617533366, AUC: 0.6668135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00784645414352417, AUC: 0.6684497500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007003918011983236, AUC: 0.6687072500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1382547098795573, AUC: 0.49200449999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011911595662434896, AUC: 0.6246729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01163492743174235, AUC: 0.6401075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0092626101175944, AUC: 0.66713325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006428608576456706, AUC: 0.659335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0074630476633707685, AUC: 0.6811092500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07312345631917318, AUC: 0.5152255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01775852330525716, AUC: 0.6178795000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015764500935872394, AUC: 0.6387575000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014313474973042806, AUC: 0.648317\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012673606236775716, AUC: 0.65534825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011645930290222168, AUC: 0.66656925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.05916407521565755, AUC: 0.504102\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015455189387003581, AUC: 0.59077475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013607269287109374, AUC: 0.63571025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01251302178700765, AUC: 0.6363740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012091420809427897, AUC: 0.653185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011062459627787272, AUC: 0.6602352499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04592007319132487, AUC: 0.59472925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01581434440612793, AUC: 0.6220332499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014274416287740071, AUC: 0.6324504999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012631422678629557, AUC: 0.6420315000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01181450621287028, AUC: 0.6484789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01126936403910319, AUC: 0.6526894999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03119165547688802, AUC: 0.50046625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020818641662597656, AUC: 0.6013\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017317930221557618, AUC: 0.6201465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014927440325419108, AUC: 0.64026275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013895987828572591, AUC: 0.6407750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013578060468037924, AUC: 0.6460469999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.050660110473632815, AUC: 0.53975525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01728528149922689, AUC: 0.58929675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015019891738891601, AUC: 0.626784\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013449231465657551, AUC: 0.6530535000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013915190696716309, AUC: 0.66708575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013293616930643718, AUC: 0.6751615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.07053558603922526, AUC: 0.5065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01712551244099935, AUC: 0.6530227500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014303717931111654, AUC: 0.678915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012982213656107584, AUC: 0.6612995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013595590909322103, AUC: 0.67441375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011929403305053711, AUC: 0.67803675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0352258046468099, AUC: 0.5538897500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014044598897298177, AUC: 0.6543534999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013815776189168294, AUC: 0.649717\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012871840159098308, AUC: 0.660366\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01166747252146403, AUC: 0.6544350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011724531809488932, AUC: 0.6666660000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09181275939941407, AUC: 0.50305425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01815019671122233, AUC: 0.6298840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015039338747660318, AUC: 0.657995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01324432404836019, AUC: 0.662176\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011767934799194336, AUC: 0.6692845000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011801658948262532, AUC: 0.67741225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03707463582356771, AUC: 0.5023584999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01809743372599284, AUC: 0.646609\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01535313351949056, AUC: 0.65820125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01393668778737386, AUC: 0.65846\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012861165046691894, AUC: 0.6614054999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011375348409016927, AUC: 0.6576424999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03653094736735026, AUC: 0.57525875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015910618782043456, AUC: 0.6524405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0137413272857666, AUC: 0.681643\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01198354689280192, AUC: 0.698758\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010935785293579102, AUC: 0.6939645000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011099060694376628, AUC: 0.6868857500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1070792744954427, AUC: 0.50199975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020499390920003257, AUC: 0.5289135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019841372172037762, AUC: 0.551725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019416621526082357, AUC: 0.5686335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01905104700724284, AUC: 0.581407\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0187668825785319, AUC: 0.588298\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.04803956604003906, AUC: 0.58902325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02094221623738607, AUC: 0.6096237500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019908166249593098, AUC: 0.62271875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01919226328531901, AUC: 0.6293805000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018521387100219725, AUC: 0.635505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018252601623535158, AUC: 0.64260975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.06815577697753906, AUC: 0.411179\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02626196543375651, AUC: 0.45467124999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023799861272176107, AUC: 0.48249649999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02192756207784017, AUC: 0.507425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020434918721516926, AUC: 0.5406035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019342798233032227, AUC: 0.5626410000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.08885225677490234, AUC: 0.5029999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023616839090983072, AUC: 0.46210075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.021774770736694336, AUC: 0.49466625000000003\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.02056900723775228, AUC: 0.51380925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019269681294759113, AUC: 0.53516275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018724326451619467, AUC: 0.5595490000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02669282404581706, AUC: 0.5297499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.022843027114868165, AUC: 0.58343775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02149360720316569, AUC: 0.59860625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020874640782674155, AUC: 0.608769\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02040539042154948, AUC: 0.6225529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02024791399637858, AUC: 0.6228750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03763049570719401, AUC: 0.5572945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023276461919148762, AUC: 0.602257\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019876455942789712, AUC: 0.628934\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018293451309204102, AUC: 0.63669275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017353838602701822, AUC: 0.6458174999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016881285985310874, AUC: 0.64659\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.09013915761311848, AUC: 0.35025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023153777440388996, AUC: 0.51247325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0218327153523763, AUC: 0.5256965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020900216420491537, AUC: 0.53996975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02003130785624186, AUC: 0.5532575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019249781926472982, AUC: 0.5708790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0521379992167155, AUC: 0.534407\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020111589431762694, AUC: 0.52797125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01891073226928711, AUC: 0.55025275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01817427635192871, AUC: 0.5684739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01760384813944499, AUC: 0.57773\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017327292760213216, AUC: 0.58774675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.061462228139241536, AUC: 0.5057505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.023149015426635743, AUC: 0.5617885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02173636563618978, AUC: 0.59152825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020826215744018554, AUC: 0.6106977499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02013681666056315, AUC: 0.62430425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01945805867513021, AUC: 0.6341554999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1382056376139323, AUC: 0.49824825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.02084566561381022, AUC: 0.620411\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020260929743448893, AUC: 0.62384225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019679572423299152, AUC: 0.628764\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019543267567952473, AUC: 0.629732\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.019062891006469726, AUC: 0.6358557499999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class SMOTE \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-5, 1e-6, 5e-7, 1e-7, 1e-8]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.SoftmaxLogisticRegression(3, shape=32*32*3)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "    \n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86c3a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/auc_analysis_other_methods.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb487777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>num_classes</th>\n",
       "      <th>classes_used</th>\n",
       "      <th>ratio</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>mean_0</th>\n",
       "      <th>variance_0</th>\n",
       "      <th>mean_10</th>\n",
       "      <th>variance_10</th>\n",
       "      <th>mean_20</th>\n",
       "      <th>variance_20</th>\n",
       "      <th>mean_30</th>\n",
       "      <th>variance_30</th>\n",
       "      <th>mean_40</th>\n",
       "      <th>variance_40</th>\n",
       "      <th>mean_50</th>\n",
       "      <th>variance_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>focal_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(100, 1)</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.548379</td>\n",
       "      <td>0.009424</td>\n",
       "      <td>0.509104</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.544317</td>\n",
       "      <td>0.003692</td>\n",
       "      <td>0.533671</td>\n",
       "      <td>0.003843</td>\n",
       "      <td>0.522565</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.560341</td>\n",
       "      <td>0.003718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>focal_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(100, 1)</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.534905</td>\n",
       "      <td>0.008606</td>\n",
       "      <td>0.705996</td>\n",
       "      <td>0.007951</td>\n",
       "      <td>0.751608</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.680877</td>\n",
       "      <td>0.015726</td>\n",
       "      <td>0.659826</td>\n",
       "      <td>0.017640</td>\n",
       "      <td>0.743499</td>\n",
       "      <td>0.014839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>focal_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(100, 1)</td>\n",
       "      <td>5.000000e-04</td>\n",
       "      <td>0.527536</td>\n",
       "      <td>0.008331</td>\n",
       "      <td>0.528728</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.576301</td>\n",
       "      <td>0.005670</td>\n",
       "      <td>0.569312</td>\n",
       "      <td>0.006928</td>\n",
       "      <td>0.567822</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>0.553500</td>\n",
       "      <td>0.003843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>focal_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(100, 1)</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.511716</td>\n",
       "      <td>0.005766</td>\n",
       "      <td>0.709210</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.733657</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.741482</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.747115</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.753203</td>\n",
       "      <td>0.000319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>focal_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(100, 1)</td>\n",
       "      <td>5.000000e-05</td>\n",
       "      <td>0.514619</td>\n",
       "      <td>0.003869</td>\n",
       "      <td>0.749702</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.700334</td>\n",
       "      <td>0.016223</td>\n",
       "      <td>0.766723</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>0.759732</td>\n",
       "      <td>0.005828</td>\n",
       "      <td>0.770309</td>\n",
       "      <td>0.004765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>focal_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(100, 1)</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.485289</td>\n",
       "      <td>0.005268</td>\n",
       "      <td>0.627792</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.632842</td>\n",
       "      <td>0.002343</td>\n",
       "      <td>0.637827</td>\n",
       "      <td>0.002302</td>\n",
       "      <td>0.643297</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>0.647572</td>\n",
       "      <td>0.002134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>focal_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(100, 1)</td>\n",
       "      <td>5.000000e-07</td>\n",
       "      <td>0.525999</td>\n",
       "      <td>0.008032</td>\n",
       "      <td>0.640772</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.642537</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.646301</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.649294</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.652700</td>\n",
       "      <td>0.001028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name  num_classes classes_used     ratio  learning_rate    mean_0   \n",
       "0  focal_loss            2       (0, 1)  (100, 1)   1.000000e-03  0.548379  \\\n",
       "1  focal_loss            2       (0, 1)  (100, 1)   1.000000e-04  0.534905   \n",
       "2  focal_loss            2       (0, 1)  (100, 1)   5.000000e-04  0.527536   \n",
       "3  focal_loss            2       (0, 1)  (100, 1)   1.000000e-05  0.511716   \n",
       "4  focal_loss            2       (0, 1)  (100, 1)   5.000000e-05  0.514619   \n",
       "5  focal_loss            2       (0, 1)  (100, 1)   1.000000e-06  0.485289   \n",
       "6  focal_loss            2       (0, 1)  (100, 1)   5.000000e-07  0.525999   \n",
       "\n",
       "   variance_0   mean_10  variance_10   mean_20  variance_20   mean_30   \n",
       "0    0.009424  0.509104     0.000462  0.544317     0.003692  0.533671  \\\n",
       "1    0.008606  0.705996     0.007951  0.751608     0.011673  0.680877   \n",
       "2    0.008331  0.528728     0.000818  0.576301     0.005670  0.569312   \n",
       "3    0.005766  0.709210     0.000450  0.733657     0.000285  0.741482   \n",
       "4    0.003869  0.749702     0.001833  0.700334     0.016223  0.766723   \n",
       "5    0.005268  0.627792     0.002347  0.632842     0.002343  0.637827   \n",
       "6    0.008032  0.640772     0.000721  0.642537     0.000811  0.646301   \n",
       "\n",
       "   variance_30   mean_40  variance_40   mean_50  variance_50  \n",
       "0     0.003843  0.522565     0.000763  0.560341     0.003718  \n",
       "1     0.015726  0.659826     0.017640  0.743499     0.014839  \n",
       "2     0.006928  0.567822     0.004212  0.553500     0.003843  \n",
       "3     0.000189  0.747115     0.000269  0.753203     0.000319  \n",
       "4     0.005824  0.759732     0.005828  0.770309     0.004765  \n",
       "5     0.002302  0.643297     0.002234  0.647572     0.002134  \n",
       "6     0.000891  0.649294     0.000932  0.652700     0.001028  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02884ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('results/auc_analysis_other_methods.csv', index=False)\n",
    "# df.to_csv('results/auc_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e783a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
