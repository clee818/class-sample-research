{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378c3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dda12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "NUM_CLASSES_REDUCED = 2\n",
    "nums = (0, 1)\n",
    "ratio = (100, 1)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b57e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "             \"mean_40\", \"variance_40\",\n",
    "             \"mean_50\", \"variance_50\",\n",
    "             \"cap\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c2a1ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor() ]))  \n",
    "\n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()]))\n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "    \n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums)\n",
    "\n",
    "triplet_train_CIFAR10 = class_sampling.ForTripletLoss(reduced_train_CIFAR10, smote=False)\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED)\n",
    "triplet_smote_train_CIFAR10= class_sampling.ForTripletLoss(smote_train_CIFAR10, smote=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13159c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000   50]\n"
     ]
    }
   ],
   "source": [
    "targets = ratio_train_CIFAR10.labels \n",
    "\n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "print(class_count)\n",
    "\n",
    "weight = 1. / class_count\n",
    "\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= class_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de0d4cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.999 \n",
    "\n",
    "exp = np.empty_like(targets)\n",
    "for i, count in enumerate(class_count):\n",
    "    exp[targets==i] = count\n",
    "effective_weights = (1 - beta) / ( 1 - (beta ** torch.from_numpy(exp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af8cd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss = DataLoader(triplet_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_smote = DataLoader(triplet_smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e2e8c09f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001968830943107605, AUC: 0.6361665000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005542738735675812, AUC: 0.885664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005292963385581971, AUC: 0.8979130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004977731555700302, AUC: 0.9077430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032240989208221437, AUC: 0.38664699999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006063661277294159, AUC: 0.855749\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005620519518852234, AUC: 0.8903450000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005382097363471985, AUC: 0.898165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006873347997665405, AUC: 0.683021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005775293409824372, AUC: 0.867693\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004782587885856628, AUC: 0.9038889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00045187199115753177, AUC: 0.9145770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001451125681400299, AUC: 0.47390000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006520158350467682, AUC: 0.7826169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006005813181400299, AUC: 0.869262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005808910429477692, AUC: 0.8839089999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031172895431518556, AUC: 0.6896909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005760127604007721, AUC: 0.8859184999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005759528279304505, AUC: 0.890328\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005142679214477539, AUC: 0.9147670000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00219118869304657, AUC: 0.37103100000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005964301228523254, AUC: 0.8728420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005374825298786163, AUC: 0.902899\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000544882744550705, AUC: 0.900375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006955941677093506, AUC: 0.3322345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005800726115703583, AUC: 0.8929210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005413680970668792, AUC: 0.9098769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000514941155910492, AUC: 0.9136345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028511343002319335, AUC: 0.5939905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000573833703994751, AUC: 0.8740805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005463034212589264, AUC: 0.891224\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005187967419624329, AUC: 0.9042855000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011438738107681274, AUC: 0.351393\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005868805348873138, AUC: 0.8747965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005481753051280975, AUC: 0.9026535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005193610191345215, AUC: 0.9129545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015125332474708558, AUC: 0.429251\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005813519060611725, AUC: 0.8786199999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005619006156921387, AUC: 0.8882220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005398068428039551, AUC: 0.889149\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034982739686965944, AUC: 0.618384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941048204898835, AUC: 0.5019874999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936376094818116, AUC: 0.504489\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934143602848053, AUC: 0.501498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011465153098106384, AUC: 0.401948\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005416077971458435, AUC: 0.90378\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005318052768707275, AUC: 0.8990750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00048347583413124083, AUC: 0.9135070000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009435354471206665, AUC: 0.577937\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006637665331363678, AUC: 0.7110139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935104429721832, AUC: 0.498063\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933853924274444, AUC: 0.499049\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029533997774124146, AUC: 0.545541\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932328343391418, AUC: 0.499006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932272613048554, AUC: 0.499495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932301223278045, AUC: 0.499504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038374502658843996, AUC: 0.565988\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005938313007354736, AUC: 0.8660625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005148913562297821, AUC: 0.911864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00049067023396492, AUC: 0.921967\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001448841392993927, AUC: 0.5810709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005838621556758881, AUC: 0.9087160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005997822880744934, AUC: 0.8527130000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004748252034187317, AUC: 0.9237920000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004549663305282592, AUC: 0.44000300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005573736131191254, AUC: 0.8895370000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005237226784229279, AUC: 0.90449\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005322802066802979, AUC: 0.9018970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028102716207504272, AUC: 0.5378565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005881844162940979, AUC: 0.875293\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005218585729599, AUC: 0.910688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004902279824018478, AUC: 0.9198445000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018956378698349, AUC: 0.37419199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005440128147602082, AUC: 0.8936314999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004857201725244522, AUC: 0.917146\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046266598999500277, AUC: 0.9193445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001775826096534729, AUC: 0.554507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006459864675998687, AUC: 0.7649400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005977490544319153, AUC: 0.846576\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005115328282117844, AUC: 0.9130775000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS normal\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-4, 1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f8f1d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d211694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0017137413620948792, AUC: 0.34950099999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012429256439208983, AUC: 0.7940069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011818664668515177, AUC: 0.864704\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017334770560264588, AUC: 0.857535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000619699061101321, AUC: 0.9280039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016367412209510803, AUC: 0.8775399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005310259188645252, AUC: 0.966544\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001116765558719635, AUC: 0.5430269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001598490059375763, AUC: 0.694131\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001009908124878265, AUC: 0.7530680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016057812571525574, AUC: 0.7648419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008823918577025433, AUC: 0.821452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001576744794845581, AUC: 0.803816\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008004976997133529, AUC: 0.881732\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009831752300262451, AUC: 0.39679100000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001676112174987793, AUC: 0.730593\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008864151638480696, AUC: 0.793768\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014211297035217284, AUC: 0.7897289999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009215684260914822, AUC: 0.8928879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018931840658187866, AUC: 0.771968\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006790035094438803, AUC: 0.90096\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013066926002502442, AUC: 0.5133\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011528908610343933, AUC: 0.7471315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015582376748028367, AUC: 0.810964\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012417744398117065, AUC: 0.831506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009810643587814698, AUC: 0.9170239999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015098519921302796, AUC: 0.8611340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006323435179667897, AUC: 0.94208\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007577472925186157, AUC: 0.636292\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001345901370048523, AUC: 0.771662\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011148011618026413, AUC: 0.8550519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014434826374053954, AUC: 0.808284\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008712424669820483, AUC: 0.896088\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001240996539592743, AUC: 0.8533139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008732791956846076, AUC: 0.925624\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027764800786972046, AUC: 0.4881195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014432362914085389, AUC: 0.782922\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009564629613910571, AUC: 0.851548\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016698249578475952, AUC: 0.844475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006890537556592781, AUC: 0.8934759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018161503076553345, AUC: 0.863981\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005943306476467907, AUC: 0.934012\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010153146982192992, AUC: 0.62556\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001262729525566101, AUC: 0.8005819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011123225328945878, AUC: 0.863988\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018099974393844605, AUC: 0.842795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006397132579200338, AUC: 0.916556\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001517596960067749, AUC: 0.863342\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006431830478924336, AUC: 0.940124\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003342249035835266, AUC: 0.494811\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013585245013236998, AUC: 0.8040759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001008233560046347, AUC: 0.845044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017321531772613525, AUC: 0.848322\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006622557585487271, AUC: 0.9164\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00092261603474617, AUC: 0.8593135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015927514817455028, AUC: 0.9537439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002506653547286987, AUC: 0.4589705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012428303956985474, AUC: 0.8434809999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009687709030095894, AUC: 0.909284\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015631653666496276, AUC: 0.8535280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006489730808920789, AUC: 0.9347040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002239511489868164, AUC: 0.840339\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005805417686461074, AUC: 0.944524\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009060979187488556, AUC: 0.5624939999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012041149735450745, AUC: 0.719261\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015574717949522604, AUC: 0.7819319999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013586844205856323, AUC: 0.789456\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010369148341440918, AUC: 0.8594799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022159544229507448, AUC: 0.8586060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006337337018829761, AUC: 0.9244600000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS ratio\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "learning_rate_train_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                _, train_auc = metric_utils.auc_sigmoid(train_loader_ratio, network) \n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f2d0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f1ccb0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.005691136598587036, AUC: 0.7166170000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000588998019695282, AUC: 0.858235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005738674998283386, AUC: 0.8568835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000547566682100296, AUC: 0.864155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024350250959396364, AUC: 0.573198\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006682571172714234, AUC: 0.6593775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006437829434871674, AUC: 0.686171\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006789233386516571, AUC: 0.644326\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002518909811973572, AUC: 0.47361\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005086382031440735, AUC: 0.861209\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012186179161071777, AUC: 0.863998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014082735776901245, AUC: 0.857443\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012410336136817932, AUC: 0.405966\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007035685777664185, AUC: 0.39492449999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006275011897087097, AUC: 0.823426\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00088487908244133, AUC: 0.843707\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038197344541549684, AUC: 0.272004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006127392649650573, AUC: 0.8226249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005858472883701325, AUC: 0.839917\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005321000218391418, AUC: 0.8312549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013948385119438172, AUC: 0.4044835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006064178347587586, AUC: 0.8118620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005522237718105316, AUC: 0.836232\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005868338644504547, AUC: 0.7515910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026865620613098144, AUC: 0.3946695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005852950513362884, AUC: 0.8483590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005711329281330109, AUC: 0.8395555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005642571151256562, AUC: 0.816999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019509876370429992, AUC: 0.547291\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006756735444068909, AUC: 0.6119295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005742033720016479, AUC: 0.838554\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008748687803745269, AUC: 0.8437619999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025394957065582274, AUC: 0.633893\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006159831285476685, AUC: 0.7870250000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006629352569580078, AUC: 0.6932835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006924188435077668, AUC: 0.732165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017310360670089722, AUC: 0.5722005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006567726433277131, AUC: 0.765111\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005512991845607757, AUC: 0.8718000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005515256524085999, AUC: 0.861332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002939437508583069, AUC: 0.563555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006314303278923035, AUC: 0.706082\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012731393575668335, AUC: 0.648748\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022744207382202146, AUC: 0.716482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008252851068973542, AUC: 0.6032989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018195168375968933, AUC: 0.695447\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005728540658950806, AUC: 0.690301\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0057329535484313965, AUC: 0.684577\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001132174551486969, AUC: 0.400508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009344134032726288, AUC: 0.589728\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030594894886016846, AUC: 0.45559599999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022871530055999754, AUC: 0.565234\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010400078892707826, AUC: 0.5479905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931792199611664, AUC: 0.4980055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000715867280960083, AUC: 0.5413429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003392560124397278, AUC: 0.823512\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011575499176979065, AUC: 0.639311\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929903626441956, AUC: 0.49061400000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007422293424606323, AUC: 0.645052\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001354758381843567, AUC: 0.5095205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018678699135780334, AUC: 0.43198000000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011401005983352661, AUC: 0.712514\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014176532030105592, AUC: 0.713692\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005547755002975464, AUC: 0.7414639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008667053282260895, AUC: 0.6099764999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001788466989994049, AUC: 0.7582169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003058842658996582, AUC: 0.775567\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002191431522369385, AUC: 0.703206\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001195160150527954, AUC: 0.5285805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006799123585224152, AUC: 0.639027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001596990406513214, AUC: 0.667326\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002838463068008423, AUC: 0.6188389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012705932259559631, AUC: 0.4969279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010228420495986938, AUC: 0.781873\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015879844427108764, AUC: 0.793261\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017532774209976197, AUC: 0.66409\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004795622110366821, AUC: 0.319283\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010547017455101013, AUC: 0.577423\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003481634497642517, AUC: 0.618579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0066692931652069095, AUC: 0.6045940000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS oversampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52d8c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:14]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1541b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.002664048075675964, AUC: 0.35103100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008118733763694763, AUC: 0.39093300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007359330058097839, AUC: 0.42988400000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007080410122871399, AUC: 0.476849\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005757878303527832, AUC: 0.4101775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008514148592948913, AUC: 0.663575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006793743073940277, AUC: 0.6388240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006851064562797546, AUC: 0.599984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001174522042274475, AUC: 0.498127\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009026037454605102, AUC: 0.569188\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007478025853633881, AUC: 0.5202549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007039127349853516, AUC: 0.5297069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009038377702236175, AUC: 0.47197\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007971824407577515, AUC: 0.48862799999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007626304626464844, AUC: 0.494027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007488877177238464, AUC: 0.4992150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034414209127426147, AUC: 0.5659004999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008925645649433136, AUC: 0.602172\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007607427835464478, AUC: 0.5276164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007198239862918854, AUC: 0.509627\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013543962240219116, AUC: 0.49844299999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013344860672950744, AUC: 0.5426559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008173213005065918, AUC: 0.520809\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007598233222961425, AUC: 0.41432599999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009839903712272643, AUC: 0.54916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008945295512676239, AUC: 0.46157500000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007561358511447906, AUC: 0.477264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007167462408542633, AUC: 0.498566\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001238984763622284, AUC: 0.47820850000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009365684688091278, AUC: 0.374864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008359749019145966, AUC: 0.3937505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007830325365066528, AUC: 0.417717\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008678676128387451, AUC: 0.6484215000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009037670493125915, AUC: 0.427843\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007472198605537415, AUC: 0.4185105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007187381088733673, AUC: 0.446337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007866494059562683, AUC: 0.5668679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007531993091106415, AUC: 0.488706\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007111433148384094, AUC: 0.49999199999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007000260055065155, AUC: 0.515335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038360543251037597, AUC: 0.294262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009016560912132263, AUC: 0.505367\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088973104953766, AUC: 0.5929039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011563763022422791, AUC: 0.606266\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006004724025726318, AUC: 0.478491\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0041501855850219725, AUC: 0.427829\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030867953300476076, AUC: 0.404122\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026625494956970214, AUC: 0.434574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017807592153549195, AUC: 0.5781095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011875814199447633, AUC: 0.6403829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009512988924980164, AUC: 0.6518860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008521502912044526, AUC: 0.6516249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007921732366085052, AUC: 0.615845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008575631380081177, AUC: 0.646551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009552640914916992, AUC: 0.646782\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008757838904857636, AUC: 0.6430809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012470539212226868, AUC: 0.417589\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001292936086654663, AUC: 0.45840000000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013544394373893739, AUC: 0.473034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001133813738822937, AUC: 0.46033199999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010617620944976807, AUC: 0.63819\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010061159133911133, AUC: 0.634423\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009557005167007447, AUC: 0.6285029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009136235415935516, AUC: 0.61536\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018809281587600709, AUC: 0.6420239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001633428931236267, AUC: 0.638927\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014178273677825928, AUC: 0.634879\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012286875247955321, AUC: 0.6292800000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016851001381874085, AUC: 0.338503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010225446820259095, AUC: 0.45910999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011303470730781556, AUC: 0.5157820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001235684871673584, AUC: 0.543991\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002234177231788635, AUC: 0.47880599999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001637177050113678, AUC: 0.588411\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001996301531791687, AUC: 0.6044080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018329355120658875, AUC: 0.602075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010110423564910888, AUC: 0.42448849999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008554110527038574, AUC: 0.44827599999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008604062795639038, AUC: 0.453961\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008695433139801025, AUC: 0.447998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS undersampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3dafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Class Weighted Loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['pos_weight'] = torch.tensor([weight[1]])\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                _, train_auc = metric_utils.auc_sigmoid(train_loader_ratio, network) \n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9eed2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c5a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 CLASS SMOTE\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf4753",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfecae8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.002174391269683838, AUC: 0.493485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001345380187034607, AUC: 0.737005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014745190143585205, AUC: 0.823094\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001579261302947998, AUC: 0.8384860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008028465509414673, AUC: 0.481804\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005763685703277588, AUC: 0.82903\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002095910429954529, AUC: 0.79007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014553953409194946, AUC: 0.8611660000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004671881914138794, AUC: 0.434292\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00045436549186706545, AUC: 0.875008\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012930932641029359, AUC: 0.8662929999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015099110007286071, AUC: 0.858296\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009448454380035401, AUC: 0.38943150000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00204328989982605, AUC: 0.6934210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008080962002277374, AUC: 0.832444\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008111295104026795, AUC: 0.8687119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021964836120605467, AUC: 0.276298\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016957995295524596, AUC: 0.6403719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006421829164028167, AUC: 0.831952\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046772484481334687, AUC: 0.8775109999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031312047243118287, AUC: 0.3405245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016629098057746887, AUC: 0.609567\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012474647760391236, AUC: 0.7549680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009316090047359467, AUC: 0.856276\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009632619321346283, AUC: 0.53584\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001713748037815094, AUC: 0.594131\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013703943490982055, AUC: 0.7032719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009412908554077148, AUC: 0.7709159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001209743320941925, AUC: 0.469074\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018361815214157104, AUC: 0.7249490000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001989833414554596, AUC: 0.854033\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011067346930503845, AUC: 0.881904\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00285483992099762, AUC: 0.556775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019113221764564515, AUC: 0.6736849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017779849171638488, AUC: 0.641289\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001578715980052948, AUC: 0.7034239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003599430441856384, AUC: 0.468728\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005966219305992127, AUC: 0.8655329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009098819196224213, AUC: 0.8990390000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001225925087928772, AUC: 0.9030839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009507868885993958, AUC: 0.6392909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00063413006067276, AUC: 0.829817\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006332595348358154, AUC: 0.8302850000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005391051471233367, AUC: 0.8704385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034711266756057737, AUC: 0.34269099999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006449199020862579, AUC: 0.7898319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000603088766336441, AUC: 0.845608\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006038400828838348, AUC: 0.843566\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012144817113876342, AUC: 0.5806305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006229795515537262, AUC: 0.825751\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000580595314502716, AUC: 0.867144\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006203400492668152, AUC: 0.8343330000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006922641038894653, AUC: 0.333821\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005892142653465271, AUC: 0.8640825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005757260322570801, AUC: 0.866771\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006372291743755341, AUC: 0.7930550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030042797327041624, AUC: 0.3306605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000601535975933075, AUC: 0.8250455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000640325278043747, AUC: 0.8091865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006479507088661193, AUC: 0.8009665000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003928661704063415, AUC: 0.62247\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693362981081009, AUC: 0.488562\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932762563228608, AUC: 0.4910495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693373292684555, AUC: 0.490547\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005347926139831543, AUC: 0.6413489999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936891376972199, AUC: 0.489515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936222612857818, AUC: 0.49102799999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693583995103836, AUC: 0.49004649999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000763323575258255, AUC: 0.552565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006195923388004303, AUC: 0.8510989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005787133872509003, AUC: 0.8669850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006304331421852112, AUC: 0.825109\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010691565275192261, AUC: 0.667979\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006198270618915558, AUC: 0.8408410000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006009622514247894, AUC: 0.8621650000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005451369285583496, AUC: 0.8589939999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002433639883995056, AUC: 0.5546615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000620361715555191, AUC: 0.825541\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005378367304801941, AUC: 0.886467\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006109955012798309, AUC: 0.8300684999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012717214226722717, AUC: 0.603449\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005810734331607819, AUC: 0.8730434999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006152071058750152, AUC: 0.845757\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000606498509645462, AUC: 0.8516885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014686033129692078, AUC: 0.362004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006023291051387787, AUC: 0.826258\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006693426370620728, AUC: 0.733561\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005830575227737427, AUC: 0.8623474999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008698516488075257, AUC: 0.54724\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006038571000099182, AUC: 0.8353690000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006631026268005371, AUC: 0.76945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006122806966304779, AUC: 0.8353050000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017069737911224365, AUC: 0.43228200000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006196925938129425, AUC: 0.8363130000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000617479383945465, AUC: 0.843767\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000626991480588913, AUC: 0.8305060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007652827024459839, AUC: 0.5466120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006368909180164337, AUC: 0.8085715000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000575814962387085, AUC: 0.8594530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005294672548770905, AUC: 0.8726865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025274738073349, AUC: 0.45867650000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006418247818946838, AUC: 0.8188444999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006219926774501801, AUC: 0.8269280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005931970775127411, AUC: 0.8546530000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001242193341255188, AUC: 0.49758499999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932232677936554, AUC: 0.497008\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00069314244389534, AUC: 0.502497\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931471228599548, AUC: 0.5010074999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003604241967201233, AUC: 0.3363525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006018182337284089, AUC: 0.8429000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006553382575511933, AUC: 0.7611285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005584169328212738, AUC: 0.8664580000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001270480751991272, AUC: 0.5603629999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006275672614574433, AUC: 0.803437\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006347024440765381, AUC: 0.8008265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006130314767360687, AUC: 0.8231039999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016833193302154541, AUC: 0.5450495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006699742376804352, AUC: 0.7335479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006106870174407959, AUC: 0.83953\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005844770371913909, AUC: 0.8555389999999999\n",
      "\n",
      "[['capped_smote', 2, (0, 1), (100, 1), 0.0005, 0.44462520000000005, 0.00681700709361, 0.7242701, 0.009445679363890003, 0.7996454, 0.005640064275240002, 0.8419774999999999, 0.003240343165050001, 1], ['capped_smote', 2, (0, 1), (100, 1), 0.0005, 0.52661185, 0.016880514633852494, 0.7630085999999999, 0.019103176368290002, 0.7816689000000001, 0.021543018134940004, 0.7637124000000002, 0.019176680151740004, 5], ['capped_smote', 2, (0, 1), (100, 1), 0.0005, 0.48896135, 0.007168321469952497, 0.78752925, 0.010554006866012502, 0.7782898, 0.010048547763510005, 0.8153295, 0.011208786899550006, 10]]\n"
     ]
    }
   ],
   "source": [
    "# 2 Class Capped SMOTE \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_sigmoid_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for i in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[i][0]\n",
    "    auc_variance = cap_aucs[i][1]\n",
    "    cap = caps[i]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "606da0f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "18 columns passed, passed data had 14 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:934\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 934\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_or_indexify_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:981\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    982\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    983\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    984\u001b[0m     )\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 18 columns passed, passed data had 14 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/convnet_aucs.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m df2 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df1, df2])\n\u001b[1;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/convnet_aucs.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:781\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    780\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 781\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    783\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    789\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    790\u001b[0m         arrays,\n\u001b[1;32m    791\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    794\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    795\u001b[0m     )\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:498\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 498\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:840\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    837\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    838\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 840\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:937\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    934\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[0;32m--> 937\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[1;32m    940\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: 18 columns passed, passed data had 14 columns"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = (col_names[0:14]))\n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f22d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 CLASS Focal Loss\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SigmoidFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d438d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5f6eb22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf2\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7924e1f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# distance + capped loss\n",
    "momentum=0\n",
    "learning_rates = [5e-3, 5e-4]\n",
    "\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_euclidean_distance(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"distance_capped_smote_fixed\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4428ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a4d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine distance + capped loss\n",
    "momentum=0\n",
    "learning_rates = [5e-2]\n",
    "\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_cosine_distance(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"cosine_distance_capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2819b2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15fa5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capped loss with everything capped + cosine distance \n",
    "momentum=0\n",
    "learning_rates = [5e-4]\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_cosine_distance(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.AllCappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"cosine_distance_all_capped\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecfcd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17af0e21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0028201372623443605, AUC: 0.44846\n",
      "\n",
      "Train loss: 32.97782490549574\n",
      "Train loss: 3.142428423189054\n",
      "Train loss: 1.9498858821999496\n",
      "Train loss: 1.8392061918594276\n",
      "Train loss: 0.6934513358553503\n",
      "Train loss: 0.8904903161867409\n",
      "Train loss: 0.8959774654002706\n",
      "Train loss: 0.6325528285685619\n",
      "Train loss: 0.5551665841014521\n",
      "Train loss: 0.5445601497865786\n",
      "Train loss: 0.49642970901765643\n",
      "Train loss: 0.5221448669767683\n",
      "Train loss: 0.5505812513600489\n",
      "Train loss: 0.4043862621305854\n",
      "Train loss: 0.5262504804665875\n",
      "\n",
      "Test set: Avg. loss: 0.0006939134299755096, AUC: 0.499004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006939105689525605, AUC: 0.498506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006939051747322083, AUC: 0.498506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938981711864472, AUC: 0.498506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938954889774323, AUC: 0.498506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938913762569428, AUC: 0.498506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938845515251159, AUC: 0.499004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938792765140533, AUC: 0.49900300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938726603984833, AUC: 0.49900300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938668191432953, AUC: 0.49900300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693861186504364, AUC: 0.49900300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938583552837372, AUC: 0.49900300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938557326793671, AUC: 0.49900300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938537061214447, AUC: 0.49900300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938503682613373, AUC: 0.49900300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938461065292359, AUC: 0.49900300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938441693782806, AUC: 0.499002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693839967250824, AUC: 0.499002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938364207744598, AUC: 0.499002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938320398330688, AUC: 0.499002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938313841819763, AUC: 0.499002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938287615776062, AUC: 0.499002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938247084617615, AUC: 0.499002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938236653804779, AUC: 0.499002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938188970088959, AUC: 0.499002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938153803348541, AUC: 0.499002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938126087188721, AUC: 0.499002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693808913230896, AUC: 0.499002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938066482543945, AUC: 0.499002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938032805919647, AUC: 0.499002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937995553016662, AUC: 0.499002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937924325466156, AUC: 0.498504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937872171401977, AUC: 0.498503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937827467918396, AUC: 0.498503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937784850597382, AUC: 0.498503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937741935253143, AUC: 0.498503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937695741653443, AUC: 0.498503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937663853168488, AUC: 0.498502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937631666660309, AUC: 0.498502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937599182128907, AUC: 0.498502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00069375741481781, AUC: 0.498502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937543749809265, AUC: 0.498502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937516629695892, AUC: 0.498502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937484741210937, AUC: 0.498502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937459111213684, AUC: 0.498502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937437951564789, AUC: 0.498502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937397420406342, AUC: 0.498502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937367022037506, AUC: 0.498502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937330663204193, AUC: 0.498502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937296688556672, AUC: 0.498502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005309300899505615, AUC: 0.58125\n",
      "\n",
      "Train loss: 70.12017831566986\n",
      "Train loss: 2.3268971333078516\n",
      "Train loss: 1.3163974788158563\n",
      "Train loss: 0.9212468081409004\n",
      "Train loss: 1.030966617783923\n",
      "Train loss: 0.8661671666203031\n",
      "Train loss: 0.7016416696985815\n",
      "Train loss: 0.5679251817381306\n",
      "Train loss: 0.6362715624510102\n",
      "Train loss: 0.47439795846392396\n",
      "Train loss: 0.4575251157686209\n",
      "Train loss: 0.5020181157976199\n",
      "Train loss: 0.44838133406866887\n",
      "Train loss: 0.4760592661465809\n",
      "Train loss: 0.40143183111005526\n",
      "\n",
      "Test set: Avg. loss: 0.000693749725818634, AUC: 0.49253399999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937425136566162, AUC: 0.492536\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937333941459656, AUC: 0.490568\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937283277511597, AUC: 0.49106150000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693723976612091, AUC: 0.48957300000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937176883220673, AUC: 0.491559\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937088370323182, AUC: 0.4910645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937020421028137, AUC: 0.491561\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936967968940735, AUC: 0.49155699999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693691223859787, AUC: 0.491559\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936895251274109, AUC: 0.49255200000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693685919046402, AUC: 0.49156000000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936849653720856, AUC: 0.492555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936815977096558, AUC: 0.492555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936765611171722, AUC: 0.49353800000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936711370944977, AUC: 0.49353800000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936661899089813, AUC: 0.493537\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936617791652679, AUC: 0.493537\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936583518981934, AUC: 0.49353800000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936516761779785, AUC: 0.49353800000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936466991901397, AUC: 0.49353800000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693643182516098, AUC: 0.493537\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936386823654174, AUC: 0.493527\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693632185459137, AUC: 0.493526\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936224102973938, AUC: 0.49302850000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936186254024505, AUC: 0.4940235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936173141002655, AUC: 0.4940235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693610280752182, AUC: 0.49302850000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936064064502716, AUC: 0.49302850000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936034858226777, AUC: 0.49302850000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935965120792389, AUC: 0.49302850000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935919821262359, AUC: 0.49452199999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935877203941345, AUC: 0.494521\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935829818248749, AUC: 0.49451999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935780942440033, AUC: 0.494519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935750842094421, AUC: 0.494519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935728788375854, AUC: 0.494519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935687959194184, AUC: 0.4950165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935667991638184, AUC: 0.4950165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935642957687378, AUC: 0.4950165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935608386993408, AUC: 0.495514\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935592293739319, AUC: 0.495514\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935570240020752, AUC: 0.4950165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935534477233887, AUC: 0.49601249999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935506463050842, AUC: 0.49651\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935485303401947, AUC: 0.49601449999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935463845729828, AUC: 0.49601249999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935434043407441, AUC: 0.49651\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935395002365112, AUC: 0.49651\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935366988182068, AUC: 0.49750599999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000988310217857361, AUC: 0.6187969999999999\n",
      "\n",
      "Train loss: 95.6922761414461\n",
      "Train loss: 4.324190375911202\n",
      "Train loss: 2.6355566359629297\n",
      "Train loss: 1.855891635463496\n",
      "Train loss: 1.6249122668983071\n",
      "Train loss: 1.4495760942720304\n",
      "Train loss: 1.0793101233281908\n",
      "Train loss: 0.7140921521338688\n",
      "Train loss: 0.7714093710966171\n",
      "Train loss: 0.636501373948565\n",
      "Train loss: 0.5691139222520172\n",
      "Train loss: 0.6303390815948985\n",
      "Train loss: 0.5256420575129758\n",
      "Train loss: 0.4956415467391348\n",
      "Train loss: 0.6592324696908332\n",
      "\n",
      "Test set: Avg. loss: 0.000692930817604065, AUC: 0.506511\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929300129413605, AUC: 0.506511\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929313838481904, AUC: 0.5080045000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929336190223693, AUC: 0.5070145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929305791854858, AUC: 0.507515\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006929296255111695, AUC: 0.507491\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929322481155396, AUC: 0.50749\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929299533367157, AUC: 0.5060000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929353177547455, AUC: 0.5074420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929371654987336, AUC: 0.508437\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929371654987336, AUC: 0.5089350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000692937970161438, AUC: 0.5089350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929370760917664, AUC: 0.508935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00069293674826622, AUC: 0.507443\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929394900798797, AUC: 0.5074649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929413378238678, AUC: 0.5074649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929428577423096, AUC: 0.50647\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929481625556946, AUC: 0.50647\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929449141025543, AUC: 0.5059724999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929453313350677, AUC: 0.5059724999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929460763931274, AUC: 0.5059724999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929470598697662, AUC: 0.5044799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929532587528228, AUC: 0.50647\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000692952960729599, AUC: 0.50647\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000692953646183014, AUC: 0.50647\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929529309272766, AUC: 0.505475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929521262645722, AUC: 0.505476\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929510831832886, AUC: 0.505476\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929520964622498, AUC: 0.505476\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929584443569184, AUC: 0.5059724999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929586231708526, AUC: 0.5059724999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929610371589661, AUC: 0.506471\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929616332054139, AUC: 0.506471\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929630935192108, AUC: 0.5059735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929653286933899, AUC: 0.50647\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929650008678436, AUC: 0.5074649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929655373096466, AUC: 0.5079604999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929660439491272, AUC: 0.5079614999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929684579372406, AUC: 0.506469\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929731965065002, AUC: 0.50647\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000692973643541336, AUC: 0.5059714999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929763853549957, AUC: 0.5054730000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929748952388764, AUC: 0.505474\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929782032966614, AUC: 0.5059714999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929806172847748, AUC: 0.5059705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929824054241181, AUC: 0.5059705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929826438426971, AUC: 0.5059715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929821074008941, AUC: 0.5059715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929812133312225, AUC: 0.5049765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929845213890076, AUC: 0.5049755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 class triplet loss no ratio \n",
    "# no smote \n",
    "\n",
    "# note: sometimes can get a very high accuracy but may diverge\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(1e-7, 1e-5)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(3): \n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(complete_network.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(15):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss, embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "        for epoch in range(50):\n",
    "            _, _ = train.train_linear_probe(epoch, train_loader_reduced, complete_network, linear_optimizer, verbose=False)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"triplet_loss\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], \n",
    "           auc_mean[i][4], auc_variance[i][4],\n",
    "           auc_mean[i][5], auc_variance[i][5],\n",
    "           None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8afa13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c6d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplet loss with ratio \n",
    "# need to make a new train loader if running this \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(1e-7, 1e-3)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(2):\n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(linear_probe.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss, complete_network.embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "            \n",
    "    #    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network)\n",
    "        for epoch in range(50):\n",
    "            _, _ = train.train_linear_probe(epoch, train_loader_reduced, complete_network.embed_network, complete_network.linear_probe, linear_optimizer, verbose=False)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"triplet_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], \n",
    "           auc_mean[i][4], auc_variance[i][4],\n",
    "           auc_mean[i][5], auc_variance[i][5],\n",
    "           None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65b5279",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a99dbdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011072651147842408, AUC: 0.5656115\n",
      "\n",
      "Train loss: 143.97858251431944\n",
      "Train loss: 2.1544351000694713\n",
      "Train loss: 0.5219840806001311\n",
      "Train loss: 0.27294657982079085\n",
      "Train loss: 0.09577784568640836\n",
      "Train loss: 0.44660394510645773\n",
      "Train loss: 0.16766894395184365\n",
      "Train loss: 0.0\n",
      "Train loss: 0.27340593041887706\n",
      "Train loss: 0.1337364266632469\n",
      "Train loss: 0.04634139036676686\n",
      "Train loss: 0.012778389985394326\n",
      "Train loss: 0.015714880767141937\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(train_losses))))\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m---> 25\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid_cosine_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomplete_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCappedBCELoss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, complete_network)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:187\u001b[0m, in \u001b[0;36mtrain_sigmoid_cosine_distance\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target, smote_target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m    185\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 187\u001b[0m     output, embeds \u001b[38;5;241m=\u001b[39m network(data)\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    190\u001b[0m         dist \u001b[38;5;241m=\u001b[39m cosine_distance(embeds, target, smote_target)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# triplet loss with SMOTE and cosine distance capped loss \n",
    "# REDO THIS (match above code)\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(1e-7, 1e-3)]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(3): \n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(complete_network.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(15):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss_smote, embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "        for epoch in range(50):\n",
    "            _, _ = train.train_sigmoid_cosine_distance(epoch, train_loader_smote, complete_network, linear_optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"cosine_distance_capped_smote_triplet_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], \n",
    "           auc_mean[i][4], auc_variance[i][4],\n",
    "           auc_mean[i][5], auc_variance[i][5],\n",
    "           None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de718f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc1dd64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES_REDUCED = 3\n",
    "nums = (0, 3, 1)\n",
    "ratio = (20, 2, 1)\n",
    "\n",
    "\n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums)\n",
    "targets = ratio_train_CIFAR10.labels \n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED)\n",
    "\n",
    "\n",
    "weight = 1. / class_count\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= max(class_count)\n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "800c1e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004159178098042806, AUC: 0.5\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 16\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     18\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_softmax(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/ml/train.py:41\u001b[0m, in \u001b[0;36mtrain_softmax\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze(), target\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     43\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/ml/models.py:19\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2_drop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)), \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m250\u001b[39m) \u001b[38;5;66;03m#(320)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_jit_internal.py:484\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3 class normal\n",
    "\n",
    "learning_rates = [1e-4, 1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 3, nums, (1, 1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6be4998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6452da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0028148902257283527, AUC: 0.5499715000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012899534304936728, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011812520821889241, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003198623657227, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012160149812698365, AUC: 0.496695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012602541049321493, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011723772684733072, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012640552123387655, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001084180474281311, AUC: 0.47396475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012727203766504925, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010816088120142618, AUC: 0.5045000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011214701334635417, AUC: 0.5035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026806603272755943, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012301311095555623, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001151394208272298, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107947838306427, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020958302021026613, AUC: 0.44825000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012716478904088338, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011440247694651285, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012925329605738322, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026947441101074217, AUC: 0.47152499999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011879764000574749, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011960159142812093, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012670242389043172, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017743062178293865, AUC: 0.50675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001074000636736552, AUC: 0.5057499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012203034957249958, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011130955616633098, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014793120384216308, AUC: 0.44375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012164912223815918, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010818771521250406, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011993260780970255, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00679119078318278, AUC: 0.48125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012626315355300903, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001144296924273173, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001217811703681946, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007157065073649088, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001255599578221639, AUC: 0.501499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011938397884368897, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010313888788223266, AUC: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class ratio\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d28e8f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1752b5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.003038533369700114, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010433460474014282, AUC: 0.574998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011814462741216024, AUC: 0.7483075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001038165827592214, AUC: 0.7227574999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007914267698923746, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010645556449890136, AUC: 0.37466975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001031708836555481, AUC: 0.6914997500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011311002572377523, AUC: 0.75428325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006719241301218669, AUC: 0.5075000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010695095856984456, AUC: 0.5922592499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010078495343526204, AUC: 0.66097125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009656443595886231, AUC: 0.68574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002060540755589803, AUC: 0.5615957500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010401540199915568, AUC: 0.3627755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009678711692492167, AUC: 0.6725415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009192776083946228, AUC: 0.675171\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00151268204053243, AUC: 0.5225000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010918419361114502, AUC: 0.40822674999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010981494585673014, AUC: 0.4126655000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010762500762939453, AUC: 0.684442\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0108524964650472, AUC: 0.499499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013274700244267782, AUC: 0.6495525000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001373440424601237, AUC: 0.6880609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882926384607951, AUC: 0.5351134999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004438136577606201, AUC: 0.50352525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012416810989379883, AUC: 0.5918582499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013266102472941081, AUC: 0.6148197500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019509809811909993, AUC: 0.6372085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005216264883677164, AUC: 0.51125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010727325280507406, AUC: 0.499001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010864347219467162, AUC: 0.4997515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010905816157658894, AUC: 0.49875600000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003298620859781901, AUC: 0.51497625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011997053623199463, AUC: 0.64312625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001198989232381185, AUC: 0.648871\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001623408595720927, AUC: 0.6290437499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013772049347559611, AUC: 0.4500199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006249984105427, AUC: 0.47525675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973472197850546, AUC: 0.46956325000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010804060300191245, AUC: 0.5838855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class oversampled \n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "423b94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c14c3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0020289310614267984, AUC: 0.4756965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001091553012530009, AUC: 0.48456375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010886570612589517, AUC: 0.4860155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010853137572606406, AUC: 0.5097382500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019787512222925823, AUC: 0.57222475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011532610654830932, AUC: 0.49576575000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145702044169108, AUC: 0.4937625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001140582799911499, AUC: 0.49350675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005084774017333984, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010802105665206909, AUC: 0.50175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010755322376887005, AUC: 0.502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010662730137507121, AUC: 0.5022505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009249690055847169, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010924884875615438, AUC: 0.50599775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882760683695474, AUC: 0.5044992500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010864359935124715, AUC: 0.50449775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008460322062174478, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003564596176148, AUC: 0.5434515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010977611939112346, AUC: 0.527579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001091849128405253, AUC: 0.502861\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037917500336964926, AUC: 0.50300075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001107357660929362, AUC: 0.50625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011036542654037475, AUC: 0.50575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008251905441284, AUC: 0.503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00853164831797282, AUC: 0.48180575000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011391439437866211, AUC: 0.4952675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011303770144780478, AUC: 0.49850675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011254301071166992, AUC: 0.49128400000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017762763341267904, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011278401215871175, AUC: 0.46372199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021624008814494, AUC: 0.42442375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011078615188598632, AUC: 0.43288249999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007273403485616049, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011379459698994954, AUC: 0.47927200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011329193909962972, AUC: 0.4857515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011250438690185546, AUC: 0.53704125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00682664426167806, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001064486066500346, AUC: 0.4997495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010623377164204915, AUC: 0.49874950000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010606383085250855, AUC: 0.49924975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class undersampled  \n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaec9e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c779b879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.00608756939570109, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010771948893864949, AUC: 0.5007499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001078349749247233, AUC: 0.49949999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010784133275349936, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00372534704208374, AUC: 0.50023525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010370986064275106, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010473136504491171, AUC: 0.49649774999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010565840800603231, AUC: 0.5224875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020539817810058592, AUC: 0.50625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010878965854644776, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087069312731425, AUC: 0.49999999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010845698912938435, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006659661134084066, AUC: 0.49401975000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010928993225097657, AUC: 0.53216375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010782272815704345, AUC: 0.54923675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010686718225479126, AUC: 0.5544079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009116797844568889, AUC: 0.48513425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010646411975224813, AUC: 0.497999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010692731142044067, AUC: 0.502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010714724858601888, AUC: 0.50150125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015166940053304037, AUC: 0.48724999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011384790738423664, AUC: 0.49775050000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011099223693211873, AUC: 0.51426275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013285338083902994, AUC: 0.59251075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018370418151219686, AUC: 0.4615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010755572319030762, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010769031047821046, AUC: 0.5002502499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010774298906326295, AUC: 0.49999925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003305099407831828, AUC: 0.503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010899808804194133, AUC: 0.49999999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010876678625742595, AUC: 0.5000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010835784276326497, AUC: 0.5007499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010739267667134603, AUC: 0.502463\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001126469651858012, AUC: 0.47123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011237830718358358, AUC: 0.49575225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011101373434066772, AUC: 0.468339\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01887862459818522, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010957230726877849, AUC: 0.4975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010916781425476074, AUC: 0.50075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010887458721796672, AUC: 0.5009999999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class weighted\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['weight'] = torch.from_numpy(weight).float()\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8692b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81115d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011305590867996215, AUC: 0.4987425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101138710975647, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011463310718536376, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011780770619710286, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038984591166178386, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011603130102157593, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011915287176767985, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012100194692611695, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006762343406677246, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145007332166036, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011761978069941203, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011957573493321736, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017103184858957927, AUC: 0.4808077500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011710822582244873, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012000585397084554, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012189826170603433, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003416938861211141, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011359240611394246, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011698009570439657, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011919792890548707, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030593673388163247, AUC: 0.475337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000983761727809906, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982903023560842, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982608477274577, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001518441875775655, AUC: 0.46025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010580919981002807, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103336493174235, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001135496457417806, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003099643548329671, AUC: 0.5017499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001077566663424174, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010170272588729858, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001027000625928243, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026799618403116864, AUC: 0.4685945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011022898356119791, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011425824165344238, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001169986605644226, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001287020762761434, AUC: 0.513392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011173804601033528, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011543748378753662, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001180996815363566, AUC: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class focal loss\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['reduction'] = 'mean'\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SoftmaxFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf8eaef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d48470b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004159178098042806, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010713002681732179, AUC: 0.49825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010870064496994018, AUC: 0.49875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010937161048253376, AUC: 0.4980015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001217594623565674, AUC: 0.5495857500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010883294343948364, AUC: 0.54002475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009588491717974345, AUC: 0.6908785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010892333984375, AUC: 0.6341857500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012967588504155477, AUC: 0.44993225000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010879942576090494, AUC: 0.49974900000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010939101775487264, AUC: 0.4984995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010963983138402302, AUC: 0.4980035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009516664822896322, AUC: 0.4979995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010874385436375935, AUC: 0.502996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010935383637746174, AUC: 0.50224575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010963813463846842, AUC: 0.50149875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008401273687680562, AUC: 0.473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001117892305056254, AUC: 0.50074975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110655943552653, AUC: 0.49974875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101777672767639, AUC: 0.49874875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018197454214096069, AUC: 0.642566\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010871400435765585, AUC: 0.502739\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010544216235478718, AUC: 0.7166375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009863032698631286, AUC: 0.7337182500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014161507288614908, AUC: 0.6221422499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001115713357925415, AUC: 0.49924975000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011057165066401164, AUC: 0.5000005000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011015222469965616, AUC: 0.50025025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00627077309290568, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003833611806233, AUC: 0.51025525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096467614173889, AUC: 0.5330164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010366939703623454, AUC: 0.669718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014923743406931558, AUC: 0.6397665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001040031870206197, AUC: 0.7275147499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010351263682047526, AUC: 0.6475892499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011037707328796387, AUC: 0.54751375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01015039348602295, AUC: 0.49\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010485761562983194, AUC: 0.66979875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009980746308962504, AUC: 0.723652\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009460805654525757, AUC: 0.700749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class SMOTE\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "717e9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bfe9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.005159725666046142, AUC: 0.47875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013107466697692871, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014245965083440144, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013205681641896565, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036626233259836835, AUC: 0.5045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012807045380274454, AUC: 0.55175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008384430607159932, AUC: 0.6527735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013193607727686565, AUC: 0.58771125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001882831374804179, AUC: 0.568451\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014629533290863037, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013556772470474244, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012844537496566773, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005783709685007731, AUC: 0.5153915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014496474663416544, AUC: 0.68025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013263502518335978, AUC: 0.72725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005901109576225281, AUC: 0.6950000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015779575983683267, AUC: 0.47575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017115784088770549, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001690352439880371, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016737443208694458, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024884181340535484, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016858530044555663, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016851926644643148, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016747065782546997, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014013916651407878, AUC: 0.4033884999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001159618854522705, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012145692507425944, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006874272624651591, AUC: 0.7312500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005693643728892008, AUC: 0.49025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016551088094711303, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016597933371861775, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001664273738861084, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004257233619689942, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017054282824198405, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001689716895421346, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016794553200403849, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009325793266296387, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016158297061920166, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018122962315877279, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001287239154179891, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011229380289713542, AUC: 0.4915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009665106336275737, AUC: 0.6430379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009809208512306213, AUC: 0.7527905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010630707343419392, AUC: 0.5388919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002801464796066284, AUC: 0.4945689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010990440448125204, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098304033279419, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097338914871216, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011395713488260904, AUC: 0.44925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010731459856033324, AUC: 0.5862499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009436763922373454, AUC: 0.63125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001167917807896932, AUC: 0.5477055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003414260149002075, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010740158557891845, AUC: 0.5370075000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010959016879399618, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010975220600763958, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005730849742889404, AUC: 0.50174975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098487933476766, AUC: 0.49950649999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005215644836425, AUC: 0.56338675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011249771118164063, AUC: 0.65596625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005885673840840658, AUC: 0.505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011182666222254436, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001144629160563151, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011247365872065227, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01593700949350993, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096144715944926, AUC: 0.4999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097902218500773, AUC: 0.49725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003881295522054, AUC: 0.4957484999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010915645360946656, AUC: 0.43125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099284609158834, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010995355049769084, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991565783818563, AUC: 0.5025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023588163852691652, AUC: 0.497\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002196073532104, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011001572211583456, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099966843922933, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003621602535247803, AUC: 0.5055245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010969852209091187, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988662242889405, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010969030062357584, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008199299812316894, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011139464775721231, AUC: 0.5786862499999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001256147066752116, AUC: 0.5299999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011905717055002848, AUC: 0.5347545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029200942516326906, AUC: 0.50149925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984888474146525, AUC: 0.49974999999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011014104684193928, AUC: 0.49749899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099079688390096, AUC: 0.4964985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013806982835133871, AUC: 0.5180030000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998592774073284, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011013609170913697, AUC: 0.49625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106422464052836, AUC: 0.49724299999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008860953013102213, AUC: 0.49525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010979174375534057, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005378564198811, AUC: 0.496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002195278803507, AUC: 0.49825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026838178634643554, AUC: 0.498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992279847462972, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998342831929524, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989136298497518, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00352239465713501, AUC: 0.4201495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992273886998494, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991250673929851, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096076528231303, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003739197254180908, AUC: 0.5009999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010914430618286133, AUC: 0.7057055000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009601728518803914, AUC: 0.67756775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001175796906153361, AUC: 0.5605000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007436930020650228, AUC: 0.49375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989174445470175, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984821716944378, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095758557319641, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011664613405863444, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973004500071208, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010976841449737548, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010974336862564088, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001008460839589437, AUC: 0.46063449999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010979483524958292, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099142074584961, AUC: 0.4995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class capped loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-2]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args, smote=True)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for i in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[i][0]\n",
    "    auc_variance = cap_aucs[i][1]\n",
    "    cap = caps[i]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", 3, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b3e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c453da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0967e450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
