{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c3c2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378c3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91dda12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "NUM_CLASSES_REDUCED = 2\n",
    "nums = (0, 1)\n",
    "ratio = (100, 1)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b57e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "          #   \"mean_40\", \"variance_40\",\n",
    "          #   \"mean_50\", \"variance_50\",\n",
    "             \"cap\", \"normalization\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c2a1ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "norm=True\n",
    "\n",
    "if norm:\n",
    "    transform=torchvision.transforms.Compose([torchvision.transforms.Normalize(mean=[143.8888, 127.1705, 117.5357], std=[69.8313, 64.5137, 66.9933])])\n",
    "else:\n",
    "   # transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "    transform=None\n",
    "\n",
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "    \n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums, transform=transform)\n",
    "\n",
    "triplet_train_CIFAR10 = class_sampling.ForTripletLoss(reduced_train_CIFAR10, smote=False, transform=transform)\n",
    "triplet_ratio_train_CIFAR10 = class_sampling.ForTripletLoss(ratio_train_CIFAR10, smote=False, transform=transform)\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED, transform=transform)\n",
    "triplet_smote_train_CIFAR10 = class_sampling.ForTripletLoss(smote_train_CIFAR10, smote=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13159c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000   50]\n"
     ]
    }
   ],
   "source": [
    "targets = ratio_train_CIFAR10.labels \n",
    "\n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "print(class_count)\n",
    "\n",
    "weight = 1. / class_count\n",
    "\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "undersampler_smote = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * 50 * NUM_CLASSES_REDUCED), replacement=False)\n",
    "weight *= class_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af8cd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_smote_undersampled = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler_smote)\n",
    "\n",
    "train_loader_tripletloss = DataLoader(triplet_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_ratio = DataLoader(triplet_ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_smote = DataLoader(triplet_smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97b7ae03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006989752054214478, AUC: 0.306551\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 18\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     20\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:25\u001b[0m, in \u001b[0;36mtrain_sigmoid\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     24\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2 CLASS normal\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2, 1e-2, 1e-3, 5e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8f1d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d211694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006964986324310303, AUC: 0.36889099999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020484524965286256, AUC: 0.7839530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008004228307837897, AUC: 0.795284\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019669443964958192, AUC: 0.8054380000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007447379432839922, AUC: 0.845868\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001964762210845947, AUC: 0.8220825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006831325510387668, AUC: 0.8975879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007002675533294678, AUC: 0.29741149999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002268509268760681, AUC: 0.7684610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000813441066587768, AUC: 0.779452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001961479306221008, AUC: 0.8012969999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007347470497952239, AUC: 0.8548\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001975426435470581, AUC: 0.8130399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006815541962548943, AUC: 0.888204\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006969135999679566, AUC: 0.396233\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019460168480873107, AUC: 0.77575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008138178086074272, AUC: 0.786416\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018313562273979188, AUC: 0.7987635000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007782826017551493, AUC: 0.83284\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019774017333984376, AUC: 0.8090884999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007228488456642273, AUC: 0.8668359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006919987499713898, AUC: 0.590628\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025043778419494627, AUC: 0.7681610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000824363611656988, AUC: 0.7828759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001947782337665558, AUC: 0.812111\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007398436336529137, AUC: 0.8474480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018197112679481505, AUC: 0.825343\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006961135962738259, AUC: 0.8962600000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007070669531822204, AUC: 0.324542\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022070237398147583, AUC: 0.777849\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007834381481982989, AUC: 0.8004519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017794649600982666, AUC: 0.804418\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007518135676953462, AUC: 0.869884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018613035678863524, AUC: 0.821823\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006768840757927092, AUC: 0.905208\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006920409202575684, AUC: 0.5614465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019371829628944397, AUC: 0.7898210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007722149164148486, AUC: 0.814532\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019019254446029663, AUC: 0.8066430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007144288805377956, AUC: 0.8728039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019621884822845458, AUC: 0.8117289999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006771543613717993, AUC: 0.8938480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006985616087913513, AUC: 0.402626\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022849451303482054, AUC: 0.7709100000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007966125646772066, AUC: 0.7914239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019928234815597533, AUC: 0.8113159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007288691906793283, AUC: 0.850916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021587210893630982, AUC: 0.8282889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006852457171656412, AUC: 0.8815680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006967081129550934, AUC: 0.490978\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002309197545051575, AUC: 0.7767790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008197864576584042, AUC: 0.787224\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016714023947715759, AUC: 0.796902\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008205616820861798, AUC: 0.868436\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001921132743358612, AUC: 0.814014\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935946207198471, AUC: 0.882064\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006814700961112976, AUC: 0.6930545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017630945444107055, AUC: 0.797293\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007966739678810728, AUC: 0.826472\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018730688691139222, AUC: 0.8231759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006986736052549712, AUC: 0.8817240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001585574746131897, AUC: 0.8531839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006644394956897981, AUC: 0.9270039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006954295635223388, AUC: 0.4636135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002018611252307892, AUC: 0.751193\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007984958703417589, AUC: 0.774552\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017926790118217469, AUC: 0.7881119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007826962842061968, AUC: 0.847824\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001984425067901611, AUC: 0.8044880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006981643691363901, AUC: 0.886428\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006964466571807861, AUC: 0.408804\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002466729164123535, AUC: 0.7191185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008746101727506311, AUC: 0.7285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023755837678909302, AUC: 0.7475769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000830049450407819, AUC: 0.7640520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002291519284248352, AUC: 0.769038\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008008942412821078, AUC: 0.790772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006984891295433044, AUC: 0.2990615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024695838689804076, AUC: 0.702512\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000903661599899135, AUC: 0.703584\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002255721092224121, AUC: 0.733269\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008449238319281895, AUC: 0.7426200000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002216058373451233, AUC: 0.745498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00081563748529937, AUC: 0.76258\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007006736695766449, AUC: 0.27604300000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002342958450317383, AUC: 0.709482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008892802150901591, AUC: 0.716492\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023358142375946044, AUC: 0.7353419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008375033682747053, AUC: 0.748888\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002135712385177612, AUC: 0.761386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008083367728545229, AUC: 0.779436\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929198205471039, AUC: 0.617357\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023249754905700685, AUC: 0.702419\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008810730036901365, AUC: 0.707332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022707459926605226, AUC: 0.723219\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008420162158326643, AUC: 0.737148\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022084383964538572, AUC: 0.751656\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008105676031053657, AUC: 0.7706759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006976652443408966, AUC: 0.31618199999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00242640483379364, AUC: 0.708145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008835200035004038, AUC: 0.717596\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002294458746910095, AUC: 0.7325029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000837876483761143, AUC: 0.745504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022259188890457154, AUC: 0.7540900000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008166247896739457, AUC: 0.76966\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948408782482148, AUC: 0.4378925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025573383569717405, AUC: 0.713814\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009001902463191216, AUC: 0.7248199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002298957109451294, AUC: 0.7394749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008347357915585289, AUC: 0.75346\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00230900776386261, AUC: 0.7645065000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00081061886752596, AUC: 0.7800199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006877087652683258, AUC: 0.691464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024140385389328, AUC: 0.698046\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008903174916522043, AUC: 0.70268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002340320587158203, AUC: 0.7352985000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008448387669528475, AUC: 0.7441639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002200485348701477, AUC: 0.756923\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008142537423547837, AUC: 0.7689999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988822519779206, AUC: 0.43333200000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002531559944152832, AUC: 0.7074045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008929316163763846, AUC: 0.7121\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023130635023117067, AUC: 0.7368629999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008417200209517586, AUC: 0.746704\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021978071928024294, AUC: 0.754598\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008112246306188921, AUC: 0.7669400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006983757019042968, AUC: 0.3209125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002440286636352539, AUC: 0.7050354999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008984846457617707, AUC: 0.706744\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0022208908796310426, AUC: 0.738314\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008432061213188537, AUC: 0.747452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021523762941360472, AUC: 0.7596109999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008099005716198152, AUC: 0.772908\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932830810546875, AUC: 0.540304\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002492153882980347, AUC: 0.69481\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008906742870475691, AUC: 0.70906\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002333818554878235, AUC: 0.721683\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008443353652474608, AUC: 0.739868\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002207614302635193, AUC: 0.743633\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000816104515907493, AUC: 0.7657440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693915456533432, AUC: 0.470027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019597501158714293, AUC: 0.6219365000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010087698723862666, AUC: 0.6026600000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025770732164382935, AUC: 0.649457\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009415403474583336, AUC: 0.63322\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026130679845809937, AUC: 0.668488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009363414068697112, AUC: 0.6562439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952630281448364, AUC: 0.44003899999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002121282696723938, AUC: 0.501671\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011203794852636829, AUC: 0.462036\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002640716314315796, AUC: 0.5757490000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001002348891763699, AUC: 0.545164\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026532732248306275, AUC: 0.618418\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009733501744299831, AUC: 0.596864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006954418122768402, AUC: 0.4471715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019567978382110597, AUC: 0.5443659999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001200668271446582, AUC: 0.5197800000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002592808127403259, AUC: 0.6007549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010081923288283961, AUC: 0.584496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026791830062866213, AUC: 0.635221\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009875340786495127, AUC: 0.6246160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946364641189576, AUC: 0.437509\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018418793678283692, AUC: 0.44055199999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001196526876180479, AUC: 0.44020000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002501891493797302, AUC: 0.569048\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009530799596173928, AUC: 0.5647760000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026047686338424685, AUC: 0.612415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009452534362784412, AUC: 0.610228\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918039619922638, AUC: 0.601547\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002175205945968628, AUC: 0.531949\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000998632746684079, AUC: 0.516276\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002629355549812317, AUC: 0.607276\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009485418508105939, AUC: 0.598236\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002678037881851196, AUC: 0.643075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009430882653756307, AUC: 0.638256\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006879782378673554, AUC: 0.6277915000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022157384157180785, AUC: 0.5750029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010036093595963303, AUC: 0.56244\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027285748720169067, AUC: 0.6079819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009884885016201747, AUC: 0.595452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027752447128295897, AUC: 0.633263\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009839284465196405, AUC: 0.6226480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000696372777223587, AUC: 0.395862\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020386130809783936, AUC: 0.5864360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009720208211848051, AUC: 0.590588\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025379515886306764, AUC: 0.6347349999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009204692729186303, AUC: 0.637304\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026244809627532957, AUC: 0.657347\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009247063012738334, AUC: 0.6611159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943791210651398, AUC: 0.45462250000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020633881092071535, AUC: 0.576015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009746211608595187, AUC: 0.5702480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002581100583076477, AUC: 0.637748\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009229023426328555, AUC: 0.6324919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002632107138633728, AUC: 0.6657649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009244124916852406, AUC: 0.662368\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952486932277679, AUC: 0.4182815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018663351535797119, AUC: 0.545241\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011194773346628294, AUC: 0.552808\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025204519033432005, AUC: 0.6163200000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009360123755834481, AUC: 0.623728\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026386983394622804, AUC: 0.6517930000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009299963736666901, AUC: 0.6604\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006928751170635223, AUC: 0.4950935000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022486521005630495, AUC: 0.5687530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009940252438335134, AUC: 0.555052\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002693565011024475, AUC: 0.616304\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009704065832798138, AUC: 0.6050719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027260791063308714, AUC: 0.6472209999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009642241993863689, AUC: 0.638994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870812177658081, AUC: 0.739843\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008490297496318817, AUC: 0.5857295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004328208394569925, AUC: 0.552808\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017542111873626708, AUC: 0.5989365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001168501495503553, AUC: 0.5705520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002265569806098938, AUC: 0.616207\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009791869404587414, AUC: 0.591996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000696898102760315, AUC: 0.5182770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008060028553009033, AUC: 0.278211\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006648399871174652, AUC: 0.2956880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017908037900924683, AUC: 0.4555199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012544574736073465, AUC: 0.43443200000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023184224367141723, AUC: 0.5466409999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009757834696902497, AUC: 0.52138\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006959914863109588, AUC: 0.4148805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011752314567565919, AUC: 0.576725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002258190761993427, AUC: 0.568176\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020100303888320923, AUC: 0.5972729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010602153155325662, AUC: 0.590644\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023770898580551146, AUC: 0.610203\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009749452314238147, AUC: 0.60552\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006949910223484039, AUC: 0.47264950000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007239996790885926, AUC: 0.6477555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006259306055484432, AUC: 0.641904\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001465491473674774, AUC: 0.6645909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014327549595053834, AUC: 0.660544\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020677268505096434, AUC: 0.663818\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010180411716498952, AUC: 0.658852\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006855813264846802, AUC: 0.704147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001160222053527832, AUC: 0.594452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002175763076779866, AUC: 0.582432\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002008086562156677, AUC: 0.611847\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001003693709942964, AUC: 0.60094\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023846813440322875, AUC: 0.62987\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009382291516233788, AUC: 0.6193000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006942566931247711, AUC: 0.46113950000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012747212052345277, AUC: 0.6141890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001783648311796755, AUC: 0.604588\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00202965784072876, AUC: 0.6359250000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009888098204489982, AUC: 0.62552\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002390943884849548, AUC: 0.6458510000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009440167406850522, AUC: 0.636112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006907444298267365, AUC: 0.6638065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001061254382133484, AUC: 0.559679\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027392128050917446, AUC: 0.5395599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019682028889656065, AUC: 0.5875830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010773851526993336, AUC: 0.571\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002398018717765808, AUC: 0.605855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982346836993895, AUC: 0.59148\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947148740291595, AUC: 0.491396\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010332340002059937, AUC: 0.639335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002572380872351108, AUC: 0.644908\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018369460701942444, AUC: 0.637613\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010801145913872387, AUC: 0.641616\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022674994468688966, AUC: 0.640206\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009589608891470597, AUC: 0.643248\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006972042322158814, AUC: 0.3433105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014003113508224488, AUC: 0.540345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001675022819844803, AUC: 0.536408\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002195003390312195, AUC: 0.5878695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009807692990075833, AUC: 0.581788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025299243927001955, AUC: 0.612595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009592226822630013, AUC: 0.60672\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006968080401420593, AUC: 0.42184750000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014318109154701233, AUC: 0.500686\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016474896108750069, AUC: 0.46987599999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002215456962585449, AUC: 0.5417299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010021077599929702, AUC: 0.5135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025008474588394166, AUC: 0.5724260000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009738422585374648, AUC: 0.5467920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929619610309601, AUC: 0.517834\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025628254413604737, AUC: 0.660749\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009607309042153382, AUC: 0.6539680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024684345722198486, AUC: 0.688854\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009149152947829502, AUC: 0.685716\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024153801202774047, AUC: 0.700696\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008824188794276797, AUC: 0.7013880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963621377944946, AUC: 0.39932549999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026565759181976316, AUC: 0.6451220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000960259028793414, AUC: 0.6375200000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002558474898338318, AUC: 0.6828970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009217527456175867, AUC: 0.682304\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023991605043411253, AUC: 0.703199\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008820832003424368, AUC: 0.706716\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006975791752338409, AUC: 0.339162\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026126396656036377, AUC: 0.658784\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009396092526356477, AUC: 0.658856\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024959851503372193, AUC: 0.6913860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009068760096030955, AUC: 0.698356\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023628296852111815, AUC: 0.710028\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000874362751140748, AUC: 0.7199519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935542821884155, AUC: 0.5349695000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026821036338806154, AUC: 0.6905810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009409535158941947, AUC: 0.6890879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025097055435180666, AUC: 0.7111405000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008983603248022275, AUC: 0.712144\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024522416591644287, AUC: 0.7268490000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008782912564860417, AUC: 0.7291799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902917921543121, AUC: 0.633925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002716934323310852, AUC: 0.656775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009502239078098889, AUC: 0.657456\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002546200513839722, AUC: 0.6893039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009040189766795329, AUC: 0.696444\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025260263681411745, AUC: 0.707875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008834564439620417, AUC: 0.7185640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006962230801582336, AUC: 0.4351415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002600910544395447, AUC: 0.678147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009459915709229979, AUC: 0.665432\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025221219062805176, AUC: 0.707263\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009089078047856837, AUC: 0.7019759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002376657962799072, AUC: 0.7258259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008787278932436268, AUC: 0.724556\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006895989775657653, AUC: 0.6795534999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026365914344787596, AUC: 0.6996745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000932357641084507, AUC: 0.696256\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024846131801605225, AUC: 0.721959\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008952125390567402, AUC: 0.72334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024358515739440916, AUC: 0.732541\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008663736498628808, AUC: 0.738012\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007075443267822266, AUC: 0.3425195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025817182064056397, AUC: 0.648787\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009326574619453733, AUC: 0.654364\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002425007462501526, AUC: 0.6846749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008935547655097919, AUC: 0.696696\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023255358934402464, AUC: 0.7044335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008635037645955782, AUC: 0.7195079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006972502768039703, AUC: 0.4017025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025414057970046997, AUC: 0.688927\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009161692572812928, AUC: 0.690264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024763214588165283, AUC: 0.713159\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008872966284835988, AUC: 0.717964\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002337065577507019, AUC: 0.726836\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008584197800569606, AUC: 0.7334040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947490572929382, AUC: 0.45900799999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025774240493774414, AUC: 0.678329\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009367089254763162, AUC: 0.674748\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025151158571243286, AUC: 0.703386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000898137059486886, AUC: 0.706268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002368255615234375, AUC: 0.7219849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008681333497498591, AUC: 0.727964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS ratio\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2, 1e-2, 1e-3, 5e-4, 5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "learning_rate_train_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                _, train_auc = metric_utils.auc_sigmoid(train_loader_ratio, network) \n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f2d0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f1ccb0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006984597146511078, AUC: 0.3728135\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 18\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_oversampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     20\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:25\u001b[0m, in \u001b[0;36mtrain_sigmoid\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     24\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2 CLASS oversampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2, 1e-2, 1e-3, 5e-3, 1e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "52d8c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b1541b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006928627490997315, AUC: 0.5610565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006705484390258789, AUC: 0.731087\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006308182775974274, AUC: 0.7689\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006022096574306488, AUC: 0.792463\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006950775682926178, AUC: 0.4702115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006825314462184907, AUC: 0.7292394999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006524708569049835, AUC: 0.754748\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006467648446559906, AUC: 0.7728849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006955726444721221, AUC: 0.3766905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007012077569961548, AUC: 0.601467\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006722729504108429, AUC: 0.7478565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006584925651550293, AUC: 0.764774\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918634176254272, AUC: 0.548633\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006640854477882385, AUC: 0.7368910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006435896456241608, AUC: 0.772193\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006132080554962159, AUC: 0.805193\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006965889036655426, AUC: 0.3598795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006798864006996155, AUC: 0.722772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006425350904464722, AUC: 0.760243\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006240421831607819, AUC: 0.7786550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006957835257053375, AUC: 0.4375025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006834719181060791, AUC: 0.711951\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006513985991477967, AUC: 0.7628889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006210027635097504, AUC: 0.7961360000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007116021811962127, AUC: 0.2701275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006652469336986542, AUC: 0.749688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006440185308456421, AUC: 0.780153\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006193165481090545, AUC: 0.7989299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007008937299251557, AUC: 0.3809365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006463136076927186, AUC: 0.7635879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005850833356380462, AUC: 0.7921960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005856163203716278, AUC: 0.8098865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006968954503536225, AUC: 0.411403\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943140625953674, AUC: 0.7122989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006632572412490845, AUC: 0.767749\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006245843768119812, AUC: 0.795972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006954509615898132, AUC: 0.42410250000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006669250130653382, AUC: 0.768473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006450056135654449, AUC: 0.786346\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006585321128368377, AUC: 0.7965205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006984304785728454, AUC: 0.3510465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006993257403373718, AUC: 0.445579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007002671062946319, AUC: 0.557559\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006986504197120666, AUC: 0.624322\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006875935792922974, AUC: 0.6208015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006796917617321014, AUC: 0.7305775000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006728697121143341, AUC: 0.732001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000661188006401062, AUC: 0.737393\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006826050281524658, AUC: 0.708147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006837069392204285, AUC: 0.688508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006812298893928528, AUC: 0.6901515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006734901666641236, AUC: 0.704798\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932272017002106, AUC: 0.5695859999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006879149675369263, AUC: 0.6111989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006847475469112396, AUC: 0.640729\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006875899136066436, AUC: 0.6689890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902289688587189, AUC: 0.6300385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006905774176120758, AUC: 0.638779\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006883034110069275, AUC: 0.677482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006861826181411743, AUC: 0.7036560000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006971223056316376, AUC: 0.4343925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006849060654640197, AUC: 0.6515434999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006698879599571228, AUC: 0.6940439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006588464975357056, AUC: 0.713968\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927119195461273, AUC: 0.519376\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000688279390335083, AUC: 0.6079015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000678226500749588, AUC: 0.666508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006683720946311951, AUC: 0.6944840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006899041533470154, AUC: 0.6205875000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006839993894100189, AUC: 0.6695689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006716804206371307, AUC: 0.7078499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006677893996238709, AUC: 0.720974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930917203426361, AUC: 0.534002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947171986103058, AUC: 0.5545225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963235139846802, AUC: 0.5801274999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006964735686779023, AUC: 0.6224324999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932185888290405, AUC: 0.5061089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940591931343078, AUC: 0.546071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943828165531158, AUC: 0.6247775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941763460636139, AUC: 0.6806765000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000687966525554657, AUC: 0.7050644999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006887478828430176, AUC: 0.6989965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894939541816712, AUC: 0.690177\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006904767751693726, AUC: 0.6740535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929472386837006, AUC: 0.520876\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930617392063141, AUC: 0.510615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929692327976226, AUC: 0.5129855000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930001080036163, AUC: 0.509326\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000698872059583664, AUC: 0.252758\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988219320774078, AUC: 0.25494649999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988555192947388, AUC: 0.258632\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988353133201599, AUC: 0.26737\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007016754448413849, AUC: 0.3065925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007013347446918487, AUC: 0.3021595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007006541788578033, AUC: 0.3044395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006997955143451691, AUC: 0.3110185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947731077671051, AUC: 0.600099\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941960752010345, AUC: 0.5978035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693750262260437, AUC: 0.5939255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934162974357605, AUC: 0.5842645000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006975911855697632, AUC: 0.3350575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006968346834182739, AUC: 0.35933800000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006962250173091889, AUC: 0.392072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006956771612167358, AUC: 0.43028\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948958039283752, AUC: 0.42235900000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947318315505981, AUC: 0.42986450000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946011483669281, AUC: 0.4361265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945200264453888, AUC: 0.44007999999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006920962631702423, AUC: 0.5704765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006916036903858184, AUC: 0.600501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006912074089050293, AUC: 0.621478\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006909322440624237, AUC: 0.6336729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006977652311325074, AUC: 0.3840535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973227560520172, AUC: 0.3848955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006969808340072631, AUC: 0.38405150000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000696704238653183, AUC: 0.383508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006994488835334778, AUC: 0.386849\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006997025310993194, AUC: 0.392687\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006996612548828125, AUC: 0.40394399999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006992768943309784, AUC: 0.42428699999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988563239574432, AUC: 0.30963050000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006964642107486725, AUC: 0.4378635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936416327953339, AUC: 0.530629\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006915313899517059, AUC: 0.585804\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936706900596618, AUC: 0.5052385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943995654582978, AUC: 0.5241214999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006942475736141204, AUC: 0.5752839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930316984653473, AUC: 0.618519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006896259486675263, AUC: 0.604004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006859594881534576, AUC: 0.636279\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.000681488573551178, AUC: 0.661619\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006757974326610565, AUC: 0.6790075000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006896138191223144, AUC: 0.610887\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893069446086884, AUC: 0.613855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006885486543178558, AUC: 0.633282\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006888088881969452, AUC: 0.6441379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000698607712984085, AUC: 0.47644650000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006970592141151428, AUC: 0.5706005000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006956733465194702, AUC: 0.6301249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951669156551361, AUC: 0.6572\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006925795376300812, AUC: 0.575475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951481699943542, AUC: 0.5105139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006970000267028808, AUC: 0.4651495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000699210911989212, AUC: 0.424679\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938398480415344, AUC: 0.49060099999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006900518834590912, AUC: 0.6081639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006852288544178009, AUC: 0.6609965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006791809201240539, AUC: 0.6885735000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006768987178802491, AUC: 0.7593004999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000679383397102356, AUC: 0.7485544999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006822780966758728, AUC: 0.7235449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006835326850414276, AUC: 0.703198\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000697054535150528, AUC: 0.412941\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006958455443382264, AUC: 0.4202635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951872408390046, AUC: 0.4582435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693785697221756, AUC: 0.5277975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936173141002655, AUC: 0.49884150000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000692090779542923, AUC: 0.5442275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006922757029533387, AUC: 0.548847\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006904305517673493, AUC: 0.5985255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006974986791610717, AUC: 0.47294749999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006974535882472992, AUC: 0.47196799999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006974373161792755, AUC: 0.470164\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006974062919616699, AUC: 0.4685745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945805549621582, AUC: 0.511066\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945959925651551, AUC: 0.512351\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946030259132385, AUC: 0.513831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946027278900146, AUC: 0.5153995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007035338878631592, AUC: 0.3031575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000703522503376007, AUC: 0.3047515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000703513115644455, AUC: 0.306492\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000703542560338974, AUC: 0.307971\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006903063356876373, AUC: 0.6405325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902830302715302, AUC: 0.6414055000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902560889720917, AUC: 0.642237\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902223825454712, AUC: 0.6432424999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973913013935089, AUC: 0.3781635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973600089550018, AUC: 0.37925449999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973490417003632, AUC: 0.37986600000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973181366920472, AUC: 0.3809785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006964112520217896, AUC: 0.37526950000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963879466056823, AUC: 0.3755745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000696378231048584, AUC: 0.3756815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963653862476349, AUC: 0.375803\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952761113643647, AUC: 0.4577645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952200829982758, AUC: 0.45907450000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695160061120987, AUC: 0.46040699999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006950892806053162, AUC: 0.462167\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000690339595079422, AUC: 0.6339035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006903114020824433, AUC: 0.634817\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902790367603302, AUC: 0.6358309999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902419328689575, AUC: 0.6368320000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006928563117980957, AUC: 0.5400750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927987933158875, AUC: 0.5429109999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927626430988312, AUC: 0.544478\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927131414413453, AUC: 0.5468745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936696171760559, AUC: 0.48374\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936245560646057, AUC: 0.48604149999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935936212539673, AUC: 0.4878675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693551242351532, AUC: 0.48999699999999996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS undersampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2, 1e-2, 1e-3, 5e-3, 1e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fda3918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cdf3dafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006929865777492523, AUC: 0.6035210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941011548042297, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01152815311261923, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933517456054688, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010536302033037242, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934399306774139, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01047680665950964, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006829564273357391, AUC: 0.7606660000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007110981047153472, AUC: 0.513034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008160775073684089, AUC: 0.4939\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006991162598133087, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009259643141586001, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932158470153809, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011024211881184342, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006900024712085724, AUC: 0.6275345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006032556593418122, AUC: 0.6915875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009716101306499822, AUC: 0.67254\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006006793975830078, AUC: 0.703085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010080904476713426, AUC: 0.68551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693379133939743, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010516728127356803, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006867653727531433, AUC: 0.6620455000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941483020782471, AUC: 0.5020020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0101606553025765, AUC: 0.5024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947364509105683, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011732829313467044, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932136416435242, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011021087063421118, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946891844272614, AUC: 0.45950349999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940916180610656, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010191532998982043, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006905839145183564, AUC: 0.5271815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010548212835104159, AUC: 0.585972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006954963207244873, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009828539650038918, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695802628993988, AUC: 0.5831175000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006958652138710022, AUC: 0.594743\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008937520402492863, AUC: 0.6741199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006913960874080657, AUC: 0.532716\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009897223864451493, AUC: 0.5484240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006926067471504212, AUC: 0.5197149999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009974045635450004, AUC: 0.543572\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007032238245010376, AUC: 0.26280099999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963876187801361, AUC: 0.5065189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009458458411811602, AUC: 0.5388360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007120804488658906, AUC: 0.567307\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008191924195478458, AUC: 0.6288479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007221527099609375, AUC: 0.5354529999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007242459070564497, AUC: 0.6090860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007022532224655151, AUC: 0.309856\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940309107303619, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01150185514204573, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932080686092377, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011013430510417069, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936182677745819, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011321447296897963, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007022981643676758, AUC: 0.279428\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933487057685852, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010538602512661773, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937902569770813, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011403333208348491, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006939776837825775, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011481297110566998, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007030169069766998, AUC: 0.26083599999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941924095153809, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010158426478357598, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933222115039825, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011132937882206227, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947585344314575, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011739103463616701, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947209239006043, AUC: 0.503085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004985811412334442, AUC: 0.8705590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008741397662918166, AUC: 0.910452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00048722681403160097, AUC: 0.8629110000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007839573692567277, AUC: 0.911974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005121163129806519, AUC: 0.831754\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006779720865853942, AUC: 0.9525359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931937634944916, AUC: 0.6319214999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005152012705802917, AUC: 0.8624559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010805733227493739, AUC: 0.8948520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005664720535278321, AUC: 0.842025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006664018843433645, AUC: 0.9242239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005163897573947907, AUC: 0.8882985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004746445078070801, AUC: 0.95862\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006993449330329895, AUC: 0.308698\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005618657171726227, AUC: 0.837395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007188617935275087, AUC: 0.8682\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005276458859443665, AUC: 0.85389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0053951541858144325, AUC: 0.9311919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005647776126861573, AUC: 0.8183910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0056284768156485985, AUC: 0.942612\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006987028419971466, AUC: 0.419359\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000473314568400383, AUC: 0.8638839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007454072691426419, AUC: 0.884772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005346084535121918, AUC: 0.8605094999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007051103150490487, AUC: 0.9267599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005636484026908874, AUC: 0.8834069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004816935280762096, AUC: 0.9464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006926956474781036, AUC: 0.62321\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005920693576335907, AUC: 0.872232\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00950404413855902, AUC: 0.898072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000488565668463707, AUC: 0.86691\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005895164248966934, AUC: 0.8858999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005560655891895294, AUC: 0.870534\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004082345602535966, AUC: 0.9216799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00069858717918396, AUC: 0.3564225000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005433701276779174, AUC: 0.8413849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007599443150038766, AUC: 0.8711\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005662823319435119, AUC: 0.8394060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0061812017282637036, AUC: 0.9136279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006344362497329712, AUC: 0.863313\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004362780357351398, AUC: 0.933392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006920470595359802, AUC: 0.626553\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005774954557418823, AUC: 0.8625749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005773622346396493, AUC: 0.8950400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005901984572410584, AUC: 0.8413899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005424396850094937, AUC: 0.9351479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005415751934051514, AUC: 0.844387\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005204110599980496, AUC: 0.9476119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006909263730049133, AUC: 0.608957\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005072842240333557, AUC: 0.848906\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006716722314900691, AUC: 0.88254\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000501204401254654, AUC: 0.8605349999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005464349566119732, AUC: 0.916816\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005297437012195587, AUC: 0.852255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006041818249343646, AUC: 0.951712\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929771304130555, AUC: 0.6099144999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005701894760131835, AUC: 0.810224\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007652453461495957, AUC: 0.9052879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005346051752567291, AUC: 0.865758\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004242133191316434, AUC: 0.94032\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005565256476402282, AUC: 0.8667680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006119210065001308, AUC: 0.932944\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973126232624054, AUC: 0.389163\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0005637988150119782, AUC: 0.828475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00714115304521995, AUC: 0.9039240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005115714669227601, AUC: 0.872054\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0058704357690150194, AUC: 0.9506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005990622639656067, AUC: 0.854916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004999826568188054, AUC: 0.9478759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893006265163422, AUC: 0.592972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005413131713867187, AUC: 0.838742\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009401522638774154, AUC: 0.873108\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005082505941390992, AUC: 0.8643450000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007597315765843533, AUC: 0.93034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005349463224411011, AUC: 0.819852\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005960197472336269, AUC: 0.952184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006917398273944855, AUC: 0.5357335000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005320677161216736, AUC: 0.872421\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007882021895729669, AUC: 0.885336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047688519954681394, AUC: 0.8764339999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006409961439595364, AUC: 0.9162440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004914150685071945, AUC: 0.8588819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005879358218447997, AUC: 0.94408\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951043009757996, AUC: 0.4402385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005084710121154785, AUC: 0.8479289999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007602337244713661, AUC: 0.887208\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004919102936983108, AUC: 0.8564430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006338312826534309, AUC: 0.9233879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005057655572891235, AUC: 0.839117\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006557884688424592, AUC: 0.94724\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006957819759845733, AUC: 0.554511\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005380403399467468, AUC: 0.8291689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009521948453223352, AUC: 0.906452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004899816066026687, AUC: 0.8586830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00712057093582531, AUC: 0.9333940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005030784308910369, AUC: 0.850487\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0057055700031837615, AUC: 0.94844\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007004860043525696, AUC: 0.3607595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00053057861328125, AUC: 0.8529720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0070074458405523015, AUC: 0.886212\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004892897009849548, AUC: 0.868004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005604852668129571, AUC: 0.920404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000458210751414299, AUC: 0.875929\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006420788363655015, AUC: 0.946916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940570473670959, AUC: 0.4731449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005776671469211579, AUC: 0.8586539999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012373634175498888, AUC: 0.8687280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00044246050715446473, AUC: 0.889369\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0070867665805438955, AUC: 0.9181079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005300732553005219, AUC: 0.8762265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005396853927338478, AUC: 0.960264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929945051670075, AUC: 0.564638\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005563854575157166, AUC: 0.8648770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009520969921999639, AUC: 0.923976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005157967507839203, AUC: 0.8525540000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006059927674803403, AUC: 0.939404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000465511754155159, AUC: 0.877534\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006717685177774713, AUC: 0.948132\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006980034112930298, AUC: 0.392889\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005355749130249024, AUC: 0.8342959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007846096653749447, AUC: 0.8555839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004474954456090927, AUC: 0.879305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00643831486159032, AUC: 0.898184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046650290489196777, AUC: 0.887073\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00500908629138871, AUC: 0.929724\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006906140744686127, AUC: 0.6270205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005572724044322967, AUC: 0.85573\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008027610229973746, AUC: 0.898772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005564309954643249, AUC: 0.8470209999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004956791843518172, AUC: 0.911748\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004649658203125, AUC: 0.866714\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006907176440305049, AUC: 0.956392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938843429088593, AUC: 0.47562899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000519424855709076, AUC: 0.8551110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007493531125606877, AUC: 0.8831279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000504085659980774, AUC: 0.8609840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006028514882125476, AUC: 0.9416640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004914458394050599, AUC: 0.867466\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006038065062891139, AUC: 0.96608\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963664293289185, AUC: 0.38524\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006432399749755859, AUC: 0.7625959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008959613684380408, AUC: 0.7848919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005628331005573272, AUC: 0.826678\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00864995011598757, AUC: 0.84258\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005056944489479065, AUC: 0.8590329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00824941654016476, AUC: 0.871856\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006910701990127564, AUC: 0.599355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000616879791021347, AUC: 0.7707120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01059920307433251, AUC: 0.781044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005687816441059113, AUC: 0.799205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009738183611690408, AUC: 0.8164119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005393176674842834, AUC: 0.8172925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008101473804747704, AUC: 0.842912\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006939460039138794, AUC: 0.54028\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006021830439567566, AUC: 0.7970139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010496961374093991, AUC: 0.803984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005550967454910278, AUC: 0.828073\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009545001192848282, AUC: 0.839732\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005240070223808289, AUC: 0.844389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0091742208452508, AUC: 0.8625879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006914302408695221, AUC: 0.6094385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005888742804527283, AUC: 0.824057\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01029107907030842, AUC: 0.83066\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005443206429481506, AUC: 0.8330359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009114989762259001, AUC: 0.849788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005262280106544494, AUC: 0.853632\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008392372603463654, AUC: 0.878884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006837622821331024, AUC: 0.691576\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005895999670028686, AUC: 0.782758\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009808489296695973, AUC: 0.80072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005589506924152375, AUC: 0.806367\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009164169448437077, AUC: 0.8281080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005357769429683685, AUC: 0.8292949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00911985157149853, AUC: 0.8574439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945233047008514, AUC: 0.48311099999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006134748458862304, AUC: 0.806477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01052536406139336, AUC: 0.808596\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005562895834445953, AUC: 0.822084\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009412364829884898, AUC: 0.833688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005309949219226837, AUC: 0.8396819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007839170440588848, AUC: 0.857212\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937495470046997, AUC: 0.48171200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006372389495372772, AUC: 0.819923\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0120068094163838, AUC: 0.8215159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005695891678333283, AUC: 0.822279\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009782834808425149, AUC: 0.840788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005112173855304718, AUC: 0.8443940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008199451105429394, AUC: 0.868456\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006921443343162536, AUC: 0.5599354999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005984731018543243, AUC: 0.8115479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010888017746481566, AUC: 0.82364\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005443423092365265, AUC: 0.8276000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009130165565131914, AUC: 0.8468919999999999\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0005055390298366547, AUC: 0.849865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008350928868397627, AUC: 0.8736079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963136792182922, AUC: 0.39968349999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005984225273132324, AUC: 0.771389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009001542176350508, AUC: 0.78562\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000558992862701416, AUC: 0.8067799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008595412568290635, AUC: 0.829488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000538530021905899, AUC: 0.82298\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007287021987509019, AUC: 0.8586119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006968409717082977, AUC: 0.38355749999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006436823904514313, AUC: 0.803137\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01135210377154964, AUC: 0.7909520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000583594560623169, AUC: 0.8077380000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01014432730060993, AUC: 0.820932\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005280269980430603, AUC: 0.833491\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008627873015875864, AUC: 0.8455999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870892643928528, AUC: 0.656811\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006438588798046112, AUC: 0.785184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011279767680876325, AUC: 0.77802\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005855887830257415, AUC: 0.790768\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009658517200167816, AUC: 0.799412\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005556738078594208, AUC: 0.8051299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008975181662210143, AUC: 0.823952\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007047882378101349, AUC: 0.3414305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006809703409671784, AUC: 0.7605970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010495741154887888, AUC: 0.7505620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006465067863464356, AUC: 0.7901370000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010764568288727561, AUC: 0.7835139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006005278527736664, AUC: 0.7971060000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010074336410749077, AUC: 0.8006639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952670216560364, AUC: 0.435129\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006432079970836639, AUC: 0.7717779999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010464417521316226, AUC: 0.7932920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005900460183620453, AUC: 0.7872790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009503405046935129, AUC: 0.809028\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005679526627063752, AUC: 0.8025730000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008954270877460441, AUC: 0.825492\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006970075964927673, AUC: 0.3704055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006620379984378815, AUC: 0.8133829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010983516284734896, AUC: 0.824464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005913313031196594, AUC: 0.8132949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009973079898569843, AUC: 0.826076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005637726485729218, AUC: 0.8200820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009417588061625414, AUC: 0.837888\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006897422671318054, AUC: 0.598884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006093161702156067, AUC: 0.7742269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00976294757115959, AUC: 0.785188\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005706937611103058, AUC: 0.7944450000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008533435092113986, AUC: 0.8078\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005440973341464996, AUC: 0.820147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008820098668041796, AUC: 0.837528\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000686633974313736, AUC: 0.66552\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006343769133090973, AUC: 0.7708459999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00998953613904443, AUC: 0.77996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000589023768901825, AUC: 0.781485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009906717373593018, AUC: 0.791872\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005734374821186065, AUC: 0.7928010000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009384710352019508, AUC: 0.8104199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006928159296512603, AUC: 0.5240104999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006608406603336335, AUC: 0.7565659999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010737750010915321, AUC: 0.761488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006106067299842835, AUC: 0.7730439999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010074301509573908, AUC: 0.781744\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005812602043151855, AUC: 0.7897865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009442508126249407, AUC: 0.805176\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943551301956176, AUC: 0.5027984999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006626200675964356, AUC: 0.8112789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010643097292078603, AUC: 0.82374\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005914508402347564, AUC: 0.8046445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010367770584503022, AUC: 0.8259\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005529929101467133, AUC: 0.8113150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009098475008907885, AUC: 0.837008\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006970823407173157, AUC: 0.3750015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006655546128749848, AUC: 0.788638\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011142519086894422, AUC: 0.803384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005962426960468292, AUC: 0.7917019999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010091654510781317, AUC: 0.800416\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005699568390846253, AUC: 0.8046564999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009837563828666612, AUC: 0.8201919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006922467052936554, AUC: 0.5355885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006529574692249298, AUC: 0.762027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011207672263136004, AUC: 0.763824\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006132130324840545, AUC: 0.778235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010650047831016012, AUC: 0.7884599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005724292099475861, AUC: 0.790807\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00923902967188618, AUC: 0.8058000000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 Class Weighted Loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2, 1e-2, 5e-3, 1e-3, 5e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['pos_weight'] = torch.tensor([weight[1]])\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                _, train_auc = metric_utils.auc_sigmoid(train_loader_ratio, network) \n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9eed2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b7c5a840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006901857256889344, AUC: 0.6220315000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008553669452667236, AUC: 0.9138239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014686028361320495, AUC: 0.9061444999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017553408741950989, AUC: 0.886091\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006922211647033691, AUC: 0.565235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001127453625202179, AUC: 0.8912439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015964934825897217, AUC: 0.880198\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013375398516654968, AUC: 0.885392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000696012943983078, AUC: 0.603688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007503098547458649, AUC: 0.892879\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00129351145029068, AUC: 0.9026019999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012972863912582397, AUC: 0.855303\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006966092586517334, AUC: 0.5215055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009291221201419831, AUC: 0.8892810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008040020763874054, AUC: 0.898658\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017433229088783264, AUC: 0.8796330000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006977330446243286, AUC: 0.404773\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005630791485309601, AUC: 0.910266\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016352534890174866, AUC: 0.885416\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013343620896339417, AUC: 0.8936780000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000691230446100235, AUC: 0.5547385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007620195448398591, AUC: 0.883143\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006794710755348205, AUC: 0.8399070000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016237211227416992, AUC: 0.8573169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006968526542186737, AUC: 0.4018345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009450372755527496, AUC: 0.8874509999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012348506450653076, AUC: 0.8850199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002453988432884216, AUC: 0.82448\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951379179954528, AUC: 0.397533\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010436472594738007, AUC: 0.907822\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016297143697738647, AUC: 0.8992685000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002018626093864441, AUC: 0.8498209999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007021846175193786, AUC: 0.32037250000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010350289344787597, AUC: 0.876231\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012405900359153748, AUC: 0.9007609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016026598811149596, AUC: 0.891613\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006897753477096558, AUC: 0.6965105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008582993149757385, AUC: 0.8845770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012737736701965332, AUC: 0.886461\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018909823894500731, AUC: 0.876094\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948190629482269, AUC: 0.4760675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010080679059028626, AUC: 0.8555240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008508298099040985, AUC: 0.899805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011516088247299195, AUC: 0.886862\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006864569187164306, AUC: 0.671481\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007861338555812836, AUC: 0.866819\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008237427175045014, AUC: 0.8950379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017165331840515137, AUC: 0.8490299999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006872381567955017, AUC: 0.6785105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006143057942390442, AUC: 0.892696\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001054575264453888, AUC: 0.875094\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011076526045799256, AUC: 0.88234\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000692339301109314, AUC: 0.5541655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006036542356014252, AUC: 0.8996939999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000754842221736908, AUC: 0.8937110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930708587169647, AUC: 0.898335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931361854076386, AUC: 0.5170895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005433129072189331, AUC: 0.8727400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008253436684608459, AUC: 0.893324\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013004844188690187, AUC: 0.8565069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006906343102455139, AUC: 0.606333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005504031181335449, AUC: 0.9004179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006820035576820373, AUC: 0.900205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009654996693134308, AUC: 0.8897789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006906748116016388, AUC: 0.656041\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010030428171157837, AUC: 0.737005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000897089958190918, AUC: 0.8864400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012927242517471313, AUC: 0.8792959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944410502910614, AUC: 0.47686399999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005283983051776886, AUC: 0.902988\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008559342324733734, AUC: 0.896749\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013502340316772462, AUC: 0.879469\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006909123659133911, AUC: 0.6002335000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000634221464395523, AUC: 0.8818410000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013389257192611694, AUC: 0.8580110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013744723200798034, AUC: 0.876027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006905958354473114, AUC: 0.771349\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000837964653968811, AUC: 0.821427\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011401160955429076, AUC: 0.879561\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012560948729515077, AUC: 0.881369\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006922280490398407, AUC: 0.6079110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005008593648672104, AUC: 0.8652790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005890430808067322, AUC: 0.8978169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008568320870399475, AUC: 0.891869\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006921758651733398, AUC: 0.6202765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005149215161800385, AUC: 0.8753\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006580804586410522, AUC: 0.8868790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007403334975242615, AUC: 0.8929279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006924903094768524, AUC: 0.58918\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005173097848892212, AUC: 0.871326\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005324308872222901, AUC: 0.8999360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007019243538379669, AUC: 0.8963970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007030869424343109, AUC: 0.30496650000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005607136785984039, AUC: 0.8610945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007600637674331666, AUC: 0.8930819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011471549868583679, AUC: 0.870947\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006986162066459655, AUC: 0.32891349999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005190083682537079, AUC: 0.878211\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006851171255111694, AUC: 0.900366\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008238690495491027, AUC: 0.899673\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937920451164245, AUC: 0.5926784999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005446333885192872, AUC: 0.8839710000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007170597910881042, AUC: 0.88554\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006953013837337494, AUC: 0.862679\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000691821038722992, AUC: 0.5492885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005423327684402466, AUC: 0.870659\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000510511338710785, AUC: 0.9001859999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007822311520576477, AUC: 0.8873110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000697955310344696, AUC: 0.5062785000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000593943178653717, AUC: 0.8810669999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000742489755153656, AUC: 0.87711\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008794473111629486, AUC: 0.8725260000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006919487714767456, AUC: 0.581564\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005216581225395202, AUC: 0.888127\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005324023365974426, AUC: 0.909804\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007356446087360382, AUC: 0.898289\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943679749965668, AUC: 0.462395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005809506475925446, AUC: 0.8541889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934134960174561, AUC: 0.8910769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008193813562393189, AUC: 0.894914\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000698040246963501, AUC: 0.411863\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000610311895608902, AUC: 0.7618959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005486262738704682, AUC: 0.821666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000531688779592514, AUC: 0.8466899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927800178527832, AUC: 0.545199\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006162319779396057, AUC: 0.7750279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005593936145305633, AUC: 0.8077649999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005271974503993988, AUC: 0.8411755000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006914125978946685, AUC: 0.56717\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.000618213951587677, AUC: 0.7882954999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005512105226516724, AUC: 0.8245439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005293850600719452, AUC: 0.847711\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006969168782234192, AUC: 0.413656\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000612224131822586, AUC: 0.765173\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005718506872653961, AUC: 0.7946544999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005359521210193634, AUC: 0.8355079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935161054134369, AUC: 0.5336654999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000620194137096405, AUC: 0.757107\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005708503723144532, AUC: 0.788429\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000546052247285843, AUC: 0.8274469999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930411756038666, AUC: 0.5264515000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006314632296562195, AUC: 0.7641144999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005780141651630402, AUC: 0.791547\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005373639762401581, AUC: 0.834433\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006924226582050323, AUC: 0.6349295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000601875752210617, AUC: 0.770088\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000555262714624405, AUC: 0.814772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005279605090618134, AUC: 0.8477735000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007037942111492157, AUC: 0.2900315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006244870722293854, AUC: 0.77932\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005641791224479675, AUC: 0.800211\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005193292200565338, AUC: 0.8489395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007010957896709442, AUC: 0.353634\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005935918688774109, AUC: 0.789633\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005444709658622742, AUC: 0.824536\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005048559159040451, AUC: 0.8650190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693207859992981, AUC: 0.5663755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006279979646205903, AUC: 0.796206\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005920657813549042, AUC: 0.8310825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005615807175636292, AUC: 0.8602040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943883895874024, AUC: 0.48671949999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006859140992164611, AUC: 0.728529\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006765582263469696, AUC: 0.782863\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000666590005159378, AUC: 0.7884815000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006926088035106659, AUC: 0.6308835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006909513175487518, AUC: 0.687868\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006890037655830384, AUC: 0.7208245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006864253878593445, AUC: 0.7374455000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006854447126388549, AUC: 0.651871\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006807336211204529, AUC: 0.6985349999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006744709610939026, AUC: 0.7347125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006655621230602265, AUC: 0.757134\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006998248100280761, AUC: 0.3035655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006849582493305206, AUC: 0.6940000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006739082932472229, AUC: 0.7321355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006642336547374725, AUC: 0.742264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006987900733947754, AUC: 0.39294599999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006920880079269409, AUC: 0.563752\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000686019480228424, AUC: 0.7538505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006788695454597473, AUC: 0.776144\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006865765154361725, AUC: 0.707079\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000679928183555603, AUC: 0.7242580000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006732113659381866, AUC: 0.728086\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000666094720363617, AUC: 0.7304955000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006996052861213684, AUC: 0.336298\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947443783283234, AUC: 0.470945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006907279491424561, AUC: 0.5854215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006869761645793915, AUC: 0.6440655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931250989437104, AUC: 0.6262624999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006886908113956452, AUC: 0.747395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006843686997890472, AUC: 0.7705165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006795947849750519, AUC: 0.7761905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943590044975281, AUC: 0.441496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936343312263489, AUC: 0.4751765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929740309715271, AUC: 0.5086975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000692169576883316, AUC: 0.5454654999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006903612613677979, AUC: 0.682798\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006788653135299682, AUC: 0.8015135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006658481657505035, AUC: 0.80889\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006547509431838989, AUC: 0.8055849999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS SMOTE\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2, 1e-2, 5e-3, 1e-3, 1e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "66655269",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfecae8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0008589592576026916, AUC: 0.610939\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014835682511329651, AUC: 0.7226130000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001802017629146576, AUC: 0.747778\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016669080257415772, AUC: 0.7606769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014676600098609924, AUC: 0.632443\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011303189992904663, AUC: 0.767101\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008886577188968658, AUC: 0.8482649999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010313032269477845, AUC: 0.855079\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011315805912017823, AUC: 0.626282\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001719098687171936, AUC: 0.8240619999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010683817267417909, AUC: 0.861991\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009965504705905914, AUC: 0.873041\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001854050636291504, AUC: 0.549836\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016986330151557922, AUC: 0.7641640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016748768091201782, AUC: 0.8111499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008317158222198486, AUC: 0.859653\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004316972017288208, AUC: 0.5750379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014464004635810852, AUC: 0.7821085000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001291241466999054, AUC: 0.835337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000702275812625885, AUC: 0.85231\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005551239252090454, AUC: 0.7095159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009146456718444825, AUC: 0.8399679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006132896542549133, AUC: 0.882249\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005638539791107178, AUC: 0.8850049999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004852016448974609, AUC: 0.34383050000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010471785664558412, AUC: 0.755272\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005431660115718842, AUC: 0.8418399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007019554972648621, AUC: 0.8834139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008205856084823608, AUC: 0.662277\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005063630938529968, AUC: 0.8503080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005063291639089585, AUC: 0.8561860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047590817511081694, AUC: 0.8726369999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012812973856925964, AUC: 0.518579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008795447945594788, AUC: 0.796864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012950129508972168, AUC: 0.8266469999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016271740198135376, AUC: 0.840008\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003707490086555481, AUC: 0.3175175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017894402742385865, AUC: 0.791996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001506696105003357, AUC: 0.8111809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010145802497863769, AUC: 0.836327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008295180201530457, AUC: 0.606415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025020800828933717, AUC: 0.565468\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000597316026687622, AUC: 0.814934\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013427149653434754, AUC: 0.821645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002524172306060791, AUC: 0.5648690000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017761843800544739, AUC: 0.6158849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016764238476753234, AUC: 0.6731240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001382877767086029, AUC: 0.703181\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009337529838085174, AUC: 0.45383\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020695263147354126, AUC: 0.525357\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020653613805770876, AUC: 0.594084\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020202330350875854, AUC: 0.6477079999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006341984748840332, AUC: 0.548899\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015960598587989806, AUC: 0.818163\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024162321090698243, AUC: 0.805618\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025939264297485352, AUC: 0.8085659999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0045012452602386475, AUC: 0.5570515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001616586148738861, AUC: 0.8047839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000741992861032486, AUC: 0.8490609999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010059026777744294, AUC: 0.86404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035758501291275023, AUC: 0.7027699999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020322672724723814, AUC: 0.6914339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016390472054481507, AUC: 0.6951879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018370933532714845, AUC: 0.723343\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001033681571483612, AUC: 0.475789\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002128912329673767, AUC: 0.43105799999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017820977568626403, AUC: 0.5025780000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017787338495254517, AUC: 0.570817\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009110206961631775, AUC: 0.525798\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010960239171981811, AUC: 0.7888350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005965421795845031, AUC: 0.86071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006593474149703979, AUC: 0.875137\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009664712250232697, AUC: 0.56963\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002559073805809021, AUC: 0.575514\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023786275386810302, AUC: 0.636227\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002123912334442139, AUC: 0.6617949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013562902212142945, AUC: 0.5332239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001564046621322632, AUC: 0.69399\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00116363525390625, AUC: 0.807909\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017350338697433472, AUC: 0.804951\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002764394760131836, AUC: 0.554037\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031668816804885863, AUC: 0.413359\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002367334246635437, AUC: 0.44242099999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022000706195831297, AUC: 0.4711525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012446025013923645, AUC: 0.334299\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021820892095565795, AUC: 0.583105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020404903888702392, AUC: 0.648216\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002024314820766449, AUC: 0.681209\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003298522114753723, AUC: 0.5834885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027435652017593383, AUC: 0.5890599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019298773407936097, AUC: 0.609164\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001615216851234436, AUC: 0.628682\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021159266233444214, AUC: 0.5615115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001970057785511017, AUC: 0.612746\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011346779465675355, AUC: 0.755604\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008024146258831024, AUC: 0.822682\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0063489582538604735, AUC: 0.5765259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030986483097076418, AUC: 0.5838599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024723620414733886, AUC: 0.560882\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021784865856170656, AUC: 0.5647835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038226916790008547, AUC: 0.639105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002087486505508423, AUC: 0.667513\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018109354376792909, AUC: 0.716287\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001730059325695038, AUC: 0.735003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002280357599258423, AUC: 0.504324\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002646989941596985, AUC: 0.39165200000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021477725505828858, AUC: 0.41529900000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021110632419586183, AUC: 0.439944\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027766032218933105, AUC: 0.561007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002738771319389343, AUC: 0.609527\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002348260045051575, AUC: 0.6862079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001673781394958496, AUC: 0.7902260000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031904937028884887, AUC: 0.2948405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025516486167907713, AUC: 0.681924\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002266431927680969, AUC: 0.6796884999999999\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 27\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid_with_smote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCappedBCELoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     29\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:56\u001b[0m, in \u001b[0;36mtrain_sigmoid_with_smote\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mfloat(), smote_label)\n\u001b[1;32m     55\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m---> 56\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2 Class Capped SMOTE \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_sigmoid_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for i in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[i][0]\n",
    "    auc_variance = cap_aucs[i][1]\n",
    "    cap = caps[i]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606da0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = (col_names))\n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f22d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 CLASS Focal Loss\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SigmoidFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d438d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5f6eb22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf2\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7924e1f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0007034576535224914, AUC: 0.350273\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005969953835010529, AUC: 0.770484\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000597990334033966, AUC: 0.7816185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006023451387882232, AUC: 0.7982774999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006960260272026062, AUC: 0.36476299999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006060459017753602, AUC: 0.768943\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005872988700866699, AUC: 0.7846535000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005800300836563111, AUC: 0.8063929999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006880538761615754, AUC: 0.6572255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006098485887050629, AUC: 0.7543479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005866237282752991, AUC: 0.790246\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005785414576530457, AUC: 0.820869\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006959007680416107, AUC: 0.437869\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006432939171791077, AUC: 0.7988230000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005786927938461303, AUC: 0.792296\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005812993943691253, AUC: 0.813682\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006853521764278412, AUC: 0.75229\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005993604958057404, AUC: 0.777921\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005757238566875458, AUC: 0.799081\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005780047178268432, AUC: 0.81665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000698248952627182, AUC: 0.33987300000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006018552780151367, AUC: 0.764054\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005995519161224365, AUC: 0.789431\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005854066014289856, AUC: 0.8199609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006925071775913239, AUC: 0.5563845000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006454500555992126, AUC: 0.7156625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000594520479440689, AUC: 0.776445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005635262429714203, AUC: 0.8153489999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000688814252614975, AUC: 0.5831584999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006107564270496368, AUC: 0.7515149999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005716030299663543, AUC: 0.803816\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005634236335754395, AUC: 0.8289449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006980474889278412, AUC: 0.36421950000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006257353127002716, AUC: 0.772481\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005740275084972382, AUC: 0.795568\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005705761313438416, AUC: 0.8149329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006886121630668641, AUC: 0.7485660000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006185167729854584, AUC: 0.796669\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005593409538269043, AUC: 0.811881\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005564782023429871, AUC: 0.832215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006908144652843475, AUC: 0.61831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005691847801208496, AUC: 0.834461\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007026033699512481, AUC: 0.8628529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000969855397939682, AUC: 0.8643609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006919503211975098, AUC: 0.5854735000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006571440398693085, AUC: 0.8290139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007473810613155365, AUC: 0.858377\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008230424225330353, AUC: 0.872895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943893432617188, AUC: 0.4555675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006340284645557404, AUC: 0.8342890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006710533797740936, AUC: 0.765735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008657899498939514, AUC: 0.8650599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006844996213912964, AUC: 0.7051545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006598311960697174, AUC: 0.8319619999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010953611731529236, AUC: 0.828903\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011425661444664002, AUC: 0.841075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006955679953098297, AUC: 0.38576900000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006204173862934113, AUC: 0.8291959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007453862726688385, AUC: 0.8484029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008284624516963959, AUC: 0.869208\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006875077188014984, AUC: 0.6786185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005375882387161255, AUC: 0.8530970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000978346973657608, AUC: 0.839971\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006238149106502533, AUC: 0.859466\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695554405450821, AUC: 0.473947\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006683248877525329, AUC: 0.8328150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007890136539936065, AUC: 0.8701960000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007169190347194672, AUC: 0.877827\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000691938579082489, AUC: 0.5933155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006572535932064057, AUC: 0.8451200000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000802250474691391, AUC: 0.857353\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007915737926959992, AUC: 0.882807\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929807960987091, AUC: 0.5788925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006303778290748597, AUC: 0.845356\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006627142727375031, AUC: 0.87822\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009959247410297393, AUC: 0.866363\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006900812983512878, AUC: 0.597478\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005529468953609466, AUC: 0.8449940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008751957416534424, AUC: 0.840394\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011264262795448302, AUC: 0.8399819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963454484939575, AUC: 0.368583\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000671286016702652, AUC: 0.686547\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006440030336380005, AUC: 0.729344\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006247979104518891, AUC: 0.7596485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693055808544159, AUC: 0.5285415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006768907904624939, AUC: 0.7657634999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006345959007740021, AUC: 0.780216\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005882263481616974, AUC: 0.7834349999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006990040838718415, AUC: 0.3461245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006863485276699066, AUC: 0.6793845000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006673316657543183, AUC: 0.7099949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000633106142282486, AUC: 0.720231\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006990968585014343, AUC: 0.39516450000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006855045258998871, AUC: 0.68612\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006598499119281769, AUC: 0.7161449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006154875457286834, AUC: 0.7527480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006969273090362549, AUC: 0.412074\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000683394193649292, AUC: 0.7100905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006497306227684021, AUC: 0.7405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006145629286766053, AUC: 0.75222\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006906314194202423, AUC: 0.6237820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006537898778915405, AUC: 0.776892\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005955623090267182, AUC: 0.776671\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005770167708396912, AUC: 0.793175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973980665206909, AUC: 0.36263\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006726363599300385, AUC: 0.721066\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006257853507995605, AUC: 0.739023\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006022973656654358, AUC: 0.761777\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000691128671169281, AUC: 0.6194335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006763252913951874, AUC: 0.775484\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006527803838253021, AUC: 0.7784819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006138690412044525, AUC: 0.7839499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006922689080238342, AUC: 0.566711\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006594015657901764, AUC: 0.76122\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006046703159809112, AUC: 0.7661695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005900995135307312, AUC: 0.7726639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988410949707031, AUC: 0.3249625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006666017472743988, AUC: 0.7422740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006063082218170166, AUC: 0.7646689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005912479162216187, AUC: 0.7822720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006906481087207794, AUC: 0.6147175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006883422136306763, AUC: 0.671021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000685589998960495, AUC: 0.701039\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00068220454454422, AUC: 0.7182789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007009784579277039, AUC: 0.273845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006942911446094512, AUC: 0.4723275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000687359094619751, AUC: 0.6318305000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006789560317993164, AUC: 0.6917044999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006907008886337281, AUC: 0.6052679999999999\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006857488453388214, AUC: 0.6640845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006802918016910553, AUC: 0.6844085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006736926138401031, AUC: 0.696304\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006890509724617004, AUC: 0.6708350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006851511299610138, AUC: 0.744216\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006813865900039673, AUC: 0.7735485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006771237552165985, AUC: 0.7873035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936484277248382, AUC: 0.47479800000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006836704015731811, AUC: 0.7268295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000674445629119873, AUC: 0.7734519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006638656854629516, AUC: 0.7779069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006897411644458771, AUC: 0.6272595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006844063699245452, AUC: 0.7086825000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006791321933269501, AUC: 0.72961\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006723710894584655, AUC: 0.7380399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006924198865890503, AUC: 0.563396\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006858779788017273, AUC: 0.7024865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006801698803901672, AUC: 0.7189635000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006742141842842102, AUC: 0.732568\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006806557476520538, AUC: 0.798945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006741758286952972, AUC: 0.80386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006679194867610931, AUC: 0.799131\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006608653366565704, AUC: 0.7951175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006975729167461395, AUC: 0.321966\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006908915042877198, AUC: 0.6378389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006857323050498962, AUC: 0.7780035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006804414093494416, AUC: 0.808985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933948993682862, AUC: 0.557557\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006890815198421479, AUC: 0.6896285000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006852592527866364, AUC: 0.7115589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006812201738357543, AUC: 0.7112310000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006916834414005279, AUC: 0.592359\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006897369027137756, AUC: 0.6735115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006882212162017822, AUC: 0.714566\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006868181228637696, AUC: 0.7406495000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935810446739197, AUC: 0.4779515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000692468136548996, AUC: 0.5387965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006917705833911896, AUC: 0.5767745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006910330057144165, AUC: 0.610922\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945132613182068, AUC: 0.451558\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006910947263240815, AUC: 0.6232475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006889475882053375, AUC: 0.712924\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006869889497756958, AUC: 0.7610735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006969058513641357, AUC: 0.4459025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693189948797226, AUC: 0.5424305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006901876032352447, AUC: 0.643663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006875587403774261, AUC: 0.719701\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945109665393829, AUC: 0.494658\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932755708694458, AUC: 0.5348095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006921696960926056, AUC: 0.572435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006911457479000092, AUC: 0.6087605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927233338356018, AUC: 0.6226010000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006891363859176636, AUC: 0.7359979999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000685926079750061, AUC: 0.7701065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006826614439487457, AUC: 0.77724\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006924867630004883, AUC: 0.545513\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006908670961856842, AUC: 0.6277649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006891733109951019, AUC: 0.667434\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00068719083070755, AUC: 0.6916169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006928185820579528, AUC: 0.5807485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000690379649400711, AUC: 0.61598\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000688027173280716, AUC: 0.6428849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000685593843460083, AUC: 0.663471\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006960352063179016, AUC: 0.4603945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947144865989685, AUC: 0.4877435\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     27\u001b[0m     loss_fn_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_cap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m loss_cap\n\u001b[0;32m---> 28\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCappedBCELoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     30\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network, embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:130\u001b[0m, in \u001b[0;36mtrain_sigmoid_distance\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m    128\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(output\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mfloat(), smote_target, embeds\u001b[38;5;241m=\u001b[39membeds)\n\u001b[1;32m    129\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m--> 130\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# distance + capped loss\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-3, 5e-4, 1e-4, 5e-5]\n",
    "\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_cap = 1.0\n",
    "loss_fn_args['distance'] = 'euclidean'\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(start_epoch):\n",
    "            loss_fn_args['loss_cap'] = None\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "        for epoch in range(start_epoch, n_epochs + 1):\n",
    "            loss_fn_args['loss_cap'] = loss_cap\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"distance_capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], loss_fn_args['loss_cap'], norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3b0f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(learning_rates) - 1): \n",
    "    row = [\"distance_capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], loss_fn_args['loss_cap'], norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd4a6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f921d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cosine distance + capped loss\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-3, 5e-4, 1e-4, 5e-5]\n",
    "\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = 5.0\n",
    "loss_fn_args['distance'] = 'cosine'\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(start_epoch):\n",
    "            loss_fn_args['loss_cap'] = None\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "        for epoch in range(start_epoch, n_epochs + 1):\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"cosine_distance_capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], loss_fn_args['loss_cap'], norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab098399",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17af0e21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006970875263214111, AUC: 0.5179245\n",
      "\n",
      "Train loss: 0.27050119904196185\n",
      "Train loss: 0.2508804725993211\n",
      "Train loss: 0.2295536493799489\n",
      "Train loss: 0.23650908014576905\n",
      "Train loss: 0.26522073852028816\n",
      "Train loss: 0.2877479203187736\n",
      "Train loss: 0.24342828267698835\n",
      "Train loss: 0.2656087279319763\n",
      "Train loss: 0.24802333306355082\n",
      "Train loss: 0.23219373499511914\n",
      "Train loss: 0.23283238889305455\n",
      "Train loss: 0.2749264088405925\n",
      "Train loss: 0.23505860794881347\n",
      "Train loss: 0.22109104569550533\n",
      "Train loss: 0.22770290124188564\n",
      "\n",
      "Test set: Avg. loss: 0.0006762307286262512, AUC: 0.744272\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006601359844207764, AUC: 0.774519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006447486281394958, AUC: 0.78154\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006311907172203064, AUC: 0.786457\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006190038323402405, AUC: 0.7906310000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006070325672626496, AUC: 0.795851\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005986305773258209, AUC: 0.8024519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000586746096611023, AUC: 0.807191\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000578653335571289, AUC: 0.813972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005686893761157989, AUC: 0.819787\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005630890429019928, AUC: 0.8243100000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005563066303730011, AUC: 0.8315590000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005492294132709503, AUC: 0.8339989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005441325604915619, AUC: 0.8399679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005367833375930786, AUC: 0.8431339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005343281328678131, AUC: 0.84854\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005320078432559967, AUC: 0.851919\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005275746881961823, AUC: 0.8554499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005220678150653839, AUC: 0.859131\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005174184739589691, AUC: 0.862691\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005150319337844848, AUC: 0.866432\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005093210935592651, AUC: 0.869156\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005078133344650268, AUC: 0.873593\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000503547340631485, AUC: 0.875814\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004974544793367386, AUC: 0.877419\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004951775372028351, AUC: 0.8812519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000494452565908432, AUC: 0.8837260000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004883580207824707, AUC: 0.88334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004841071516275406, AUC: 0.886687\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004838077574968338, AUC: 0.8882089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007133534848690032, AUC: 0.22769799999999998\n",
      "\n",
      "Train loss: 0.3014722208308566\n",
      "Train loss: 0.335937455581252\n",
      "Train loss: 0.3535766123206752\n",
      "Train loss: 0.3588402692679387\n",
      "Train loss: 0.35056528733794096\n",
      "Train loss: 0.4185634335135199\n",
      "Train loss: 0.2839110546810612\n",
      "Train loss: 0.3721709464006363\n",
      "Train loss: 0.30416134778101733\n",
      "Train loss: 0.33496738505211604\n",
      "Train loss: 0.309860969425007\n",
      "Train loss: 0.3350027195966927\n",
      "Train loss: 0.338246029653367\n",
      "Train loss: 0.3346106406230076\n",
      "Train loss: 0.3215553411252939\n",
      "\n",
      "Test set: Avg. loss: 0.0007001563012599946, AUC: 0.34397500000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936496198177338, AUC: 0.49112900000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894029080867768, AUC: 0.6103575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006842100322246551, AUC: 0.7041845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006770964860916138, AUC: 0.7529140000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006660743951797485, AUC: 0.7731259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006517412662506104, AUC: 0.7826445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006336460709571838, AUC: 0.7865350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006176429986953735, AUC: 0.792916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006037728190422058, AUC: 0.797614\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005915506184101105, AUC: 0.8029810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005818419456481934, AUC: 0.8051360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005746455490589141, AUC: 0.8108770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005675255060195923, AUC: 0.8174009999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005613057911396026, AUC: 0.827779\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005555008947849274, AUC: 0.830713\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005498598515987396, AUC: 0.836857\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005424348413944245, AUC: 0.840509\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005357863008975983, AUC: 0.8452220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005321557223796845, AUC: 0.8488349999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005287462770938873, AUC: 0.8541360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005250332355499267, AUC: 0.856433\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005187397599220276, AUC: 0.8606119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005134570002555848, AUC: 0.8645220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005081571936607361, AUC: 0.8663839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005035769641399384, AUC: 0.8696229999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005008548200130463, AUC: 0.8698069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004978715777397155, AUC: 0.876355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004936823695898057, AUC: 0.8733500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004914387315511703, AUC: 0.8779600000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007054819762706756, AUC: 0.282213\n",
      "\n",
      "Train loss: 0.24167257585343282\n",
      "Train loss: 0.21683010515893342\n",
      "Train loss: 0.2450069223239923\n",
      "Train loss: 0.24539157501451528\n",
      "Train loss: 0.24932515621185303\n",
      "Train loss: 0.20205302375137427\n",
      "Train loss: 0.2312577457944299\n",
      "Train loss: 0.231532895261315\n",
      "Train loss: 0.22345968796189425\n",
      "Train loss: 0.25985963443282306\n",
      "Train loss: 0.23562251724255312\n",
      "Train loss: 0.17437375313157488\n",
      "Train loss: 0.2054279712355061\n",
      "Train loss: 0.23142437000942837\n",
      "Train loss: 0.2514593711324558\n",
      "\n",
      "Test set: Avg. loss: 0.0006692471504211425, AUC: 0.7267399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006376748979091644, AUC: 0.7670334999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006104510426521301, AUC: 0.7776235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000594084769487381, AUC: 0.7839160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005837447643280029, AUC: 0.790523\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00057659912109375, AUC: 0.79424\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005702949464321136, AUC: 0.7992980000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005638820528984069, AUC: 0.803792\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005590019226074218, AUC: 0.808187\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005563409626483918, AUC: 0.813504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005517008006572724, AUC: 0.818517\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005475625097751618, AUC: 0.823134\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005427576005458831, AUC: 0.8268340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005387462675571442, AUC: 0.832124\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000535504162311554, AUC: 0.835487\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005319998860359192, AUC: 0.8385050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005296002328395844, AUC: 0.844119\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005238275229930878, AUC: 0.845669\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000521075040102005, AUC: 0.852505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005165559351444245, AUC: 0.8543419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005115041136741638, AUC: 0.8562749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005071846246719361, AUC: 0.8593130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005043738484382629, AUC: 0.8630439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005007966607809066, AUC: 0.8653129999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000496901884675026, AUC: 0.8675080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004947603791952133, AUC: 0.869729\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004921582937240601, AUC: 0.8723580000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004866652190685272, AUC: 0.8723400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00048461295664310456, AUC: 0.8737750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004839456528425217, AUC: 0.875918\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007033359706401825, AUC: 0.39147899999999997\n",
      "\n",
      "Train loss: 0.2264507803947303\n",
      "Train loss: 0.2493441761678951\n",
      "Train loss: 0.22984266736704834\n",
      "Train loss: 0.21082301504293066\n",
      "Train loss: 0.2076721609018411\n",
      "Train loss: 0.2646031896020197\n",
      "Train loss: 0.22108323103303362\n",
      "Train loss: 0.16802240822725234\n",
      "Train loss: 0.20636420796631247\n",
      "Train loss: 0.21501884604715238\n",
      "Train loss: 0.20195242174112113\n",
      "Train loss: 0.20963617039334242\n",
      "Train loss: 0.21823341869244908\n",
      "Train loss: 0.1766183163709701\n",
      "Train loss: 0.1892275886171183\n",
      "\n",
      "Test set: Avg. loss: 0.0006623582243919373, AUC: 0.7710859999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006413186192512512, AUC: 0.78701\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006244334876537323, AUC: 0.7913570000000001\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006099905967712402, AUC: 0.7951569999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000596579909324646, AUC: 0.7971199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005863858163356781, AUC: 0.801263\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00057879638671875, AUC: 0.80675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000572771966457367, AUC: 0.811958\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005666111409664154, AUC: 0.8163039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000559524118900299, AUC: 0.8201769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005542269051074982, AUC: 0.8243260000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005485452115535736, AUC: 0.8279209999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005436902046203613, AUC: 0.832715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005399761497974396, AUC: 0.8367309999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005338923335075379, AUC: 0.838973\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005322109758853912, AUC: 0.8437689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005274032056331635, AUC: 0.8485179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005236106216907501, AUC: 0.8523729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005193279385566712, AUC: 0.856134\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005149158537387848, AUC: 0.859769\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005103896558284759, AUC: 0.8633639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005056613087654114, AUC: 0.8668099999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000501801148056984, AUC: 0.8707530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004957093894481659, AUC: 0.873371\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004910053014755249, AUC: 0.8760190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004873826950788498, AUC: 0.8797670000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00048719318211078643, AUC: 0.883866\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004804772436618805, AUC: 0.8850969999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047764965891838076, AUC: 0.88803\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004737325459718704, AUC: 0.8904329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007115857899188995, AUC: 0.566001\n",
      "\n",
      "Train loss: 0.282108270058966\n",
      "Train loss: 0.328307897042317\n",
      "Train loss: 0.30232203386391804\n",
      "Train loss: 0.34793881625886175\n",
      "Train loss: 0.3282173993481193\n",
      "Train loss: 0.3492243536718332\n",
      "Train loss: 0.2824000742784731\n",
      "Train loss: 0.29489359886023647\n",
      "Train loss: 0.3277479049506461\n",
      "Train loss: 0.31624698221303854\n",
      "Train loss: 0.30845797859179747\n",
      "Train loss: 0.3175832203998687\n",
      "Train loss: 0.31186593717830197\n",
      "Train loss: 0.31021728940830107\n",
      "Train loss: 0.29311991611104105\n",
      "\n",
      "Test set: Avg. loss: 0.0006833215355873108, AUC: 0.7485440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006717347800731659, AUC: 0.7836249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006577662825584411, AUC: 0.7901419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006402826011180878, AUC: 0.7915534999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006199146211147309, AUC: 0.793005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006019279658794403, AUC: 0.796392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005874082148075104, AUC: 0.8026380000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005759837627410889, AUC: 0.810843\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005658689141273498, AUC: 0.8182\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005567056238651275, AUC: 0.8259310000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005486019849777221, AUC: 0.832257\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005408168435096741, AUC: 0.8390820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005350074470043182, AUC: 0.846648\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005256423950195313, AUC: 0.8522810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005207732319831848, AUC: 0.859791\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000514547049999237, AUC: 0.863904\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005086543262004852, AUC: 0.8688979999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005037811100482941, AUC: 0.873389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004993184059858322, AUC: 0.8770650000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004959745556116104, AUC: 0.880595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004907459765672684, AUC: 0.8809130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004844515472650528, AUC: 0.8830589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004816789925098419, AUC: 0.8869529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004792306125164032, AUC: 0.887759\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047550903260707854, AUC: 0.889381\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004727445244789124, AUC: 0.889092\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047145040333271027, AUC: 0.891733\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004691614806652069, AUC: 0.8927259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046523839235305785, AUC: 0.8938139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046418409049510953, AUC: 0.894168\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933468282222748, AUC: 0.5070764999999999\n",
      "\n",
      "Train loss: 0.2413686157032183\n",
      "Train loss: 0.23451158271473685\n",
      "Train loss: 0.20200065025098765\n",
      "Train loss: 0.21952570319935016\n",
      "Train loss: 0.20098364846721575\n",
      "Train loss: 0.1910101767558201\n",
      "Train loss: 0.2032882430750853\n",
      "Train loss: 0.19550621243798808\n",
      "Train loss: 0.18009676143621942\n",
      "Train loss: 0.1892723695487733\n",
      "Train loss: 0.23208488210750994\n",
      "Train loss: 0.20064730135498532\n",
      "Train loss: 0.1735089102368446\n",
      "Train loss: 0.1917156068382749\n",
      "Train loss: 0.18277080613336746\n",
      "\n",
      "Test set: Avg. loss: 0.0006818731129169465, AUC: 0.7532485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006727496385574341, AUC: 0.8025190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006625185012817383, AUC: 0.816831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006518848538398743, AUC: 0.8209245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006411920487880707, AUC: 0.821762\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006286712288856506, AUC: 0.820811\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006165320575237275, AUC: 0.8198799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000603670209646225, AUC: 0.8193335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005904943346977234, AUC: 0.8184969999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005792211890220642, AUC: 0.818836\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005706440508365631, AUC: 0.8210200000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005634638965129852, AUC: 0.8242099999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00055939581990242, AUC: 0.8274689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005524302124977112, AUC: 0.8310420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005480090081691742, AUC: 0.833938\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005441724956035614, AUC: 0.8394625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005396417677402496, AUC: 0.8423970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005374679267406464, AUC: 0.8463309999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005320470333099365, AUC: 0.849275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005280445516109467, AUC: 0.854349\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005241619348526, AUC: 0.857181\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005200570523738861, AUC: 0.860336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005174140930175781, AUC: 0.862586\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005129047036170959, AUC: 0.8643810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005100338459014893, AUC: 0.867698\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005062006562948227, AUC: 0.8700709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004996050894260407, AUC: 0.8707100000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004965450018644333, AUC: 0.874019\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004933423548936843, AUC: 0.877322\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004926126301288605, AUC: 0.87862\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007194790244102478, AUC: 0.276393\n",
      "\n",
      "Train loss: 0.15697926586600625\n",
      "Train loss: 0.18217567691377773\n",
      "Train loss: 0.17292748628907903\n",
      "Train loss: 0.19580148777384668\n",
      "Train loss: 0.16828548414692\n",
      "Train loss: 0.16848752528998504\n",
      "Train loss: 0.16413680345389495\n",
      "Train loss: 0.19105200820667728\n",
      "Train loss: 0.13830619784677103\n",
      "Train loss: 0.18869896527308566\n",
      "Train loss: 0.17385781190957234\n",
      "Train loss: 0.1491287589832476\n",
      "Train loss: 0.14435341024095086\n",
      "Train loss: 0.15058315673451514\n",
      "Train loss: 0.1596347469433098\n",
      "\n",
      "Test set: Avg. loss: 0.0006823883354663848, AUC: 0.6669035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006624938249588013, AUC: 0.7531654999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006465267837047577, AUC: 0.764988\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006341229379177093, AUC: 0.772888\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000621725708246231, AUC: 0.7787919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000612126350402832, AUC: 0.786096\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006044217646121978, AUC: 0.789898\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005962004065513611, AUC: 0.8001400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005880303978919983, AUC: 0.804448\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005811279714107514, AUC: 0.8090849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000571574866771698, AUC: 0.812263\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005658712089061737, AUC: 0.8181814999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00056162229180336, AUC: 0.820846\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005573521554470062, AUC: 0.827167\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005514912605285644, AUC: 0.831882\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0005457632243633271, AUC: 0.8351440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005399615466594696, AUC: 0.8389530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005355960130691528, AUC: 0.842231\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005317294597625733, AUC: 0.848512\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005271710455417633, AUC: 0.8521449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005212545394897461, AUC: 0.8550009999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005165989398956299, AUC: 0.859765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005129876136779786, AUC: 0.863526\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005102378427982331, AUC: 0.86945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005042073428630829, AUC: 0.871496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004981806427240372, AUC: 0.874105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004947265684604645, AUC: 0.8766989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004910622239112854, AUC: 0.879653\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004856421500444412, AUC: 0.882077\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004833685904741287, AUC: 0.8857980000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006880944669246673, AUC: 0.6294545\n",
      "\n",
      "Train loss: 0.16886905196366037\n",
      "Train loss: 0.18825904351131173\n",
      "Train loss: 0.16323023882641155\n",
      "Train loss: 0.144447962569583\n",
      "Train loss: 0.17219472500928648\n",
      "Train loss: 0.15335565084105085\n",
      "Train loss: 0.1574192012950873\n",
      "Train loss: 0.1802229718038231\n",
      "Train loss: 0.1330905959105036\n",
      "Train loss: 0.16140922011843153\n",
      "Train loss: 0.14106032764835721\n",
      "Train loss: 0.12714075890316326\n",
      "Train loss: 0.16537748134819566\n",
      "Train loss: 0.140576841724906\n",
      "Train loss: 0.13418386468462123\n",
      "\n",
      "Test set: Avg. loss: 0.0006763900518417358, AUC: 0.7570749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006670749485492706, AUC: 0.7863249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006560753583908081, AUC: 0.7993410000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006438340842723847, AUC: 0.804744\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006281707286834717, AUC: 0.806184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006115095317363739, AUC: 0.807391\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005961151719093323, AUC: 0.81142\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005822727680206298, AUC: 0.8145399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005745165348052979, AUC: 0.8193729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005656524896621704, AUC: 0.825863\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000556539922952652, AUC: 0.828768\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005516160130500794, AUC: 0.834201\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005488419532775879, AUC: 0.8396129999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005429102778434753, AUC: 0.842058\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005388636887073517, AUC: 0.8452580000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005335430800914765, AUC: 0.8489\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005291613638401031, AUC: 0.8511\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005246857702732086, AUC: 0.8544339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005196473300457001, AUC: 0.856208\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005186508595943451, AUC: 0.858645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005144793093204499, AUC: 0.8608119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005085039734840394, AUC: 0.864067\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005054698288440705, AUC: 0.8664750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005004764646291733, AUC: 0.868371\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004981708228588104, AUC: 0.8708210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004904505759477615, AUC: 0.872495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004888229221105576, AUC: 0.87524\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004862315356731415, AUC: 0.877281\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004840891659259796, AUC: 0.879982\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004767947196960449, AUC: 0.8816729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933152675628662, AUC: 0.544653\n",
      "\n",
      "Train loss: 0.27375231275133266\n",
      "Train loss: 0.2828253351958694\n",
      "Train loss: 0.2862266958898799\n",
      "Train loss: 0.30147991780262845\n",
      "Train loss: 0.32293547262811356\n",
      "Train loss: 0.2876329482740657\n",
      "Train loss: 0.32200231635646454\n",
      "Train loss: 0.2824154036819555\n",
      "Train loss: 0.2851420830769144\n",
      "Train loss: 0.2842872116216429\n",
      "Train loss: 0.29300111152563885\n",
      "Train loss: 0.26386117024026856\n",
      "Train loss: 0.26859890389594304\n",
      "Train loss: 0.2970562558265249\n",
      "Train loss: 0.2749405081864375\n",
      "\n",
      "Test set: Avg. loss: 0.0006707668602466584, AUC: 0.735374\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006536027491092682, AUC: 0.7605395000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006356818675994873, AUC: 0.77396\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006181473731994629, AUC: 0.782845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006007250845432282, AUC: 0.794438\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005873383581638336, AUC: 0.806335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005747479796409607, AUC: 0.810359\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005639373958110809, AUC: 0.820622\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005544420778751374, AUC: 0.8305600000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005458514988422394, AUC: 0.8381884999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005381325483322144, AUC: 0.8420999999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005310440361499787, AUC: 0.8457955000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005233037769794464, AUC: 0.8501040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005157012045383454, AUC: 0.855606\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005093472003936768, AUC: 0.8587689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000505829781293869, AUC: 0.8627929999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004989646822214127, AUC: 0.866083\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004945629388093948, AUC: 0.869025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004896151125431061, AUC: 0.8714549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004885567277669907, AUC: 0.872763\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00048591315746307376, AUC: 0.874092\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00048081009089946745, AUC: 0.87694\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004768858850002289, AUC: 0.878545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000474134162068367, AUC: 0.878582\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046906907856464385, AUC: 0.881373\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046855051815509796, AUC: 0.8833989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000468600794672966, AUC: 0.8832920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004679757207632065, AUC: 0.8834930000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046303653717041015, AUC: 0.885932\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004618158042430878, AUC: 0.8871469999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006887815594673156, AUC: 0.6098405\n",
      "\n",
      "Train loss: 0.22489499628164206\n",
      "Train loss: 0.17027938631689474\n",
      "Train loss: 0.16907513483314757\n",
      "Train loss: 0.17814510073631432\n",
      "Train loss: 0.19139882143895337\n",
      "Train loss: 0.19397678838413993\n",
      "Train loss: 0.16489396049718189\n",
      "Train loss: 0.16588816938886217\n",
      "Train loss: 0.1600673635294483\n",
      "Train loss: 0.15363535569731596\n",
      "Train loss: 0.14298894602781648\n",
      "Train loss: 0.17704810762101678\n",
      "Train loss: 0.15052171117940527\n",
      "Train loss: 0.17639382971320183\n",
      "Train loss: 0.15854367746669015\n",
      "\n",
      "Test set: Avg. loss: 0.0006554527282714844, AUC: 0.7653430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006384008526802063, AUC: 0.771884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000622253805398941, AUC: 0.77444\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006096687018871307, AUC: 0.778691\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006001061797142029, AUC: 0.782491\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005911051630973816, AUC: 0.788332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005833947360515594, AUC: 0.792843\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005790476500988007, AUC: 0.7997225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005723702311515808, AUC: 0.806658\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005667882263660431, AUC: 0.813197\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005611466169357299, AUC: 0.820459\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005565719306468964, AUC: 0.828449\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005528772175312042, AUC: 0.8371230000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005438002645969391, AUC: 0.842263\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005394275188446045, AUC: 0.849221\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005350631773471833, AUC: 0.8578159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005272654592990875, AUC: 0.863027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005256368815898895, AUC: 0.869147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005172924399375916, AUC: 0.871581\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005123894512653351, AUC: 0.874671\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005114701092243195, AUC: 0.878517\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000504142478108406, AUC: 0.8786700000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005001888424158097, AUC: 0.880574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005000960230827331, AUC: 0.883866\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004952629655599595, AUC: 0.885462\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004897051900625229, AUC: 0.8858819999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004860337823629379, AUC: 0.8865995000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000486377015709877, AUC: 0.8885319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004802228510379791, AUC: 0.888687\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.00048418672382831576, AUC: 0.8903570000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007154200375080108, AUC: 0.49844649999999996\n",
      "\n",
      "Train loss: 0.29820947472456916\n",
      "Train loss: 0.19961200132491483\n",
      "Train loss: 0.2934751742205043\n",
      "Train loss: 0.3064685365197006\n",
      "Train loss: 0.24654365648889237\n",
      "Train loss: 0.2999751810814924\n",
      "Train loss: 0.28235246193636754\n",
      "Train loss: 0.30309595537793105\n",
      "Train loss: 0.3140808393241494\n",
      "Train loss: 0.30847976124210724\n",
      "Train loss: 0.23305495833135714\n",
      "Train loss: 0.25309754784699456\n",
      "Train loss: 0.25631907297547457\n",
      "Train loss: 0.21590502740471226\n",
      "Train loss: 0.3219998133410314\n",
      "\n",
      "Test set: Avg. loss: 0.0006911126673221588, AUC: 0.650932\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006829962134361267, AUC: 0.7456495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006768400073051453, AUC: 0.7868360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006702893078327179, AUC: 0.801085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006626879274845123, AUC: 0.8048610000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006542136073112488, AUC: 0.804765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006443548202514648, AUC: 0.8024439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006344436705112458, AUC: 0.800728\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006233394145965576, AUC: 0.8001135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006135433912277221, AUC: 0.8008065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006047964990139007, AUC: 0.800877\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005958747267723084, AUC: 0.801251\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005886502861976624, AUC: 0.803424\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005830150544643403, AUC: 0.8044835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005782446265220642, AUC: 0.80769\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005755068063735962, AUC: 0.809877\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005718098282814026, AUC: 0.811861\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005684893131256103, AUC: 0.8139200000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005652849972248077, AUC: 0.8165264999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005620139241218567, AUC: 0.8178500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005585586428642273, AUC: 0.8205460000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000556520015001297, AUC: 0.822534\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005553761124610901, AUC: 0.824958\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005530216097831726, AUC: 0.8280305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005498533844947815, AUC: 0.8293079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005483139455318451, AUC: 0.8325719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005461876690387726, AUC: 0.834342\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005443792343139648, AUC: 0.837486\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005424173772335053, AUC: 0.8395739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005395532846450805, AUC: 0.8420285000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933454871177673, AUC: 0.497348\n",
      "\n",
      "Train loss: 0.2706556407509336\n",
      "Train loss: 0.26525715202282946\n",
      "Train loss: 0.24215913654133014\n",
      "Train loss: 0.26657851562378515\n",
      "Train loss: 0.25414817272477846\n",
      "Train loss: 0.2649650019445237\n",
      "Train loss: 0.2725582255679331\n",
      "Train loss: 0.2748076858793854\n",
      "Train loss: 0.2724566543178194\n",
      "Train loss: 0.272238046879981\n",
      "Train loss: 0.2663549879553971\n",
      "Train loss: 0.2616280738715154\n",
      "Train loss: 0.2525444452170354\n",
      "Train loss: 0.26604671007508685\n",
      "Train loss: 0.290806812845218\n",
      "\n",
      "Test set: Avg. loss: 0.0006892602741718293, AUC: 0.608695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006851469278335571, AUC: 0.678458\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006807456910610199, AUC: 0.7139854999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000675945669412613, AUC: 0.734008\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006698699295520782, AUC: 0.7466410000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006626792550086975, AUC: 0.7542899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000654373288154602, AUC: 0.7586590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006445948183536529, AUC: 0.7619385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006352960765361786, AUC: 0.7650310000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006260733604431152, AUC: 0.76817\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006172137260437012, AUC: 0.770996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006097570955753326, AUC: 0.7743829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006031337380409241, AUC: 0.778141\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005968605577945709, AUC: 0.7812319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005916741788387299, AUC: 0.7846639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005876010954380035, AUC: 0.788516\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005832341611385346, AUC: 0.7920189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005798372328281403, AUC: 0.7958544999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005754792392253876, AUC: 0.79937\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005728688538074494, AUC: 0.802515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005700832903385162, AUC: 0.8063859999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005666529834270478, AUC: 0.8095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000564730554819107, AUC: 0.813414\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005615500509738922, AUC: 0.8162119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005581766366958618, AUC: 0.8192149999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005552025735378266, AUC: 0.821557\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000551630973815918, AUC: 0.824745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005477049648761749, AUC: 0.827303\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005446484684944153, AUC: 0.8308549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005429054796695709, AUC: 0.834881\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951982080936432, AUC: 0.573293\n",
      "\n",
      "Train loss: 0.30526904999070864\n",
      "Train loss: 0.3258733806336761\n",
      "Train loss: 0.30488752217809106\n",
      "Train loss: 0.3312543417997421\n",
      "Train loss: 0.2932194714333601\n",
      "Train loss: 0.2968553927294008\n",
      "Train loss: 0.30485304259950186\n",
      "Train loss: 0.26448326733461613\n",
      "Train loss: 0.2422902352491002\n",
      "Train loss: 0.31339734488991416\n",
      "Train loss: 0.3319407553429816\n",
      "Train loss: 0.3111930945117003\n",
      "Train loss: 0.31675089126939227\n",
      "Train loss: 0.31881645644546314\n",
      "Train loss: 0.2791730343915854\n",
      "\n",
      "Test set: Avg. loss: 0.0006791761517524719, AUC: 0.6796359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006649464964866638, AUC: 0.726755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006478756666183471, AUC: 0.754338\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006328641474246979, AUC: 0.769616\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006196373701095582, AUC: 0.77797\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006081178188323974, AUC: 0.7821245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005989486277103424, AUC: 0.7866264999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005904869139194489, AUC: 0.790681\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005842151641845704, AUC: 0.794454\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005815061032772064, AUC: 0.799721\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005777669548988342, AUC: 0.8032810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005744918882846832, AUC: 0.807235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005698123276233673, AUC: 0.811105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005658320486545562, AUC: 0.8149089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005622171759605408, AUC: 0.818587\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005585551261901855, AUC: 0.822481\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005542243719100952, AUC: 0.8248374999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005518650412559509, AUC: 0.8285739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005482037365436554, AUC: 0.832041\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005453794002532959, AUC: 0.8347979999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005418746769428253, AUC: 0.838098\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005411350727081299, AUC: 0.841577\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005375902950763702, AUC: 0.8440179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005346887409687042, AUC: 0.8465625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005333230793476104, AUC: 0.849378\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005308325886726379, AUC: 0.852105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005269505381584168, AUC: 0.853739\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005247549712657928, AUC: 0.855798\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005217035710811615, AUC: 0.857873\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005203298926353455, AUC: 0.86042\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007158085405826568, AUC: 0.41284200000000004\n",
      "\n",
      "Train loss: 0.2960048963309853\n",
      "Train loss: 0.34782758108369866\n",
      "Train loss: 0.37795116567307974\n",
      "Train loss: 0.3721876326639941\n",
      "Train loss: 0.3516643430776657\n",
      "Train loss: 0.3255758471549696\n",
      "Train loss: 0.3368575542595736\n",
      "Train loss: 0.3123558391431335\n",
      "Train loss: 0.3552315945078613\n",
      "Train loss: 0.32845238287737416\n",
      "Train loss: 0.3375946306119299\n",
      "Train loss: 0.33711254444851235\n",
      "Train loss: 0.3214192823239952\n",
      "Train loss: 0.29673414207567833\n",
      "Train loss: 0.31403497677699777\n",
      "\n",
      "Test set: Avg. loss: 0.0006831664741039276, AUC: 0.615191\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006672499477863312, AUC: 0.7201044999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006534515023231506, AUC: 0.7574869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000638128250837326, AUC: 0.777471\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006245045065879822, AUC: 0.784751\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006122897863388062, AUC: 0.790363\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006018477082252502, AUC: 0.793475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005927428007125855, AUC: 0.796139\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005850924253463745, AUC: 0.798675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005791719257831573, AUC: 0.8010760000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005736169219017029, AUC: 0.8044505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005692993998527527, AUC: 0.8074489999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005645809173583984, AUC: 0.8098320000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005614774823188782, AUC: 0.8125439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005582036674022674, AUC: 0.8153499999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005544720292091369, AUC: 0.818101\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005512963831424714, AUC: 0.821227\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005466761589050293, AUC: 0.824775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000542959064245224, AUC: 0.827258\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005388966798782348, AUC: 0.8302149999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005353261232376098, AUC: 0.8333920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005325390994548797, AUC: 0.836337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005292962193489075, AUC: 0.8401855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000525256484746933, AUC: 0.8426279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005217847228050232, AUC: 0.846345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005184147357940674, AUC: 0.8487709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00051447793841362, AUC: 0.852312\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005108059942722321, AUC: 0.8552629999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005072192251682282, AUC: 0.8578929999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000504113957285881, AUC: 0.8600779999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007220182716846466, AUC: 0.39724499999999996\n",
      "\n",
      "Train loss: 0.3300186665194809\n",
      "Train loss: 0.3358853933917489\n",
      "Train loss: 0.3257146663726515\n",
      "Train loss: 0.35120238392216385\n",
      "Train loss: 0.46468677072768\n",
      "Train loss: 0.4210891427507826\n",
      "Train loss: 0.4044683666745569\n",
      "Train loss: 0.3995680862171635\n",
      "Train loss: 0.327331074483835\n",
      "Train loss: 0.40556822347033555\n",
      "Train loss: 0.43778240490870873\n",
      "Train loss: 0.3868963965185129\n",
      "Train loss: 0.32619898638148215\n",
      "Train loss: 0.340770107166023\n",
      "Train loss: 0.4165717587349521\n",
      "\n",
      "Test set: Avg. loss: 0.0006749932467937469, AUC: 0.7133435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006613436639308929, AUC: 0.7333264999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006515329778194427, AUC: 0.7394549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006407862305641175, AUC: 0.7458645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006303763389587402, AUC: 0.751925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006221433281898499, AUC: 0.7568980000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000615655243396759, AUC: 0.7596130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006090421378612518, AUC: 0.764473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006041221320629119, AUC: 0.7683949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006004619896411896, AUC: 0.771689\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005959418416023255, AUC: 0.775557\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005919107794761658, AUC: 0.7779189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005884385108947754, AUC: 0.782158\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005849074721336365, AUC: 0.7865789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005822020769119262, AUC: 0.7900650000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005780588388442994, AUC: 0.7947204999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005752671658992767, AUC: 0.79709\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005712324380874634, AUC: 0.8023935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005679351687431335, AUC: 0.8065865000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005648612976074218, AUC: 0.80913\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005599505305290222, AUC: 0.8143309999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000556719183921814, AUC: 0.818555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005529569685459137, AUC: 0.8241999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005503007173538208, AUC: 0.827188\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005465859472751618, AUC: 0.832267\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005443116724491119, AUC: 0.834217\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005402808785438537, AUC: 0.837657\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005370054841041565, AUC: 0.8421409999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005335800647735595, AUC: 0.8463309999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005303688943386078, AUC: 0.848344\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006840086281299591, AUC: 0.6814270000000001\n",
      "\n",
      "Train loss: 0.17619577980345222\n",
      "Train loss: 0.1758550899043964\n",
      "Train loss: 0.18714553307575785\n",
      "Train loss: 0.1775080516080188\n",
      "Train loss: 0.2270784981691154\n",
      "Train loss: 0.2138310363338252\n",
      "Train loss: 0.19467284914794242\n",
      "Train loss: 0.19290888423372984\n",
      "Train loss: 0.2112513359185237\n",
      "Train loss: 0.19189017061974592\n",
      "Train loss: 0.19200191642068754\n",
      "Train loss: 0.17362309717069005\n",
      "Train loss: 0.18574165766406212\n",
      "Train loss: 0.17335529502030392\n",
      "Train loss: 0.2162516713142395\n",
      "\n",
      "Test set: Avg. loss: 0.0006716095805168152, AUC: 0.7828645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006610568463802337, AUC: 0.803472\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006508012115955353, AUC: 0.8101895000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006408543288707733, AUC: 0.811961\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006305415034294128, AUC: 0.8114525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006209536790847778, AUC: 0.810357\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006117241680622101, AUC: 0.810636\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006021948456764221, AUC: 0.8108129999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005950473845005035, AUC: 0.8112349999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005871616005897522, AUC: 0.812085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005802070796489716, AUC: 0.814721\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005745445787906647, AUC: 0.8172860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00057154780626297, AUC: 0.820016\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00056943079829216, AUC: 0.8236340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005660333335399628, AUC: 0.825799\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005640954971313476, AUC: 0.829496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005615778863430023, AUC: 0.8316954999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005585732161998749, AUC: 0.8344290000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000555778443813324, AUC: 0.8365710000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005532196164131165, AUC: 0.8387510000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005515329241752624, AUC: 0.841742\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005483656823635101, AUC: 0.843628\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005459973812103272, AUC: 0.8450329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005432744920253754, AUC: 0.847051\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005405437648296356, AUC: 0.848756\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005383824110031128, AUC: 0.850528\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005369285941123962, AUC: 0.852989\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005348294377326966, AUC: 0.8545269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005324771702289582, AUC: 0.8565179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005318953096866608, AUC: 0.8586179999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006970923244953156, AUC: 0.4937605\n",
      "\n",
      "Train loss: 0.24714923208686196\n",
      "Train loss: 0.2557293241191062\n",
      "Train loss: 0.24347007236662943\n",
      "Train loss: 0.26363628153588364\n",
      "Train loss: 0.2660020107676269\n",
      "Train loss: 0.25344365265718694\n",
      "Train loss: 0.25310299806534103\n",
      "Train loss: 0.252519845582877\n",
      "Train loss: 0.26507462096062434\n",
      "Train loss: 0.2841436411165128\n",
      "Train loss: 0.2440513383810687\n",
      "Train loss: 0.27061921813685424\n",
      "Train loss: 0.2689234546035718\n",
      "Train loss: 0.2865497659725748\n",
      "Train loss: 0.26027744363068017\n",
      "\n",
      "Test set: Avg. loss: 0.0006829453408718109, AUC: 0.6703614999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006720106303691864, AUC: 0.739126\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006624498069286347, AUC: 0.764002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006512903869152069, AUC: 0.7755700000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006391447186470031, AUC: 0.781228\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000628093034029007, AUC: 0.7848735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006185566484928131, AUC: 0.787669\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006097819805145263, AUC: 0.7897825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000601970762014389, AUC: 0.7921209999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005957014560699463, AUC: 0.7951280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005898561179637909, AUC: 0.7974289999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005856561958789826, AUC: 0.800459\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005823198556900024, AUC: 0.803725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005782137215137482, AUC: 0.806104\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005725001692771912, AUC: 0.8081250000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005673769116401672, AUC: 0.810791\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005636438429355621, AUC: 0.814115\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0005596866309642792, AUC: 0.8170459999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005575256943702697, AUC: 0.820464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005538226664066315, AUC: 0.822648\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005499895215034485, AUC: 0.8254350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005461176037788391, AUC: 0.8279155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005429751574993134, AUC: 0.8308119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005409910380840302, AUC: 0.8338159999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005378718376159668, AUC: 0.8368190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005346839129924774, AUC: 0.839609\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005313617289066315, AUC: 0.842791\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005279151797294617, AUC: 0.8445159999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005251319706439972, AUC: 0.847218\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005225062668323517, AUC: 0.8493565000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007142808437347412, AUC: 0.283389\n",
      "\n",
      "Train loss: 0.19184402257773528\n",
      "Train loss: 0.18174743804202717\n",
      "Train loss: 0.2037607344092837\n",
      "Train loss: 0.1799795039140495\n",
      "Train loss: 0.1632969941303229\n",
      "Train loss: 0.12392588557711073\n",
      "Train loss: 0.16034877528050903\n",
      "Train loss: 0.16917293960121788\n",
      "Train loss: 0.17411004585824955\n",
      "Train loss: 0.16491744244933887\n",
      "Train loss: 0.1660806783445322\n",
      "Train loss: 0.21618413583488222\n",
      "Train loss: 0.20794027018698918\n",
      "Train loss: 0.18584716206143617\n",
      "Train loss: 0.1696517782606137\n",
      "\n",
      "Test set: Avg. loss: 0.0006815493106842041, AUC: 0.722275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006679500937461853, AUC: 0.7744479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000658944457769394, AUC: 0.7839269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006518393456935882, AUC: 0.7867685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006445080041885376, AUC: 0.7897609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006375613510608673, AUC: 0.792724\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006310611963272095, AUC: 0.7949350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006234194040298462, AUC: 0.7970185000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006172767579555511, AUC: 0.7986\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006104931831359863, AUC: 0.800227\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006037557423114777, AUC: 0.801739\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005975601077079773, AUC: 0.803011\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005916907191276551, AUC: 0.804921\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00058670973777771, AUC: 0.80729\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005816972851753235, AUC: 0.8095619999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005769111812114715, AUC: 0.8115539999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005740401148796082, AUC: 0.814474\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005693597197532654, AUC: 0.817105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005657927691936493, AUC: 0.8200510000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005619331896305084, AUC: 0.823071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005590586960315705, AUC: 0.8265879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005556273758411407, AUC: 0.8298140000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005524579286575317, AUC: 0.8331710000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005494749248027801, AUC: 0.8358760000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005460039973258972, AUC: 0.8386619999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005426205396652222, AUC: 0.841389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005397015511989594, AUC: 0.8442680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005365166962146759, AUC: 0.846853\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005338336527347565, AUC: 0.848809\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005307880342006684, AUC: 0.851185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006971733868122101, AUC: 0.5134445\n",
      "\n",
      "Train loss: 0.2958370048529024\n",
      "Train loss: 0.25718140374323367\n",
      "Train loss: 0.28508474075110857\n",
      "Train loss: 0.28734655592851577\n",
      "Train loss: 0.2835466816167163\n",
      "Train loss: 0.29762345134832296\n",
      "Train loss: 0.28947731340007415\n",
      "Train loss: 0.29978158253773\n",
      "Train loss: 0.29324400842569437\n",
      "Train loss: 0.29396174819606125\n",
      "Train loss: 0.2843467065483142\n",
      "Train loss: 0.29663974843966734\n",
      "Train loss: 0.3122081175731246\n",
      "Train loss: 0.26930578195365373\n",
      "Train loss: 0.2713651148376951\n",
      "\n",
      "Test set: Avg. loss: 0.0006901585459709168, AUC: 0.611118\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006856660544872283, AUC: 0.6831705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006819352507591248, AUC: 0.7226305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006782605350017548, AUC: 0.7459125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006742610931396484, AUC: 0.7610569999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006695941090583801, AUC: 0.769992\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006655969023704528, AUC: 0.7747769999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006610913276672364, AUC: 0.779086\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006572428941726685, AUC: 0.782323\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006525532305240631, AUC: 0.7848739999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006485293209552765, AUC: 0.7873\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006435282230377197, AUC: 0.7884220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006395919322967529, AUC: 0.7892459999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006350986659526825, AUC: 0.7905439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006294893622398376, AUC: 0.790648\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006230201125144958, AUC: 0.7902279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006173456013202667, AUC: 0.7901420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006109423935413361, AUC: 0.7913334999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006043239831924438, AUC: 0.7920200000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005985826551914215, AUC: 0.7934939999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005927558243274688, AUC: 0.795685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005877851247787475, AUC: 0.7965145000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005827850103378296, AUC: 0.798165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000580410748720169, AUC: 0.800698\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005753988921642304, AUC: 0.8034380000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005715765655040741, AUC: 0.805606\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000569170206785202, AUC: 0.80873\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005654279291629792, AUC: 0.8109799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005625342726707459, AUC: 0.8137709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005593529343605042, AUC: 0.8158099999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006949527263641357, AUC: 0.4900965\n",
      "\n",
      "Train loss: 0.29527600138050736\n",
      "Train loss: 0.28284014410274044\n",
      "Train loss: 0.22809498666957684\n",
      "Train loss: 0.2881656004365083\n",
      "Train loss: 0.24533416473182143\n",
      "Train loss: 0.25167316122419514\n",
      "Train loss: 0.29445471049873695\n",
      "Train loss: 0.23982757937376667\n",
      "Train loss: 0.2572348151996637\n",
      "Train loss: 0.2055988004253169\n",
      "Train loss: 0.23501088133283482\n",
      "Train loss: 0.2636840324492971\n",
      "Train loss: 0.2435467664603215\n",
      "Train loss: 0.21437438658088637\n",
      "Train loss: 0.23789841316308186\n",
      "\n",
      "Test set: Avg. loss: 0.0006718525588512421, AUC: 0.7504730000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006580987870693207, AUC: 0.7762169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006451113820075989, AUC: 0.782465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006341781914234162, AUC: 0.785466\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006238905489444733, AUC: 0.787208\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006142018437385559, AUC: 0.7889209999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006067226231098175, AUC: 0.790875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005992954075336456, AUC: 0.7924745000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005935932993888855, AUC: 0.795611\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005875649452209472, AUC: 0.797572\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005838717818260193, AUC: 0.7997059999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005796324610710144, AUC: 0.802402\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005761690437793732, AUC: 0.8047770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005730340480804443, AUC: 0.806459\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000570714682340622, AUC: 0.8091769999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005671060979366302, AUC: 0.81199\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005644032061100006, AUC: 0.8143500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005609679222106933, AUC: 0.8165899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005588594377040863, AUC: 0.819364\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005568288266658783, AUC: 0.821479\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005545039176940918, AUC: 0.8240185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000551036685705185, AUC: 0.8258620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005480175912380219, AUC: 0.828126\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000545448362827301, AUC: 0.8304039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005437239408493043, AUC: 0.832402\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000541215181350708, AUC: 0.8352090000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005391958951950073, AUC: 0.8379695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005362535417079926, AUC: 0.839993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005347079634666443, AUC: 0.8421500000000001\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0005330676138401031, AUC: 0.84423\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006898444890975952, AUC: 0.590473\n",
      "\n",
      "Train loss: 0.33411584158611907\n",
      "Train loss: 0.38215364240537025\n",
      "Train loss: 0.3364601951495857\n",
      "Train loss: 0.3138343433665622\n",
      "Train loss: 0.3149105917875934\n",
      "Train loss: 0.39542520881458454\n",
      "Train loss: 0.3406064931754094\n",
      "Train loss: 0.3016549432353609\n",
      "Train loss: 0.33445700984092275\n",
      "Train loss: 0.3150544045077767\n",
      "Train loss: 0.32435027732970606\n",
      "Train loss: 0.3593109580361919\n",
      "Train loss: 0.3516343228376595\n",
      "Train loss: 0.36579364500228007\n",
      "Train loss: 0.33241842392903226\n",
      "\n",
      "Test set: Avg. loss: 0.0006890428960323334, AUC: 0.6059000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006883248388767242, AUC: 0.6183669999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006875491738319397, AUC: 0.630964\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006868465542793274, AUC: 0.6415029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00068608957529068, AUC: 0.6521685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006853469312191009, AUC: 0.6616955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006845857203006744, AUC: 0.6705494999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006837658882141113, AUC: 0.6788835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006830263733863831, AUC: 0.685861\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006822454333305359, AUC: 0.692457\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006813961863517762, AUC: 0.698964\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006805436611175537, AUC: 0.7047829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006796346604824066, AUC: 0.7106805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006787144541740418, AUC: 0.716183\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006777288913726807, AUC: 0.721046\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006767899394035339, AUC: 0.7251920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006758464872837067, AUC: 0.7293229999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006747672557830811, AUC: 0.7328365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006736683249473572, AUC: 0.736475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006725755631923675, AUC: 0.7396389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000671376496553421, AUC: 0.7428294999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006701891124248504, AUC: 0.7455650000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006689204573631286, AUC: 0.7482155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000667718380689621, AUC: 0.7505449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006664215326309204, AUC: 0.7528239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006649744808673859, AUC: 0.7547035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006635185778141022, AUC: 0.7565700000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006620490550994873, AUC: 0.7582800000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000660600334405899, AUC: 0.7600235000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006589950919151306, AUC: 0.7615135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006886384487152099, AUC: 0.6478825\n",
      "\n",
      "Train loss: 0.3010790382221246\n",
      "Train loss: 0.3135720935596782\n",
      "Train loss: 0.36051595476782244\n",
      "Train loss: 0.27582995792862713\n",
      "Train loss: 0.29806906060808025\n",
      "Train loss: 0.33710631367507254\n",
      "Train loss: 0.2829943196788715\n",
      "Train loss: 0.28678670582497956\n",
      "Train loss: 0.3005243699262097\n",
      "Train loss: 0.22920894812626444\n",
      "Train loss: 0.2829197371841236\n",
      "Train loss: 0.2846777363187948\n",
      "Train loss: 0.27380981756623385\n",
      "Train loss: 0.30748922809673723\n",
      "Train loss: 0.3436726722747657\n",
      "\n",
      "Test set: Avg. loss: 0.0006831853985786438, AUC: 0.6874880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006783798336982727, AUC: 0.7142530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000673965334892273, AUC: 0.7331534999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006699150204658508, AUC: 0.7457330000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006662011444568634, AUC: 0.7550585000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006627539992332458, AUC: 0.761268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000659181296825409, AUC: 0.766394\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006556666791439057, AUC: 0.770853\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006525187492370606, AUC: 0.773226\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006493756771087647, AUC: 0.77599\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006461537182331085, AUC: 0.7780119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006434026062488555, AUC: 0.779282\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006405854821205139, AUC: 0.7803509999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006378028392791748, AUC: 0.781384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006350695192813874, AUC: 0.7819955000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006325266361236572, AUC: 0.7824439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006301731169223785, AUC: 0.7833330000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006275551319122315, AUC: 0.783975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006251139044761658, AUC: 0.7846934999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006229832172393799, AUC: 0.784948\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006208216845989227, AUC: 0.7855429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006190652549266816, AUC: 0.7859399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006170299053192139, AUC: 0.7861855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006151198744773865, AUC: 0.7866769999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006131869256496429, AUC: 0.786915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006113179624080658, AUC: 0.787569\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006096697747707367, AUC: 0.787856\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006078928709030151, AUC: 0.7885090000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006064049601554871, AUC: 0.788841\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006049715876579284, AUC: 0.78928\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947072148323059, AUC: 0.497153\n",
      "\n",
      "Train loss: 0.28803555297244127\n",
      "Train loss: 0.2508950279017163\n",
      "Train loss: 0.26412330805116396\n",
      "Train loss: 0.2631133965625884\n",
      "Train loss: 0.27455296941623564\n",
      "Train loss: 0.3014781581368416\n",
      "Train loss: 0.31359825468367075\n",
      "Train loss: 0.2796356207246234\n",
      "Train loss: 0.28640255502834444\n",
      "Train loss: 0.28725807757893945\n",
      "Train loss: 0.266351873327972\n",
      "Train loss: 0.29643187477330496\n",
      "Train loss: 0.2890690405657337\n",
      "Train loss: 0.32370964926519213\n",
      "Train loss: 0.3188817588387022\n",
      "\n",
      "Test set: Avg. loss: 0.0006932390928268432, AUC: 0.528399\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006916220188140869, AUC: 0.561766\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006901281177997589, AUC: 0.589881\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006886523067951202, AUC: 0.6149709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006872221231460572, AUC: 0.6357679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006858396828174591, AUC: 0.6527475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006844979226589202, AUC: 0.6668860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000683144599199295, AUC: 0.6790970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006818187236785889, AUC: 0.688786\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006805245876312256, AUC: 0.6969020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006792093813419342, AUC: 0.7037915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006779174208641052, AUC: 0.7095975000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006766347289085388, AUC: 0.714585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006752700805664063, AUC: 0.7192620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006739252507686615, AUC: 0.7232500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006725789606571198, AUC: 0.7265990000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006711584627628327, AUC: 0.729305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006697956621646881, AUC: 0.731777\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006682954728603363, AUC: 0.73392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006668470203876495, AUC: 0.735981\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006653151214122772, AUC: 0.7379525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006637997031211853, AUC: 0.739729\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006623509824275971, AUC: 0.741392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006607320904731751, AUC: 0.7429100000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006591616272926331, AUC: 0.7442469999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006575883030891419, AUC: 0.745337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006558734774589538, AUC: 0.746683\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006542018949985504, AUC: 0.747744\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006525775790214538, AUC: 0.7487600000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006508595645427704, AUC: 0.7496035000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006787140667438508, AUC: 0.7279225\n",
      "\n",
      "Train loss: 0.23329416543814788\n",
      "Train loss: 0.1965074793548341\n",
      "Train loss: 0.2175286818461813\n",
      "Train loss: 0.21503547118727567\n",
      "Train loss: 0.22078192234039307\n",
      "Train loss: 0.20269066664823301\n",
      "Train loss: 0.22665006909400795\n",
      "Train loss: 0.22748865139712193\n",
      "Train loss: 0.20414392469794887\n",
      "Train loss: 0.2193483144614347\n",
      "Train loss: 0.22251512328530573\n",
      "Train loss: 0.20230341185430054\n",
      "Train loss: 0.23820869482246934\n",
      "Train loss: 0.2129288729588697\n",
      "Train loss: 0.2235608575450387\n",
      "\n",
      "Test set: Avg. loss: 0.0006755665838718414, AUC: 0.74317\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00067304927110672, AUC: 0.7520585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006705779731273651, AUC: 0.7588244999999999\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006680652797222137, AUC: 0.76432\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006656407713890076, AUC: 0.7686085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006630861163139343, AUC: 0.771946\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006605594158172607, AUC: 0.7745869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006580725610256195, AUC: 0.776539\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006558189690113068, AUC: 0.7780509999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006531240344047546, AUC: 0.7795895000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006505233943462372, AUC: 0.7808269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006481728553771973, AUC: 0.7818244999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006457369327545166, AUC: 0.782664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006431747376918793, AUC: 0.7834165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006409286558628082, AUC: 0.7842640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006386128962039948, AUC: 0.7848859999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006362033784389496, AUC: 0.785704\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006338813006877899, AUC: 0.786359\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006314097344875336, AUC: 0.7870425000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006290288865566253, AUC: 0.78764\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006267174184322357, AUC: 0.7881665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006243877410888672, AUC: 0.7887715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006222697794437408, AUC: 0.789388\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000620128720998764, AUC: 0.7897799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000618078887462616, AUC: 0.7903830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006160578429698944, AUC: 0.790723\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006138507723808289, AUC: 0.7911900000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000611699640750885, AUC: 0.7914380000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000609741598367691, AUC: 0.7921185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006077866852283478, AUC: 0.792665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007173434495925903, AUC: 0.407621\n",
      "\n",
      "Train loss: 0.39365054885293266\n",
      "Train loss: 0.3641681291495159\n",
      "Train loss: 0.3465496483881762\n",
      "Train loss: 0.32092021642976504\n",
      "Train loss: 0.265318449515446\n",
      "Train loss: 0.42787748271492637\n",
      "Train loss: 0.34172322112283887\n",
      "Train loss: 0.41493082957662597\n",
      "Train loss: 0.33332757661297063\n",
      "Train loss: 0.32604747972670634\n",
      "Train loss: 0.3670035695574086\n",
      "Train loss: 0.3464245374794978\n",
      "Train loss: 0.33668789780063996\n",
      "Train loss: 0.3419307781632539\n",
      "Train loss: 0.3646772894889686\n",
      "\n",
      "Test set: Avg. loss: 0.0007039355635643005, AUC: 0.4754195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006949189901351929, AUC: 0.5744024999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006885329484939575, AUC: 0.6423110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006832610368728638, AUC: 0.6742699999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006788694262504578, AUC: 0.6893585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006748995780944824, AUC: 0.697733\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006716192662715912, AUC: 0.702445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006686002910137177, AUC: 0.70618\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006658430099487305, AUC: 0.7090835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006631563305854797, AUC: 0.711311\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006608803868293762, AUC: 0.7139040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006583410203456878, AUC: 0.7162289999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006560188233852386, AUC: 0.718201\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006540336012840271, AUC: 0.7204299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006519377529621125, AUC: 0.721893\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006498225331306457, AUC: 0.7233945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006481603682041168, AUC: 0.725144\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006460151374340057, AUC: 0.7263270000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006440410614013672, AUC: 0.7276990000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006421054303646088, AUC: 0.729052\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006403257548809051, AUC: 0.7305999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006386339366436005, AUC: 0.7320114999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006367703378200531, AUC: 0.733208\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006350530683994294, AUC: 0.7342360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006333273649215698, AUC: 0.735364\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006316655874252319, AUC: 0.7365630000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006299312114715576, AUC: 0.7376955000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006283926069736481, AUC: 0.738761\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006268594563007355, AUC: 0.7398465000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006254784762859345, AUC: 0.7410384999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006962255537509919, AUC: 0.46200300000000005\n",
      "\n",
      "Train loss: 0.30364012756165426\n",
      "Train loss: 0.23850242916945438\n",
      "Train loss: 0.24859627037291315\n",
      "Train loss: 0.2799286747434337\n",
      "Train loss: 0.2451590075614346\n",
      "Train loss: 0.2709951415942733\n",
      "Train loss: 0.24531035894041608\n",
      "Train loss: 0.2611055298216024\n",
      "Train loss: 0.3098360338028829\n",
      "Train loss: 0.201128424352901\n",
      "Train loss: 0.2736832833593818\n",
      "Train loss: 0.2591888604650072\n",
      "Train loss: 0.26124088475658636\n",
      "Train loss: 0.25895490350237316\n",
      "Train loss: 0.2598064116611602\n",
      "\n",
      "Test set: Avg. loss: 0.0006922892034053802, AUC: 0.5303964999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006873971223831177, AUC: 0.598874\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006831969916820526, AUC: 0.639581\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006793823838233948, AUC: 0.665784\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006757814288139343, AUC: 0.683828\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006724271476268768, AUC: 0.6969495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006695016324520111, AUC: 0.705975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006667966246604919, AUC: 0.7132320000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006641430854797363, AUC: 0.7186315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006616580486297608, AUC: 0.723374\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006590839922428131, AUC: 0.7272289999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006567186713218689, AUC: 0.7306459999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006543518304824829, AUC: 0.7334694999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006518784761428833, AUC: 0.736035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006496481001377106, AUC: 0.7384189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006473829746246338, AUC: 0.7404855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006452116370201111, AUC: 0.742543\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006429639756679535, AUC: 0.744253\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006407952010631561, AUC: 0.745774\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006387487351894378, AUC: 0.7473515000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000636654645204544, AUC: 0.7486649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006347189247608185, AUC: 0.7500469999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006328026950359344, AUC: 0.751363\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006310788094997407, AUC: 0.752611\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006291984915733338, AUC: 0.7538334999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006274810135364533, AUC: 0.754955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006258191466331482, AUC: 0.7560435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006240284144878388, AUC: 0.7570865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006225227117538452, AUC: 0.758025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006209666430950165, AUC: 0.7590520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941999197006226, AUC: 0.497598\n",
      "\n",
      "Train loss: 0.3925849783952069\n",
      "Train loss: 0.39452828571295284\n",
      "Train loss: 0.3222807816639068\n",
      "Train loss: 0.35014789765048177\n",
      "Train loss: 0.33800217338428373\n",
      "Train loss: 0.34013378012711837\n",
      "Train loss: 0.30945806556446537\n",
      "Train loss: 0.31280585800766186\n",
      "Train loss: 0.367869383969884\n",
      "Train loss: 0.2952500141350327\n",
      "Train loss: 0.34971759007994535\n",
      "Train loss: 0.3153393044593228\n",
      "Train loss: 0.34719970119986565\n",
      "Train loss: 0.35217134398259936\n",
      "Train loss: 0.33156726390692837\n",
      "\n",
      "Test set: Avg. loss: 0.0006918652057647705, AUC: 0.533955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000689908504486084, AUC: 0.5634790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006881396174430847, AUC: 0.587118\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000686536967754364, AUC: 0.6057285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006850473582744598, AUC: 0.6205375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006835942566394805, AUC: 0.632019\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006822648048400879, AUC: 0.6412559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006809954643249512, AUC: 0.6490469999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006796869039535523, AUC: 0.656039\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006784494817256928, AUC: 0.6620360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006773348748683929, AUC: 0.666644\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006761601567268371, AUC: 0.6712135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006749694645404816, AUC: 0.6752879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000673870325088501, AUC: 0.6789485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000672756016254425, AUC: 0.682609\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000671602487564087, AUC: 0.685945\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006704702079296112, AUC: 0.6895560000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006694221794605255, AUC: 0.6925425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006682207584381104, AUC: 0.695418\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006670372188091278, AUC: 0.6981189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000665871649980545, AUC: 0.700533\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006646739840507508, AUC: 0.703076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006634033322334289, AUC: 0.7056069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006621023416519165, AUC: 0.7081339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006607334017753601, AUC: 0.7107690000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006594657003879547, AUC: 0.713323\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006581801772117615, AUC: 0.7155935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006569226384162903, AUC: 0.7179595000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006557233035564422, AUC: 0.720136\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006544747352600097, AUC: 0.7222489999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006923116147518158, AUC: 0.5312720000000001\n",
      "\n",
      "Train loss: 0.2566044330596924\n",
      "Train loss: 0.25784841274759573\n",
      "Train loss: 0.25508197591562937\n",
      "Train loss: 0.2470536630624419\n",
      "Train loss: 0.2405956076208953\n",
      "Train loss: 0.2363429396015823\n",
      "Train loss: 0.2494381730723533\n",
      "Train loss: 0.2725364930310826\n",
      "Train loss: 0.26361004334346505\n",
      "Train loss: 0.2633491644434109\n",
      "Train loss: 0.2614168312139572\n",
      "Train loss: 0.26099264963417296\n",
      "Train loss: 0.27066130509042435\n",
      "Train loss: 0.27644768755906707\n",
      "Train loss: 0.2561367678034837\n",
      "\n",
      "Test set: Avg. loss: 0.0006892262399196625, AUC: 0.583526\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006865724325180053, AUC: 0.6194314999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006838291883468628, AUC: 0.6481629999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006811134517192841, AUC: 0.670153\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006786915063858032, AUC: 0.685936\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006763208210468293, AUC: 0.6976704999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006741483509540558, AUC: 0.706938\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006718654334545135, AUC: 0.7143575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006696116626262664, AUC: 0.7203919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006674436032772064, AUC: 0.725835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006652948558330536, AUC: 0.730609\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006631795763969422, AUC: 0.734046\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006610250174999237, AUC: 0.737406\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006589305102825165, AUC: 0.739911\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006568245887756348, AUC: 0.742128\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006548852026462555, AUC: 0.744082\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000652977466583252, AUC: 0.7459175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006510161459445953, AUC: 0.747709\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006493759751319885, AUC: 0.749196\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006474750638008118, AUC: 0.7505739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000645648717880249, AUC: 0.7518339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006440695822238922, AUC: 0.753106\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000642239660024643, AUC: 0.754343\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006407683789730072, AUC: 0.755212\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006390129029750824, AUC: 0.7557149999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006374710202217102, AUC: 0.7570189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006359499096870422, AUC: 0.758037\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006345787942409516, AUC: 0.758658\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006333024799823761, AUC: 0.759216\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000631923496723175, AUC: 0.76002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006974876523017883, AUC: 0.45843500000000004\n",
      "\n",
      "Train loss: 0.278844271116196\n",
      "Train loss: 0.3018906222786873\n",
      "Train loss: 0.2577036186388344\n",
      "Train loss: 0.2825034226581549\n",
      "Train loss: 0.2807199746180492\n",
      "Train loss: 0.2848745728753934\n",
      "Train loss: 0.2652761780532302\n",
      "Train loss: 0.30896090170380414\n",
      "Train loss: 0.2609475828280115\n",
      "Train loss: 0.2384151998598864\n",
      "Train loss: 0.28925603742052797\n",
      "Train loss: 0.28858925392673274\n",
      "Train loss: 0.25090904296583433\n",
      "Train loss: 0.2828464618154392\n",
      "Train loss: 0.2591556238520677\n",
      "\n",
      "Test set: Avg. loss: 0.0006955741047859192, AUC: 0.48538749999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006939609944820404, AUC: 0.5136975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006925079524517059, AUC: 0.543229\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000691136360168457, AUC: 0.5733529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000689860463142395, AUC: 0.602503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006886804699897766, AUC: 0.6290515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006873566210269928, AUC: 0.6565585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006861407160758973, AUC: 0.6790655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006849177479743958, AUC: 0.6983015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006837712228298187, AUC: 0.7135389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006825787723064423, AUC: 0.7265169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006813441812992095, AUC: 0.737493\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006801340579986572, AUC: 0.7461535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006788329482078552, AUC: 0.7534489999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006775401532649993, AUC: 0.7587740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006760889291763305, AUC: 0.763139\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006747019290924073, AUC: 0.766311\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000673281341791153, AUC: 0.7687160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006718424558639527, AUC: 0.7703789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006702761054039002, AUC: 0.7716705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006687882840633393, AUC: 0.7721629999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006672575175762177, AUC: 0.7727595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006657455265522003, AUC: 0.772853\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006640039682388306, AUC: 0.772891\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006624569594860076, AUC: 0.7728475000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006607206761837006, AUC: 0.77281\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006589910686016083, AUC: 0.7727669999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006572491526603699, AUC: 0.772238\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006553426086902619, AUC: 0.772123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006533944606781006, AUC: 0.771907\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007073167264461518, AUC: 0.31173500000000004\n",
      "\n",
      "Train loss: 0.18354031842225677\n",
      "Train loss: 0.20713381934317815\n",
      "Train loss: 0.22100003433834975\n",
      "Train loss: 0.22084650909824735\n",
      "Train loss: 0.19355524687250708\n",
      "Train loss: 0.17030072364078205\n",
      "Train loss: 0.17729513242745856\n",
      "Train loss: 0.23036302274959103\n",
      "Train loss: 0.2012517566134216\n",
      "Train loss: 0.1909838558002642\n",
      "Train loss: 0.22966037206589038\n",
      "Train loss: 0.19411911212714614\n",
      "Train loss: 0.19351224458900987\n",
      "Train loss: 0.19092586693490388\n",
      "Train loss: 0.1847699607253834\n",
      "\n",
      "Test set: Avg. loss: 0.0007039954960346222, AUC: 0.344534\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007007471919059753, AUC: 0.38452949999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006976049542427063, AUC: 0.4331365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006949351727962494, AUC: 0.47961550000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006924784183502198, AUC: 0.5256460000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006903257966041565, AUC: 0.5660390000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006883292496204376, AUC: 0.600932\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006864057183265686, AUC: 0.631123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006844946742057801, AUC: 0.6576115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006825700104236603, AUC: 0.6801429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006807696521282196, AUC: 0.6978769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006789461076259614, AUC: 0.7128485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006771513819694519, AUC: 0.7248595000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000675249695777893, AUC: 0.7363990000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006733392477035522, AUC: 0.745149\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006713018119335175, AUC: 0.7532650000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006694532930850983, AUC: 0.759726\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000667517602443695, AUC: 0.7650860000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006655682325363159, AUC: 0.7696795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006633988916873932, AUC: 0.773916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006612575054168701, AUC: 0.776961\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006590418219566346, AUC: 0.779622\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006566565930843354, AUC: 0.7821139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006544488966464996, AUC: 0.7841115000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006521991491317749, AUC: 0.786253\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000649906039237976, AUC: 0.788076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000647485226392746, AUC: 0.789386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006451266407966614, AUC: 0.790343\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006429085433483124, AUC: 0.791369\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006405390202999115, AUC: 0.792479\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000718257486820221, AUC: 0.282124\n",
      "\n",
      "Train loss: 0.24279541650395484\n",
      "Train loss: 0.21561232797659127\n",
      "Train loss: 0.21026910907903296\n",
      "Train loss: 0.22105312119623657\n",
      "Train loss: 0.24160270326456446\n",
      "Train loss: 0.20978607265812577\n",
      "Train loss: 0.20176735994922126\n",
      "Train loss: 0.1943474138618275\n",
      "Train loss: 0.19081071114084522\n",
      "Train loss: 0.1840556727093496\n",
      "Train loss: 0.19481274162887768\n",
      "Train loss: 0.1982407649611212\n",
      "Train loss: 0.21892993047738532\n",
      "Train loss: 0.18922295160354322\n",
      "Train loss: 0.16525371553032261\n",
      "\n",
      "Test set: Avg. loss: 0.0006855151653289795, AUC: 0.6416220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006722566485404968, AUC: 0.7267460000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006618347465991974, AUC: 0.745714\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006519446969032287, AUC: 0.752426\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006433593630790711, AUC: 0.7565330000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006352792382240296, AUC: 0.760345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006276961266994477, AUC: 0.763269\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006205072104930878, AUC: 0.766061\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006149581372737884, AUC: 0.7689180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006095667779445649, AUC: 0.771495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006046305596828461, AUC: 0.774471\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006002231538295746, AUC: 0.777876\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005965810418128967, AUC: 0.7807080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005922438502311706, AUC: 0.78367\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005892052352428436, AUC: 0.786622\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005862601697444916, AUC: 0.7897550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005841680765151978, AUC: 0.7926974999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000580964595079422, AUC: 0.795039\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000578916758298874, AUC: 0.7976960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005762392282485962, AUC: 0.8004735000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005729475915431976, AUC: 0.803247\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005699922144412994, AUC: 0.8055450000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005677993595600129, AUC: 0.8083120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005660646557807922, AUC: 0.811585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005643628537654877, AUC: 0.8142339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005625787675380706, AUC: 0.816853\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005600066781044006, AUC: 0.8196379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005573868155479431, AUC: 0.822014\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005546629428863526, AUC: 0.824776\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005518604516983033, AUC: 0.8268089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007088316977024078, AUC: 0.50386\n",
      "\n",
      "Train loss: 0.3775375641075669\n",
      "Train loss: 0.4605535067570437\n",
      "Train loss: 0.33195132358818297\n",
      "Train loss: 0.3480214894197549\n",
      "Train loss: 0.3521298777525592\n",
      "Train loss: 0.36635084516683203\n",
      "Train loss: 0.380381567462994\n",
      "Train loss: 0.35262075047584096\n",
      "Train loss: 0.3618609476241337\n",
      "Train loss: 0.3121037794526216\n",
      "Train loss: 0.3326571386331206\n",
      "Train loss: 0.2951301127482372\n",
      "Train loss: 0.38917634707347604\n",
      "Train loss: 0.3030783739059594\n",
      "Train loss: 0.2858227366095136\n",
      "\n",
      "Test set: Avg. loss: 0.0006793177425861358, AUC: 0.696913\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006706357896327972, AUC: 0.744602\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000662370353937149, AUC: 0.7645570000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006539131999015809, AUC: 0.775309\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006449054479599, AUC: 0.7825139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006367073655128479, AUC: 0.7853070000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006294398307800293, AUC: 0.7866550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006220313906669617, AUC: 0.787127\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006144852340221405, AUC: 0.7890355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006064879596233368, AUC: 0.7910919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006006953120231629, AUC: 0.792106\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005951392650604248, AUC: 0.793738\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005885851979255676, AUC: 0.7982450000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00058387491106987, AUC: 0.800707\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005793068110942841, AUC: 0.803362\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005738622546195984, AUC: 0.8071329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005684905052185059, AUC: 0.8113329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005643735527992249, AUC: 0.815752\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005617674589157105, AUC: 0.8188690000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005566402077674865, AUC: 0.8213940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005527609586715699, AUC: 0.8245625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005497868955135346, AUC: 0.8276459999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005458910465240478, AUC: 0.8318099999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000541364073753357, AUC: 0.836215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005375449955463409, AUC: 0.83977\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000533954381942749, AUC: 0.841224\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005315234065055847, AUC: 0.843995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005274874567985535, AUC: 0.847281\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005233792066574096, AUC: 0.848637\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005200292468070984, AUC: 0.8520270000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006859288513660431, AUC: 0.6275275\n",
      "\n",
      "Train loss: 0.29534203877114945\n",
      "Train loss: 0.2445476901759008\n",
      "Train loss: 0.2403881629561163\n",
      "Train loss: 0.262374908301481\n",
      "Train loss: 0.32654329204255605\n",
      "Train loss: 0.26135346768008677\n",
      "Train loss: 0.24527620206213302\n",
      "Train loss: 0.25475897416946996\n",
      "Train loss: 0.2271808423813741\n",
      "Train loss: 0.2756528508890966\n",
      "Train loss: 0.250697721341613\n",
      "Train loss: 0.2828122403970949\n",
      "Train loss: 0.24424752080516451\n",
      "Train loss: 0.23007288138577892\n",
      "Train loss: 0.2077400475550609\n",
      "\n",
      "Test set: Avg. loss: 0.000675718903541565, AUC: 0.692323\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006669366359710693, AUC: 0.7163609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000657539963722229, AUC: 0.731213\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006492651402950287, AUC: 0.738108\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006410962045192718, AUC: 0.744613\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006340934932231903, AUC: 0.750642\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006263099610805511, AUC: 0.7571270000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006187093257904052, AUC: 0.7622434999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006125571131706238, AUC: 0.766321\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006064384281635284, AUC: 0.7717809999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006014975309371948, AUC: 0.775365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005952989161014557, AUC: 0.7813830000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005911889076232911, AUC: 0.7864945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005857743322849274, AUC: 0.7908355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005811223685741424, AUC: 0.7951440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005768184661865234, AUC: 0.800185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005718823075294495, AUC: 0.8031609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005689612329006195, AUC: 0.8075640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005649995803833008, AUC: 0.810719\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005616684257984161, AUC: 0.815508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005572797954082489, AUC: 0.8175479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005547177195549012, AUC: 0.8230889999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005512160360813141, AUC: 0.8257270000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005485152304172515, AUC: 0.8311755000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005431767404079438, AUC: 0.830565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005416803061962128, AUC: 0.836998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005374867618083953, AUC: 0.838076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000536211758852005, AUC: 0.8431960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000532742977142334, AUC: 0.8446319999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005283772647380829, AUC: 0.845598\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007118547260761261, AUC: 0.32816500000000004\n",
      "\n",
      "Train loss: 0.2989620380340868\n",
      "Train loss: 0.32434611183822537\n",
      "Train loss: 0.3508803293963147\n",
      "Train loss: 0.3251055205703541\n",
      "Train loss: 0.3134303601684084\n",
      "Train loss: 0.327706948966737\n",
      "Train loss: 0.25673715779735784\n",
      "Train loss: 0.33074645973314903\n",
      "Train loss: 0.2796965947576389\n",
      "Train loss: 0.2843618681476374\n",
      "Train loss: 0.3464306163939701\n",
      "Train loss: 0.2531654922066221\n",
      "Train loss: 0.35150144320384713\n",
      "Train loss: 0.28923378097023933\n",
      "Train loss: 0.2864012144933081\n",
      "\n",
      "Test set: Avg. loss: 0.0007002629041671753, AUC: 0.3786165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952589154243469, AUC: 0.46660050000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006919208168983459, AUC: 0.55202\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006889106333255768, AUC: 0.622323\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006858068108558655, AUC: 0.6743560000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006824159622192383, AUC: 0.710034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000678071141242981, AUC: 0.7358359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006730778515338897, AUC: 0.7509064999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006670894026756287, AUC: 0.7610525000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000660363346338272, AUC: 0.7685660000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006531411111354827, AUC: 0.7724639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006443365514278412, AUC: 0.7746190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006359018683433533, AUC: 0.7765575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006282975375652314, AUC: 0.778751\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006207510530948639, AUC: 0.7814180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006139645278453827, AUC: 0.782877\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006078597009181976, AUC: 0.7846439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006020857095718384, AUC: 0.786891\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005966466069221496, AUC: 0.789455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005916291773319244, AUC: 0.792065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005874954760074616, AUC: 0.7947960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005829669535160065, AUC: 0.7967825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005796265900135041, AUC: 0.7996159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005758908689022064, AUC: 0.8031689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005722416937351227, AUC: 0.8066930000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005683653354644775, AUC: 0.809961\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000565098762512207, AUC: 0.8137719999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00056249338388443, AUC: 0.817414\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005592448115348816, AUC: 0.820941\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005559235215187073, AUC: 0.8236490000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007025113403797149, AUC: 0.4011085\n",
      "\n",
      "Train loss: 0.35063541315163776\n",
      "Train loss: 0.2682289883589289\n",
      "Train loss: 0.3466424326987783\n",
      "Train loss: 0.2318961468471843\n",
      "Train loss: 0.2870749451552227\n",
      "Train loss: 0.2962500384658765\n",
      "Train loss: 0.28442059960334926\n",
      "Train loss: 0.2697556345326126\n",
      "Train loss: 0.2814817906944615\n",
      "Train loss: 0.24805204238101936\n",
      "Train loss: 0.23255143499678108\n",
      "Train loss: 0.25675445681164977\n",
      "Train loss: 0.27637745999986196\n",
      "Train loss: 0.24145863086554656\n",
      "Train loss: 0.2504461551927457\n",
      "\n",
      "Test set: Avg. loss: 0.0006898408830165863, AUC: 0.5709845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006796177923679351, AUC: 0.713311\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006701429188251495, AUC: 0.7732160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006582845747470856, AUC: 0.7985314999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006451598405838013, AUC: 0.8074925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006308509111404419, AUC: 0.8097520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000617968738079071, AUC: 0.8092459999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006060036718845368, AUC: 0.809097\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005964435636997223, AUC: 0.8105559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005889615118503571, AUC: 0.811401\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005817767679691314, AUC: 0.814097\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005752489268779755, AUC: 0.8150649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005706054866313935, AUC: 0.818189\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005659703612327575, AUC: 0.8207169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005617236495018005, AUC: 0.823853\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005577794313430787, AUC: 0.8263529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005531076490879059, AUC: 0.826704\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005499606132507325, AUC: 0.8313459999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005474047362804413, AUC: 0.8331415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000545157790184021, AUC: 0.83717\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000541665256023407, AUC: 0.8394519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005402414500713348, AUC: 0.841442\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005378512442111969, AUC: 0.8431269999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005360589921474456, AUC: 0.84755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005324837863445282, AUC: 0.8473920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005299736857414246, AUC: 0.849843\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005282538831233978, AUC: 0.8530349999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005256597697734833, AUC: 0.8535809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005228540599346161, AUC: 0.8548349999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005214717090129853, AUC: 0.8582449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006976809203624725, AUC: 0.4578315\n",
      "\n",
      "Train loss: 0.3476544182012036\n",
      "Train loss: 0.29523958484078666\n",
      "Train loss: 0.34301167195010335\n",
      "Train loss: 0.3158453247349733\n",
      "Train loss: 0.33614978972513965\n",
      "Train loss: 0.3159587789493002\n",
      "Train loss: 0.2664221187306058\n",
      "Train loss: 0.3836206292650502\n",
      "Train loss: 0.2919499088244833\n",
      "Train loss: 0.32409198117104304\n",
      "Train loss: 0.3359652013535712\n",
      "Train loss: 0.3256896529228065\n",
      "Train loss: 0.3501369489985666\n",
      "Train loss: 0.2944463707838848\n",
      "Train loss: 0.3156575401117847\n",
      "\n",
      "Test set: Avg. loss: 0.0006922675669193268, AUC: 0.541553\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000688874751329422, AUC: 0.611731\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000685577243566513, AUC: 0.659528\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006820595562458038, AUC: 0.6913119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006775754392147064, AUC: 0.7093974999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006727660000324249, AUC: 0.7202745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006667549610137939, AUC: 0.727922\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000659960150718689, AUC: 0.7330490000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006527735888957977, AUC: 0.738284\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006455252766609192, AUC: 0.742014\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006384370625019073, AUC: 0.7454949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006322335302829743, AUC: 0.749091\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000626126766204834, AUC: 0.752943\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006202944815158844, AUC: 0.7559049999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006148532629013061, AUC: 0.7594755000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006111995875835419, AUC: 0.762783\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006074793934822083, AUC: 0.7657090000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006026257574558258, AUC: 0.7688969999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005990254580974579, AUC: 0.771104\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005953487753868103, AUC: 0.773487\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005927825570106506, AUC: 0.7760385000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005896938145160675, AUC: 0.778167\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005868479013442994, AUC: 0.780215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005843957960605621, AUC: 0.782814\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005825934112071991, AUC: 0.7847280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005800623595714569, AUC: 0.787174\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005770065486431122, AUC: 0.7894360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005744734704494477, AUC: 0.792049\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005731655657291412, AUC: 0.7939799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005702040791511536, AUC: 0.7963829999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006903973519802093, AUC: 0.5523885\n",
      "\n",
      "Train loss: 0.2633548705441177\n",
      "Train loss: 0.2955473460209597\n",
      "Train loss: 0.29439605155568216\n",
      "Train loss: 0.2924051561932655\n",
      "Train loss: 0.27325438352147485\n",
      "Train loss: 0.28010429014825516\n",
      "Train loss: 0.2967426332698506\n",
      "Train loss: 0.248883890498216\n",
      "Train loss: 0.26709455099834756\n",
      "Train loss: 0.28456285007440363\n",
      "Train loss: 0.2820380705936699\n",
      "Train loss: 0.27662535989360443\n",
      "Train loss: 0.29026038251864683\n",
      "Train loss: 0.26229506199526936\n",
      "Train loss: 0.27110565468004555\n",
      "\n",
      "Test set: Avg. loss: 0.0006806291341781616, AUC: 0.6520889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006728332936763764, AUC: 0.7028300000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006657307744026184, AUC: 0.731559\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006580401659011841, AUC: 0.7495879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006499513387680054, AUC: 0.760304\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006419650614261628, AUC: 0.7677999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006344489455223084, AUC: 0.7739940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006259636282920837, AUC: 0.778263\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006180939078330994, AUC: 0.782113\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006095927953720093, AUC: 0.785737\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006022898852825164, AUC: 0.789154\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000595382422208786, AUC: 0.7931819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005886983275413513, AUC: 0.796825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005819515287876129, AUC: 0.800314\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005764863789081573, AUC: 0.803455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005713963210582733, AUC: 0.807051\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0005665676593780518, AUC: 0.8105120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000561883419752121, AUC: 0.814373\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00055844646692276, AUC: 0.818042\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005541107356548309, AUC: 0.821893\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005503147840499878, AUC: 0.8247199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005475920438766479, AUC: 0.827519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005445378422737121, AUC: 0.830985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005404637753963471, AUC: 0.8352550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005381945669651031, AUC: 0.8373069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005364443063735962, AUC: 0.840086\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000533218115568161, AUC: 0.843809\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005298088490962983, AUC: 0.8462309999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005272234380245209, AUC: 0.848664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005233173668384552, AUC: 0.851064\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973285675048828, AUC: 0.5184565\n",
      "\n",
      "Train loss: 0.27460544997719444\n",
      "Train loss: 0.2617756883809521\n",
      "Train loss: 0.2834964136409152\n",
      "Train loss: 0.2879892287740282\n",
      "Train loss: 0.2757835483095448\n",
      "Train loss: 0.25849739950933276\n",
      "Train loss: 0.2399688495951853\n",
      "Train loss: 0.27038310895300216\n",
      "Train loss: 0.28002384665665353\n",
      "Train loss: 0.29268804515243335\n",
      "Train loss: 0.26656224355576147\n",
      "Train loss: 0.27910895408338804\n",
      "Train loss: 0.2665805486357136\n",
      "Train loss: 0.27351509309878014\n",
      "Train loss: 0.25814950580050233\n",
      "\n",
      "Test set: Avg. loss: 0.000685533583164215, AUC: 0.656349\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006747876405715943, AUC: 0.7276204999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006632292568683624, AUC: 0.7549325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000651152789592743, AUC: 0.7651869999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006385098993778229, AUC: 0.7698995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006267256140708923, AUC: 0.773003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006171038448810577, AUC: 0.7757055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006086076200008392, AUC: 0.777682\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006021746397018433, AUC: 0.7802530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000595744639635086, AUC: 0.7825070000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005912047624588013, AUC: 0.785725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005875353813171387, AUC: 0.78773\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005834050476551056, AUC: 0.790817\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005790715515613556, AUC: 0.793188\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005759238302707672, AUC: 0.796061\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005739046335220337, AUC: 0.799292\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000570812165737152, AUC: 0.8026479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005676121115684509, AUC: 0.806203\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005650791525840759, AUC: 0.8089879999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005620984733104706, AUC: 0.811306\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005590918362140655, AUC: 0.814331\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005564588904380798, AUC: 0.816763\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005541290640830994, AUC: 0.820241\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005522591471672058, AUC: 0.82218\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005496344864368438, AUC: 0.825508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005467975437641144, AUC: 0.828541\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005453228056430816, AUC: 0.831219\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005421369075775147, AUC: 0.833428\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005398148894309998, AUC: 0.836448\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005368064641952515, AUC: 0.83905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006995826959609985, AUC: 0.422173\n",
      "\n",
      "Train loss: 0.27854905576462957\n",
      "Train loss: 0.22796926149137461\n",
      "Train loss: 0.2599353706760771\n",
      "Train loss: 0.22122593556240105\n",
      "Train loss: 0.2444842150256892\n",
      "Train loss: 0.23127188652184358\n",
      "Train loss: 0.2367039319056614\n",
      "Train loss: 0.20711312210483915\n",
      "Train loss: 0.22679210202709124\n",
      "Train loss: 0.22823786204028282\n",
      "Train loss: 0.20676036055680294\n",
      "Train loss: 0.21065809506519584\n",
      "Train loss: 0.22770334770725031\n",
      "Train loss: 0.20515604079908625\n",
      "Train loss: 0.185137517892631\n",
      "\n",
      "Test set: Avg. loss: 0.0006858318150043488, AUC: 0.6329275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000673928290605545, AUC: 0.7188190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006626125872135162, AUC: 0.744378\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006522134542465209, AUC: 0.7538215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006413453221321106, AUC: 0.761034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006321808397769928, AUC: 0.764706\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006232659518718719, AUC: 0.767956\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006161397099494934, AUC: 0.7708155000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006115421354770661, AUC: 0.773282\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006058960556983948, AUC: 0.7761669999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006011187732219696, AUC: 0.77944\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005973632633686066, AUC: 0.7822430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005931018590927124, AUC: 0.785498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005894049108028412, AUC: 0.788941\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005861968696117401, AUC: 0.7919479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000582897961139679, AUC: 0.7949029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005791110396385192, AUC: 0.7979580000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005760253369808197, AUC: 0.8009780000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005720489621162414, AUC: 0.803834\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005697618126869202, AUC: 0.807117\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005671678185462951, AUC: 0.8087509999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005634013712406159, AUC: 0.812554\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005607081353664398, AUC: 0.816398\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005586934089660644, AUC: 0.8176640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005552931129932403, AUC: 0.821051\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005520919859409333, AUC: 0.823942\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005489878356456756, AUC: 0.8273964999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005447323024272919, AUC: 0.8306369999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005416851341724396, AUC: 0.835067\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005398655533790589, AUC: 0.8383090000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007560735642910003, AUC: 0.376822\n",
      "\n",
      "Train loss: 0.4134020433304416\n",
      "Train loss: 0.37008381649187416\n",
      "Train loss: 0.3431662070523402\n",
      "Train loss: 0.3253183182637403\n",
      "Train loss: 0.3637054167735349\n",
      "Train loss: 0.3673566887333135\n",
      "Train loss: 0.39156732399752187\n",
      "Train loss: 0.34499567443398155\n",
      "Train loss: 0.32493804746372684\n",
      "Train loss: 0.32621797254890395\n",
      "Train loss: 0.33237425878549076\n",
      "Train loss: 0.34114362707563267\n",
      "Train loss: 0.3218216151948188\n",
      "Train loss: 0.3342897645227469\n",
      "Train loss: 0.280481772058329\n",
      "\n",
      "Test set: Avg. loss: 0.0006931832432746887, AUC: 0.5439365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006758662760257721, AUC: 0.746365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006609989404678345, AUC: 0.7671735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006473539769649506, AUC: 0.7695150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006329608559608459, AUC: 0.7735780000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000621952086687088, AUC: 0.775762\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006107365489006043, AUC: 0.7786305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006025086045265197, AUC: 0.7814099999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005954236388206482, AUC: 0.784243\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005895255506038666, AUC: 0.7875420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005839668214321136, AUC: 0.790967\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005797304809093475, AUC: 0.7951590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005759171843528748, AUC: 0.798516\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005714214444160462, AUC: 0.8023039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005694416463375092, AUC: 0.804597\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005652609169483184, AUC: 0.808328\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005612103939056397, AUC: 0.8117749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005578757822513581, AUC: 0.8155340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005544147193431854, AUC: 0.8190020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005509757697582245, AUC: 0.821804\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000547719657421112, AUC: 0.8249299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005447686016559601, AUC: 0.827534\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005416778922080994, AUC: 0.8305830000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005381516814231872, AUC: 0.8333090000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005339490473270416, AUC: 0.83568\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005316562056541442, AUC: 0.8390525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005282561480998993, AUC: 0.841912\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000525954157114029, AUC: 0.843925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005226964950561523, AUC: 0.8456950000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005204899609088897, AUC: 0.848009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 class triplet loss no ratio \n",
    "# no smote \n",
    "\n",
    "# note: sometimes can get a very high accuracy but may diverge\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(5e-6, 1e-3), (1e-6, 5e-4), (1e-6, 1e-4), (5e-6, 5e-4)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "    \n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10): \n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(complete_network.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(15):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss, embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_reduced, complete_network, linear_optimizer, verbose=False)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"triplet_loss\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], 0.0, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8afa13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b889a171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.000679101675748825, AUC: 0.7387214999999999\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m15\u001b[39m):\n\u001b[0;32m---> 27\u001b[0m     _, train_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_triplet_loss_smote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_tripletloss_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(train_losses))))\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:201\u001b[0m, in \u001b[0;36mtrain_triplet_loss_smote\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn_args)\u001b[0m\n\u001b[1;32m    199\u001b[0m neg_embeds \u001b[38;5;241m=\u001b[39m network(neg_data)\n\u001b[1;32m    200\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(anchor_embeds, pos_embeds, neg_embeds)\n\u001b[0;32m--> 201\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "# 2 class triplet loss with capped SMOTE \n",
    "\n",
    "# note: sometimes can get a very high accuracy but may diverge\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(5e-6, 1e-3), (1e-6, 5e-4), (1e-6, 1e-4), (5e-6, 5e-4)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = 5.0\n",
    "loss_fn_args['distance'] = 'euclidean'\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10): \n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(complete_network.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(15):\n",
    "            _, train_losses = train.train_triplet_loss_smote(epoch, train_loader_tripletloss_smote, embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_reduced, complete_network, linear_optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"triplet_loss\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], 0.0, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f34cc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0007769797742366791, AUC: 0.581019\n",
      "\n",
      "Train loss: 16.096790712090986\n",
      "Train loss: 7.95101783849016\n",
      "Train loss: 1.6742434924161886\n",
      "Train loss: 0.0\n",
      "Train loss: 6.369154097158698\n",
      "Train loss: 1.0685905746266813\n",
      "Train loss: 0.9919730077815961\n",
      "Train loss: 4.3510085480122624\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 1.729099128819719\n",
      "Train loss: 0.0\n",
      "Train loss: 2.9074361294130737\n",
      "Train loss: 0.0\n",
      "\n",
      "Test set: Avg. loss: 0.0007081505060195923, AUC: 0.595766\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918745040893554, AUC: 0.599685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006822091042995453, AUC: 0.606827\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006758022308349609, AUC: 0.614831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006717154681682586, AUC: 0.620797\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006670982539653778, AUC: 0.6299465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006633270978927612, AUC: 0.641555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006615970730781555, AUC: 0.647514\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006594435572624207, AUC: 0.6563729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006575994789600372, AUC: 0.666123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006559013724327088, AUC: 0.6735810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006542154252529145, AUC: 0.6845415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006518358290195465, AUC: 0.698683\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006498202979564666, AUC: 0.7103269999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006485094726085662, AUC: 0.717825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006470210254192352, AUC: 0.7272419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000646136075258255, AUC: 0.7334495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000645109623670578, AUC: 0.7394789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006443145573139191, AUC: 0.7454485000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000643904983997345, AUC: 0.746941\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006429647505283355, AUC: 0.7538564999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000642709881067276, AUC: 0.7577389999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006426226794719696, AUC: 0.760882\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006425316333770752, AUC: 0.764992\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006421145796775818, AUC: 0.768756\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006414653062820435, AUC: 0.773288\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006413009464740754, AUC: 0.774419\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006411257684230805, AUC: 0.7756689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000640982300043106, AUC: 0.7791325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006405025720596314, AUC: 0.780958\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006400455534458161, AUC: 0.7839640000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006398386657238006, AUC: 0.7868439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006397152841091156, AUC: 0.7885960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006394894123077393, AUC: 0.791209\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006395555436611175, AUC: 0.7915060000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006393527686595916, AUC: 0.7925110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006391496956348419, AUC: 0.794049\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006389279961585999, AUC: 0.795107\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006387557983398437, AUC: 0.7956540000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000638495922088623, AUC: 0.79725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006383010745048523, AUC: 0.797925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006380480825901032, AUC: 0.7989930000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006377894580364227, AUC: 0.8008280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006376721858978271, AUC: 0.8018989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006373582482337952, AUC: 0.8024609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006369325518608093, AUC: 0.803453\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006367611885070801, AUC: 0.8036590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006366349756717682, AUC: 0.8035285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006367577910423278, AUC: 0.8037650000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006367309987545014, AUC: 0.8036895000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023003580570220947, AUC: 0.5429729999999999\n",
      "\n",
      "Train loss: 136.23961547658413\n",
      "Train loss: 17.986870584608635\n",
      "Train loss: 15.341035212142557\n",
      "Train loss: 4.752364243133159\n",
      "Train loss: 4.356625907028778\n",
      "Train loss: 0.7004882715925386\n",
      "Train loss: 1.1259002685546875\n",
      "Train loss: 6.454442579534989\n",
      "Train loss: 4.747905163825313\n",
      "Train loss: 0.3334539147871959\n",
      "Train loss: 3.8129760645612887\n",
      "Train loss: 0.0\n",
      "Train loss: 3.7372857588755934\n",
      "Train loss: 3.130930067617682\n",
      "Train loss: 3.3125161279605915\n",
      "\n",
      "Test set: Avg. loss: 0.0007151560485363006, AUC: 0.45533100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007060944736003875, AUC: 0.4688575000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007020347714424133, AUC: 0.4808285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007004459500312805, AUC: 0.4868415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006991749107837677, AUC: 0.49186949999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006983735859394073, AUC: 0.49412800000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006977417469024659, AUC: 0.5011685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006969517469406128, AUC: 0.511384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006968002021312714, AUC: 0.5048075000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006965127289295197, AUC: 0.5116275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006962951421737671, AUC: 0.514332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006961811780929565, AUC: 0.5129704999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006960669755935669, AUC: 0.505204\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695921242237091, AUC: 0.5012044999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006958386301994324, AUC: 0.4976050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006957364678382874, AUC: 0.49709449999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006956190168857574, AUC: 0.500152\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006955111622810364, AUC: 0.493364\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006954317688941955, AUC: 0.48901300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006953255832195282, AUC: 0.48931600000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952438652515411, AUC: 0.4857020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951936483383179, AUC: 0.48956999999999984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951353847980499, AUC: 0.495209\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695084810256958, AUC: 0.49530450000000015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006950275301933289, AUC: 0.49494200000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006949717700481415, AUC: 0.49411399999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006949259042739868, AUC: 0.49331900000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948856711387634, AUC: 0.493639\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948323547840119, AUC: 0.49492200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947891116142273, AUC: 0.498717\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00069475057721138, AUC: 0.503362\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947134435176849, AUC: 0.505495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946909725666046, AUC: 0.508237\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946579813957214, AUC: 0.504954\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946228444576263, AUC: 0.5067439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945843696594238, AUC: 0.5100250000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945613324642182, AUC: 0.506382\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000694526344537735, AUC: 0.5097929999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944917440414429, AUC: 0.507986\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944626867771149, AUC: 0.506527\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944490969181061, AUC: 0.506454\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000694442331790924, AUC: 0.5044559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944300532341003, AUC: 0.502201\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944151818752288, AUC: 0.5003179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943970620632171, AUC: 0.49821999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943894028663635, AUC: 0.4986275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943775415420532, AUC: 0.49697849999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943643987178803, AUC: 0.500112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943606734275818, AUC: 0.50076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943549513816834, AUC: 0.49980600000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001080885648727417, AUC: 0.41414299999999993\n",
      "\n",
      "Train loss: 24.389437904840783\n",
      "Train loss: 0.8292029658450356\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 1.7853070995475673\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 3.6756908561609967\n",
      "Train loss: 0.0\n",
      "\n",
      "Test set: Avg. loss: 0.0007218130528926849, AUC: 0.518904\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006989753544330597, AUC: 0.540902\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946681439876556, AUC: 0.5388165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931599974632263, AUC: 0.5372144999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918653249740601, AUC: 0.5308125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006907552778720856, AUC: 0.5248035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006905039250850678, AUC: 0.5236075\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006901566684246063, AUC: 0.5230110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006899235546588897, AUC: 0.5261325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006896623373031616, AUC: 0.5253125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894396543502808, AUC: 0.5309490000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006897644698619842, AUC: 0.52842\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006899001598358154, AUC: 0.525083\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006898137331008911, AUC: 0.5242764999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006899007260799408, AUC: 0.5234755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006897071897983551, AUC: 0.5284414999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894522905349732, AUC: 0.5316019999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894703507423401, AUC: 0.5321385000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006892255246639252, AUC: 0.5359525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006892721951007843, AUC: 0.5328990000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006891983449459076, AUC: 0.537638\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006890941560268402, AUC: 0.5369265000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006891008913516998, AUC: 0.53901\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893084347248077, AUC: 0.5359465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893291175365448, AUC: 0.5363020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893750429153442, AUC: 0.535502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000689245194196701, AUC: 0.536058\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006888734996318817, AUC: 0.5422319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006889899671077729, AUC: 0.538168\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006888663470745087, AUC: 0.5418495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006884230673313141, AUC: 0.5464029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006882088780403137, AUC: 0.550612\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006883909702301025, AUC: 0.546473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006878928244113922, AUC: 0.5528475000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006881430745124817, AUC: 0.5479885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000688222199678421, AUC: 0.546259\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006877389848232269, AUC: 0.5549315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006872588694095611, AUC: 0.5582575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870459020137787, AUC: 0.5611119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006872363984584809, AUC: 0.5597035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870978772640228, AUC: 0.5620409999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006871461272239685, AUC: 0.561431\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870918273925781, AUC: 0.561923\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006871341466903687, AUC: 0.5619075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000687317818403244, AUC: 0.551667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870152950286865, AUC: 0.5539374999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006868258118629455, AUC: 0.5586725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006866075992584228, AUC: 0.560983\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006869733929634094, AUC: 0.5588695000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006868163049221039, AUC: 0.5607644999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 class triplet loss with ratio \n",
    "# no smote \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(1e-7, 1e-5)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(3): \n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(complete_network.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(15):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss_ratio, embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "        for epoch in range(50):\n",
    "            _, _ = train.train_linear_probe(epoch, train_loader_reduced, complete_network, linear_optimizer, verbose=False)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"triplet_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], \n",
    "           auc_mean[i][4], auc_variance[i][4],\n",
    "           auc_mean[i][5], auc_variance[i][5],\n",
    "           None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10048fce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "14 columns passed, passed data had 18 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:934\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 934\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_or_indexify_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:981\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    982\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    983\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    984\u001b[0m     )\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 14 columns passed, passed data had 18 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/convnet_aucs.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m df2 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcol_names\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df1, df2])\n\u001b[1;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/convnet_aucs.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:781\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    780\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 781\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    783\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    789\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    790\u001b[0m         arrays,\n\u001b[1;32m    791\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    794\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    795\u001b[0m     )\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:498\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 498\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:840\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    837\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    838\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 840\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:937\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    934\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[0;32m--> 937\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[1;32m    940\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: 14 columns passed, passed data had 18 columns"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de718f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "041af005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006894645094871521, AUC: 0.6804574999999999\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     29\u001b[0m     loss_fn_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_cap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m loss_cap\n\u001b[0;32m---> 30\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_triplet_capped_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_tripletloss_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcap_calc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTripletLoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCappedBCELoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     32\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network, embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:269\u001b[0m, in \u001b[0;36mtrain_triplet_capped_loss\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, cap_calc, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m    266\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    268\u001b[0m anchor_output, anchor_embeds \u001b[38;5;241m=\u001b[39m network(anchor_data\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m--> 269\u001b[0m _, pos_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m _, neg_embeds \u001b[38;5;241m=\u001b[39m network(neg_data\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m    272\u001b[0m cap \u001b[38;5;241m=\u001b[39m cap_calc(anchor_embeds, pos_embeds, neg_embeds)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/models.py:42\u001b[0m, in \u001b[0;36mConvNetWithEmbeddings.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 42\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1_drop(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     43\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)), \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     44\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m250\u001b[39m) \n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# capped smote using triplet loss\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-3, 1e-3, 1e-2]\n",
    "\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "loss_caps = [1, 5, 10]\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(5): \n",
    "            print(loss_cap)\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(2)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                _, _ = train.train_triplet_capped_loss(epoch, train_loader_tripletloss_smote, network, optimizer, verbose=False, cap_calc=loss_fns.TripletLoss,loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"triplet_loss_capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], loss_cap, norm]\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e4764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "511a56e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_fns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m start_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     13\u001b[0m loss_caps \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m cap_calc \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fns\u001b[49m\u001b[38;5;241m.\u001b[39mTripletLossWithAverage\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loss_cap \u001b[38;5;129;01min\u001b[39;00m loss_caps:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m learning_rate \u001b[38;5;129;01min\u001b[39;00m learning_rates:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_fns' is not defined"
     ]
    }
   ],
   "source": [
    "# capped smote using triplet loss w/ average \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-3, 1e-3, 1e-2]\n",
    "\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "loss_caps = [1, 5, 10]\n",
    "cap_calc = loss_fns.TripletLossWithAverage\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(5): \n",
    "            print(loss_cap)\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(2)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                _, _ = train.train_triplet_capped_loss(epoch, train_loader_tripletloss_smote, network, optimizer, verbose=False, cap_calc=cap_calc, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"triplet_loss_capped_smote_average\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], loss_cap, norm]\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7278387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aa73e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006921598315238952, AUC: 0.5632135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012470570206642151, AUC: 0.7233750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013748981952667236, AUC: 0.7771020000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013903254866600037, AUC: 0.807651\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000689566433429718, AUC: 0.6339159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001366079866886139, AUC: 0.759145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013456925749778747, AUC: 0.780262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001352324604988098, AUC: 0.8117350000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006955260336399078, AUC: 0.364984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012048510313034057, AUC: 0.7615790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013354987502098084, AUC: 0.806033\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001402545928955078, AUC: 0.8174680000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006994863450527191, AUC: 0.3235535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001231974244117737, AUC: 0.744665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013687712550163268, AUC: 0.7914589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013790038824081422, AUC: 0.8012819999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006958338618278504, AUC: 0.4455605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012480974793434142, AUC: 0.7790835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001200647532939911, AUC: 0.812688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013523554801940917, AUC: 0.8305199999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006863028407096863, AUC: 0.7004435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001561657726764679, AUC: 0.572488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015748396515846252, AUC: 0.6567399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00134136039018631, AUC: 0.694971\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006980693936347961, AUC: 0.26639100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015346712470054626, AUC: 0.60413\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00154194712638855, AUC: 0.681176\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013370873928070068, AUC: 0.702694\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006933815777301789, AUC: 0.5073019999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015096882581710817, AUC: 0.5958939999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014330532550811768, AUC: 0.690552\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011835205554962158, AUC: 0.7153319999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006886812150478363, AUC: 0.6946855000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009307905733585358, AUC: 0.6967679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010540494918823242, AUC: 0.723033\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011437109112739563, AUC: 0.734716\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006976078748703003, AUC: 0.46697049999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014517637491226196, AUC: 0.6255609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013296366333961486, AUC: 0.686639\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012349424958229065, AUC: 0.704991\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000693293958902359, AUC: 0.4923235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012241790890693664, AUC: 0.827404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005047559738159, AUC: 0.8478140000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013191110491752624, AUC: 0.851334\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006935112476348877, AUC: 0.479819\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012350835800170898, AUC: 0.8207519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013129203319549561, AUC: 0.85213\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013719419836997985, AUC: 0.854548\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006914593577384949, AUC: 0.6132475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014812179207801819, AUC: 0.809393\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014218631982803344, AUC: 0.850916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018798137903213502, AUC: 0.839475\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006919685304164887, AUC: 0.594222\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014740896821022033, AUC: 0.812515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010685132145881653, AUC: 0.844366\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016855565905570984, AUC: 0.847835\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006946231424808503, AUC: 0.49076200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015229471325874328, AUC: 0.809684\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013850536346435547, AUC: 0.848045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018160623908042909, AUC: 0.840114\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.000694040447473526, AUC: 0.48431050000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011827890276908874, AUC: 0.744298\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013519347906112672, AUC: 0.789686\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014733164310455323, AUC: 0.806422\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.000693232923746109, AUC: 0.49885650000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001122897982597351, AUC: 0.777504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012883987426757812, AUC: 0.8092579999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013939679265022279, AUC: 0.817615\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006891491115093232, AUC: 0.695659\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011895716190338135, AUC: 0.7591819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013252856135368347, AUC: 0.798462\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013407520055770874, AUC: 0.819223\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006961319148540497, AUC: 0.39895549999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011996761560440063, AUC: 0.75079\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012966253757476806, AUC: 0.7967635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014839352369308471, AUC: 0.8087410000000002\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006925540268421174, AUC: 0.5795984999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001355758011341095, AUC: 0.710366\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013774314522743225, AUC: 0.751304\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013380625247955322, AUC: 0.7966709999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0007008523344993592, AUC: 0.475669\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013151673078536988, AUC: 0.6792710000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012554534673690795, AUC: 0.7220089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012443227767944336, AUC: 0.728746\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.000694794237613678, AUC: 0.45336050000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012269565463066102, AUC: 0.700483\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011091956496238709, AUC: 0.7385409999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011187997460365296, AUC: 0.747538\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006910423040390014, AUC: 0.674372\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012350799441337586, AUC: 0.647083\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011486090421676635, AUC: 0.7002680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011409287452697754, AUC: 0.7117600000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006949552297592163, AUC: 0.5725665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010592284202575683, AUC: 0.696091\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010815622806549072, AUC: 0.717181\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011523696184158325, AUC: 0.723992\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006918987333774567, AUC: 0.567306\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014218735694885255, AUC: 0.626509\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012867481708526611, AUC: 0.6912170000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012059167623519897, AUC: 0.713389\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006900597214698792, AUC: 0.6891025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013834730982780457, AUC: 0.8162429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001025064766407013, AUC: 0.8558520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014152864217758178, AUC: 0.850301\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006863984763622284, AUC: 0.7320325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015049086213111877, AUC: 0.7962099999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014538233280181885, AUC: 0.820638\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012426711916923522, AUC: 0.868339\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.000691572368144989, AUC: 0.5833459999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012380607724189758, AUC: 0.804972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010192102491855622, AUC: 0.5083005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015678569674491883, AUC: 0.850638\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006989287734031677, AUC: 0.37948899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013903325200080871, AUC: 0.8076300000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012960513830184935, AUC: 0.838335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017057555913925172, AUC: 0.8204400000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006881595253944397, AUC: 0.657149\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001455705225467682, AUC: 0.808458\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013453017473220826, AUC: 0.841771\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014921228289604188, AUC: 0.8636439999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006940659582614899, AUC: 0.46338100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012335270643234254, AUC: 0.757069\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012548251748085023, AUC: 0.8054749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014439223408699035, AUC: 0.8233950000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006964159309864044, AUC: 0.37077499999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001184098184108734, AUC: 0.7718909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013753706216812133, AUC: 0.8011400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011591293811798096, AUC: 0.826225\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0007007008492946624, AUC: 0.335313\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012136767506599425, AUC: 0.744562\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012937182188034059, AUC: 0.792217\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013339343070983888, AUC: 0.8134770000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006943095922470093, AUC: 0.4270065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009600128531455994, AUC: 0.73343\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012829274535179139, AUC: 0.7855930000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013778572082519531, AUC: 0.8141099999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0007034328579902649, AUC: 0.2648275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012563759088516236, AUC: 0.7556390000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013986634612083435, AUC: 0.785158\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013595693111419678, AUC: 0.808511\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006898211240768433, AUC: 0.66229\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011816049218177796, AUC: 0.6702199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010327804684638977, AUC: 0.723724\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010605663657188415, AUC: 0.724789\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006936001777648926, AUC: 0.5081075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012493101954460144, AUC: 0.656124\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011874300241470337, AUC: 0.7047410000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001150161385536194, AUC: 0.7237859999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006923040151596069, AUC: 0.527953\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011280679106712341, AUC: 0.680736\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00119078528881073, AUC: 0.7085760000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012809017300605774, AUC: 0.724596\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006926771104335785, AUC: 0.549755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010948650240898132, AUC: 0.6895015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001139619767665863, AUC: 0.718095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011930285692214966, AUC: 0.730982\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006932551860809326, AUC: 0.600077\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008879028558731079, AUC: 0.7255324999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010932807326316833, AUC: 0.741028\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012548867464065552, AUC: 0.749073\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006961268782615662, AUC: 0.41802700000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012942067980766297, AUC: 0.826802\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011923564076423644, AUC: 0.846108\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016668501496315002, AUC: 0.8389610000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006952577829360962, AUC: 0.42054100000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013249553442001344, AUC: 0.8137460000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000906592845916748, AUC: 0.883143\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012671335339546205, AUC: 0.852954\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006929630637168884, AUC: 0.5119039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014076567888259889, AUC: 0.7928660000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001311193585395813, AUC: 0.8343180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018300474882125854, AUC: 0.825908\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006893717646598816, AUC: 0.6550805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001314487874507904, AUC: 0.8143610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014213076829910278, AUC: 0.840531\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001453961431980133, AUC: 0.852763\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0007071491181850434, AUC: 0.30161\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013747764825820922, AUC: 0.807883\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013649266958236693, AUC: 0.8373499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013550484776496887, AUC: 0.8474889999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# capped smote using triplet loss w/ positive dist squared \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-3, 1e-3, 1e-2]\n",
    "\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "loss_caps = [1, 5, 10]\n",
    "cap_calc = loss_fns.TripletLossSquaredPositive\n",
    "\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(5): \n",
    "            print(loss_cap)\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(2)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                _, _ = train.train_triplet_capped_loss(epoch, train_loader_tripletloss_smote, network, optimizer, verbose=False, cap_calc=cap_calc, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"triplet_loss_capped_smote_pos_squared\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], loss_cap, norm]\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09944b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc1dd64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 classes\n",
    "\n",
    "NUM_CLASSES_REDUCED = 3\n",
    "nums = (0, 3, 1)\n",
    "ratio = (20, 2, 1)\n",
    "\n",
    "norm=True\n",
    "\n",
    "if norm:\n",
    "    transform=torchvision.transforms.Compose([torchvision.transforms.Normalize(mean=[-0.1390, -0.0688,  0.0125], std=[[1.0098, 0.9996, 0.9935]])])\n",
    "else:\n",
    "    transform=None\n",
    "\n",
    "\n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums, transform=transform)\n",
    "targets = ratio_train_CIFAR10.labels \n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED, transform=transform)\n",
    "\n",
    "\n",
    "weight = 1. / class_count\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= max(class_count)\n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "800c1e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004159178098042806, AUC: 0.5\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 16\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     18\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_softmax(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/ml/train.py:41\u001b[0m, in \u001b[0;36mtrain_softmax\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze(), target\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     43\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/ml/models.py:19\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2_drop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)), \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m250\u001b[39m) \u001b[38;5;66;03m#(320)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_jit_internal.py:484\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3 class normal\n",
    "\n",
    "learning_rates = [1e-4, 1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 3, nums, (1, 1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6be4998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6452da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0028148902257283527, AUC: 0.5499715000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012899534304936728, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011812520821889241, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003198623657227, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012160149812698365, AUC: 0.496695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012602541049321493, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011723772684733072, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012640552123387655, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001084180474281311, AUC: 0.47396475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012727203766504925, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010816088120142618, AUC: 0.5045000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011214701334635417, AUC: 0.5035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026806603272755943, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012301311095555623, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001151394208272298, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107947838306427, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020958302021026613, AUC: 0.44825000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012716478904088338, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011440247694651285, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012925329605738322, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026947441101074217, AUC: 0.47152499999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011879764000574749, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011960159142812093, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012670242389043172, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017743062178293865, AUC: 0.50675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001074000636736552, AUC: 0.5057499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012203034957249958, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011130955616633098, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014793120384216308, AUC: 0.44375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012164912223815918, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010818771521250406, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011993260780970255, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00679119078318278, AUC: 0.48125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012626315355300903, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001144296924273173, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001217811703681946, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007157065073649088, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001255599578221639, AUC: 0.501499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011938397884368897, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010313888788223266, AUC: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class ratio\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d28e8f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1752b5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.003038533369700114, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010433460474014282, AUC: 0.574998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011814462741216024, AUC: 0.7483075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001038165827592214, AUC: 0.7227574999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007914267698923746, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010645556449890136, AUC: 0.37466975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001031708836555481, AUC: 0.6914997500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011311002572377523, AUC: 0.75428325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006719241301218669, AUC: 0.5075000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010695095856984456, AUC: 0.5922592499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010078495343526204, AUC: 0.66097125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009656443595886231, AUC: 0.68574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002060540755589803, AUC: 0.5615957500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010401540199915568, AUC: 0.3627755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009678711692492167, AUC: 0.6725415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009192776083946228, AUC: 0.675171\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00151268204053243, AUC: 0.5225000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010918419361114502, AUC: 0.40822674999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010981494585673014, AUC: 0.4126655000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010762500762939453, AUC: 0.684442\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0108524964650472, AUC: 0.499499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013274700244267782, AUC: 0.6495525000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001373440424601237, AUC: 0.6880609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882926384607951, AUC: 0.5351134999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004438136577606201, AUC: 0.50352525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012416810989379883, AUC: 0.5918582499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013266102472941081, AUC: 0.6148197500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019509809811909993, AUC: 0.6372085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005216264883677164, AUC: 0.51125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010727325280507406, AUC: 0.499001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010864347219467162, AUC: 0.4997515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010905816157658894, AUC: 0.49875600000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003298620859781901, AUC: 0.51497625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011997053623199463, AUC: 0.64312625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001198989232381185, AUC: 0.648871\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001623408595720927, AUC: 0.6290437499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013772049347559611, AUC: 0.4500199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006249984105427, AUC: 0.47525675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973472197850546, AUC: 0.46956325000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010804060300191245, AUC: 0.5838855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class oversampled \n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "423b94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c14c3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0020289310614267984, AUC: 0.4756965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001091553012530009, AUC: 0.48456375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010886570612589517, AUC: 0.4860155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010853137572606406, AUC: 0.5097382500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019787512222925823, AUC: 0.57222475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011532610654830932, AUC: 0.49576575000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145702044169108, AUC: 0.4937625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001140582799911499, AUC: 0.49350675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005084774017333984, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010802105665206909, AUC: 0.50175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010755322376887005, AUC: 0.502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010662730137507121, AUC: 0.5022505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009249690055847169, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010924884875615438, AUC: 0.50599775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882760683695474, AUC: 0.5044992500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010864359935124715, AUC: 0.50449775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008460322062174478, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003564596176148, AUC: 0.5434515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010977611939112346, AUC: 0.527579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001091849128405253, AUC: 0.502861\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037917500336964926, AUC: 0.50300075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001107357660929362, AUC: 0.50625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011036542654037475, AUC: 0.50575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008251905441284, AUC: 0.503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00853164831797282, AUC: 0.48180575000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011391439437866211, AUC: 0.4952675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011303770144780478, AUC: 0.49850675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011254301071166992, AUC: 0.49128400000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017762763341267904, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011278401215871175, AUC: 0.46372199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021624008814494, AUC: 0.42442375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011078615188598632, AUC: 0.43288249999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007273403485616049, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011379459698994954, AUC: 0.47927200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011329193909962972, AUC: 0.4857515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011250438690185546, AUC: 0.53704125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00682664426167806, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001064486066500346, AUC: 0.4997495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010623377164204915, AUC: 0.49874950000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010606383085250855, AUC: 0.49924975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class undersampled  \n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaec9e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c779b879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.00608756939570109, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010771948893864949, AUC: 0.5007499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001078349749247233, AUC: 0.49949999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010784133275349936, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00372534704208374, AUC: 0.50023525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010370986064275106, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010473136504491171, AUC: 0.49649774999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010565840800603231, AUC: 0.5224875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020539817810058592, AUC: 0.50625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010878965854644776, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087069312731425, AUC: 0.49999999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010845698912938435, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006659661134084066, AUC: 0.49401975000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010928993225097657, AUC: 0.53216375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010782272815704345, AUC: 0.54923675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010686718225479126, AUC: 0.5544079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009116797844568889, AUC: 0.48513425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010646411975224813, AUC: 0.497999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010692731142044067, AUC: 0.502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010714724858601888, AUC: 0.50150125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015166940053304037, AUC: 0.48724999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011384790738423664, AUC: 0.49775050000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011099223693211873, AUC: 0.51426275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013285338083902994, AUC: 0.59251075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018370418151219686, AUC: 0.4615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010755572319030762, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010769031047821046, AUC: 0.5002502499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010774298906326295, AUC: 0.49999925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003305099407831828, AUC: 0.503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010899808804194133, AUC: 0.49999999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010876678625742595, AUC: 0.5000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010835784276326497, AUC: 0.5007499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010739267667134603, AUC: 0.502463\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001126469651858012, AUC: 0.47123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011237830718358358, AUC: 0.49575225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011101373434066772, AUC: 0.468339\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01887862459818522, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010957230726877849, AUC: 0.4975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010916781425476074, AUC: 0.50075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010887458721796672, AUC: 0.5009999999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class weighted\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['weight'] = torch.from_numpy(weight).float()\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8692b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81115d0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'any'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m network \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mConvNet(NUM_CLASSES_REDUCED)\n\u001b[1;32m     16\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(network\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, momentum\u001b[38;5;241m=\u001b[39mmomentum)\n\u001b[0;32m---> 17\u001b[0m _, auc \u001b[38;5;241m=\u001b[39m \u001b[43mmetric_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauc_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     18\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/metric_utils.py:31\u001b[0m, in \u001b[0;36mauc_softmax\u001b[0;34m(test_loader, network, average)\u001b[0m\n\u001b[1;32m     28\u001b[0m test_losses, y_preds, y_true \u001b[38;5;241m=\u001b[39m inference\u001b[38;5;241m.\u001b[39mrun_inference_softmax(test_loader, network)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average: \n\u001b[0;32m---> 31\u001b[0m     network_auc \u001b[38;5;241m=\u001b[39m \u001b[43mauc\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     network_auc \u001b[38;5;241m=\u001b[39m multiclass_auc(y_preds, y_true)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/metric_utils.py:63\u001b[0m, in \u001b[0;36mauc\u001b[0;34m(y_pred, y_true)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauc\u001b[39m(y_pred, y_true):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:551\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    549\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    550\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 551\u001b[0m y_score \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    554\u001b[0m     y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    555\u001b[0m ):\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;66;03m# do not support partial ROC computation for multiclass\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m max_fpr \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:110\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_object_dtype_isnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m():\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput contains NaN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# We need only consider float arrays, hence can early return for all else.\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'any'"
     ]
    }
   ],
   "source": [
    "#  3 class focal loss\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['reduction'] = 'mean'\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SoftmaxFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf8eaef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d48470b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (3) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m network \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mConvNet(NUM_CLASSES_REDUCED)\n\u001b[1;32m     15\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(network\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, momentum\u001b[38;5;241m=\u001b[39mmomentum)\n\u001b[0;32m---> 16\u001b[0m _, auc \u001b[38;5;241m=\u001b[39m \u001b[43mmetric_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauc_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     17\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/metric_utils.py:26\u001b[0m, in \u001b[0;36mauc_softmax\u001b[0;34m(test_loader, network, average, embeddings)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauc_softmax\u001b[39m(test_loader, network, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 26\u001b[0m     test_losses, y_preds, y_true \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_inference_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m average: \n\u001b[1;32m     29\u001b[0m         network_auc \u001b[38;5;241m=\u001b[39m auc(y_preds, y_true)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/inference.py:44\u001b[0m, in \u001b[0;36mrun_inference_softmax\u001b[0;34m(dataloader, network, embeddings)\u001b[0m\n\u001b[1;32m     41\u001b[0m loss_fn\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     45\u001b[0m         output \u001b[38;5;241m=\u001b[39m network(data)\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m embeddings: \n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/class_sampling.py:34\u001b[0m, in \u001b[0;36mReduce.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     32\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 34\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (image, label)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torchvision/transforms/functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torchvision/transforms/_functional_tensor.py:928\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m std\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    927\u001b[0m     std \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 928\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (3) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "#  3 class SMOTE\n",
    "\n",
    "learning_rates = [1e-4, 5e-4, 1e-5, 1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "717e9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06bfe9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004159178098042806, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010975551207860312, AUC: 0.6297499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006656141678492228, AUC: 0.7525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010653618971506755, AUC: 0.6115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001217594623565674, AUC: 0.5495857500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011367549101511637, AUC: 0.6367499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010483775536219279, AUC: 0.6514999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007630239725112915, AUC: 0.7260000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012967588504155477, AUC: 0.44993225000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010262293815612793, AUC: 0.6295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008588801821072897, AUC: 0.683\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006734636823336283, AUC: 0.78375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009516664822896322, AUC: 0.4979995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00063530965646108, AUC: 0.69475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000929928461710612, AUC: 0.6829999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007287528316179911, AUC: 0.74675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008401273687680562, AUC: 0.473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009506293336550395, AUC: 0.636959\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009607645273208618, AUC: 0.63535425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007702702085177104, AUC: 0.7275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018197454214096069, AUC: 0.642566\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005856151978174846, AUC: 0.75725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013211110432942708, AUC: 0.57475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010714397033055624, AUC: 0.5885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014161507288614908, AUC: 0.6221422499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008691601355870565, AUC: 0.6571120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008344510793685913, AUC: 0.6910065000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008789121111234029, AUC: 0.6564294999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00627077309290568, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007714670101801554, AUC: 0.68025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009927304983139039, AUC: 0.633\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001188183585802714, AUC: 0.5665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014923743406931558, AUC: 0.6397665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007305695414543152, AUC: 0.7214999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007611584663391114, AUC: 0.7315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008980198303858439, AUC: 0.662\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01015039348602295, AUC: 0.49\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001585029443105062, AUC: 0.552\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013031953970591228, AUC: 0.60825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012743113040924073, AUC: 0.5595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017152368227640788, AUC: 0.49150499999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006319928765296936, AUC: 0.75025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007423277099927266, AUC: 0.7175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008685394128163656, AUC: 0.6675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015195043087005615, AUC: 0.4752734999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014631412426630657, AUC: 0.51075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009681601921717325, AUC: 0.638\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011530553102493287, AUC: 0.62075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0068232992490132646, AUC: 0.5097499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007140132188796997, AUC: 0.7082499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008448924223581949, AUC: 0.6845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008831818501154582, AUC: 0.6757500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0162394323348999, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007203754186630249, AUC: 0.73725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006663846174875895, AUC: 0.7585000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006526186466217041, AUC: 0.759\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009562163988749187, AUC: 0.50025025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009007019797960918, AUC: 0.6865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008860059976577759, AUC: 0.6970000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008917976816495259, AUC: 0.7015000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008379616101582846, AUC: 0.4925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008464243412017822, AUC: 0.66525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007587121923764546, AUC: 0.70875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001005786379178365, AUC: 0.6279999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014540130297342937, AUC: 0.49967025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010335294206937154, AUC: 0.631\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007619184056917827, AUC: 0.7272500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008936668237050374, AUC: 0.6885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014362625281016032, AUC: 0.60427475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001139803965886434, AUC: 0.61625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008609330654144287, AUC: 0.6785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008365250627199809, AUC: 0.68925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004167644659678141, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009807932376861572, AUC: 0.644\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008850908875465394, AUC: 0.6915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015805523792902629, AUC: 0.5642499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003894431432088216, AUC: 0.508385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013399926026662191, AUC: 0.542\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014994232654571534, AUC: 0.52125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014919991890589397, AUC: 0.565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032936851978302003, AUC: 0.44232899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010130736231803894, AUC: 0.5740175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010271922747294108, AUC: 0.621\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008953987161318461, AUC: 0.6647500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005828616937001546, AUC: 0.49775025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014115378061930338, AUC: 0.504999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009735130469004313, AUC: 0.6126025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012178641160329182, AUC: 0.5822499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011171425501505534, AUC: 0.46174999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011258708238601684, AUC: 0.54575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008865743676821391, AUC: 0.6525000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007689760327339172, AUC: 0.6915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019177785317103068, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011179452737172444, AUC: 0.5174225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00121778937180837, AUC: 0.52726925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010796918869018554, AUC: 0.578416\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015408532460530599, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013170512517293295, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013639211654663086, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001255187431971232, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00226270850499471, AUC: 0.5105089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011977490981419881, AUC: 0.5110012499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009848896662394205, AUC: 0.56875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009220691124598186, AUC: 0.61909525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032962547143300376, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001387450893719991, AUC: 0.5030002499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011427743832270305, AUC: 0.54076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014284003973007201, AUC: 0.527504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010787606875101725, AUC: 0.4985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014681175549825032, AUC: 0.5115000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014118368625640869, AUC: 0.5537495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015168241659800211, AUC: 0.54677475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002233516534169515, AUC: 0.5347500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011471472581227622, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010156599084536234, AUC: 0.5039999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008470589717229207, AUC: 0.5882499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006666666030883789, AUC: 0.4964975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008849486112594605, AUC: 0.5985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00069130406777064, AUC: 0.7115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007107996344566345, AUC: 0.7132499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005378442923227946, AUC: 0.5002505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010483744144439697, AUC: 0.568998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010385095278422037, AUC: 0.5807010000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010713615814844767, AUC: 0.55364475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037749646504720053, AUC: 0.47023875000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011022014220555623, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001066104213396708, AUC: 0.5926400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010722426573435465, AUC: 0.553951\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005266088962554932, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010970422426859537, AUC: 0.626636\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011182560125986736, AUC: 0.506246\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011456472476323446, AUC: 0.504502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00298089075088501, AUC: 0.49472900000000003\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010305375258127848, AUC: 0.6913247499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010350863933563232, AUC: 0.6280855000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010418128967285157, AUC: 0.5992064999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03673971430460612, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010967044830322266, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010927757024765015, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00109928297996521, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009726393699645995, AUC: 0.48325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010913198391596477, AUC: 0.56702925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010592830578486125, AUC: 0.5896625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00109747048219045, AUC: 0.541984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013137797037760416, AUC: 0.49675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010657407840092977, AUC: 0.7087220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011155157089233398, AUC: 0.5314895000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011556921402613321, AUC: 0.5110129999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004617925008138021, AUC: 0.5070490000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011286824941635133, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110752534866333, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011012270053227743, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.016669163386027017, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010504453976949055, AUC: 0.6175824999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010885148048400879, AUC: 0.5237605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001111813465754191, AUC: 0.5292649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032121910254160565, AUC: 0.46281025000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010838578542073567, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001030877431233724, AUC: 0.7040495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010622098445892333, AUC: 0.569897\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005499816258748373, AUC: 0.4629150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010803848107655842, AUC: 0.6971802499999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001057974378267924, AUC: 0.66081525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010510002772013347, AUC: 0.6371439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027720227241516114, AUC: 0.37133825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100282629330953, AUC: 0.72390275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010707162221272786, AUC: 0.6865695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010874239206314087, AUC: 0.5755472500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00188139804204305, AUC: 0.503289\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086582581202189, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001089125116666158, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001090404470761617, AUC: 0.50075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00198538879553477, AUC: 0.5524595000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011342737674713134, AUC: 0.550976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010761692523956299, AUC: 0.6525359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010808565616607666, AUC: 0.60348875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027240955034891763, AUC: 0.51137975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00099675848086675, AUC: 0.690015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009240483045578003, AUC: 0.73414775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010368161598841349, AUC: 0.5858220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038253275553385415, AUC: 0.510201\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010566739638646443, AUC: 0.6456307499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001051866332689921, AUC: 0.5896450000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001061085343360901, AUC: 0.555655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001382793347040812, AUC: 0.48925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010509036382039389, AUC: 0.6267175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001039019505182902, AUC: 0.61541725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001051597515741984, AUC: 0.5731567500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014516881942749024, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001151285171508789, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001129536787668864, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001054709235827128, AUC: 0.704878\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003324142932891846, AUC: 0.505257\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010856000979741415, AUC: 0.6503510000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001044483701388041, AUC: 0.7219310000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001047504464785258, AUC: 0.6032815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006436972141265869, AUC: 0.53495825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010288987159729004, AUC: 0.62854875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010188900629679361, AUC: 0.6056704999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009908483823140462, AUC: 0.655329\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009068726221720377, AUC: 0.499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001123564918835958, AUC: 0.6403722500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011447450717290242, AUC: 0.6234495000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011120458443959554, AUC: 0.6947255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005306102752685547, AUC: 0.5885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001127474069595337, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011242369016011555, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001121023654937744, AUC: 0.5002494999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006944515069325765, AUC: 0.5267499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010755825042724609, AUC: 0.6103427499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010677810907363892, AUC: 0.621092\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010732324123382568, AUC: 0.5845257500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031805087725321453, AUC: 0.52475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011366676489512125, AUC: 0.49925025000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011330976088841755, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011296690702438355, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003709184805552165, AUC: 0.52422275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001053947687149048, AUC: 0.7144419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010236427386601766, AUC: 0.7290747500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010113393068313598, AUC: 0.69986225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015917202234268187, AUC: 0.5048739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001047765890757243, AUC: 0.48376874999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010516340335210163, AUC: 0.5204444999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010441278616587322, AUC: 0.5773935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016138941844304404, AUC: 0.6107965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011180264155069986, AUC: 0.509995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011150464614232381, AUC: 0.5414305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010947375694910686, AUC: 0.64092575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004556914965311686, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010929491917292277, AUC: 0.6354922500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001080975850423177, AUC: 0.69916575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010818928082784018, AUC: 0.6413610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004589003086090088, AUC: 0.505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011493717829386394, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011443336804707845, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011083608468373616, AUC: 0.72585525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001200760324796041, AUC: 0.57075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011402225494384765, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011360338926315308, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011322333812713622, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032604951858520507, AUC: 0.5568744999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010879574219385783, AUC: 0.5644085000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001092010219891866, AUC: 0.545238\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110060183207194, AUC: 0.54675725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005285548369089762, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010683266719182332, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001025087316830953, AUC: 0.624031\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010506339073181152, AUC: 0.56024825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027359028657277427, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010591832002003988, AUC: 0.6131832500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010624086459477741, AUC: 0.57442325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010561026334762573, AUC: 0.580099\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009625364621480306, AUC: 0.4950125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010050710439682008, AUC: 0.607521\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011637854178746542, AUC: 0.56324725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011045797268549602, AUC: 0.5608685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006338486671447754, AUC: 0.4540310000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010315340360005697, AUC: 0.6843125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010497285922368368, AUC: 0.588686\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010563398202260335, AUC: 0.570149\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008947574933369954, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011193767786026002, AUC: 0.61758\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010644810597101846, AUC: 0.6154572500000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011546129782994588, AUC: 0.50325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016673778295516967, AUC: 0.5095\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011017917394638061, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011000678539276124, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989853541056316, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012646420160929362, AUC: 0.47450000000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010960377852121989, AUC: 0.6284795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010728757381439208, AUC: 0.5803695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011331966718037922, AUC: 0.5152655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023012900352478026, AUC: 0.50575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001067526618639628, AUC: 0.537042\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001066692630449931, AUC: 0.539551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011165502866109212, AUC: 0.51325075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005958204746246338, AUC: 0.5135000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010548818508783976, AUC: 0.610457\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001052526871363322, AUC: 0.5903295000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001153125564257304, AUC: 0.5192565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004796805699666341, AUC: 0.52476475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010926293532053629, AUC: 0.621903\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010789188146591187, AUC: 0.60643675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010915964047114055, AUC: 0.563888\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006229605992635091, AUC: 0.39975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001141523281733195, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103006641070048, AUC: 0.6385029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010894367297490438, AUC: 0.5960624999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028797560532887777, AUC: 0.41903\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011121308008829752, AUC: 0.50025025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011110671758651734, AUC: 0.49974999999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011104142268498738, AUC: 0.49974999999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008994462966918946, AUC: 0.5027535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001118641455968221, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011117239793141684, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106630841890971, AUC: 0.50199975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015098811785380045, AUC: 0.521009\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010285453001658122, AUC: 0.635089\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010578515926996867, AUC: 0.5190015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009844126303990683, AUC: 0.6817725000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023552542527516683, AUC: 0.4528245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010949519077936808, AUC: 0.51\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010855381886164348, AUC: 0.5517565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010566818714141846, AUC: 0.6277745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004073406934738159, AUC: 0.41662875000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010068363746007284, AUC: 0.6882299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010097815195719401, AUC: 0.6423420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001037907918294271, AUC: 0.5447315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008052815914154052, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001118549664815267, AUC: 0.5034987500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011064444780349732, AUC: 0.5197394999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010736923615137736, AUC: 0.5769244999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004365557511647542, AUC: 0.50075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001067453106244405, AUC: 0.6697072499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010618601640065512, AUC: 0.61062125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010790412028630575, AUC: 0.5495295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013300448656082154, AUC: 0.5458562499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010548566579818726, AUC: 0.7298565000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010721299648284912, AUC: 0.589707\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010924956401189168, AUC: 0.5403374999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006615845203399659, AUC: 0.50030125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010822104612986246, AUC: 0.70681775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00102922519048055, AUC: 0.6558124999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010151251951853433, AUC: 0.6902862500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009616997241973877, AUC: 0.49699999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010699758132298788, AUC: 0.50075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010708796580632528, AUC: 0.5025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001072011391321818, AUC: 0.54075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016420459349950154, AUC: 0.528\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009985286593437194, AUC: 0.6360699999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010161267121632893, AUC: 0.6433625000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009802197615305583, AUC: 0.678502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036003258228302004, AUC: 0.4615935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001155247728029887, AUC: 0.5726082499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011424423853556315, AUC: 0.584388\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011352591911951702, AUC: 0.57893275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017295799652735392, AUC: 0.46725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010735944112141927, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010751252174377442, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010766595602035522, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007779232660929362, AUC: 0.59075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010640310843785603, AUC: 0.6801740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010556456247965494, AUC: 0.6863115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010503278573354086, AUC: 0.66131\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002807686646779378, AUC: 0.48249699999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001119636058807373, AUC: 0.457952\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098007837931315, AUC: 0.42814399999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011078639030456543, AUC: 0.46986425000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016860313018163045, AUC: 0.50275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001111672600110372, AUC: 0.68321325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010923396746317547, AUC: 0.71771425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011171891291936239, AUC: 0.5910165000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003883177200953166, AUC: 0.504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010796475807825724, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010804632902145386, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010811487038930258, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.038931207021077475, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011111186345418294, AUC: 0.49849975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011080533663431803, AUC: 0.489\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010978318452835084, AUC: 0.6390264999999999\n",
      "\n",
      "[['capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.001, 0.5364242250000001, 0.004731031713818123, 0.6595821, 0.002944479432089998, 0.664386075, 0.0026849814496256257, 0.66284295, 0.005799479982322499, 1, False], ['capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0005, 0.508160875, 0.0011112581592281252, 0.64915, 0.005520389999999997, 0.682275, 0.003798518125000002, 0.65595, 0.003408897500000005, 1, False], ['capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0001, 0.49420857499999993, 0.0005791681168006258, 0.52664405, 0.0010765723811100008, 0.579238125, 0.0042558641816906285, 0.601229, 0.004464774047212501, 1, False], ['capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.001, 0.4915077500000001, 0.00019162859982499997, 0.57805425, 0.005836140147775001, 0.5656134500000001, 0.003972044294522502, 0.536346325, 0.0010066087098756238, 5, False], ['capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0005, 0.49412977500000005, 0.0021946263786931258, 0.6213072000000001, 0.005695828450959999, 0.626673225, 0.006007817972368127, 0.599505225, 0.0028797415718931235, 5, False], ['capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0001, 0.535489325, 0.0014483440016256253, 0.559466325, 0.006122588975825624, 0.5734907, 0.007017111012047507, 0.60648985, 0.006845006958727501, 5, False], ['capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.001, 0.5010168, 0.0006318581545099986, 0.5863233750000001, 0.0031878474769406243, 0.572133275, 0.001231564663093127, 0.536914475, 0.0008021006862681254, 10, False], ['capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0005, 0.47836167500000004, 0.0024265998788631235, 0.585828475, 0.007662156869318123, 0.56783575, 0.0028470377398250022, 0.568277025, 0.002806235619730628, 10, False], ['capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0001, 0.503414175, 0.0011789626593506253, 0.5736085, 0.008017908466624998, 0.570723275, 0.008933586426055625, 0.584968825, 0.005830720074938126, 10, False]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for i in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[i][0]\n",
    "    auc_variance = cap_aucs[i][1]\n",
    "    cap = caps[i]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "838b3e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c453da4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0078015821774800615, AUC: 0.49625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001081034223238627, AUC: 0.51192\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010818347930908203, AUC: 0.5092574999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010877530177434286, AUC: 0.59471175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006712500254313151, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011000588337580362, AUC: 0.5037455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011020750999450684, AUC: 0.5004995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001104021390279134, AUC: 0.4999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001641412854194641, AUC: 0.5240275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011246995925903321, AUC: 0.564403\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011237755616505941, AUC: 0.5504524999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011289521853129069, AUC: 0.58037975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004778544743855795, AUC: 0.5345000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011649170319239298, AUC: 0.5652305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001163023034731547, AUC: 0.59888525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011398308277130126, AUC: 0.598623\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010933431307474772, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010769203901290893, AUC: 0.50125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010792320171991985, AUC: 0.50075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010814727942148844, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003806600570678711, AUC: 0.49075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010765077273050944, AUC: 0.5140075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010792068640391032, AUC: 0.50525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010816334088643391, AUC: 0.50375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006420979499816895, AUC: 0.42650000000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011930530865987142, AUC: 0.62415675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001139781673749288, AUC: 0.64630475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001186063806215922, AUC: 0.6306612500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022019866307576496, AUC: 0.49425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012133020957310994, AUC: 0.601514\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011549022595087686, AUC: 0.6202062500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001157347838083903, AUC: 0.5710902499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00599648110071818, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010966438055038453, AUC: 0.505246\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010983812808990478, AUC: 0.50224975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011000104745229086, AUC: 0.5014989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027281310558319094, AUC: 0.5057385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012588430643081666, AUC: 0.61279775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001200169563293457, AUC: 0.63602175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012310056686401368, AUC: 0.642783\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024534427642822265, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011087461709976196, AUC: 0.5132500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011549708445866903, AUC: 0.5897359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011030149062474569, AUC: 0.63771175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006747143268585205, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012413901885350545, AUC: 0.6310115000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010895591974258423, AUC: 0.7675489999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001059328516324361, AUC: 0.6148899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004022144794464112, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011222544511159261, AUC: 0.50075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011424777507781983, AUC: 0.50175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011638484001159667, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0069988695780436195, AUC: 0.45899999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011403009096781413, AUC: 0.54425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011573975880940755, AUC: 0.5309999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011634063323338827, AUC: 0.5009999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003451640764872233, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001176724672317505, AUC: 0.5480330000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011134016116460164, AUC: 0.56101175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010796163082122802, AUC: 0.5646035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007572885513305664, AUC: 0.5225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011175174315770466, AUC: 0.53075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001155137300491333, AUC: 0.57525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011368136803309122, AUC: 0.50275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029685304164886474, AUC: 0.54019375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010921700795491537, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011123814582824707, AUC: 0.5132500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00112034805615743, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003021557013193766, AUC: 0.52125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011162529786427815, AUC: 0.50575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011929986476898193, AUC: 0.5287470000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001151958187421163, AUC: 0.5255000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010383518536885579, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095157504081726, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097788691520691, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992683172225952, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00436355193456014, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00113641623655955, AUC: 0.5544675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001158361832300822, AUC: 0.7154652499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011658074061075847, AUC: 0.612\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004303484916687012, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096583604812622, AUC: 0.6752600000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010884890556335449, AUC: 0.6556835000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087139089902242, AUC: 0.7441457499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006738119125366211, AUC: 0.486265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010819751818974812, AUC: 0.5024989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010847816864649454, AUC: 0.50075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010891348123550416, AUC: 0.50175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017149168252944946, AUC: 0.4775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010800637404123942, AUC: 0.49900100000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010822492837905884, AUC: 0.49950049999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010842357873916625, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024736140569051107, AUC: 0.51300475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001093860705693563, AUC: 0.5045112500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010971054633458455, AUC: 0.50274775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011019357442855835, AUC: 0.5084945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00922234280904134, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011670323610305786, AUC: 0.6325972499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001142394224802653, AUC: 0.6009099999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010924520095189412, AUC: 0.6410849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005277036031087239, AUC: 0.48299200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010917553504308066, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001094180981318156, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010955576499303182, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010092356363932293, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010606398979822794, AUC: 0.49399325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001071586767832438, AUC: 0.5538249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010738330284754435, AUC: 0.5179655000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002840666135152181, AUC: 0.43653299999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010719456672668457, AUC: 0.5152155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010763492981592814, AUC: 0.53190275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010793944597244263, AUC: 0.517945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01110190518697103, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011076199213663736, AUC: 0.4917575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011097734769185384, AUC: 0.50973375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011120569705963135, AUC: 0.50674675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008843225836753845, AUC: 0.49771\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011571987867355347, AUC: 0.502976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011852587461471558, AUC: 0.4992485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012513719399770101, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030762850443522135, AUC: 0.51997275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010922926664352418, AUC: 0.4999997499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010931973457336426, AUC: 0.49999999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010939019521077474, AUC: 0.5007492499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007049403190612793, AUC: 0.512497\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011527343193689981, AUC: 0.50175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011512453158696492, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011496795415878295, AUC: 0.49974999999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0075025513966878255, AUC: 0.51199725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012419941822687786, AUC: 0.5893840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011943827867507935, AUC: 0.6711465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012028979460398355, AUC: 0.64601225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024347769419352213, AUC: 0.54825\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001166247844696045, AUC: 0.48674625000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001174075166384379, AUC: 0.48724100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011709243059158324, AUC: 0.490257\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009658121744791666, AUC: 0.5005000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011268941561381023, AUC: 0.47115225000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011330329577128092, AUC: 0.4739225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012209938367207845, AUC: 0.579674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010735984245936075, AUC: 0.52853025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011176213026046753, AUC: 0.53512575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011161125103632608, AUC: 0.5316354999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001127867062886556, AUC: 0.549112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003836607058842977, AUC: 0.50138575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010821324189503989, AUC: 0.5089839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010832757155100504, AUC: 0.51515325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098487655321757, AUC: 0.550893\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009752304951349894, AUC: 0.5526425000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001160635789235433, AUC: 0.56717325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001175346334775289, AUC: 0.566401\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011575175921122232, AUC: 0.60270725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00512715212504069, AUC: 0.43700000000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011687794526418051, AUC: 0.49273550000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011635773181915283, AUC: 0.49624700000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011810110807418824, AUC: 0.49650525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001971488396326701, AUC: 0.42839425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010664573907852174, AUC: 0.49999999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010682994922002157, AUC: 0.49974999999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001069926381111145, AUC: 0.49974999999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.025793635686238606, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011122318108876545, AUC: 0.50075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011212788422902426, AUC: 0.50425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011345349152882894, AUC: 0.5179999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010563562711079916, AUC: 0.49488299999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011565153201421102, AUC: 0.5515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011405647993087769, AUC: 0.509\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011598007678985595, AUC: 0.54825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008897570610046387, AUC: 0.47875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011027261813481648, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011000010569890341, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010964361429214478, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003762861251831055, AUC: 0.46554175000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011314114332199097, AUC: 0.5085000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011253054539362589, AUC: 0.5049999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011466267108917237, AUC: 0.5432499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025937561988830567, AUC: 0.542\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011555614868799845, AUC: 0.6686532500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011650006373723348, AUC: 0.579988\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145788590113322, AUC: 0.58499425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01994966952006022, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010985822280248007, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011128174463907877, AUC: 0.5335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012478423913319906, AUC: 0.638\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007603826920191447, AUC: 0.54838525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010899901390075684, AUC: 0.59568875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010835384130477906, AUC: 0.50849875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010858776569366456, AUC: 0.51025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015720778147379558, AUC: 0.44775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011271460056304931, AUC: 0.53340675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001136118253072103, AUC: 0.527\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011150607268015545, AUC: 0.5015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00766020679473877, AUC: 0.4985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010689200560251872, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010711604356765747, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010739969412485758, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009535351753234863, AUC: 0.53425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011380306879679363, AUC: 0.5984440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011020881334940593, AUC: 0.5015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011394408941268921, AUC: 0.5647500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027770806948343913, AUC: 0.44919875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011186947425206502, AUC: 0.47948275000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001135025382041931, AUC: 0.492\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011479076147079467, AUC: 0.499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018082295258839926, AUC: 0.5052180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096133828163147, AUC: 0.49999999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010956933895746868, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010955317815144856, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001219605525334676, AUC: 0.48751525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001073895812034607, AUC: 0.5032285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010749948422114053, AUC: 0.4915095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001078894813855489, AUC: 0.5160784999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006283337275187174, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011731402079264324, AUC: 0.6079545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012628846565882364, AUC: 0.5479715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012555795907974244, AUC: 0.5549885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0048173964818318685, AUC: 0.49999999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011116517384847004, AUC: 0.55727075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011161040862401326, AUC: 0.5635625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011345978180567423, AUC: 0.6134255000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007991501649220785, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012359501520792643, AUC: 0.5276989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012230838537216186, AUC: 0.5107440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001169662078221639, AUC: 0.511242\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006120059967041016, AUC: 0.5055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011344719330469767, AUC: 0.495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011358980735143025, AUC: 0.509\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001136518398920695, AUC: 0.5065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020380603869756065, AUC: 0.569768\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012267388502756754, AUC: 0.580069\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011771796941757201, AUC: 0.6325850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001154836614926656, AUC: 0.5998495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005287721633911133, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010965311527252198, AUC: 0.49974100000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010970792770385743, AUC: 0.5132617500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010960846741994222, AUC: 0.5882255000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005409732182820638, AUC: 0.502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010802576144536336, AUC: 0.50552\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010958816607793173, AUC: 0.67629425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105203111966451, AUC: 0.7234175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005787942886352539, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001107224702835083, AUC: 0.5905405000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011384952863057454, AUC: 0.665423\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011142903963724771, AUC: 0.6340269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012107685804367066, AUC: 0.56475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011108010212580364, AUC: 0.5009997499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011107036670049031, AUC: 0.50075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011106461683909098, AUC: 0.5015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009412595431009928, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001071484406789144, AUC: 0.504998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010697315136591592, AUC: 0.5015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010740696986516318, AUC: 0.5035062499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035412507057189942, AUC: 0.50225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088220477104187, AUC: 0.50075075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010879761775334676, AUC: 0.49975074999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088096022605896, AUC: 0.49925075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007933681805928548, AUC: 0.473761\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011397211154301962, AUC: 0.47625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001187015692392985, AUC: 0.6565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012106592655181884, AUC: 0.67225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015093008677164713, AUC: 0.496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001132037878036499, AUC: 0.493009\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011322906812032065, AUC: 0.4947555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011422838767369589, AUC: 0.49576200000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001788552443186442, AUC: 0.516176\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011330342292785644, AUC: 0.5235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011348271369934083, AUC: 0.5427500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001139047622680664, AUC: 0.5818490000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002080832600593567, AUC: 0.48411425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011403302748998006, AUC: 0.49874950000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011391813755035401, AUC: 0.49874975000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001156262993812561, AUC: 0.495748\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001705811619758606, AUC: 0.5480827500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011754041910171508, AUC: 0.532629\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012903087536493938, AUC: 0.53393825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00134560227394104, AUC: 0.52968825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005548909982045491, AUC: 0.4260195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011066407362620035, AUC: 0.6257185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001109532912572225, AUC: 0.6433625000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011481906175613403, AUC: 0.7056715000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020296188195546468, AUC: 0.49605999999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001171825091044108, AUC: 0.6084999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011494208971659342, AUC: 0.562747\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011372947295506795, AUC: 0.57775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001779158353805542, AUC: 0.51225675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011607282559076945, AUC: 0.6646394999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011160573561986287, AUC: 0.5167475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011168999274571738, AUC: 0.5087492499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009055037498474121, AUC: 0.60698225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011230660279591877, AUC: 0.5509835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011054041385650634, AUC: 0.5140175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011122498512268065, AUC: 0.553062\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0051953005790710445, AUC: 0.5005000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011084355513254801, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011035503546396892, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011158474286397297, AUC: 0.5672499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014019058863321941, AUC: 0.51225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010755784511566162, AUC: 0.5570139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005804936091105, AUC: 0.62191725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010793394247690837, AUC: 0.511\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00379930321375529, AUC: 0.5192570000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001091899275779724, AUC: 0.5154955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010889174938201905, AUC: 0.5165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010828911463419597, AUC: 0.5062500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017278546492258708, AUC: 0.5351895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011475948890050252, AUC: 0.55899575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011994705994923908, AUC: 0.5072587500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011360195477803548, AUC: 0.523249\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015554540554682413, AUC: 0.37945825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011042903264363608, AUC: 0.5087499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006051301956177, AUC: 0.5039999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011053547461827597, AUC: 0.5287499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002834061066309611, AUC: 0.5011595000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010939277410507203, AUC: 0.51775375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087912917137146, AUC: 0.5105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001091808795928955, AUC: 0.54475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019129021962483723, AUC: 0.50425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011593480904897055, AUC: 0.6292300000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011515321334203085, AUC: 0.55675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011220805644989013, AUC: 0.5295000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002323791265487671, AUC: 0.5582779999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011307435830434163, AUC: 0.6188155000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001138014833132426, AUC: 0.662249\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001182223598162333, AUC: 0.713817\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0063361152013142906, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011406030257542928, AUC: 0.5646825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001174927512804667, AUC: 0.666698\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011349745194117228, AUC: 0.53325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003330778519312541, AUC: 0.50250425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001335960586865743, AUC: 0.5897697499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011870774825414022, AUC: 0.70748825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012303256591161091, AUC: 0.6282747499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004146814028422038, AUC: 0.52646775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001082275072733561, AUC: 0.5402772499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011094582080841064, AUC: 0.6262604999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011240882873535156, AUC: 0.660328\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026322948932647706, AUC: 0.4057195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011167777776718139, AUC: 0.5902085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011308948198954264, AUC: 0.6097545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011337016423543294, AUC: 0.604477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001967432498931885, AUC: 0.5183599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011230528354644776, AUC: 0.5503579999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001136027971903483, AUC: 0.6980249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011852296988169352, AUC: 0.68345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003892947753270467, AUC: 0.51\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012223012844721475, AUC: 0.6144265000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011769430239995321, AUC: 0.60379075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011510656277338664, AUC: 0.689886\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007001494566599528, AUC: 0.48397525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011377573808034262, AUC: 0.5876857500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011536659002304077, AUC: 0.6323544999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011449573040008545, AUC: 0.5859874999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.018307286580403644, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010808561642964682, AUC: 0.5693767499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002917289733886, AUC: 0.662592\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011442521413167319, AUC: 0.673304\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015141593217849732, AUC: 0.5246195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001081487735112508, AUC: 0.50901025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010932341416676839, AUC: 0.5734299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001104562520980835, AUC: 0.6595310000000001\n",
      "\n",
      "[['distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 5e-05, 0.49722659999999996, 0.0007258583662899996, 0.5504271, 0.0021798571809275016, 0.556987725, 0.003432341548330627, 0.5623747499999999, 0.0028781164538375014, 1, False], ['distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0005, 0.5042443750000001, 0.0004074952847656253, 0.5328512000000001, 0.0014758752363100008, 0.5783759, 0.007598348519352494, 0.545995525, 0.0028386029700056253, 1, False], ['distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0001, 0.4894004749999999, 0.00040631006983062566, 0.531831075, 0.0038538157197631254, 0.535455175, 0.002586813728363126, 0.5438132499999999, 0.006108176059349995, 1, False], ['distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 5e-05, 0.5041169750000001, 0.0015541349135056262, 0.5153050749999999, 0.0012537512362506258, 0.524124675, 0.0029967681766756237, 0.541541, 0.0026069667513750005, 5, False], ['distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0005, 0.5010060000000001, 0.0009665504239625008, 0.545744275, 0.003020728824093128, 0.516873675, 0.0005600662315506247, 0.540899425, 0.001836289287975627, 5, False], ['distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0001, 0.5019199999999999, 0.0007628823332125009, 0.52559655, 0.0016047476759599988, 0.54371785, 0.003639585557965, 0.5612977, 0.004641225296260005, 5, False], ['distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 5e-05, 0.50111535, 0.0013188755321650002, 0.5247144999999999, 0.0020152603381875015, 0.5537479750000001, 0.0046532687013181254, 0.561925275, 0.005921846460468128, 10, False], ['distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0005, 0.506736325, 0.002746633848313124, 0.5611111999999999, 0.0028195363351474993, 0.5310938, 0.0013281728886225, 0.535031025, 0.0005587976595056244, 10, False], ['distance_capped_smote', 3, (0, 3, 1), (20, 2, 1), 0.0001, 0.5029174249999999, 0.001420715182888124, 0.5734610750000001, 0.0010366552664756276, 0.64426425, 0.0016362618387750015, 0.643230525, 0.0027337223701056276, 10, False]]\n"
     ]
    }
   ],
   "source": [
    "# 3 class euclidean distance capped smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-5, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['distance'] = 'euclidean'\n",
    "\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for i in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[i][0]\n",
    "    auc_variance = cap_aucs[i][1]\n",
    "    cap = caps[i]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"distance_capped_smote\", NUM_CLASSES_REDUCED, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0967e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ae6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
