{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c3c2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378c3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91dda12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "NUM_CLASSES_REDUCED = 2\n",
    "nums = (0, 1)\n",
    "ratio = (100, 1)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b57e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "             \"mean_40\", \"variance_40\",\n",
    "             \"mean_50\", \"variance_50\",\n",
    "             \"cap\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c2a1ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor() ]))  \n",
    "\n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()]))\n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "    \n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums)\n",
    "\n",
    "triplet_train_CIFAR10 = class_sampling.ForTripletLoss(reduced_train_CIFAR10, smote=False)\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED)\n",
    "triplet_train_CIFAR10_smote = class_sampling.ForTripletLoss(smote_train_CIFAR10, smote=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13159c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000   50]\n"
     ]
    }
   ],
   "source": [
    "targets = ratio_train_CIFAR10.labels \n",
    "\n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "print(class_count)\n",
    "\n",
    "weight = 1. / class_count\n",
    "\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= class_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de0d4cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.999 \n",
    "\n",
    "exp = np.empty_like(targets)\n",
    "for i, count in enumerate(class_count):\n",
    "    exp[targets==i] = count\n",
    "effective_weights = (1 - beta) / ( 1 - (beta ** torch.from_numpy(exp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af8cd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss = DataLoader(triplet_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_smote = DataLoader(triplet_train_CIFAR10_smote, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e2e8c09f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001968830943107605, AUC: 0.6361665000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005542738735675812, AUC: 0.885664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005292963385581971, AUC: 0.8979130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004977731555700302, AUC: 0.9077430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032240989208221437, AUC: 0.38664699999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006063661277294159, AUC: 0.855749\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005620519518852234, AUC: 0.8903450000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005382097363471985, AUC: 0.898165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006873347997665405, AUC: 0.683021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005775293409824372, AUC: 0.867693\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004782587885856628, AUC: 0.9038889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00045187199115753177, AUC: 0.9145770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001451125681400299, AUC: 0.47390000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006520158350467682, AUC: 0.7826169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006005813181400299, AUC: 0.869262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005808910429477692, AUC: 0.8839089999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031172895431518556, AUC: 0.6896909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005760127604007721, AUC: 0.8859184999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005759528279304505, AUC: 0.890328\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005142679214477539, AUC: 0.9147670000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00219118869304657, AUC: 0.37103100000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005964301228523254, AUC: 0.8728420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005374825298786163, AUC: 0.902899\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000544882744550705, AUC: 0.900375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006955941677093506, AUC: 0.3322345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005800726115703583, AUC: 0.8929210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005413680970668792, AUC: 0.9098769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000514941155910492, AUC: 0.9136345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028511343002319335, AUC: 0.5939905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000573833703994751, AUC: 0.8740805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005463034212589264, AUC: 0.891224\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005187967419624329, AUC: 0.9042855000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011438738107681274, AUC: 0.351393\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005868805348873138, AUC: 0.8747965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005481753051280975, AUC: 0.9026535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005193610191345215, AUC: 0.9129545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015125332474708558, AUC: 0.429251\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005813519060611725, AUC: 0.8786199999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005619006156921387, AUC: 0.8882220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005398068428039551, AUC: 0.889149\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034982739686965944, AUC: 0.618384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941048204898835, AUC: 0.5019874999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936376094818116, AUC: 0.504489\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934143602848053, AUC: 0.501498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011465153098106384, AUC: 0.401948\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005416077971458435, AUC: 0.90378\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005318052768707275, AUC: 0.8990750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00048347583413124083, AUC: 0.9135070000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009435354471206665, AUC: 0.577937\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006637665331363678, AUC: 0.7110139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935104429721832, AUC: 0.498063\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933853924274444, AUC: 0.499049\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029533997774124146, AUC: 0.545541\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932328343391418, AUC: 0.499006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932272613048554, AUC: 0.499495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932301223278045, AUC: 0.499504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038374502658843996, AUC: 0.565988\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005938313007354736, AUC: 0.8660625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005148913562297821, AUC: 0.911864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00049067023396492, AUC: 0.921967\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001448841392993927, AUC: 0.5810709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005838621556758881, AUC: 0.9087160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005997822880744934, AUC: 0.8527130000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004748252034187317, AUC: 0.9237920000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004549663305282592, AUC: 0.44000300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005573736131191254, AUC: 0.8895370000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005237226784229279, AUC: 0.90449\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005322802066802979, AUC: 0.9018970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028102716207504272, AUC: 0.5378565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005881844162940979, AUC: 0.875293\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005218585729599, AUC: 0.910688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004902279824018478, AUC: 0.9198445000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018956378698349, AUC: 0.37419199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005440128147602082, AUC: 0.8936314999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004857201725244522, AUC: 0.917146\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046266598999500277, AUC: 0.9193445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001775826096534729, AUC: 0.554507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006459864675998687, AUC: 0.7649400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005977490544319153, AUC: 0.846576\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005115328282117844, AUC: 0.9130775000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS normal\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-4, 1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f8f1d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7d211694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0019287337064743042, AUC: 0.518386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014777783751487733, AUC: 0.746022\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00104189484610711, AUC: 0.8004319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001442479908466339, AUC: 0.8411460000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008427958854354254, AUC: 0.877404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011749097108840942, AUC: 0.868549\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009405695706015767, AUC: 0.9168040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011988715529441833, AUC: 0.556991\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021464204788208007, AUC: 0.8249695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007761876094201118, AUC: 0.7954079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015006399750709535, AUC: 0.856152\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007861524836925587, AUC: 0.8672519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001119160532951355, AUC: 0.887005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009549586371620102, AUC: 0.912044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001965500712394714, AUC: 0.48217\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020545467138290406, AUC: 0.808807\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007406754061432168, AUC: 0.828048\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016655817031860352, AUC: 0.821081\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007073351064014553, AUC: 0.8833319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013160868883132935, AUC: 0.814012\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009515452547238605, AUC: 0.903524\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013815407156944274, AUC: 0.517711\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015325177311897279, AUC: 0.7847900000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008979990720601365, AUC: 0.8432\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015435771346092224, AUC: 0.844348\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006992896222094498, AUC: 0.925264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015264580249786377, AUC: 0.87893\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000566807387230715, AUC: 0.9620039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000916945070028305, AUC: 0.410497\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013779096007347108, AUC: 0.7751300000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011010685474565714, AUC: 0.7823720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014025141596794128, AUC: 0.7968609999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010231879290820349, AUC: 0.803012\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016610566973686218, AUC: 0.809115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007667976958990687, AUC: 0.85858\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020729939341545107, AUC: 0.39433950000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013543757796287536, AUC: 0.7996530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001047509663028292, AUC: 0.828628\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014721465110778809, AUC: 0.8186379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008545397222042084, AUC: 0.870828\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020374441146850586, AUC: 0.79545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006473699848324355, AUC: 0.9030360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004707336902618408, AUC: 0.6373630000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012800355553627015, AUC: 0.759399\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012825752189843961, AUC: 0.796596\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014130505919456482, AUC: 0.805319\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009584719945888708, AUC: 0.8653639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012986513376235962, AUC: 0.8357809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009062094875786564, AUC: 0.9034080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01781408405303955, AUC: 0.654745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015463224053382874, AUC: 0.835103\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008266044160959744, AUC: 0.8326999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014237256050109862, AUC: 0.864918\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008004593764348785, AUC: 0.882996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012769156694412232, AUC: 0.8659590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008428762642906444, AUC: 0.9213480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014245012402534485, AUC: 0.5852345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001658763825893402, AUC: 0.8170390000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007605639404368283, AUC: 0.8649439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013760349750518799, AUC: 0.847286\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00080482135572941, AUC: 0.9197520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015656830668449402, AUC: 0.84578\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006399273289607303, AUC: 0.9433560000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00336314058303833, AUC: 0.313304\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016906937360763549, AUC: 0.7734209999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008637543446799316, AUC: 0.779352\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015704018473625184, AUC: 0.8213159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008057524691713918, AUC: 0.838932\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013763497471809387, AUC: 0.8308530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008804763908876051, AUC: 0.8831279999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS ratio\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "learning_rate_train_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                _, train_auc = metric_utils.auc_sigmoid(train_loader_ratio, network) \n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0f2d0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f1ccb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0009614106118679046, AUC: 0.614011\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006774698793888092, AUC: 0.6585589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006662350296974182, AUC: 0.713206\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006555393636226654, AUC: 0.7449055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006931622505187988, AUC: 0.532071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006511834263801575, AUC: 0.705865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006510878801345825, AUC: 0.7048525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006323479413986206, AUC: 0.74831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012074116468429566, AUC: 0.45304300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006792399287223815, AUC: 0.6182110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006664896011352539, AUC: 0.667305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006620110869407654, AUC: 0.6808749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005842076301574707, AUC: 0.35606649999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006702626347541809, AUC: 0.6863610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006543796062469482, AUC: 0.7483445000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006360483169555665, AUC: 0.7825044999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009045130610466003, AUC: 0.455444\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006217218935489655, AUC: 0.8241620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005998185575008392, AUC: 0.8498125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005988564193248749, AUC: 0.8472009999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007873946130275727, AUC: 0.505924\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006304432153701782, AUC: 0.745096\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005960119962692261, AUC: 0.790896\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005686694979667664, AUC: 0.8086879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018394469618797302, AUC: 0.342069\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006031354665756225, AUC: 0.780868\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005784178674221039, AUC: 0.804662\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005471040308475494, AUC: 0.84365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026135700941085816, AUC: 0.615929\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988285779953002, AUC: 0.508404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006910592019557952, AUC: 0.566112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006852890253067017, AUC: 0.5973975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002733421802520752, AUC: 0.42795249999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006736321449279785, AUC: 0.6940394999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000640917718410492, AUC: 0.8093619999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000628568172454834, AUC: 0.827172\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009439133703708648, AUC: 0.654072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000649003803730011, AUC: 0.7196885000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006312101185321808, AUC: 0.768405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006372729241847991, AUC: 0.7558340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007115027904510498, AUC: 0.616657\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006401776671409607, AUC: 0.720353\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006331292688846589, AUC: 0.763507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006281391382217407, AUC: 0.7875584999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019356902241706847, AUC: 0.568219\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006451549232006073, AUC: 0.6998120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006452490091323853, AUC: 0.7160934999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006427935063838959, AUC: 0.72821\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001419282615184784, AUC: 0.40512800000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695315808057785, AUC: 0.48694499999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948222219944001, AUC: 0.4929665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006942886710166931, AUC: 0.5003135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008247624337673188, AUC: 0.525384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006887524425983429, AUC: 0.5578150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006850159466266632, AUC: 0.569787\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006808511018753052, AUC: 0.5959215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004765978336334228, AUC: 0.348692\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006693117916584015, AUC: 0.6508544999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006629041135311126, AUC: 0.6958869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006600531339645385, AUC: 0.7041590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007654413282871246, AUC: 0.621819\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006669045090675354, AUC: 0.6428965000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006625311076641083, AUC: 0.6809704999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006586202681064606, AUC: 0.6952625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034120362997055054, AUC: 0.388479\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006689565479755401, AUC: 0.716342\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006688528954982757, AUC: 0.716347\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006670557856559753, AUC: 0.7320850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005057451725006104, AUC: 0.592154\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006919226050376892, AUC: 0.5435785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894824206829071, AUC: 0.5413675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000687130481004715, AUC: 0.563974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003564469575881958, AUC: 0.574443\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006921630799770355, AUC: 0.572517\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918762624263763, AUC: 0.563254\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006903278231620789, AUC: 0.577599\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012052770853042602, AUC: 0.355636\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006900358200073242, AUC: 0.537617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006885618567466736, AUC: 0.554183\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006862815916538239, AUC: 0.570273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS oversampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52d8c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1541b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.002664048075675964, AUC: 0.35103100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008118733763694763, AUC: 0.39093300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007359330058097839, AUC: 0.42988400000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007080410122871399, AUC: 0.476849\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005757878303527832, AUC: 0.4101775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008514148592948913, AUC: 0.663575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006793743073940277, AUC: 0.6388240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006851064562797546, AUC: 0.599984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001174522042274475, AUC: 0.498127\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009026037454605102, AUC: 0.569188\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007478025853633881, AUC: 0.5202549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007039127349853516, AUC: 0.5297069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009038377702236175, AUC: 0.47197\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007971824407577515, AUC: 0.48862799999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007626304626464844, AUC: 0.494027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007488877177238464, AUC: 0.4992150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034414209127426147, AUC: 0.5659004999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008925645649433136, AUC: 0.602172\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007607427835464478, AUC: 0.5276164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007198239862918854, AUC: 0.509627\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013543962240219116, AUC: 0.49844299999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013344860672950744, AUC: 0.5426559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008173213005065918, AUC: 0.520809\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007598233222961425, AUC: 0.41432599999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009839903712272643, AUC: 0.54916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008945295512676239, AUC: 0.46157500000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007561358511447906, AUC: 0.477264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007167462408542633, AUC: 0.498566\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001238984763622284, AUC: 0.47820850000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009365684688091278, AUC: 0.374864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008359749019145966, AUC: 0.3937505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007830325365066528, AUC: 0.417717\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008678676128387451, AUC: 0.6484215000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009037670493125915, AUC: 0.427843\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007472198605537415, AUC: 0.4185105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007187381088733673, AUC: 0.446337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007866494059562683, AUC: 0.5668679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007531993091106415, AUC: 0.488706\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007111433148384094, AUC: 0.49999199999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007000260055065155, AUC: 0.515335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038360543251037597, AUC: 0.294262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009016560912132263, AUC: 0.505367\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088973104953766, AUC: 0.5929039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011563763022422791, AUC: 0.606266\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006004724025726318, AUC: 0.478491\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0041501855850219725, AUC: 0.427829\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030867953300476076, AUC: 0.404122\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026625494956970214, AUC: 0.434574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017807592153549195, AUC: 0.5781095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011875814199447633, AUC: 0.6403829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009512988924980164, AUC: 0.6518860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008521502912044526, AUC: 0.6516249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007921732366085052, AUC: 0.615845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008575631380081177, AUC: 0.646551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009552640914916992, AUC: 0.646782\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008757838904857636, AUC: 0.6430809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012470539212226868, AUC: 0.417589\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001292936086654663, AUC: 0.45840000000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013544394373893739, AUC: 0.473034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001133813738822937, AUC: 0.46033199999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010617620944976807, AUC: 0.63819\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010061159133911133, AUC: 0.634423\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009557005167007447, AUC: 0.6285029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009136235415935516, AUC: 0.61536\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018809281587600709, AUC: 0.6420239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001633428931236267, AUC: 0.638927\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014178273677825928, AUC: 0.634879\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012286875247955321, AUC: 0.6292800000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016851001381874085, AUC: 0.338503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010225446820259095, AUC: 0.45910999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011303470730781556, AUC: 0.5157820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001235684871673584, AUC: 0.543991\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002234177231788635, AUC: 0.47880599999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001637177050113678, AUC: 0.588411\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001996301531791687, AUC: 0.6044080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018329355120658875, AUC: 0.602075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010110423564910888, AUC: 0.42448849999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008554110527038574, AUC: 0.44827599999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008604062795639038, AUC: 0.453961\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008695433139801025, AUC: 0.447998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS undersampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3dafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Class Weighted Loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['pos_weight'] = torch.tensor([weight[1]])\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                _, train_auc = metric_utils.auc_sigmoid(train_loader_ratio, network) \n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9eed2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7c5a840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001231140434741974, AUC: 0.42799699999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004646242409944534, AUC: 0.8889039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005343631803989411, AUC: 0.8919079999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008686412274837494, AUC: 0.8901119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001083509087562561, AUC: 0.53226\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006845002472400665, AUC: 0.653123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006125326752662659, AUC: 0.8488089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005919314324855804, AUC: 0.8557605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0039026875495910647, AUC: 0.506627\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937935054302216, AUC: 0.498991\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933347284793854, AUC: 0.49950300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932948827743531, AUC: 0.497489\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013873992562294006, AUC: 0.7195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005967761874198914, AUC: 0.82448\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005734991431236267, AUC: 0.8349440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007070224285125733, AUC: 0.785533\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002357735753059387, AUC: 0.482152\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936296820640564, AUC: 0.48415649999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938667595386505, AUC: 0.4802235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006942165791988373, AUC: 0.487563\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036210461854934693, AUC: 0.523385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937761902809143, AUC: 0.5009855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934431791305542, AUC: 0.49848999999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932702362537384, AUC: 0.498487\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025560494661331177, AUC: 0.32735000000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930984556674958, AUC: 0.498506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930361986160279, AUC: 0.4950305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930350363254547, AUC: 0.494524\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015529027581214904, AUC: 0.474617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006194843351840973, AUC: 0.837767\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006279045939445496, AUC: 0.8462059999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006999724805355072, AUC: 0.767437\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012249009609222412, AUC: 0.620241\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006041952669620514, AUC: 0.873287\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007532914876937866, AUC: 0.8782405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006726002991199494, AUC: 0.8323440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01455846118927002, AUC: 0.639461\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931689977645874, AUC: 0.499509\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931595504283905, AUC: 0.4990065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931029856204986, AUC: 0.49901\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014588987231254577, AUC: 0.3418255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006358098983764649, AUC: 0.8169600000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006213722229003907, AUC: 0.8379335000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005967464148998261, AUC: 0.868117\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004154715538024902, AUC: 0.663772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945523023605347, AUC: 0.49701850000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943893432617188, AUC: 0.496503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006942522823810578, AUC: 0.495525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004893231153488159, AUC: 0.522625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006549744606018066, AUC: 0.7453135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006052753925323486, AUC: 0.8331200000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006558211147785187, AUC: 0.7792520000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004606489419937134, AUC: 0.3185075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938447058200836, AUC: 0.5138165000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940377056598664, AUC: 0.5133340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006939546763896942, AUC: 0.5196854999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007986662089824677, AUC: 0.5418645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006440240740776062, AUC: 0.7755520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006362338960170746, AUC: 0.7949105000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006253111064434051, AUC: 0.8168529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018555140495300293, AUC: 0.425004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006363131999969483, AUC: 0.777339\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006264924705028533, AUC: 0.807383\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006382323503494263, AUC: 0.7971630000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001258825659751892, AUC: 0.510613\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006386227607727051, AUC: 0.7993314999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000617032140493393, AUC: 0.8133179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006372943520545959, AUC: 0.8054589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004171834945678711, AUC: 0.621041\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006373399198055268, AUC: 0.7957860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006196101009845734, AUC: 0.8158980000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006130297482013703, AUC: 0.8392575000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021911412477493286, AUC: 0.457905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951816082000732, AUC: 0.491057\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947838366031647, AUC: 0.49203\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945332288742066, AUC: 0.49502749999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007901264131069183, AUC: 0.7051749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005447923541069031, AUC: 0.8485389999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000490674301981926, AUC: 0.861569\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004823784530162811, AUC: 0.875722\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035094506740570067, AUC: 0.4869675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006126205623149871, AUC: 0.8411069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005894834697246551, AUC: 0.863031\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006183668673038483, AUC: 0.837372\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002124825477600098, AUC: 0.5139849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693384736776352, AUC: 0.4989545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932052671909332, AUC: 0.498004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934086084365845, AUC: 0.495482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027600611448287964, AUC: 0.5918680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006007645428180694, AUC: 0.8397330000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006115467846393585, AUC: 0.847848\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000565569132566452, AUC: 0.8785409999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010338220596313476, AUC: 0.561124\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00069463711977005, AUC: 0.500472\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941149830818176, AUC: 0.501969\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938323378562928, AUC: 0.49899250000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005264483451843262, AUC: 0.41430500000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000630477398633957, AUC: 0.806809\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000600333571434021, AUC: 0.845023\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005916058719158173, AUC: 0.8497254999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002107487916946411, AUC: 0.557177\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005829608142375947, AUC: 0.8499970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005906616151332855, AUC: 0.8557620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005705581307411194, AUC: 0.8572625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992025136947632, AUC: 0.432855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006437636613845825, AUC: 0.7785299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005796646773815155, AUC: 0.824011\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007460331320762635, AUC: 0.889765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011212921142578125, AUC: 0.432425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005817952752113343, AUC: 0.8627610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005758479833602905, AUC: 0.8727155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005439314842224122, AUC: 0.8740650000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015276249647140504, AUC: 0.342524\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006016400456428527, AUC: 0.833069\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005816873908042907, AUC: 0.8589514999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005960991084575653, AUC: 0.851675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012753783464431762, AUC: 0.40852999999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006214221119880676, AUC: 0.8184144999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005740155875682831, AUC: 0.8740189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005707530081272126, AUC: 0.873696\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS SMOTE\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 1e-4, 5e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edcf4753",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfecae8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.002174391269683838, AUC: 0.493485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001345380187034607, AUC: 0.737005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014745190143585205, AUC: 0.823094\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001579261302947998, AUC: 0.8384860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008028465509414673, AUC: 0.481804\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005763685703277588, AUC: 0.82903\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002095910429954529, AUC: 0.79007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014553953409194946, AUC: 0.8611660000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004671881914138794, AUC: 0.434292\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00045436549186706545, AUC: 0.875008\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012930932641029359, AUC: 0.8662929999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015099110007286071, AUC: 0.858296\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009448454380035401, AUC: 0.38943150000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00204328989982605, AUC: 0.6934210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008080962002277374, AUC: 0.832444\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008111295104026795, AUC: 0.8687119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021964836120605467, AUC: 0.276298\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016957995295524596, AUC: 0.6403719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006421829164028167, AUC: 0.831952\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046772484481334687, AUC: 0.8775109999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031312047243118287, AUC: 0.3405245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016629098057746887, AUC: 0.609567\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012474647760391236, AUC: 0.7549680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009316090047359467, AUC: 0.856276\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009632619321346283, AUC: 0.53584\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001713748037815094, AUC: 0.594131\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013703943490982055, AUC: 0.7032719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009412908554077148, AUC: 0.7709159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001209743320941925, AUC: 0.469074\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018361815214157104, AUC: 0.7249490000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001989833414554596, AUC: 0.854033\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011067346930503845, AUC: 0.881904\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00285483992099762, AUC: 0.556775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019113221764564515, AUC: 0.6736849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017779849171638488, AUC: 0.641289\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001578715980052948, AUC: 0.7034239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003599430441856384, AUC: 0.468728\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005966219305992127, AUC: 0.8655329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009098819196224213, AUC: 0.8990390000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001225925087928772, AUC: 0.9030839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009507868885993958, AUC: 0.6392909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00063413006067276, AUC: 0.829817\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006332595348358154, AUC: 0.8302850000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005391051471233367, AUC: 0.8704385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034711266756057737, AUC: 0.34269099999999997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 Class Capped SMOTE \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_sigmoid_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for i in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[i][0]\n",
    "    auc_variance = cap_aucs[i][1]\n",
    "    cap = caps[i]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606da0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = (col_names))\n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f22d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 CLASS Focal Loss\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SigmoidFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d438d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5f6eb22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf2\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7924e1f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# distance + capped loss\n",
    "momentum=0\n",
    "learning_rates = [5e-3, 5e-4]\n",
    "\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_euclidean_distance(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"distance_capped_smote_fixed\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca87cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf33bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine distance + capped loss\n",
    "momentum=0\n",
    "learning_rates = [5e-2]\n",
    "\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_cosine_distance(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"cosine_distance_capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd0c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d18f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capped loss with everything capped + cosine distance \n",
    "momentum=0\n",
    "learning_rates = [5e-4]\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_cosine_distance(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.AllCappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"cosine_distance_all_capped\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecfcd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af0e21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2 class triplet loss no ratio \n",
    "# no smote \n",
    "\n",
    "# note: sometimes can get a very high accuracy but may diverge\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(1e-6, 5e-4)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(3): \n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(complete_network.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs / 2):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss, embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "        for epoch in range(50):\n",
    "            _, _ = train.train_linear_probe(epoch, train_loader_reduced, complete_network, linear_optimizer, verbose=False)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"triplet_loss\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], \n",
    "           auc_mean[i][4], auc_variance[i][4],\n",
    "           auc_mean[i][5], auc_variance[i][5],\n",
    "           None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8afa13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf0669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplet loss with ratio \n",
    "# need to make a new train loader if running this \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(1e-7, 1e-3)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(2):\n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(linear_probe.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss, complete_network.embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "            \n",
    "    #    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network)\n",
    "        for epoch in range(50):\n",
    "            _, _ = train.train_linear_probe(epoch, train_loader_reduced, complete_network.embed_network, complete_network.linear_probe, linear_optimizer, verbose=False)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"triplet_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], \n",
    "           auc_mean[i][4], auc_variance[i][4],\n",
    "           auc_mean[i][5], auc_variance[i][5],\n",
    "           None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a3e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a99dbdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010404332876205445, AUC: 0.561752\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 22\u001b[0m     _, train_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_triplet_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_tripletloss_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(train_losses))))\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:143\u001b[0m, in \u001b[0;36mtrain_triplet_loss\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn_args)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (anchor_data, pos_data, neg_data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m    141\u001b[0m         train_loader):\n\u001b[1;32m    142\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 143\u001b[0m     _, anchor_embeds \u001b[38;5;241m=\u001b[39m network(anchor_data\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m    144\u001b[0m     _, pos_embeds \u001b[38;5;241m=\u001b[39m network(pos_data\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m    145\u001b[0m     _, neg_embeds \u001b[38;5;241m=\u001b[39m network(neg_data\u001b[38;5;241m.\u001b[39mfloat())\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# 2 class triplet loss w/ SMOTE and ratio \n",
    "# REDO THIS (match above code)\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(1e-7, 1e-3)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(2):\n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(linear_probe.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss_smote, complete_network.embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "        for epoch in range(50):\n",
    "            _, _ = train.train_linear_probe(epoch, train_loader_reduced, complete_network, linear_optimizer, loss_fn=loss_fns.CappedBCELoss, verbose=False)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"capped_smote_triplet_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], \n",
    "           auc_mean[i][4], auc_variance[i][4],\n",
    "           auc_mean[i][5], auc_variance[i][5],\n",
    "           None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de718f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc1dd64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES_REDUCED = 3\n",
    "nums = (0, 3, 1)\n",
    "ratio = (20, 2, 1)\n",
    "\n",
    "\n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums)\n",
    "targets = ratio_train_CIFAR10.labels \n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED)\n",
    "\n",
    "\n",
    "weight = 1. / class_count\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= max(class_count)\n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "800c1e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004159178098042806, AUC: 0.5\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 16\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     18\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_softmax(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/ml/train.py:41\u001b[0m, in \u001b[0;36mtrain_softmax\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze(), target\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     43\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/ml/models.py:19\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2_drop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)), \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m250\u001b[39m) \u001b[38;5;66;03m#(320)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_jit_internal.py:484\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3 class normal\n",
    "\n",
    "learning_rates = [1e-4, 1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 3, nums, (1, 1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6be4998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6452da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0028148902257283527, AUC: 0.5499715000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012899534304936728, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011812520821889241, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003198623657227, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012160149812698365, AUC: 0.496695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012602541049321493, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011723772684733072, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012640552123387655, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001084180474281311, AUC: 0.47396475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012727203766504925, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010816088120142618, AUC: 0.5045000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011214701334635417, AUC: 0.5035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026806603272755943, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012301311095555623, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001151394208272298, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107947838306427, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020958302021026613, AUC: 0.44825000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012716478904088338, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011440247694651285, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012925329605738322, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026947441101074217, AUC: 0.47152499999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011879764000574749, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011960159142812093, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012670242389043172, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017743062178293865, AUC: 0.50675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001074000636736552, AUC: 0.5057499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012203034957249958, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011130955616633098, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014793120384216308, AUC: 0.44375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012164912223815918, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010818771521250406, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011993260780970255, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00679119078318278, AUC: 0.48125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012626315355300903, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001144296924273173, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001217811703681946, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007157065073649088, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001255599578221639, AUC: 0.501499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011938397884368897, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010313888788223266, AUC: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class ratio\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d28e8f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1752b5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.003038533369700114, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010433460474014282, AUC: 0.574998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011814462741216024, AUC: 0.7483075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001038165827592214, AUC: 0.7227574999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007914267698923746, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010645556449890136, AUC: 0.37466975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001031708836555481, AUC: 0.6914997500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011311002572377523, AUC: 0.75428325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006719241301218669, AUC: 0.5075000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010695095856984456, AUC: 0.5922592499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010078495343526204, AUC: 0.66097125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009656443595886231, AUC: 0.68574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002060540755589803, AUC: 0.5615957500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010401540199915568, AUC: 0.3627755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009678711692492167, AUC: 0.6725415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009192776083946228, AUC: 0.675171\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00151268204053243, AUC: 0.5225000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010918419361114502, AUC: 0.40822674999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010981494585673014, AUC: 0.4126655000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010762500762939453, AUC: 0.684442\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0108524964650472, AUC: 0.499499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013274700244267782, AUC: 0.6495525000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001373440424601237, AUC: 0.6880609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882926384607951, AUC: 0.5351134999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004438136577606201, AUC: 0.50352525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012416810989379883, AUC: 0.5918582499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013266102472941081, AUC: 0.6148197500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019509809811909993, AUC: 0.6372085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005216264883677164, AUC: 0.51125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010727325280507406, AUC: 0.499001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010864347219467162, AUC: 0.4997515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010905816157658894, AUC: 0.49875600000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003298620859781901, AUC: 0.51497625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011997053623199463, AUC: 0.64312625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001198989232381185, AUC: 0.648871\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001623408595720927, AUC: 0.6290437499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013772049347559611, AUC: 0.4500199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006249984105427, AUC: 0.47525675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973472197850546, AUC: 0.46956325000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010804060300191245, AUC: 0.5838855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class oversampled \n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "423b94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c14c3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0020289310614267984, AUC: 0.4756965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001091553012530009, AUC: 0.48456375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010886570612589517, AUC: 0.4860155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010853137572606406, AUC: 0.5097382500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019787512222925823, AUC: 0.57222475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011532610654830932, AUC: 0.49576575000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145702044169108, AUC: 0.4937625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001140582799911499, AUC: 0.49350675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005084774017333984, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010802105665206909, AUC: 0.50175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010755322376887005, AUC: 0.502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010662730137507121, AUC: 0.5022505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009249690055847169, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010924884875615438, AUC: 0.50599775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882760683695474, AUC: 0.5044992500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010864359935124715, AUC: 0.50449775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008460322062174478, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003564596176148, AUC: 0.5434515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010977611939112346, AUC: 0.527579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001091849128405253, AUC: 0.502861\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037917500336964926, AUC: 0.50300075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001107357660929362, AUC: 0.50625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011036542654037475, AUC: 0.50575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008251905441284, AUC: 0.503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00853164831797282, AUC: 0.48180575000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011391439437866211, AUC: 0.4952675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011303770144780478, AUC: 0.49850675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011254301071166992, AUC: 0.49128400000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017762763341267904, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011278401215871175, AUC: 0.46372199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021624008814494, AUC: 0.42442375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011078615188598632, AUC: 0.43288249999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007273403485616049, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011379459698994954, AUC: 0.47927200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011329193909962972, AUC: 0.4857515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011250438690185546, AUC: 0.53704125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00682664426167806, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001064486066500346, AUC: 0.4997495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010623377164204915, AUC: 0.49874950000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010606383085250855, AUC: 0.49924975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class undersampled  \n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaec9e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c779b879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.00608756939570109, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010771948893864949, AUC: 0.5007499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001078349749247233, AUC: 0.49949999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010784133275349936, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00372534704208374, AUC: 0.50023525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010370986064275106, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010473136504491171, AUC: 0.49649774999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010565840800603231, AUC: 0.5224875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020539817810058592, AUC: 0.50625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010878965854644776, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087069312731425, AUC: 0.49999999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010845698912938435, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006659661134084066, AUC: 0.49401975000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010928993225097657, AUC: 0.53216375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010782272815704345, AUC: 0.54923675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010686718225479126, AUC: 0.5544079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009116797844568889, AUC: 0.48513425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010646411975224813, AUC: 0.497999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010692731142044067, AUC: 0.502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010714724858601888, AUC: 0.50150125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015166940053304037, AUC: 0.48724999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011384790738423664, AUC: 0.49775050000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011099223693211873, AUC: 0.51426275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013285338083902994, AUC: 0.59251075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018370418151219686, AUC: 0.4615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010755572319030762, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010769031047821046, AUC: 0.5002502499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010774298906326295, AUC: 0.49999925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003305099407831828, AUC: 0.503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010899808804194133, AUC: 0.49999999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010876678625742595, AUC: 0.5000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010835784276326497, AUC: 0.5007499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010739267667134603, AUC: 0.502463\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001126469651858012, AUC: 0.47123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011237830718358358, AUC: 0.49575225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011101373434066772, AUC: 0.468339\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01887862459818522, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010957230726877849, AUC: 0.4975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010916781425476074, AUC: 0.50075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010887458721796672, AUC: 0.5009999999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class weighted\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['weight'] = torch.from_numpy(weight).float()\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8692b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81115d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011305590867996215, AUC: 0.4987425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101138710975647, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011463310718536376, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011780770619710286, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038984591166178386, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011603130102157593, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011915287176767985, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012100194692611695, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006762343406677246, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145007332166036, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011761978069941203, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011957573493321736, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017103184858957927, AUC: 0.4808077500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011710822582244873, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012000585397084554, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012189826170603433, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003416938861211141, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011359240611394246, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011698009570439657, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011919792890548707, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030593673388163247, AUC: 0.475337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000983761727809906, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982903023560842, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982608477274577, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001518441875775655, AUC: 0.46025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010580919981002807, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103336493174235, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001135496457417806, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003099643548329671, AUC: 0.5017499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001077566663424174, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010170272588729858, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001027000625928243, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026799618403116864, AUC: 0.4685945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011022898356119791, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011425824165344238, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001169986605644226, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001287020762761434, AUC: 0.513392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011173804601033528, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011543748378753662, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001180996815363566, AUC: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class focal loss\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['reduction'] = 'mean'\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SoftmaxFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf8eaef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d48470b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004159178098042806, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010713002681732179, AUC: 0.49825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010870064496994018, AUC: 0.49875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010937161048253376, AUC: 0.4980015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001217594623565674, AUC: 0.5495857500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010883294343948364, AUC: 0.54002475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009588491717974345, AUC: 0.6908785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010892333984375, AUC: 0.6341857500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012967588504155477, AUC: 0.44993225000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010879942576090494, AUC: 0.49974900000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010939101775487264, AUC: 0.4984995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010963983138402302, AUC: 0.4980035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009516664822896322, AUC: 0.4979995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010874385436375935, AUC: 0.502996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010935383637746174, AUC: 0.50224575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010963813463846842, AUC: 0.50149875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008401273687680562, AUC: 0.473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001117892305056254, AUC: 0.50074975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110655943552653, AUC: 0.49974875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101777672767639, AUC: 0.49874875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018197454214096069, AUC: 0.642566\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010871400435765585, AUC: 0.502739\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010544216235478718, AUC: 0.7166375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009863032698631286, AUC: 0.7337182500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014161507288614908, AUC: 0.6221422499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001115713357925415, AUC: 0.49924975000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011057165066401164, AUC: 0.5000005000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011015222469965616, AUC: 0.50025025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00627077309290568, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003833611806233, AUC: 0.51025525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096467614173889, AUC: 0.5330164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010366939703623454, AUC: 0.669718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014923743406931558, AUC: 0.6397665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001040031870206197, AUC: 0.7275147499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010351263682047526, AUC: 0.6475892499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011037707328796387, AUC: 0.54751375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01015039348602295, AUC: 0.49\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010485761562983194, AUC: 0.66979875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009980746308962504, AUC: 0.723652\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009460805654525757, AUC: 0.700749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class SMOTE\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "717e9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bfe9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.005159725666046142, AUC: 0.47875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013107466697692871, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014245965083440144, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013205681641896565, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036626233259836835, AUC: 0.5045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012807045380274454, AUC: 0.55175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008384430607159932, AUC: 0.6527735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013193607727686565, AUC: 0.58771125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001882831374804179, AUC: 0.568451\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014629533290863037, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013556772470474244, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012844537496566773, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005783709685007731, AUC: 0.5153915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014496474663416544, AUC: 0.68025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013263502518335978, AUC: 0.72725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005901109576225281, AUC: 0.6950000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015779575983683267, AUC: 0.47575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017115784088770549, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001690352439880371, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016737443208694458, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024884181340535484, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016858530044555663, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016851926644643148, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016747065782546997, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014013916651407878, AUC: 0.4033884999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001159618854522705, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012145692507425944, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006874272624651591, AUC: 0.7312500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005693643728892008, AUC: 0.49025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016551088094711303, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016597933371861775, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001664273738861084, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004257233619689942, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017054282824198405, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001689716895421346, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016794553200403849, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009325793266296387, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016158297061920166, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018122962315877279, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001287239154179891, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011229380289713542, AUC: 0.4915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009665106336275737, AUC: 0.6430379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009809208512306213, AUC: 0.7527905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010630707343419392, AUC: 0.5388919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002801464796066284, AUC: 0.4945689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010990440448125204, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098304033279419, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097338914871216, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011395713488260904, AUC: 0.44925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010731459856033324, AUC: 0.5862499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009436763922373454, AUC: 0.63125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001167917807896932, AUC: 0.5477055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003414260149002075, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010740158557891845, AUC: 0.5370075000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010959016879399618, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010975220600763958, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005730849742889404, AUC: 0.50174975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098487933476766, AUC: 0.49950649999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005215644836425, AUC: 0.56338675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011249771118164063, AUC: 0.65596625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005885673840840658, AUC: 0.505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011182666222254436, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001144629160563151, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011247365872065227, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01593700949350993, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096144715944926, AUC: 0.4999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097902218500773, AUC: 0.49725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003881295522054, AUC: 0.4957484999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010915645360946656, AUC: 0.43125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099284609158834, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010995355049769084, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991565783818563, AUC: 0.5025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023588163852691652, AUC: 0.497\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002196073532104, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011001572211583456, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099966843922933, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003621602535247803, AUC: 0.5055245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010969852209091187, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988662242889405, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010969030062357584, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008199299812316894, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011139464775721231, AUC: 0.5786862499999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001256147066752116, AUC: 0.5299999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011905717055002848, AUC: 0.5347545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029200942516326906, AUC: 0.50149925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984888474146525, AUC: 0.49974999999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011014104684193928, AUC: 0.49749899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099079688390096, AUC: 0.4964985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013806982835133871, AUC: 0.5180030000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998592774073284, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011013609170913697, AUC: 0.49625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106422464052836, AUC: 0.49724299999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008860953013102213, AUC: 0.49525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010979174375534057, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005378564198811, AUC: 0.496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002195278803507, AUC: 0.49825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026838178634643554, AUC: 0.498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992279847462972, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998342831929524, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989136298497518, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00352239465713501, AUC: 0.4201495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992273886998494, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991250673929851, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096076528231303, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003739197254180908, AUC: 0.5009999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010914430618286133, AUC: 0.7057055000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009601728518803914, AUC: 0.67756775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001175796906153361, AUC: 0.5605000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007436930020650228, AUC: 0.49375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989174445470175, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984821716944378, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095758557319641, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011664613405863444, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973004500071208, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010976841449737548, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010974336862564088, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001008460839589437, AUC: 0.46063449999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010979483524958292, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099142074584961, AUC: 0.4995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class capped loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-2]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args, smote=True)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for i in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[i][0]\n",
    "    auc_variance = cap_aucs[i][1]\n",
    "    cap = caps[i]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", 3, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b3e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c453da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0967e450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
