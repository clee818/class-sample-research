{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c3c2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378c3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91dda12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "NUM_CLASSES_REDUCED = 2\n",
    "nums = (0, 1)\n",
    "ratio = (100, 1)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b57e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "            \"mean_40\", \"variance_40\",\n",
    "            \"mean_50\", \"variance_50\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c2a1ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor() ]))  \n",
    "\n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()]))\n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "    \n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums)\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13159c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000   50]\n"
     ]
    }
   ],
   "source": [
    "targets = ratio_train_CIFAR10.labels \n",
    "\n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "print(class_count)\n",
    "\n",
    "weight = 1. / class_count\n",
    "\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= class_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de0d4cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.999 \n",
    "\n",
    "exp = np.empty_like(targets)\n",
    "for i, count in enumerate(class_count):\n",
    "    exp[targets==i] = count\n",
    "effective_weights = (1 - beta) / ( 1 - (beta ** torch.from_numpy(exp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af8cd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2e8c09f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001968830943107605, AUC: 0.6361665000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006069920063018799, AUC: 0.8331379999999999\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 18\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     20\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/ml/train.py:20\u001b[0m, in \u001b[0;36mtrain_sigmoid\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     19\u001b[0m pred\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2 CLASS normal\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f1d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "\n",
    "df2.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d211694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001968830943107605, AUC: 0.6361665000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028516278266906736, AUC: 0.5899970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025376933813095093, AUC: 0.6142540000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022351186275482177, AUC: 0.631213\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001262024223804474, AUC: 0.5179750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036277343034744262, AUC: 0.451289\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032277283668518068, AUC: 0.47812699999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002840754508972168, AUC: 0.490997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014940328001976013, AUC: 0.5863039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004338119268417358, AUC: 0.355806\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0039295399188995364, AUC: 0.38401599999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003611718535423279, AUC: 0.40321599999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034400452375411986, AUC: 0.48185000000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003534223437309265, AUC: 0.5466075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002973232626914978, AUC: 0.595231\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002541086196899414, AUC: 0.6146119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003438815712928772, AUC: 0.417288\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003965676546096802, AUC: 0.40975900000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034930930137634275, AUC: 0.444622\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033193113803863523, AUC: 0.48642599999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005195510864257812, AUC: 0.5519015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038592629432678224, AUC: 0.540808\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031717004776000974, AUC: 0.6203755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028933166265487672, AUC: 0.6688225000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000978255033493042, AUC: 0.601167\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037130323648452757, AUC: 0.597758\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003394280433654785, AUC: 0.60338\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003216369152069092, AUC: 0.627105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007666639685630798, AUC: 0.5573060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002731086254119873, AUC: 0.627084\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023691245317459106, AUC: 0.615294\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021873852014541624, AUC: 0.6435109999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004819110631942749, AUC: 0.446156\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003851627707481384, AUC: 0.579202\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027788195610046387, AUC: 0.5772595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030230313539505007, AUC: 0.610449\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022173867225646973, AUC: 0.744175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029739172458648684, AUC: 0.6515909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003142796754837036, AUC: 0.651929\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029972847700119017, AUC: 0.6419765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007359807252883911, AUC: 0.56999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007687550067901611, AUC: 0.569072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0059759230613708495, AUC: 0.5384169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004907165050506592, AUC: 0.5126465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009441473186016083, AUC: 0.553817\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031278812885284423, AUC: 0.508879\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003693561553955078, AUC: 0.5183249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037564566135406493, AUC: 0.523801\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025664455890655516, AUC: 0.5203535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003067721962928772, AUC: 0.519094\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002948760747909546, AUC: 0.5257149999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002927612066268921, AUC: 0.521189\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0039877592325210575, AUC: 0.589023\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0050818660259246825, AUC: 0.58621\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004325702905654907, AUC: 0.5557234999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038247548341751097, AUC: 0.5310039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015584539771080017, AUC: 0.61194\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004594267845153809, AUC: 0.500473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004243391752243042, AUC: 0.462033\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003881990671157837, AUC: 0.43223100000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001934466540813446, AUC: 0.7069635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011837327480316163, AUC: 0.49404549999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017638294100761414, AUC: 0.49302499999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022053698301315308, AUC: 0.502332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007971093356609344, AUC: 0.47903799999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016412882208824157, AUC: 0.6123750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018253124952316284, AUC: 0.628907\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017445732951164245, AUC: 0.626074\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001009637802839279, AUC: 0.48053499999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001799607276916504, AUC: 0.48565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002908296823501587, AUC: 0.479954\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032813555002212526, AUC: 0.475307\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001696766436100006, AUC: 0.6105499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023335320949554443, AUC: 0.557841\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002563801646232605, AUC: 0.547795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002678507924079895, AUC: 0.540913\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003499551773071289, AUC: 0.581766\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037614872455596924, AUC: 0.46397199999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003901851773262024, AUC: 0.453811\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003799800753593445, AUC: 0.45079699999999995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS ratio\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "learning_rate_train_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                _, train_auc = metric_utils.auc_sigmoid(train_loader_ratio, network) \n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f2d0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f1ccb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0009614106118679046, AUC: 0.614011\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006774698793888092, AUC: 0.6585589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006662350296974182, AUC: 0.713206\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006555393636226654, AUC: 0.7449055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006931622505187988, AUC: 0.532071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006511834263801575, AUC: 0.705865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006510878801345825, AUC: 0.7048525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006323479413986206, AUC: 0.74831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012074116468429566, AUC: 0.45304300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006792399287223815, AUC: 0.6182110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006664896011352539, AUC: 0.667305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006620110869407654, AUC: 0.6808749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005842076301574707, AUC: 0.35606649999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006702626347541809, AUC: 0.6863610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006543796062469482, AUC: 0.7483445000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006360483169555665, AUC: 0.7825044999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009045130610466003, AUC: 0.455444\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006217218935489655, AUC: 0.8241620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005998185575008392, AUC: 0.8498125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005988564193248749, AUC: 0.8472009999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007873946130275727, AUC: 0.505924\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006304432153701782, AUC: 0.745096\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005960119962692261, AUC: 0.790896\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005686694979667664, AUC: 0.8086879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018394469618797302, AUC: 0.342069\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006031354665756225, AUC: 0.780868\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005784178674221039, AUC: 0.804662\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005471040308475494, AUC: 0.84365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026135700941085816, AUC: 0.615929\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988285779953002, AUC: 0.508404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006910592019557952, AUC: 0.566112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006852890253067017, AUC: 0.5973975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002733421802520752, AUC: 0.42795249999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006736321449279785, AUC: 0.6940394999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000640917718410492, AUC: 0.8093619999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000628568172454834, AUC: 0.827172\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009439133703708648, AUC: 0.654072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000649003803730011, AUC: 0.7196885000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006312101185321808, AUC: 0.768405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006372729241847991, AUC: 0.7558340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007115027904510498, AUC: 0.616657\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006401776671409607, AUC: 0.720353\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006331292688846589, AUC: 0.763507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006281391382217407, AUC: 0.7875584999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019356902241706847, AUC: 0.568219\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006451549232006073, AUC: 0.6998120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006452490091323853, AUC: 0.7160934999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006427935063838959, AUC: 0.72821\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001419282615184784, AUC: 0.40512800000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695315808057785, AUC: 0.48694499999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948222219944001, AUC: 0.4929665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006942886710166931, AUC: 0.5003135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008247624337673188, AUC: 0.525384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006887524425983429, AUC: 0.5578150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006850159466266632, AUC: 0.569787\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006808511018753052, AUC: 0.5959215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004765978336334228, AUC: 0.348692\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006693117916584015, AUC: 0.6508544999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006629041135311126, AUC: 0.6958869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006600531339645385, AUC: 0.7041590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007654413282871246, AUC: 0.621819\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006669045090675354, AUC: 0.6428965000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006625311076641083, AUC: 0.6809704999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006586202681064606, AUC: 0.6952625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034120362997055054, AUC: 0.388479\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006689565479755401, AUC: 0.716342\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006688528954982757, AUC: 0.716347\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006670557856559753, AUC: 0.7320850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005057451725006104, AUC: 0.592154\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006919226050376892, AUC: 0.5435785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894824206829071, AUC: 0.5413675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000687130481004715, AUC: 0.563974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003564469575881958, AUC: 0.574443\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006921630799770355, AUC: 0.572517\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918762624263763, AUC: 0.563254\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006903278231620789, AUC: 0.577599\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012052770853042602, AUC: 0.355636\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006900358200073242, AUC: 0.537617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006885618567466736, AUC: 0.554183\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006862815916538239, AUC: 0.570273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS oversampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52d8c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1541b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.002664048075675964, AUC: 0.35103100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008118733763694763, AUC: 0.39093300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007359330058097839, AUC: 0.42988400000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007080410122871399, AUC: 0.476849\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005757878303527832, AUC: 0.4101775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008514148592948913, AUC: 0.663575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006793743073940277, AUC: 0.6388240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006851064562797546, AUC: 0.599984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001174522042274475, AUC: 0.498127\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009026037454605102, AUC: 0.569188\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007478025853633881, AUC: 0.5202549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007039127349853516, AUC: 0.5297069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009038377702236175, AUC: 0.47197\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007971824407577515, AUC: 0.48862799999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007626304626464844, AUC: 0.494027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007488877177238464, AUC: 0.4992150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034414209127426147, AUC: 0.5659004999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008925645649433136, AUC: 0.602172\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007607427835464478, AUC: 0.5276164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007198239862918854, AUC: 0.509627\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013543962240219116, AUC: 0.49844299999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013344860672950744, AUC: 0.5426559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008173213005065918, AUC: 0.520809\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007598233222961425, AUC: 0.41432599999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009839903712272643, AUC: 0.54916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008945295512676239, AUC: 0.46157500000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007561358511447906, AUC: 0.477264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007167462408542633, AUC: 0.498566\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001238984763622284, AUC: 0.47820850000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009365684688091278, AUC: 0.374864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008359749019145966, AUC: 0.3937505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007830325365066528, AUC: 0.417717\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008678676128387451, AUC: 0.6484215000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009037670493125915, AUC: 0.427843\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007472198605537415, AUC: 0.4185105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007187381088733673, AUC: 0.446337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007866494059562683, AUC: 0.5668679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007531993091106415, AUC: 0.488706\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007111433148384094, AUC: 0.49999199999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007000260055065155, AUC: 0.515335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038360543251037597, AUC: 0.294262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009016560912132263, AUC: 0.505367\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088973104953766, AUC: 0.5929039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011563763022422791, AUC: 0.606266\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006004724025726318, AUC: 0.478491\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0041501855850219725, AUC: 0.427829\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030867953300476076, AUC: 0.404122\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026625494956970214, AUC: 0.434574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017807592153549195, AUC: 0.5781095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011875814199447633, AUC: 0.6403829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009512988924980164, AUC: 0.6518860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008521502912044526, AUC: 0.6516249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007921732366085052, AUC: 0.615845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008575631380081177, AUC: 0.646551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009552640914916992, AUC: 0.646782\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008757838904857636, AUC: 0.6430809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012470539212226868, AUC: 0.417589\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001292936086654663, AUC: 0.45840000000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013544394373893739, AUC: 0.473034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001133813738822937, AUC: 0.46033199999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010617620944976807, AUC: 0.63819\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010061159133911133, AUC: 0.634423\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009557005167007447, AUC: 0.6285029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009136235415935516, AUC: 0.61536\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018809281587600709, AUC: 0.6420239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001633428931236267, AUC: 0.638927\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014178273677825928, AUC: 0.634879\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012286875247955321, AUC: 0.6292800000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016851001381874085, AUC: 0.338503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010225446820259095, AUC: 0.45910999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011303470730781556, AUC: 0.5157820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001235684871673584, AUC: 0.543991\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002234177231788635, AUC: 0.47880599999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001637177050113678, AUC: 0.588411\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001996301531791687, AUC: 0.6044080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018329355120658875, AUC: 0.602075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010110423564910888, AUC: 0.42448849999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008554110527038574, AUC: 0.44827599999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008604062795639038, AUC: 0.453961\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008695433139801025, AUC: 0.447998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS undersampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9eed2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7c5a840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.002710835576057434, AUC: 0.3797905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006392351984977722, AUC: 0.775934\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006287154555320739, AUC: 0.8036535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006199929118156433, AUC: 0.823649\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015046903491020202, AUC: 0.452239\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006398996412754059, AUC: 0.8049729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000635301262140274, AUC: 0.8110299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006301345229148865, AUC: 0.822333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003237917065620422, AUC: 0.41465399999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006488316059112549, AUC: 0.7828309999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006274860501289367, AUC: 0.8129929999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006214897632598877, AUC: 0.8296914999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0043341732025146485, AUC: 0.638036\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006287120580673218, AUC: 0.8169195000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006269447803497315, AUC: 0.8304279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006237910687923432, AUC: 0.8436055000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001160554051399231, AUC: 0.36272699999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941077411174774, AUC: 0.5093470000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006727005541324615, AUC: 0.6642309999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006592448353767395, AUC: 0.7381570000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012124891877174377, AUC: 0.489245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006397503912448883, AUC: 0.7556330000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006133291721343994, AUC: 0.812527\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006023222208023072, AUC: 0.8353060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001963006615638733, AUC: 0.539396\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006368187665939331, AUC: 0.7815854999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006136636435985565, AUC: 0.8324995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006073701381683349, AUC: 0.84874\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006302855491638184, AUC: 0.593271\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006409132480621338, AUC: 0.786085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006360244750976562, AUC: 0.8000840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005955122411251068, AUC: 0.850834\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00855223846435547, AUC: 0.4026815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006329087316989899, AUC: 0.798812\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006283422112464905, AUC: 0.8011035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000624260276556015, AUC: 0.8090275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000924219697713852, AUC: 0.652555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006466928422451019, AUC: 0.7602655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006315066814422608, AUC: 0.8079275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006142356097698212, AUC: 0.8369720000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009669749438762664, AUC: 0.3994995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006759988367557526, AUC: 0.6390659999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000670566737651825, AUC: 0.6858505000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006651373505592346, AUC: 0.7160565000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012196735739707946, AUC: 0.599332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006721514165401459, AUC: 0.663119\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006675722002983094, AUC: 0.6950510000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006639676094055176, AUC: 0.721249\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00334078848361969, AUC: 0.44292600000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006739585101604461, AUC: 0.6749769999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006641647219657897, AUC: 0.7112210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006535576581954956, AUC: 0.735226\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00427367353439331, AUC: 0.5966205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007001340985298156, AUC: 0.4788245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006965263485908508, AUC: 0.47200800000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006954674124717712, AUC: 0.47953350000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001417681932449341, AUC: 0.5604155000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006797730028629303, AUC: 0.63084\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006597400903701783, AUC: 0.664035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006520869135856628, AUC: 0.698733\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002432983160018921, AUC: 0.5864290000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948926746845245, AUC: 0.501004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006949251294136047, AUC: 0.498321\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000694829672574997, AUC: 0.488601\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018511811494827271, AUC: 0.44305249999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006634501218795776, AUC: 0.611016\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000644721508026123, AUC: 0.692332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006262083649635315, AUC: 0.7476645000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007674235582351685, AUC: 0.585197\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006692624688148499, AUC: 0.6064910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000660158395767212, AUC: 0.643483\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006538372039794921, AUC: 0.6832739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014404111504554749, AUC: 0.5836365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006691365242004394, AUC: 0.633353\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006597938239574433, AUC: 0.6884870000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006440792083740234, AUC: 0.741633\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002460719108581543, AUC: 0.381248\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006891500949859619, AUC: 0.529965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006840179860591888, AUC: 0.5782505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006742066740989685, AUC: 0.6356054999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS SMOTE\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edcf4753",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "563f22d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001035837471485138, AUC: 0.44774200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927213966846466, AUC: 0.5254145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931669712066651, AUC: 0.511691\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935113668441773, AUC: 0.5110125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005613439321517945, AUC: 0.426767\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037078361511230467, AUC: 0.5171749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026477036476135253, AUC: 0.46210599999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002063918948173523, AUC: 0.44280450000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001599363923072815, AUC: 0.5595060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017810710668563843, AUC: 0.527251\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017536671757698058, AUC: 0.530268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001468895673751831, AUC: 0.511718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009869316220283508, AUC: 0.521323\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025733143091201784, AUC: 0.44177999999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002441696286201477, AUC: 0.42422899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00228461492061615, AUC: 0.419806\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005625223875045777, AUC: 0.640735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00567081618309021, AUC: 0.490035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003996514081954956, AUC: 0.400708\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033221423625946047, AUC: 0.379003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013055719137191773, AUC: 0.359221\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002855167269706726, AUC: 0.3432545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002768984794616699, AUC: 0.369273\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002534526348114014, AUC: 0.363009\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007071554660797119, AUC: 0.6680550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020034250020980835, AUC: 0.45828099999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022772670984268187, AUC: 0.48882499999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00230745005607605, AUC: 0.509682\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009378761649131774, AUC: 0.5550744999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033345311880111695, AUC: 0.523733\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003033424973487854, AUC: 0.505011\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026757409572601316, AUC: 0.501909\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027659571170806883, AUC: 0.402637\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015370473265647888, AUC: 0.5538240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015259616374969482, AUC: 0.5430379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014122150540351868, AUC: 0.5184869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007173371315002441, AUC: 0.297585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001956529438495636, AUC: 0.5169174999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001981451392173767, AUC: 0.533161\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019487438797950744, AUC: 0.536132\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000798177808523178, AUC: 0.7362540000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006655452251434326, AUC: 0.7207349999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007001525759696961, AUC: 0.733501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007603182792663574, AUC: 0.742639\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002295616149902344, AUC: 0.376647\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009011189937591553, AUC: 0.5605964999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011427636742591858, AUC: 0.5951460000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013215634822845458, AUC: 0.605137\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00221078360080719, AUC: 0.5965469999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010793698787689208, AUC: 0.642539\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010493645191192627, AUC: 0.628214\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009857546329498292, AUC: 0.613483\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006628040313720703, AUC: 0.6315075000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008294330596923829, AUC: 0.650957\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007958059310913085, AUC: 0.6476215000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007488812685012817, AUC: 0.64287\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002516766667366028, AUC: 0.43171800000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007630343437194825, AUC: 0.431629\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007703002095222473, AUC: 0.411333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007906514704227448, AUC: 0.45406500000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002265890836715698, AUC: 0.608756\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004604844093322754, AUC: 0.649751\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038692718744277954, AUC: 0.643426\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032494683265686035, AUC: 0.635331\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015813257098197936, AUC: 0.553272\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00550286054611206, AUC: 0.640204\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0053946242332458495, AUC: 0.631832\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005209399700164795, AUC: 0.626354\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032208967208862305, AUC: 0.590365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005893768787384034, AUC: 0.47718400000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0058131103515625, AUC: 0.455178\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005506749391555786, AUC: 0.440446\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024443947076797485, AUC: 0.535904\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003024880290031433, AUC: 0.495147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002925659418106079, AUC: 0.495309\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002817286252975464, AUC: 0.48984750000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004183521747589111, AUC: 0.566368\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004094211220741272, AUC: 0.52125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037260520458221435, AUC: 0.474804\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033895738124847413, AUC: 0.42854600000000004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS Focal Loss\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['reduction'] = 'mean'\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SigmoidFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d438d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a9c9d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>num_classes</th>\n",
       "      <th>classes_used</th>\n",
       "      <th>ratio</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>mean_0</th>\n",
       "      <th>variance_0</th>\n",
       "      <th>mean_10</th>\n",
       "      <th>variance_10</th>\n",
       "      <th>mean_20</th>\n",
       "      <th>variance_20</th>\n",
       "      <th>mean_30</th>\n",
       "      <th>variance_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>focal_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(100, 1)</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.487865</td>\n",
       "      <td>0.013197</td>\n",
       "      <td>0.489767</td>\n",
       "      <td>0.003418</td>\n",
       "      <td>0.476831</td>\n",
       "      <td>0.003291</td>\n",
       "      <td>0.469356</td>\n",
       "      <td>0.003572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>focal_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(100, 1)</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.562734</td>\n",
       "      <td>0.009148</td>\n",
       "      <td>0.578999</td>\n",
       "      <td>0.008085</td>\n",
       "      <td>0.571636</td>\n",
       "      <td>0.009897</td>\n",
       "      <td>0.567872</td>\n",
       "      <td>0.010230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name  num_classes classes_used     ratio  learning_rate    mean_0   \n",
       "0  focal_loss            2       (0, 1)  (100, 1)        0.00010  0.487865  \\\n",
       "1  focal_loss            2       (0, 1)  (100, 1)        0.00001  0.562734   \n",
       "\n",
       "   variance_0   mean_10  variance_10   mean_20  variance_20   mean_30   \n",
       "0    0.013197  0.489767     0.003418  0.476831     0.003291  0.469356  \\\n",
       "1    0.009148  0.578999     0.008085  0.571636     0.009897  0.567872   \n",
       "\n",
       "   variance_30  \n",
       "0     0.003572  \n",
       "1     0.010230  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc1dd64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES_REDUCED = 3\n",
    "nums = (0, 3, 1)\n",
    "ratio = (20, 2, 1)\n",
    "\n",
    "\n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums)\n",
    "targets = ratio_train_CIFAR10.labels \n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10)\n",
    "\n",
    "weight = 1. / class_count\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= max(class_count)\n",
    "\n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "800c1e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004159178098042806, AUC: 0.5\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 16\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     18\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_softmax(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/ml/train.py:41\u001b[0m, in \u001b[0;36mtrain_softmax\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze(), target\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     43\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/ml/models.py:19\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2_drop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)), \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m250\u001b[39m) \u001b[38;5;66;03m#(320)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_jit_internal.py:484\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3 class normal\n",
    "\n",
    "learning_rates = [1e-4, 1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 3, nums, (1, 1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6be4998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6452da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.007701303323109945, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011454200744628906, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010027400255203248, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011310822168986003, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033325454394022625, AUC: 0.50905525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012397658030192057, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011350598732630412, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001227091352144877, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012566899299621582, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011679747502009075, AUC: 0.50225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012158536513646443, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011643580993016562, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019118253787358602, AUC: 0.4758649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011438403924306233, AUC: 0.5015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012043550014495849, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012455733219782512, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.03923085276285807, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011789478460947672, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011106468439102174, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010646061102549235, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015726275444030763, AUC: 0.514162\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011236273845036825, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011463644901911418, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011936803261439006, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002556024233500163, AUC: 0.4519345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001304982582728068, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105905572573344, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001253319501876831, AUC: 0.50225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005431477228800456, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013172676563262939, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012756712436676025, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011369795401891072, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008315267960230509, AUC: 0.49450625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010652623971303303, AUC: 0.5027505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013055435021718342, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012182055314381918, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009178260167439778, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012359701792399088, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001237366477648417, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001223189910252889, AUC: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class ratio\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d28e8f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1752b5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004499721844991048, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011086997588475546, AUC: 0.49224375000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001102787176767985, AUC: 0.4895015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100447694460551, AUC: 0.49403575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003900010029474894, AUC: 0.51350025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009926878809928894, AUC: 0.7083280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009727329611778259, AUC: 0.73949675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001177750587463379, AUC: 0.770915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024206600983937582, AUC: 0.5434865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001156400203704834, AUC: 0.626809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class oversampled \n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c14c3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 class undersampled  \n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec9e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c779b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  3 class weighted\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['weight'] = torch.from_numpy(weight).float()\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8692b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81115d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  3 class focal loss\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['reduction'] = 'mean'\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SoftmaxFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8eaef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d48470b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  3 class SMOTE\n",
    "# loss is really low but so is auc? going down \n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717e9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c453da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0967e450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
