{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c3c2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378c3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91dda12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "NUM_CLASSES_REDUCED = 2\n",
    "nums = (0, 1)\n",
    "ratio = (100, 1)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b57e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "          #   \"mean_40\", \"variance_40\",\n",
    "          #   \"mean_50\", \"variance_50\",\n",
    "             \"cap\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c2a1ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor() ]))  \n",
    "\n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()]))\n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "    \n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums)\n",
    "\n",
    "triplet_train_CIFAR10 = class_sampling.ForTripletLoss(reduced_train_CIFAR10, smote=False)\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED)\n",
    "triplet_smote_train_CIFAR10= class_sampling.ForTripletLoss(smote_train_CIFAR10, smote=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13159c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000   50]\n"
     ]
    }
   ],
   "source": [
    "targets = ratio_train_CIFAR10.labels \n",
    "\n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "print(class_count)\n",
    "\n",
    "weight = 1. / class_count\n",
    "\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "undersampler_smote = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * 50 * NUM_CLASSES_REDUCED), replacement=False)\n",
    "weight *= class_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de0d4cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.999 \n",
    "\n",
    "exp = np.empty_like(targets)\n",
    "for i, count in enumerate(class_count):\n",
    "    exp[targets==i] = count\n",
    "effective_weights = (1 - beta) / ( 1 - (beta ** torch.from_numpy(exp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af8cd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_smote_undersampled = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler_smote)\n",
    "\n",
    "train_loader_tripletloss = DataLoader(triplet_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_smote = DataLoader(triplet_smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e2e8c09f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001968830943107605, AUC: 0.6361665000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005542738735675812, AUC: 0.885664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005292963385581971, AUC: 0.8979130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004977731555700302, AUC: 0.9077430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032240989208221437, AUC: 0.38664699999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006063661277294159, AUC: 0.855749\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005620519518852234, AUC: 0.8903450000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005382097363471985, AUC: 0.898165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006873347997665405, AUC: 0.683021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005775293409824372, AUC: 0.867693\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004782587885856628, AUC: 0.9038889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00045187199115753177, AUC: 0.9145770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001451125681400299, AUC: 0.47390000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006520158350467682, AUC: 0.7826169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006005813181400299, AUC: 0.869262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005808910429477692, AUC: 0.8839089999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031172895431518556, AUC: 0.6896909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005760127604007721, AUC: 0.8859184999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005759528279304505, AUC: 0.890328\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005142679214477539, AUC: 0.9147670000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00219118869304657, AUC: 0.37103100000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005964301228523254, AUC: 0.8728420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005374825298786163, AUC: 0.902899\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000544882744550705, AUC: 0.900375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006955941677093506, AUC: 0.3322345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005800726115703583, AUC: 0.8929210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005413680970668792, AUC: 0.9098769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000514941155910492, AUC: 0.9136345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028511343002319335, AUC: 0.5939905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000573833703994751, AUC: 0.8740805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005463034212589264, AUC: 0.891224\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005187967419624329, AUC: 0.9042855000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011438738107681274, AUC: 0.351393\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005868805348873138, AUC: 0.8747965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005481753051280975, AUC: 0.9026535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005193610191345215, AUC: 0.9129545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015125332474708558, AUC: 0.429251\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005813519060611725, AUC: 0.8786199999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005619006156921387, AUC: 0.8882220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005398068428039551, AUC: 0.889149\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034982739686965944, AUC: 0.618384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941048204898835, AUC: 0.5019874999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936376094818116, AUC: 0.504489\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934143602848053, AUC: 0.501498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011465153098106384, AUC: 0.401948\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005416077971458435, AUC: 0.90378\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005318052768707275, AUC: 0.8990750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00048347583413124083, AUC: 0.9135070000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009435354471206665, AUC: 0.577937\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006637665331363678, AUC: 0.7110139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935104429721832, AUC: 0.498063\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933853924274444, AUC: 0.499049\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029533997774124146, AUC: 0.545541\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932328343391418, AUC: 0.499006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932272613048554, AUC: 0.499495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932301223278045, AUC: 0.499504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038374502658843996, AUC: 0.565988\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005938313007354736, AUC: 0.8660625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005148913562297821, AUC: 0.911864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00049067023396492, AUC: 0.921967\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001448841392993927, AUC: 0.5810709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005838621556758881, AUC: 0.9087160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005997822880744934, AUC: 0.8527130000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004748252034187317, AUC: 0.9237920000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004549663305282592, AUC: 0.44000300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005573736131191254, AUC: 0.8895370000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005237226784229279, AUC: 0.90449\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005322802066802979, AUC: 0.9018970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028102716207504272, AUC: 0.5378565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005881844162940979, AUC: 0.875293\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005218585729599, AUC: 0.910688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004902279824018478, AUC: 0.9198445000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018956378698349, AUC: 0.37419199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005440128147602082, AUC: 0.8936314999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004857201725244522, AUC: 0.917146\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046266598999500277, AUC: 0.9193445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001775826096534729, AUC: 0.554507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006459864675998687, AUC: 0.7649400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005977490544319153, AUC: 0.846576\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005115328282117844, AUC: 0.9130775000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS normal\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-4, 1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f8f1d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d211694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0020350258350372314, AUC: 0.6529435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018120235204696655, AUC: 0.6530240000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010293059689131113, AUC: 0.668872\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001586657702922821, AUC: 0.7140299999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010288638518293305, AUC: 0.736928\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013753380179405213, AUC: 0.739265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011858412239811207, AUC: 0.7685119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007551872730255127, AUC: 0.6657620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002787360668182373, AUC: 0.6875020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008929764428222091, AUC: 0.7381840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019024794697761536, AUC: 0.735419\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007972857817122251, AUC: 0.824272\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001780514895915985, AUC: 0.8002449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007506171086489564, AUC: 0.867892\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007656799077987671, AUC: 0.357271\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021916342973709105, AUC: 0.752474\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008010014285803727, AUC: 0.777604\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014365978837013245, AUC: 0.808182\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009090690513943681, AUC: 0.85076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002745326638221741, AUC: 0.7344769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00087369707860237, AUC: 0.755296\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038860321044921877, AUC: 0.5284265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023633899688720703, AUC: 0.633328\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008542614840524326, AUC: 0.6984360000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022927229404449464, AUC: 0.726839\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007604373006684945, AUC: 0.8144959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020038236379623414, AUC: 0.786016\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007008208180454993, AUC: 0.868596\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028474007844924926, AUC: 0.46099249999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017803604006767274, AUC: 0.764519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008698247066966378, AUC: 0.7581559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001656494915485382, AUC: 0.8066559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008072029520748275, AUC: 0.839692\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016342102885246277, AUC: 0.8376599999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007282870513672876, AUC: 0.8900119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007546884536743164, AUC: 0.5589405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002055235385894775, AUC: 0.744901\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000775805304915008, AUC: 0.811612\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017233411073684693, AUC: 0.821062\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007363590006117184, AUC: 0.886308\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017577680945396424, AUC: 0.8428729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006637122749470839, AUC: 0.9142239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031836777925491333, AUC: 0.546789\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015769251585006714, AUC: 0.5762609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012319207943902158, AUC: 0.647276\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00164082932472229, AUC: 0.681227\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009922157191108948, AUC: 0.7634439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014852824807167053, AUC: 0.657591\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011854888299609174, AUC: 0.7703840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001620980978012085, AUC: 0.4011355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002050992488861084, AUC: 0.682091\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00087490853420136, AUC: 0.6826319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001719328463077545, AUC: 0.746667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008866113243569242, AUC: 0.774852\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014194449186325073, AUC: 0.799355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001000340966747539, AUC: 0.840564\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005369068861007691, AUC: 0.36282\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003079697251319885, AUC: 0.721078\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008964818781403142, AUC: 0.774092\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002145679235458374, AUC: 0.7752165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007052172191145987, AUC: 0.856536\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021031206846237184, AUC: 0.8087389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006609652646918698, AUC: 0.8915519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009941160082817078, AUC: 0.43657100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020752947330474852, AUC: 0.606679\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009073623649702214, AUC: 0.6652999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016860944032669068, AUC: 0.724351\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009311184880904632, AUC: 0.7699480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015081595778465272, AUC: 0.753126\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010010608156571293, AUC: 0.8088559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022467935085296632, AUC: 0.48797050000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00310284948348999, AUC: 0.668835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000986019807434318, AUC: 0.667004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025377326011657717, AUC: 0.6702115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008842649512767497, AUC: 0.672464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002289165735244751, AUC: 0.6925410000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008490912293517354, AUC: 0.703512\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005732634544372559, AUC: 0.329512\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028723956346511843, AUC: 0.533339\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009654786346461809, AUC: 0.55228\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023563352823257446, AUC: 0.597919\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008797403377028975, AUC: 0.632896\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021936622858047487, AUC: 0.672167\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008376849055437758, AUC: 0.715676\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0069734807014465335, AUC: 0.368532\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002823939800262451, AUC: 0.646652\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009624598078927634, AUC: 0.62952\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002626596212387085, AUC: 0.657871\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009243738612547369, AUC: 0.645408\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002363510370254517, AUC: 0.6671870000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008907292109204106, AUC: 0.658612\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01959215545654297, AUC: 0.4975015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003949219942092895, AUC: 0.42127400000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012388900857381892, AUC: 0.433456\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033795143365859984, AUC: 0.43859200000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010923387363350186, AUC: 0.459212\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003314336895942688, AUC: 0.494413\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010604097617163074, AUC: 0.516712\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006981277704238891, AUC: 0.44336049999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003990621566772461, AUC: 0.579466\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012205294451196817, AUC: 0.5949880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031918243169784546, AUC: 0.579638\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010167563710785886, AUC: 0.5940840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00305902361869812, AUC: 0.598644\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000988440142234437, AUC: 0.61314\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00259570038318634, AUC: 0.4063665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003608007311820984, AUC: 0.631018\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011421820519740998, AUC: 0.637616\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00297004508972168, AUC: 0.621098\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010059384630974567, AUC: 0.6293920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002896303415298462, AUC: 0.6387049999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009705834248389053, AUC: 0.6512720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004442177295684815, AUC: 0.589298\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031884167194366456, AUC: 0.5733969999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010185218436440618, AUC: 0.582776\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029089962244033814, AUC: 0.595618\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009547152844174663, AUC: 0.607064\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025704939365386965, AUC: 0.5802565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009083451034667174, AUC: 0.602648\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024201889038085937, AUC: 0.5633489999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032470020055770875, AUC: 0.5253049999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010822141716385833, AUC: 0.510692\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029186619520187376, AUC: 0.5598425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009985440167238806, AUC: 0.553948\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027000287771224976, AUC: 0.585116\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009537057954781126, AUC: 0.581984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007307032644748687, AUC: 0.5528839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028895152807235716, AUC: 0.542925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009890146070796221, AUC: 0.532636\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028696520328521727, AUC: 0.563835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009720361209243979, AUC: 0.5540879999999999\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.002572161316871643, AUC: 0.5555669999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009350464684833394, AUC: 0.54104\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003061597228050232, AUC: 0.6727259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032712137699127198, AUC: 0.501175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010851466581040974, AUC: 0.507844\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030594792366027833, AUC: 0.565909\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010099006753119797, AUC: 0.566432\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026028387546539306, AUC: 0.6164085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009181819367453013, AUC: 0.605664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032295290231704713, AUC: 0.5874085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014447757601737975, AUC: 0.7830085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009910186712104496, AUC: 0.8013079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012968756556510925, AUC: 0.8132029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010575488423651988, AUC: 0.838224\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014231621026992798, AUC: 0.83391\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008838533683873639, AUC: 0.857556\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023010921478271483, AUC: 0.6184615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002285633325576782, AUC: 0.5931979999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009434462063899725, AUC: 0.6067840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001766660749912262, AUC: 0.609324\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010508064287576345, AUC: 0.68238\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017600275278091431, AUC: 0.678952\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000930723851491319, AUC: 0.753276\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008264895677566528, AUC: 0.722729\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022826236486434936, AUC: 0.628862\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009283946637101102, AUC: 0.6440279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017910045385360718, AUC: 0.721519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008845281027404978, AUC: 0.744008\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012057945728302002, AUC: 0.750984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014780547949347165, AUC: 0.7656240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012148174643516541, AUC: 0.522062\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002543883204460144, AUC: 0.621345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008810781550104961, AUC: 0.6541319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001977275609970093, AUC: 0.691835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008549915881145119, AUC: 0.724972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019476459622383118, AUC: 0.737373\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008094259886310833, AUC: 0.780776\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021162036657333375, AUC: 0.5721745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003817723274230957, AUC: 0.66016\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011155160027556121, AUC: 0.7107840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020452228784561158, AUC: 0.7379979999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008112106911174142, AUC: 0.793364\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017773056626319886, AUC: 0.766284\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008284972573049588, AUC: 0.821516\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017827187776565552, AUC: 0.6163075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017084128856658936, AUC: 0.754659\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009072887661433456, AUC: 0.740056\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001528137445449829, AUC: 0.7892379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009600773520103776, AUC: 0.779468\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012882382869720459, AUC: 0.810644\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001153135436153648, AUC: 0.81358\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037656874656677246, AUC: 0.452172\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023451154232025147, AUC: 0.582245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009036259268327515, AUC: 0.617444\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018625113368034363, AUC: 0.589104\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009977885922140414, AUC: 0.6570480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017853498458862305, AUC: 0.5887855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010357086061693653, AUC: 0.66054\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01560137939453125, AUC: 0.6343730000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028253393173217774, AUC: 0.663745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009125846218188653, AUC: 0.684036\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022675477266311645, AUC: 0.71926\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008191051172774912, AUC: 0.756804\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017850410342216492, AUC: 0.7479035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008286885253273614, AUC: 0.823436\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020437868237495422, AUC: 0.4189435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001946687161922455, AUC: 0.689937\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009012014192003424, AUC: 0.698744\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002036372900009155, AUC: 0.781695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007880830274212479, AUC: 0.790472\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018148719072341919, AUC: 0.8082389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007748577208949788, AUC: 0.825388\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009956028461456299, AUC: 0.44076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002909269690513611, AUC: 0.527169\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010032688825400455, AUC: 0.5258\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002381338834762573, AUC: 0.6424425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008805383017083796, AUC: 0.681396\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019694555997848513, AUC: 0.680312\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008679668723356606, AUC: 0.747904\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011478289365768432, AUC: 0.5513429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015690070390701294, AUC: 0.735552\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009977451243465489, AUC: 0.751236\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014823941588401795, AUC: 0.7757729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009676087363669187, AUC: 0.8232999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012902816534042358, AUC: 0.773401\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011978693903967885, AUC: 0.834192\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017196342945098877, AUC: 0.6530935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017311505079269408, AUC: 0.755336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00087021388888064, AUC: 0.7701239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001581154704093933, AUC: 0.8303649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007951241698448021, AUC: 0.877896\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00140901780128479, AUC: 0.830132\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008610952468496738, AUC: 0.9105319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008207473754882812, AUC: 0.5850905000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001566799521446228, AUC: 0.734977\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009772583348031092, AUC: 0.779044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015310409069061279, AUC: 0.7564679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009634758565242928, AUC: 0.809112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001389787495136261, AUC: 0.745485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00111627061473261, AUC: 0.8327479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004754784822463989, AUC: 0.357047\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013601994514465332, AUC: 0.793241\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099311668831523, AUC: 0.8212639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012491620779037475, AUC: 0.8528979999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010282922373844846, AUC: 0.913384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011775829792022704, AUC: 0.8487750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00112846937731351, AUC: 0.929694\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001162901520729065, AUC: 0.390592\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014512205719947814, AUC: 0.7006535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001166905612491145, AUC: 0.734584\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014830247163772584, AUC: 0.760512\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010046131259734087, AUC: 0.820292\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018969746232032777, AUC: 0.7984315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007515779792303496, AUC: 0.830184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01161625337600708, AUC: 0.6947829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016128405928611755, AUC: 0.827262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007764948402891065, AUC: 0.879404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014314205050468445, AUC: 0.8576819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007430357983944439, AUC: 0.943952\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001240342915058136, AUC: 0.8555699999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008630734925517942, AUC: 0.9618400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008358806371688843, AUC: 0.525097\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011456515789031983, AUC: 0.7513445000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016068354426044048, AUC: 0.8321679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018244597911834717, AUC: 0.8454060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006302436615460285, AUC: 0.921872\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013939085602760314, AUC: 0.869401\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006746186343676383, AUC: 0.962868\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002570245862007141, AUC: 0.38279450000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011375197768211365, AUC: 0.7442124999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017728431198266474, AUC: 0.726024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014149757623672485, AUC: 0.827115\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0009399736156262974, AUC: 0.86164\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015783169865608215, AUC: 0.8476490000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007382010094968989, AUC: 0.900012\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022254493236541746, AUC: 0.45958600000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001722590446472168, AUC: 0.711756\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000926684569014181, AUC: 0.7398999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012482515573501588, AUC: 0.8040189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011589160163213712, AUC: 0.9036\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013439211249351502, AUC: 0.8534299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007977260420523067, AUC: 0.939456\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016305903792381287, AUC: 0.399731\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001686771333217621, AUC: 0.728855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009271248459520907, AUC: 0.7390960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014608917236328125, AUC: 0.7759060000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010075070543011815, AUC: 0.814392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014361127018928529, AUC: 0.8223400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00092106845156096, AUC: 0.865092\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS ratio\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 1e-4, 5e-4, 5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "learning_rate_train_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                _, train_auc = metric_utils.auc_sigmoid(train_loader_ratio, network) \n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f2d0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f1ccb0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.005691136598587036, AUC: 0.7166170000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000588998019695282, AUC: 0.858235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005738674998283386, AUC: 0.8568835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000547566682100296, AUC: 0.864155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024350250959396364, AUC: 0.573198\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006682571172714234, AUC: 0.6593775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006437829434871674, AUC: 0.686171\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006789233386516571, AUC: 0.644326\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002518909811973572, AUC: 0.47361\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005086382031440735, AUC: 0.861209\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012186179161071777, AUC: 0.863998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014082735776901245, AUC: 0.857443\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012410336136817932, AUC: 0.405966\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007035685777664185, AUC: 0.39492449999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006275011897087097, AUC: 0.823426\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00088487908244133, AUC: 0.843707\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038197344541549684, AUC: 0.272004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006127392649650573, AUC: 0.8226249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005858472883701325, AUC: 0.839917\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005321000218391418, AUC: 0.8312549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013948385119438172, AUC: 0.4044835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006064178347587586, AUC: 0.8118620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005522237718105316, AUC: 0.836232\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005868338644504547, AUC: 0.7515910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026865620613098144, AUC: 0.3946695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005852950513362884, AUC: 0.8483590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005711329281330109, AUC: 0.8395555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005642571151256562, AUC: 0.816999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019509876370429992, AUC: 0.547291\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006756735444068909, AUC: 0.6119295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005742033720016479, AUC: 0.838554\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008748687803745269, AUC: 0.8437619999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025394957065582274, AUC: 0.633893\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006159831285476685, AUC: 0.7870250000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006629352569580078, AUC: 0.6932835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006924188435077668, AUC: 0.732165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017310360670089722, AUC: 0.5722005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006567726433277131, AUC: 0.765111\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005512991845607757, AUC: 0.8718000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005515256524085999, AUC: 0.861332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002939437508583069, AUC: 0.563555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006314303278923035, AUC: 0.706082\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012731393575668335, AUC: 0.648748\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022744207382202146, AUC: 0.716482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008252851068973542, AUC: 0.6032989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018195168375968933, AUC: 0.695447\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005728540658950806, AUC: 0.690301\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0057329535484313965, AUC: 0.684577\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001132174551486969, AUC: 0.400508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009344134032726288, AUC: 0.589728\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030594894886016846, AUC: 0.45559599999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022871530055999754, AUC: 0.565234\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010400078892707826, AUC: 0.5479905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931792199611664, AUC: 0.4980055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000715867280960083, AUC: 0.5413429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003392560124397278, AUC: 0.823512\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011575499176979065, AUC: 0.639311\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929903626441956, AUC: 0.49061400000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007422293424606323, AUC: 0.645052\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001354758381843567, AUC: 0.5095205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018678699135780334, AUC: 0.43198000000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011401005983352661, AUC: 0.712514\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014176532030105592, AUC: 0.713692\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005547755002975464, AUC: 0.7414639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008667053282260895, AUC: 0.6099764999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001788466989994049, AUC: 0.7582169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003058842658996582, AUC: 0.775567\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002191431522369385, AUC: 0.703206\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001195160150527954, AUC: 0.5285805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006799123585224152, AUC: 0.639027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001596990406513214, AUC: 0.667326\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002838463068008423, AUC: 0.6188389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012705932259559631, AUC: 0.4969279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010228420495986938, AUC: 0.781873\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015879844427108764, AUC: 0.793261\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017532774209976197, AUC: 0.66409\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004795622110366821, AUC: 0.319283\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010547017455101013, AUC: 0.577423\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003481634497642517, AUC: 0.618579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0066692931652069095, AUC: 0.6045940000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS oversampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52d8c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:14]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1541b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.002664048075675964, AUC: 0.35103100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008118733763694763, AUC: 0.39093300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007359330058097839, AUC: 0.42988400000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007080410122871399, AUC: 0.476849\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005757878303527832, AUC: 0.4101775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008514148592948913, AUC: 0.663575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006793743073940277, AUC: 0.6388240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006851064562797546, AUC: 0.599984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001174522042274475, AUC: 0.498127\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009026037454605102, AUC: 0.569188\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007478025853633881, AUC: 0.5202549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007039127349853516, AUC: 0.5297069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009038377702236175, AUC: 0.47197\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007971824407577515, AUC: 0.48862799999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007626304626464844, AUC: 0.494027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007488877177238464, AUC: 0.4992150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034414209127426147, AUC: 0.5659004999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008925645649433136, AUC: 0.602172\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007607427835464478, AUC: 0.5276164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007198239862918854, AUC: 0.509627\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013543962240219116, AUC: 0.49844299999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013344860672950744, AUC: 0.5426559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008173213005065918, AUC: 0.520809\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007598233222961425, AUC: 0.41432599999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009839903712272643, AUC: 0.54916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008945295512676239, AUC: 0.46157500000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007561358511447906, AUC: 0.477264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007167462408542633, AUC: 0.498566\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001238984763622284, AUC: 0.47820850000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009365684688091278, AUC: 0.374864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008359749019145966, AUC: 0.3937505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007830325365066528, AUC: 0.417717\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008678676128387451, AUC: 0.6484215000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009037670493125915, AUC: 0.427843\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007472198605537415, AUC: 0.4185105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007187381088733673, AUC: 0.446337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007866494059562683, AUC: 0.5668679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007531993091106415, AUC: 0.488706\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007111433148384094, AUC: 0.49999199999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007000260055065155, AUC: 0.515335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038360543251037597, AUC: 0.294262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009016560912132263, AUC: 0.505367\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088973104953766, AUC: 0.5929039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011563763022422791, AUC: 0.606266\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006004724025726318, AUC: 0.478491\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0041501855850219725, AUC: 0.427829\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030867953300476076, AUC: 0.404122\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026625494956970214, AUC: 0.434574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017807592153549195, AUC: 0.5781095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011875814199447633, AUC: 0.6403829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009512988924980164, AUC: 0.6518860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008521502912044526, AUC: 0.6516249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007921732366085052, AUC: 0.615845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008575631380081177, AUC: 0.646551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009552640914916992, AUC: 0.646782\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008757838904857636, AUC: 0.6430809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012470539212226868, AUC: 0.417589\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001292936086654663, AUC: 0.45840000000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013544394373893739, AUC: 0.473034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001133813738822937, AUC: 0.46033199999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010617620944976807, AUC: 0.63819\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010061159133911133, AUC: 0.634423\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009557005167007447, AUC: 0.6285029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009136235415935516, AUC: 0.61536\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018809281587600709, AUC: 0.6420239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001633428931236267, AUC: 0.638927\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014178273677825928, AUC: 0.634879\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012286875247955321, AUC: 0.6292800000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016851001381874085, AUC: 0.338503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010225446820259095, AUC: 0.45910999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011303470730781556, AUC: 0.5157820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001235684871673584, AUC: 0.543991\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002234177231788635, AUC: 0.47880599999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001637177050113678, AUC: 0.588411\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001996301531791687, AUC: 0.6044080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018329355120658875, AUC: 0.602075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010110423564910888, AUC: 0.42448849999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008554110527038574, AUC: 0.44827599999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008604062795639038, AUC: 0.453961\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008695433139801025, AUC: 0.447998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS undersampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdf3dafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.005942921638488769, AUC: 0.567106\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693460613489151, AUC: 0.5014949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01101493201633491, AUC: 0.498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932864785194397, AUC: 0.49949899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010916043460959255, AUC: 0.49879999999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932151615619659, AUC: 0.49949999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010851628886591089, AUC: 0.4994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002580095887184143, AUC: 0.52986\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005952333509922028, AUC: 0.8429355000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01112000550373946, AUC: 0.9091279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005518969893455505, AUC: 0.8701230000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009937099761301929, AUC: 0.9495319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005057350695133209, AUC: 0.883475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009651631164078665, AUC: 0.956386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009529277384281159, AUC: 0.445674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693619042634964, AUC: 0.49950399999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01036372478645627, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933236122131348, AUC: 0.500001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010543091297149658, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932159066200256, AUC: 0.499501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010651487182862688, AUC: 0.49989999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009260337352752686, AUC: 0.487573\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005576490461826325, AUC: 0.8761464999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012328089947747712, AUC: 0.9336720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005413561165332794, AUC: 0.8706175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010132806395540143, AUC: 0.944408\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005325608849525451, AUC: 0.8538310000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008870178484680628, AUC: 0.951468\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019186037778854371, AUC: 0.342846\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934821605682373, AUC: 0.5010004999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011260139222192293, AUC: 0.4997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932827532291412, AUC: 0.501499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011106325482377912, AUC: 0.5002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931948959827423, AUC: 0.501499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010999078372917553, AUC: 0.5002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009112397134304047, AUC: 0.40108900000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936869621276856, AUC: 0.509949\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011499585609624881, AUC: 0.5066360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934890151023865, AUC: 0.5000014999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011246178445249501, AUC: 0.507746\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932364404201507, AUC: 0.5014959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011072796677598858, AUC: 0.5083340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018737066984176635, AUC: 0.598061\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005399770736694336, AUC: 0.883624\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01103035563289529, AUC: 0.9507519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005523259043693542, AUC: 0.873992\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008670374638963453, AUC: 0.94526\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005198828279972076, AUC: 0.8742989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008047248048357445, AUC: 0.974004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005043919801712036, AUC: 0.4263775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936001479625702, AUC: 0.5019984999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011317932971633306, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693315178155899, AUC: 0.501499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011141563618537223, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932187080383301, AUC: 0.5009994999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011029406951205566, AUC: 0.49979999999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018713508248329163, AUC: 0.71854\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693433791399002, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010507272623553134, AUC: 0.4997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932770013809204, AUC: 0.49999949999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010628685007000914, AUC: 0.4997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932145059108735, AUC: 0.49999949999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010707831772247163, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086933732032776, AUC: 0.542975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931445598602295, AUC: 0.500505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011061525262228334, AUC: 0.4981\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931356191635132, AUC: 0.501501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010978521722378117, AUC: 0.4988\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931386291980743, AUC: 0.5009994999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010924689061570876, AUC: 0.49870000000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008205283880233765, AUC: 0.5254369999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006925942599773407, AUC: 0.525311\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010304348988108116, AUC: 0.545482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006537875235080719, AUC: 0.714302\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010759260807887163, AUC: 0.813016\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006208569407463074, AUC: 0.781027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01163209867949533, AUC: 0.837336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004449025392532348, AUC: 0.7239549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006821617186069488, AUC: 0.616701\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010066702861597042, AUC: 0.743002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006269781291484833, AUC: 0.7886060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010905323158396353, AUC: 0.8315520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006448910832405091, AUC: 0.7666895000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010326671883611397, AUC: 0.861054\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0040985639095306396, AUC: 0.685927\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006949299871921539, AUC: 0.502008\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009919748282668615, AUC: 0.522132\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006949068605899811, AUC: 0.502487\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00996117352259041, AUC: 0.511858\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000694685697555542, AUC: 0.5019815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010000678206434344, AUC: 0.510776\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000692509263753891, AUC: 0.604922\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006583554148674011, AUC: 0.7543150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01235714848678891, AUC: 0.799852\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006510149836540222, AUC: 0.7518015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012042915136507243, AUC: 0.874788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006351345181465149, AUC: 0.769869\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012874028163381143, AUC: 0.9042319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006502100229263306, AUC: 0.305219\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006949509084224701, AUC: 0.50101\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010219396942912943, AUC: 0.5192399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951335966587066, AUC: 0.501008\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010241817035297356, AUC: 0.510254\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006955615878105163, AUC: 0.4960495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010255760497385913, AUC: 0.500864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001411342978477478, AUC: 0.419446\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931746304035186, AUC: 0.50055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010877399373762678, AUC: 0.511688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932973265647888, AUC: 0.500497\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010872851610183715, AUC: 0.5019\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933660507202149, AUC: 0.500949\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010869365068945555, AUC: 0.5025999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008656686842441559, AUC: 0.633576\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006243113279342651, AUC: 0.8167039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01079183289320162, AUC: 0.8645280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006164305210113525, AUC: 0.8412700000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0105886443652729, AUC: 0.900088\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006011280715465545, AUC: 0.8429160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010646376291123947, AUC: 0.912704\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011180292367935182, AUC: 0.49661700000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006740418672561646, AUC: 0.6875190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010940170819216435, AUC: 0.71441\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006196146905422211, AUC: 0.823889\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01288879629408959, AUC: 0.8183360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006056763827800751, AUC: 0.852226\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012128729100274568, AUC: 0.88796\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017505425810813904, AUC: 0.464355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006939280927181244, AUC: 0.5186175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01141159445932596, AUC: 0.5750519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943604052066803, AUC: 0.501238\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011387146687743687, AUC: 0.5670279999999999\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006944212913513184, AUC: 0.5038005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0113616940054563, AUC: 0.5663979999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007881028413772582, AUC: 0.4730995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000692995399236679, AUC: 0.500478\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010630261945252371, AUC: 0.4824239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933453977108002, AUC: 0.5038505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010637967291444835, AUC: 0.503172\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934205293655395, AUC: 0.5003850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010647759791648033, AUC: 0.503188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 Class Weighted Loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 1e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['pos_weight'] = torch.tensor([weight[1]])\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                _, train_auc = metric_utils.auc_sigmoid(train_loader_ratio, network) \n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9eed2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7c5a840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001968830943107605, AUC: 0.6361665000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005302040874958038, AUC: 0.8983949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000634994238615036, AUC: 0.825839\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000631784051656723, AUC: 0.8181655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032240989208221437, AUC: 0.38664699999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933937072753906, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931578814983368, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931533217430115, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006873347997665405, AUC: 0.683021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005754781067371369, AUC: 0.821701\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006926852166652679, AUC: 0.7723930000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008418424427509307, AUC: 0.7373959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001451125681400299, AUC: 0.47390000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006959674656391144, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931877434253693, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931476294994354, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031172895431518556, AUC: 0.6896909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931331753730774, AUC: 0.501499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931317448616028, AUC: 0.5010024999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931581795215606, AUC: 0.497504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00219118869304657, AUC: 0.37103100000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007095436155796051, AUC: 0.6998199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005958462357521057, AUC: 0.835952\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00048375831544399264, AUC: 0.867326\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006955941677093506, AUC: 0.3322345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006323525309562683, AUC: 0.7104299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000527073860168457, AUC: 0.879102\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006032899916172028, AUC: 0.8253775000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028511343002319335, AUC: 0.5939905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931498348712921, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931465566158294, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931473016738892, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011438738107681274, AUC: 0.351393\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931806802749634, AUC: 0.498501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931719779968261, AUC: 0.49950000000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932362616062165, AUC: 0.49650200000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015125332474708558, AUC: 0.429251\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006255557537078858, AUC: 0.7437885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006174995303153992, AUC: 0.8239935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00067621049284935, AUC: 0.8196395000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034982739686965944, AUC: 0.618384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006882179677486419, AUC: 0.601931\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006676130592823029, AUC: 0.737554\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000661391943693161, AUC: 0.7826115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011465153098106384, AUC: 0.401948\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006201351284980774, AUC: 0.8381440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006041123569011688, AUC: 0.8530135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006021824777126312, AUC: 0.853889\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009435354471206665, AUC: 0.577937\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006610359847545624, AUC: 0.7537075000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006470790207386016, AUC: 0.7964849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006215363442897796, AUC: 0.8437495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029533997774124146, AUC: 0.545541\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006949090659618377, AUC: 0.4848730000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945248544216156, AUC: 0.48343100000000017\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943772435188294, AUC: 0.48298300000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038374502658843996, AUC: 0.565988\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000669541448354721, AUC: 0.7168109999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006380274295806885, AUC: 0.8015749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006226620674133301, AUC: 0.8302339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001448841392993927, AUC: 0.5810709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006441689431667328, AUC: 0.787606\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000623123288154602, AUC: 0.8273805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006155709624290466, AUC: 0.8330679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004549663305282592, AUC: 0.44000300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006320518553256988, AUC: 0.781259\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006132028102874755, AUC: 0.8322565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006064126789569854, AUC: 0.845758\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028102716207504272, AUC: 0.5378565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006557497084140778, AUC: 0.7658349999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006359571814537049, AUC: 0.8117945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006210537850856781, AUC: 0.8314429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018956378698349, AUC: 0.37419199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006342944204807281, AUC: 0.800203\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006094975471496582, AUC: 0.8436115000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005961551070213318, AUC: 0.8591875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001775826096534729, AUC: 0.554507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934720873832703, AUC: 0.526205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006917890012264252, AUC: 0.5641824999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006756187379360199, AUC: 0.6784055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019287337064743042, AUC: 0.518386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005559384226799011, AUC: 0.8935555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005239886045455932, AUC: 0.8936890000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006596373319625855, AUC: 0.7825115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003408196210861206, AUC: 0.7021970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006177656352519989, AUC: 0.85033\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005146624445915222, AUC: 0.8974850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005963504612445832, AUC: 0.8737524999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033094873428344725, AUC: 0.46274150000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006379089653491974, AUC: 0.8073075000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005224249064922332, AUC: 0.8907775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000467333048582077, AUC: 0.9046219999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007158293724060058, AUC: 0.573209\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009776136577129364, AUC: 0.8127230000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006105627715587616, AUC: 0.8978240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005587335526943207, AUC: 0.9086860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004602427005767822, AUC: 0.536975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006775906085968018, AUC: 0.6486004999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006156192719936371, AUC: 0.8224370000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006300158798694611, AUC: 0.8190590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023244422674179076, AUC: 0.5137155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005509713888168335, AUC: 0.8727690000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000692750483751297, AUC: 0.711758\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005922922790050506, AUC: 0.870213\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004095608711242676, AUC: 0.30450750000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005953157246112823, AUC: 0.849583\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006098766028881072, AUC: 0.8494235000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005180746614933014, AUC: 0.8935274999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006330698013305664, AUC: 0.49924100000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006022027432918549, AUC: 0.8478800000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005938524603843689, AUC: 0.866407\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005472795665264129, AUC: 0.8919670000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020437593460083006, AUC: 0.4564695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006749367415904998, AUC: 0.7311589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005225082039833069, AUC: 0.893719\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005687133967876434, AUC: 0.8766875000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025592957735061646, AUC: 0.6610935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005815093517303467, AUC: 0.8721304999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006237394511699677, AUC: 0.8247240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005508310496807098, AUC: 0.8849104999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS SMOTE\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-3, 1e-4, 1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2899080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b422fee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001968830943107605, AUC: 0.6361665000000001\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'RandomUnderSampler' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 21\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid_with_smote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_smote_undersampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     23\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:51\u001b[0m, in \u001b[0;36mtrain_sigmoid_with_smote\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m     network.train()\n\u001b[0;32m---> 51\u001b[0m     for batch_idx, (data, target, smote_label) in enumerate(train_loader):\n\u001b[1;32m     52\u001b[0m         optimizer.zero_grad()\n\u001b[1;32m     53\u001b[0m         output = network(data)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:624\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/sampler.py:254\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m    253\u001b[0m idx_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 254\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler:\n\u001b[1;32m    255\u001b[0m     batch[idx_in_batch] \u001b[38;5;241m=\u001b[39m idx\n\u001b[1;32m    256\u001b[0m     idx_in_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'RandomUnderSampler' object is not iterable"
     ]
    }
   ],
   "source": [
    "# 2 CLASS SMOTE with undersampling\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-3, 1e-4, 1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_with_smote(epoch, train_loader_smote_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled_smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edcf4753",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfecae8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.000964056670665741, AUC: 0.520941\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002801976799964905, AUC: 0.603904\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023315074443817137, AUC: 0.6346155000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001935368299484253, AUC: 0.6770020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015654934048652648, AUC: 0.5420925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012076606750488282, AUC: 0.6018205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006806973814964295, AUC: 0.745674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006647063195705414, AUC: 0.8141400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007722229480743408, AUC: 0.477479\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002731645941734314, AUC: 0.535709\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00225200891494751, AUC: 0.597371\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006625505685806275, AUC: 0.812553\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012214133143424988, AUC: 0.549326\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002234416365623474, AUC: 0.6193310000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015321336984634398, AUC: 0.7092240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010821450352668763, AUC: 0.7708145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015485901832580566, AUC: 0.640261\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00248842990398407, AUC: 0.646152\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020163779258728027, AUC: 0.70584\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018269655704498291, AUC: 0.763082\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0039055988788604737, AUC: 0.49889\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002461014151573181, AUC: 0.580205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001619136333465576, AUC: 0.664839\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013440817594528199, AUC: 0.753778\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023231953382492065, AUC: 0.617685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002575545787811279, AUC: 0.6123350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024801415205001833, AUC: 0.6435219999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002596958160400391, AUC: 0.6848249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011064586639404296, AUC: 0.408165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002185407042503357, AUC: 0.6438839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013983224034309386, AUC: 0.710449\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011335475444793702, AUC: 0.776975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013216559290885924, AUC: 0.418529\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002269905686378479, AUC: 0.651465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018026745319366455, AUC: 0.685593\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001050930440425873, AUC: 0.778413\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015292782187461852, AUC: 0.402582\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026281837224960327, AUC: 0.609267\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002344490885734558, AUC: 0.695286\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011545369625091553, AUC: 0.7973430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005974745273590088, AUC: 0.671063\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011837352514266967, AUC: 0.832149\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033103182315826415, AUC: 0.815026\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010485939979553223, AUC: 0.886463\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008579914093017577, AUC: 0.664835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020294631123542787, AUC: 0.672458\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016899308562278747, AUC: 0.7507329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012150493860244751, AUC: 0.752828\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005069618225097656, AUC: 0.6031975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005457256734371185, AUC: 0.875129\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008250180780887603, AUC: 0.888025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001563160240650177, AUC: 0.87826\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004804236888885498, AUC: 0.620153\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007836443185806275, AUC: 0.859518\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012541677951812744, AUC: 0.877231\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008345756530761719, AUC: 0.896647\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004910669803619385, AUC: 0.3238765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001646380603313446, AUC: 0.832295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004047170430421829, AUC: 0.9109000000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012764493227005005, AUC: 0.901555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001648660123348236, AUC: 0.46896899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022269333600997924, AUC: 0.624269\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018476015329360962, AUC: 0.7049920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010357484221458434, AUC: 0.7927789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0042981312274932865, AUC: 0.3444795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015390984416007996, AUC: 0.635807\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013158419728279114, AUC: 0.74302\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010194579958915711, AUC: 0.802161\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019534590244293213, AUC: 0.5050060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006502495408058167, AUC: 0.834522\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011276235580444335, AUC: 0.8694999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008604837954044342, AUC: 0.882695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008654152750968933, AUC: 0.47114999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006010472178459167, AUC: 0.8440844999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005932100117206574, AUC: 0.840112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005578492283821106, AUC: 0.8600084999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006011764526367188, AUC: 0.29192850000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007463725209236145, AUC: 0.894598\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011363054513931275, AUC: 0.9006350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001186053991317749, AUC: 0.9026909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002790385961532593, AUC: 0.28177399999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009584041535854339, AUC: 0.876717\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014251053929328918, AUC: 0.8666870000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010076404511928558, AUC: 0.886673\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002548191428184509, AUC: 0.5849150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000725983738899231, AUC: 0.892412\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000749692440032959, AUC: 0.9002319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002381552219390869, AUC: 0.860951\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017568197250366212, AUC: 0.5848495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018065781593322753, AUC: 0.6032850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001494412362575531, AUC: 0.6493610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014485009312629699, AUC: 0.6941069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017455670833587647, AUC: 0.380466\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006353239417076111, AUC: 0.8375789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006757119297981263, AUC: 0.89476\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017343769073486328, AUC: 0.8776740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004781398296356201, AUC: 0.3929645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012769671082496644, AUC: 0.860072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010610344409942628, AUC: 0.8832749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008701026737689972, AUC: 0.8818980000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003943687677383423, AUC: 0.29812500000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001194875180721283, AUC: 0.8592709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973641812801361, AUC: 0.876852\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015443329215049744, AUC: 0.874314\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007970042228698731, AUC: 0.473208\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008883583843708039, AUC: 0.785266\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010213080644607543, AUC: 0.8004580000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006914535760879516, AUC: 0.8606260000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013984680771827699, AUC: 0.5980829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010630066990852357, AUC: 0.87939\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015172980427742005, AUC: 0.8825529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008741162419319152, AUC: 0.9022779999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011132168769836426, AUC: 0.638255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006350016891956329, AUC: 0.8586389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00045628657937049865, AUC: 0.885041\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009610064625740052, AUC: 0.8888339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033210359811782835, AUC: 0.532149\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014725556969642638, AUC: 0.717386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006170571446418762, AUC: 0.820031\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007253816723823547, AUC: 0.8562420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002552252888679504, AUC: 0.561223\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006334934532642364, AUC: 0.7846520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006227342188358306, AUC: 0.8049075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006103373765945435, AUC: 0.8282804999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037488518953323366, AUC: 0.40662200000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006263749301433563, AUC: 0.8425030000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006173044145107269, AUC: 0.851136\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000611768901348114, AUC: 0.8517030000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002070085525512695, AUC: 0.590348\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006544892489910126, AUC: 0.7352115000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006303447484970093, AUC: 0.807039\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006590245068073272, AUC: 0.7533475000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007899287045001984, AUC: 0.670127\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006518673598766327, AUC: 0.773551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006446815729141235, AUC: 0.7977499999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006375725269317626, AUC: 0.8204609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003895806074142456, AUC: 0.585514\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00063050776720047, AUC: 0.8090909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006110207438468934, AUC: 0.831422\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006125293970108033, AUC: 0.8279915000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012400510311126709, AUC: 0.5677885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006437472999095917, AUC: 0.7166265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006125587224960328, AUC: 0.8170925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006116727888584137, AUC: 0.8358605000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008119668662548065, AUC: 0.6165309999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000663476824760437, AUC: 0.7330094999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006388857960700989, AUC: 0.7984014999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006321916282176971, AUC: 0.8101265000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008972242474555969, AUC: 0.401691\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006314697563648224, AUC: 0.7883199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000612924724817276, AUC: 0.8110935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006028134226799012, AUC: 0.8303904999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003174308180809021, AUC: 0.5475655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006006564497947693, AUC: 0.8371485000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006060300767421722, AUC: 0.8393040000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005922211408615112, AUC: 0.8496634999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032957570552825926, AUC: 0.5027429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006688888967037201, AUC: 0.7446620000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000649306446313858, AUC: 0.799407\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006368065774440766, AUC: 0.8159695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004990001201629638, AUC: 0.574568\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006282609403133392, AUC: 0.811007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005937656462192536, AUC: 0.8476395000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006180797815322876, AUC: 0.829514\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031396812200546265, AUC: 0.4465475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005860375165939331, AUC: 0.8429595000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005630400776863099, AUC: 0.8658610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005787071883678436, AUC: 0.8695265000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011578769683837891, AUC: 0.524138\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006228411197662354, AUC: 0.8181649999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005917013585567475, AUC: 0.852026\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000583314299583435, AUC: 0.857508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013598071932792663, AUC: 0.575604\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000623369574546814, AUC: 0.8094774999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006079152524471283, AUC: 0.8312689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005867798924446106, AUC: 0.851155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006621781826019287, AUC: 0.6869320000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006283579170703888, AUC: 0.8204770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006013086438179016, AUC: 0.8470999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000654230535030365, AUC: 0.7931049999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003582522749900818, AUC: 0.3222775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000603306382894516, AUC: 0.8286339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005767458975315094, AUC: 0.8523155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005951729416847229, AUC: 0.841295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004272486925125122, AUC: 0.352216\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006581661105155945, AUC: 0.7652889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006009359955787658, AUC: 0.850306\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006298901736736298, AUC: 0.8256475000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018105419874191284, AUC: 0.361556\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006014188528060913, AUC: 0.845718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00064871546626091, AUC: 0.7989245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006860582828521728, AUC: 0.7044365000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005167312860488891, AUC: 0.689548\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006246030628681183, AUC: 0.8230239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000616847813129425, AUC: 0.8213605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006269767284393311, AUC: 0.8256890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014786293506622314, AUC: 0.672685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006157369017601014, AUC: 0.814877\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006302497088909149, AUC: 0.8039790000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006149878799915314, AUC: 0.8154145000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002059874653816223, AUC: 0.40853300000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00068011474609375, AUC: 0.6823330000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006191925704479217, AUC: 0.8193415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006898281872272492, AUC: 0.768114\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015746417045593261, AUC: 0.599588\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006058313250541687, AUC: 0.8403395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006321284174919128, AUC: 0.8063629999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005368283390998841, AUC: 0.8789685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019499664902687072, AUC: 0.3934055000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006134471297264099, AUC: 0.8270870000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006327263116836548, AUC: 0.8153245000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005203947126865387, AUC: 0.8805489999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00403324556350708, AUC: 0.461312\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006679792702198029, AUC: 0.683534\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006318261921405792, AUC: 0.7878465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006103304028511048, AUC: 0.8349360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00837464427947998, AUC: 0.612069\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007215316593647003, AUC: 0.8526210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005961912274360657, AUC: 0.879758\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006339927017688751, AUC: 0.889991\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001660290539264679, AUC: 0.404836\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005964293479919434, AUC: 0.8500190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000520930528640747, AUC: 0.8853995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006570290625095368, AUC: 0.8048350000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013040754199028016, AUC: 0.336091\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929050385951996, AUC: 0.49907549999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005870556533336639, AUC: 0.854798\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005775222182273865, AUC: 0.8666704999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0046026728153228755, AUC: 0.4506755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006478333175182343, AUC: 0.8009510000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006295593976974488, AUC: 0.8419\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000552111804485321, AUC: 0.8759259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008104130327701569, AUC: 0.5352680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006035432815551758, AUC: 0.8578510000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000624394953250885, AUC: 0.842024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005150584578514099, AUC: 0.885518\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010002752840518952, AUC: 0.360174\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004515965580940247, AUC: 0.8932749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00042538104951381683, AUC: 0.9054739999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00044597277045249937, AUC: 0.90937\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008473179340362549, AUC: 0.546756\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006158844828605651, AUC: 0.8141305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006052950620651245, AUC: 0.8328234999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006014370918273926, AUC: 0.8336780000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002391463041305542, AUC: 0.27507249999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006232262253761292, AUC: 0.794295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006221258342266083, AUC: 0.8096895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006113168895244598, AUC: 0.8371394999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001042124629020691, AUC: 0.49809800000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006381492614746094, AUC: 0.7790399999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006332163810729981, AUC: 0.7949014999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006188197433948517, AUC: 0.8151620000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027534217834472656, AUC: 0.48419999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006390801966190339, AUC: 0.8050705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006340892314910889, AUC: 0.8114160000000001\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006174682676792144, AUC: 0.8310925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007684100151062012, AUC: 0.4070105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006202857792377472, AUC: 0.8066605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006176858246326447, AUC: 0.816376\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006103067696094513, AUC: 0.830536\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013037946224212647, AUC: 0.638735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005673413872718811, AUC: 0.789875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005420814156532288, AUC: 0.822748\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005116104185581207, AUC: 0.8447049999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007415176630020142, AUC: 0.5973919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006263005435466766, AUC: 0.8075709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005980682075023651, AUC: 0.8442180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006044952571392059, AUC: 0.84839\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000744761973619461, AUC: 0.6458790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006299040615558624, AUC: 0.7886010000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006408950388431549, AUC: 0.7723139999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006115258336067199, AUC: 0.8350675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004379341125488281, AUC: 0.7136\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000694673866033554, AUC: 0.5032479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006274721026420594, AUC: 0.8050600000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006303495764732361, AUC: 0.806403\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001051507443189621, AUC: 0.6788770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006240522563457489, AUC: 0.8185950000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006258217394351959, AUC: 0.8162265000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006147584915161133, AUC: 0.8318750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003145597815513611, AUC: 0.32849399999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005807328820228577, AUC: 0.859862\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005515931844711304, AUC: 0.8816459999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005638036727905274, AUC: 0.8819124999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002209828734397888, AUC: 0.37480100000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006220133006572723, AUC: 0.8144359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000580600619316101, AUC: 0.8555065000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005769412815570832, AUC: 0.8537370000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00147938871383667, AUC: 0.4048084999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006753892004489899, AUC: 0.674155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00057561457157135, AUC: 0.85653\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005927586257457733, AUC: 0.8447100000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009204252660274506, AUC: 0.447871\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006151717901229859, AUC: 0.8439380000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000616810828447342, AUC: 0.8363420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005670543015003205, AUC: 0.8666775000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010123408734798432, AUC: 0.517424\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005845471024513245, AUC: 0.852753\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006098836064338684, AUC: 0.8404395000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005713619887828827, AUC: 0.861746\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014559485912322998, AUC: 0.270013\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006143341958522796, AUC: 0.8433350000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006261303126811981, AUC: 0.824942\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005711425244808197, AUC: 0.8712949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038044289350509645, AUC: 0.5068900000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006150248646736145, AUC: 0.8208705000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005802111327648162, AUC: 0.8512559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005595802068710327, AUC: 0.8737389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002538438320159912, AUC: 0.561335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006038925051689148, AUC: 0.8357004999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005928950309753418, AUC: 0.8454735000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000638547956943512, AUC: 0.8162300000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014429892301559449, AUC: 0.6502439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937412917613984, AUC: 0.49700799999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935389935970307, AUC: 0.497502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934589743614197, AUC: 0.497501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0068713886737823485, AUC: 0.35051299999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006097583770751954, AUC: 0.8509719999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006007438600063324, AUC: 0.8451359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005484432280063629, AUC: 0.879217\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020305522084236144, AUC: 0.646182\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005491954684257507, AUC: 0.880623\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000489090472459793, AUC: 0.8992000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005037916302680969, AUC: 0.8954055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034704625606536863, AUC: 0.556714\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005496560633182526, AUC: 0.8886424999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006694852411746979, AUC: 0.778717\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006735021770000458, AUC: 0.8038890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008687609195709229, AUC: 0.697226\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934575140476227, AUC: 0.494508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934167444705964, AUC: 0.493515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935551762580871, AUC: 0.49002650000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033097184896469117, AUC: 0.55142\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000563425362110138, AUC: 0.8683985000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006602745354175567, AUC: 0.795951\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005559330582618713, AUC: 0.8806409999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010429821610450744, AUC: 0.565547\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005676802098751069, AUC: 0.8580479999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005945636332035064, AUC: 0.8427125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005550395250320435, AUC: 0.8714235000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005781375408172608, AUC: 0.470508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006018178164958954, AUC: 0.8408599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005976891219615936, AUC: 0.837603\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005385911762714386, AUC: 0.8708565000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004720101118087769, AUC: 0.677619\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006284879446029663, AUC: 0.8156595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005255373418331146, AUC: 0.8623749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005224584341049194, AUC: 0.878071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004078102588653564, AUC: 0.3977945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006107538044452667, AUC: 0.8471705000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005206150114536285, AUC: 0.878519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006155983507633209, AUC: 0.8729149999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010403269290924073, AUC: 0.40722050000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005747822225093842, AUC: 0.8612580000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005320911109447479, AUC: 0.8737410000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006981034576892853, AUC: 0.7420190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001469210147857666, AUC: 0.5677145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006009528040885926, AUC: 0.854181\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005945510566234589, AUC: 0.860663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006010228097438812, AUC: 0.8389049999999998\n",
      "\n",
      "[['capped_smote', 2, (0, 1), (100, 1), 0.0001, 0.50759505, 0.006265930314522501, 0.61040725, 0.0010776797476624992, 0.67924135, 0.0017468956486024994, 0.7628925499999999, 0.002033609184422501, 1], ['capped_smote', 2, (0, 1), (100, 1), 0.0005, 0.4964657999999999, 0.018171485669459995, 0.79048295, 0.009654922238622502, 0.8300173999999998, 0.004849427915640002, 0.85560875, 0.0025598142761624992, 1], ['capped_smote', 2, (0, 1), (100, 1), 0.001, 0.47647890000000004, 0.015348474091940003, 0.8170016999999999, 0.007554903506809994, 0.845925, 0.005236349520799994, 0.8583597, 0.0031836061494100024, 1], ['capped_smote', 2, (0, 1), (100, 1), 0.0001, 0.5450153, 0.006673195071459999, 0.7764775, 0.0017506326953499976, 0.8157553, 0.00031615619421000225, 0.8223794, 0.000687683591039999, 5], ['capped_smote', 2, (0, 1), (100, 1), 0.0005, 0.5206072, 0.018375386227310005, 0.8179627999999999, 0.00044454733171000137, 0.8370781, 0.0004503068599899985, 0.8213291000000001, 0.001952023578889999, 5], ['capped_smote', 2, (0, 1), (100, 1), 0.001, 0.45619519999999997, 0.008366438191510001, 0.7787086000000001, 0.013311188866390006, 0.8438228999999999, 0.0012896288829899974, 0.8594877999999999, 0.0017180018573099975, 5], ['capped_smote', 2, (0, 1), (100, 1), 0.0001, 0.5485619999999999, 0.016573135572550007, 0.77070865, 0.008085907643352505, 0.8125772999999998, 0.00035419974701000304, 0.83140485, 0.00014068539205249928, 10], ['capped_smote', 2, (0, 1), (100, 1), 0.0005, 0.44123935000000014, 0.0124266505041025, 0.789303, 0.012118831247150002, 0.81347735, 0.011294180485452502, 0.8246765, 0.012233027606099995, 10], ['capped_smote', 2, (0, 1), (100, 1), 0.001, 0.55379455, 0.009754318009372499, 0.8209348999999999, 0.01221760866609, 0.8122996499999999, 0.012514219371502506, 0.8144152, 0.013603492902059996, 10]]\n"
     ]
    }
   ],
   "source": [
    "# 2 Class Capped SMOTE \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 5e-4, 1e-3]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_sigmoid_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for i in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[i][0]\n",
    "    auc_variance = cap_aucs[i][1]\n",
    "    cap = caps[i]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "606da0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = (col_names[0:14]))\n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f22d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 CLASS Focal Loss\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SigmoidFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d438d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5f6eb22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf2\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7924e1f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0008439761102199554, AUC: 0.458455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005637407302856445, AUC: 0.8840095000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004919547736644745, AUC: 0.9045030000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000604819118976593, AUC: 0.8537779999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009120811462402343, AUC: 0.447008\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006216726899147034, AUC: 0.834169\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005918767750263214, AUC: 0.8661705000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006442052721977234, AUC: 0.8375685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017059873938560486, AUC: 0.48120300000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005866606831550598, AUC: 0.8596909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005112299025058746, AUC: 0.8946060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006069133877754211, AUC: 0.857534\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013419885039329528, AUC: 0.46782999999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006542591452598572, AUC: 0.769544\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000525863379240036, AUC: 0.8919025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005154286921024323, AUC: 0.891442\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037963229417800905, AUC: 0.47896700000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929879784584045, AUC: 0.504478\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006900709867477417, AUC: 0.5465335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929939389228821, AUC: 0.503497\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013928800821304322, AUC: 0.5142115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933024823665619, AUC: 0.4989855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932156682014465, AUC: 0.498506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005740237534046173, AUC: 0.8587929999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012375463485717773, AUC: 0.48107500000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004606181383132935, AUC: 0.896311\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005868735015392303, AUC: 0.86551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004222252666950226, AUC: 0.91819\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0053819150924682615, AUC: 0.366182\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006149508655071259, AUC: 0.8272169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005483061969280243, AUC: 0.886965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005681749880313873, AUC: 0.8830720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007074347138404846, AUC: 0.6574070000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006563288867473602, AUC: 0.774603\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000604621946811676, AUC: 0.8585160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000479815199971199, AUC: 0.9047215000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013631030917167664, AUC: 0.3875775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005463899672031403, AUC: 0.888676\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005721072256565094, AUC: 0.8614010000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047562263906002043, AUC: 0.8999494999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008625339031219483, AUC: 0.7069070000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005811516046524048, AUC: 0.8619390000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005783504247665406, AUC: 0.874839\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005979859828948975, AUC: 0.858376\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012759427428245544, AUC: 0.643196\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006127350330352783, AUC: 0.8427414999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000601469099521637, AUC: 0.852891\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005777522027492523, AUC: 0.8698009999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001058193564414978, AUC: 0.5161815000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006364794671535492, AUC: 0.7862144999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000572220504283905, AUC: 0.8682300000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005678242444992066, AUC: 0.8655260000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017843746542930603, AUC: 0.44242500000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006265249252319335, AUC: 0.8153445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000587759643793106, AUC: 0.8590475000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005430217683315277, AUC: 0.8866439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00122032231092453, AUC: 0.435823\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005837898850440979, AUC: 0.862661\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005834003686904908, AUC: 0.86952\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005688999593257905, AUC: 0.8756729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002295554041862488, AUC: 0.627077\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006150263845920563, AUC: 0.8181645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006443795263767243, AUC: 0.7821485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005783058106899262, AUC: 0.8598480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022693157196044924, AUC: 0.40804999999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005997583866119385, AUC: 0.8455429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005273479819297791, AUC: 0.9028665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005568995475769043, AUC: 0.8803615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017738425135612487, AUC: 0.511952\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005892116129398346, AUC: 0.8646739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00056046062707901, AUC: 0.8753740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005555383265018463, AUC: 0.873375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0043174057006835935, AUC: 0.4102015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006530484855175018, AUC: 0.7553004999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000608791321516037, AUC: 0.839797\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006293816566467286, AUC: 0.8061519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022468689680099486, AUC: 0.566055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006239404082298279, AUC: 0.8191989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006275454163551331, AUC: 0.8220370000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000569666862487793, AUC: 0.8655599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008367579877376556, AUC: 0.5501579999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005451313853263855, AUC: 0.8439305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000485722616314888, AUC: 0.8825350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004906994104385376, AUC: 0.889801\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004625638484954834, AUC: 0.3794624999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006529110372066498, AUC: 0.7661179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006407046616077423, AUC: 0.7811130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006216191649436951, AUC: 0.8237065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017287462949752807, AUC: 0.6017085000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006270993053913116, AUC: 0.8204334999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005987875461578369, AUC: 0.8463005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006154116988182068, AUC: 0.8385174999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030951732397079467, AUC: 0.5251790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006382147669792176, AUC: 0.7769809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006129896640777588, AUC: 0.8140655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006174193918704986, AUC: 0.8078660000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005898470401763916, AUC: 0.6444259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000577654242515564, AUC: 0.784532\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005137502551078797, AUC: 0.8489789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004722211509943008, AUC: 0.881066\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016366830468177796, AUC: 0.518339\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006395434141159058, AUC: 0.7930195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006210331618785858, AUC: 0.846544\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006086625754833222, AUC: 0.867672\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008840167820453644, AUC: 0.569688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005580012202262879, AUC: 0.8301115000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004910331666469574, AUC: 0.865863\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004668349027633667, AUC: 0.880971\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005788650989532471, AUC: 0.583539\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931572258472442, AUC: 0.49403749999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693048894405365, AUC: 0.49749699999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930306851863861, AUC: 0.498513\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0043890209197998045, AUC: 0.316538\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006316217482089996, AUC: 0.8062765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006033726334571839, AUC: 0.8494105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006015035212039948, AUC: 0.850344\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006356547594070435, AUC: 0.3503795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006243184208869935, AUC: 0.8226040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006187717914581299, AUC: 0.8282345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006105678975582122, AUC: 0.8333065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# distance + capped loss\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_euclidean_distance(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"distance_capped_smote_fixed1\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd4a6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "639f921d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011436396837234497, AUC: 0.457495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005700503587722778, AUC: 0.8682060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005558769106864929, AUC: 0.877864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007304551005363465, AUC: 0.661598\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00553279709815979, AUC: 0.2802325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947735548019409, AUC: 0.499513\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006955170035362244, AUC: 0.501002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006960488855838776, AUC: 0.502988\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016392868161201478, AUC: 0.362875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006916555464267731, AUC: 0.5249240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006274392902851104, AUC: 0.8069995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005601176917552948, AUC: 0.8731455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008223848342895508, AUC: 0.448507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005955914556980133, AUC: 0.865737\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006267938613891601, AUC: 0.8331379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006058297157287598, AUC: 0.8535895000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008242251873016357, AUC: 0.6408520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005871844887733459, AUC: 0.8728810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005207555890083313, AUC: 0.8923355000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000615071564912796, AUC: 0.8436364999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000918371707201004, AUC: 0.536765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005651862323284149, AUC: 0.856358\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005426359176635742, AUC: 0.8685500000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005213096141815185, AUC: 0.8812189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013069388270378112, AUC: 0.58745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006328843832015991, AUC: 0.8024040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005452930629253387, AUC: 0.8818340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000638193815946579, AUC: 0.8249209999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001208219826221466, AUC: 0.470757\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006046959459781647, AUC: 0.8551559999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005713726878166199, AUC: 0.8785719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004810726493597031, AUC: 0.8969955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008590931296348572, AUC: 0.6383909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005886261761188507, AUC: 0.8380164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006248479187488556, AUC: 0.8089189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005951111316680909, AUC: 0.841516\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011240931153297423, AUC: 0.49242199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005780737400054932, AUC: 0.8566495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006102285087108612, AUC: 0.8440169999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006346866488456726, AUC: 0.833603\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000697275072336197, AUC: 0.7227359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005900128483772277, AUC: 0.857107\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005580009520053863, AUC: 0.8668195000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006286512315273285, AUC: 0.8103145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004994371175765991, AUC: 0.6705915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005795387029647827, AUC: 0.8694660000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005699180364608765, AUC: 0.8658590000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005729371905326843, AUC: 0.856299\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013795417547225952, AUC: 0.586665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006315724849700928, AUC: 0.8226810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000640956461429596, AUC: 0.8096565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000603638231754303, AUC: 0.849835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016411503553390504, AUC: 0.6515529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005710045099258423, AUC: 0.8691070000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005508889257907867, AUC: 0.877851\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918166279792785, AUC: 0.64317\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017172007560729982, AUC: 0.3578335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006278906464576721, AUC: 0.826955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000604762077331543, AUC: 0.8417924999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005280996263027191, AUC: 0.8909325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012478209137916565, AUC: 0.5741499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006026541888713837, AUC: 0.8499774999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005772899687290191, AUC: 0.8672360000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006183014512062073, AUC: 0.8365860000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006380985260009766, AUC: 0.450204\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004147695004940033, AUC: 0.904407\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003687274158000946, AUC: 0.9213450000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006014271676540374, AUC: 0.9060219999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016212204694747924, AUC: 0.41917399999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006098184287548065, AUC: 0.847912\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005784734785556793, AUC: 0.8702619999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006370393335819244, AUC: 0.79496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018048741817474365, AUC: 0.34690400000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005895836055278778, AUC: 0.863846\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005726675093173981, AUC: 0.8717030000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000542557567358017, AUC: 0.8950065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008693362474441528, AUC: 0.44686800000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936838328838349, AUC: 0.500504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943668127059936, AUC: 0.5069870000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948027908802032, AUC: 0.5090080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001239137589931488, AUC: 0.5158640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006417568922042847, AUC: 0.7925600000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000630193293094635, AUC: 0.8170269999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006180804669857025, AUC: 0.8292739999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00346296226978302, AUC: 0.6133879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005664035081863404, AUC: 0.7966279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005283989608287811, AUC: 0.834187\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000499327763915062, AUC: 0.8592339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003393764615058899, AUC: 0.45321\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940672397613525, AUC: 0.4938690000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938166320323944, AUC: 0.49643399999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937651038169861, AUC: 0.49497549999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008302388489246368, AUC: 0.504591\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000646837592124939, AUC: 0.7744329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006434661746025085, AUC: 0.7915279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006417382061481476, AUC: 0.801971\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000990877866744995, AUC: 0.39449\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006310530006885529, AUC: 0.7927445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006039515733718872, AUC: 0.8356080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005904605686664581, AUC: 0.8547595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014796666502952577, AUC: 0.4171665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006375111341476441, AUC: 0.8128610000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006182823181152344, AUC: 0.8273959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006178308725357056, AUC: 0.8265345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009021728932857514, AUC: 0.396864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006370321810245514, AUC: 0.81604\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006200593709945679, AUC: 0.842328\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006152957975864411, AUC: 0.8501570000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011793015599250794, AUC: 0.392324\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006257997751235962, AUC: 0.8159225000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006256788969039917, AUC: 0.81561\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006208104193210601, AUC: 0.8205710000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006731678247451783, AUC: 0.33828250000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006072148680686951, AUC: 0.844442\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006218261420726776, AUC: 0.819703\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006114244759082794, AUC: 0.8287840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011103346943855286, AUC: 0.5835480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006344992220401763, AUC: 0.808653\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000627922922372818, AUC: 0.8299200000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006316606104373932, AUC: 0.834324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cosine distance + capped loss\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_cosine_distance(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"cosine_distance_capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], 5]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab098399",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8326eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capped loss with everything capped + cosine distance \n",
    "momentum=0\n",
    "learning_rates = [5e-4]\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_cosine_distance(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.AllCappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"cosine_distance_all_capped\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecfcd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17af0e21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m embed_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(embed_network\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate[\u001b[38;5;241m0\u001b[39m], momentum\u001b[38;5;241m=\u001b[39mmomentum)\n\u001b[1;32m     19\u001b[0m linear_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(complete_network\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate[\u001b[38;5;241m1\u001b[39m], momentum\u001b[38;5;241m=\u001b[39mmomentum)\n\u001b[0;32m---> 20\u001b[0m _, auc \u001b[38;5;241m=\u001b[39m \u001b[43mmetric_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauc_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomplete_network\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     21\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m15\u001b[39m):\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/metric_utils.py:18\u001b[0m, in \u001b[0;36mauc_sigmoid\u001b[0;34m(test_loader, network, embeddings)\u001b[0m\n\u001b[1;32m     16\u001b[0m     test_losses, y_preds, y_true \u001b[38;5;241m=\u001b[39m inference\u001b[38;5;241m.\u001b[39mrun_inference_sigmoid(test_loader, network, embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     test_losses, y_preds, y_true \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_inference_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m network_auc \u001b[38;5;241m=\u001b[39m auc(y_preds, y_true)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTest set: Avg. loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnetwork_auc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)   \n",
      "File \u001b[0;32m~/Downloads/class-sample-research/inference.py:22\u001b[0m, in \u001b[0;36mrun_inference_sigmoid\u001b[0;34m(dataloader, network, embeddings)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings: \n\u001b[1;32m     21\u001b[0m     output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 22\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fn(\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m()\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     23\u001b[0m pred\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39msigmoid(output\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     24\u001b[0m y_preds\u001b[38;5;241m.\u001b[39mextend(pred\u001b[38;5;241m.\u001b[39mfloat())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'squeeze'"
     ]
    }
   ],
   "source": [
    "# 2 class triplet loss no ratio \n",
    "# no smote \n",
    "\n",
    "# note: sometimes can get a very high accuracy but may diverge\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(1e-7, 1e-5)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(3): \n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(complete_network.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(15):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss, embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "        for epoch in range(50):\n",
    "            _, _ = train.train_linear_probe(epoch, train_loader_reduced, complete_network, linear_optimizer, verbose=False)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"triplet_loss\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], \n",
    "           auc_mean[i][4], auc_variance[i][4],\n",
    "           auc_mean[i][5], auc_variance[i][5],\n",
    "           None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8afa13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f34cc01",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m embed_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(embed_network\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate[\u001b[38;5;241m0\u001b[39m], momentum\u001b[38;5;241m=\u001b[39mmomentum)\n\u001b[1;32m     17\u001b[0m linear_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(linear_probe\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate[\u001b[38;5;241m1\u001b[39m], momentum\u001b[38;5;241m=\u001b[39mmomentum)\n\u001b[0;32m---> 18\u001b[0m _, auc \u001b[38;5;241m=\u001b[39m \u001b[43mmetric_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauc_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomplete_network\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     19\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/metric_utils.py:18\u001b[0m, in \u001b[0;36mauc_sigmoid\u001b[0;34m(test_loader, network, embeddings)\u001b[0m\n\u001b[1;32m     16\u001b[0m     test_losses, y_preds, y_true \u001b[38;5;241m=\u001b[39m inference\u001b[38;5;241m.\u001b[39mrun_inference_sigmoid(test_loader, network, embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     test_losses, y_preds, y_true \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_inference_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m network_auc \u001b[38;5;241m=\u001b[39m auc(y_preds, y_true)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTest set: Avg. loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnetwork_auc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)   \n",
      "File \u001b[0;32m~/Downloads/class-sample-research/inference.py:22\u001b[0m, in \u001b[0;36mrun_inference_sigmoid\u001b[0;34m(dataloader, network, embeddings)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings: \n\u001b[1;32m     21\u001b[0m     output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 22\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fn(\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m()\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     23\u001b[0m pred\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39msigmoid(output\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     24\u001b[0m y_preds\u001b[38;5;241m.\u001b[39mextend(pred\u001b[38;5;241m.\u001b[39mfloat())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'squeeze'"
     ]
    }
   ],
   "source": [
    "# triplet loss with ratio \n",
    "# need to make a new train loader if running this \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(1e-7, 1e-3)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(2):\n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(linear_probe.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss, complete_network.embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "            \n",
    "    #    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network)\n",
    "        for epoch in range(50):\n",
    "            _, _ = train.train_linear_probe(epoch, train_loader_reduced, complete_network.embed_network, complete_network.linear_probe, linear_optimizer, verbose=False)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"triplet_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], \n",
    "           auc_mean[i][4], auc_variance[i][4],\n",
    "           auc_mean[i][5], auc_variance[i][5],\n",
    "           None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10048fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a99dbdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.002908367872238159, AUC: 0.337541\n",
      "\n",
      "Train triplet loss: 224.1131993069011\n",
      "Train triplet loss: 14.503459689343812\n",
      "Train triplet loss: 2.3045721103431314\n",
      "Train triplet loss: 0.9209006834941306\n",
      "Train triplet loss: 1.294924501021197\n",
      "Train triplet loss: 0.7550809907305772\n",
      "Train triplet loss: 0.9270113535747406\n",
      "Train triplet loss: 0.7487862869432778\n",
      "Train triplet loss: 0.2525857892006066\n",
      "Train triplet loss: 0.45942818321240175\n",
      "Train triplet loss: 0.1847958750785536\n",
      "Train triplet loss: 0.10644819022743565\n",
      "Train triplet loss: 0.2511055499884733\n",
      "Train triplet loss: 0.08564935216478481\n",
      "Train triplet loss: 0.05430815022462492\n",
      "\n",
      "Test set: Avg. loss: 0.0006781926453113556, AUC: 0.6198600000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000667210340499878, AUC: 0.6874805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006613142788410187, AUC: 0.727465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006528576910495758, AUC: 0.7565990000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006457682251930236, AUC: 0.7813000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006435786485671997, AUC: 0.7982045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00063743194937706, AUC: 0.8135109999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006340896785259247, AUC: 0.8213185000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006330857276916503, AUC: 0.82793\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006300545036792755, AUC: 0.8336909999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006289317309856415, AUC: 0.8387704999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006278052628040314, AUC: 0.8363659999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006252728998661041, AUC: 0.8442915000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006271203756332398, AUC: 0.8422369999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006220865249633789, AUC: 0.8471285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006182534992694855, AUC: 0.8516745000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006221278607845307, AUC: 0.846864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006210695803165435, AUC: 0.847746\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006165607273578644, AUC: 0.8519945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006245328783988953, AUC: 0.8481225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006157533824443817, AUC: 0.8558350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006128311753273011, AUC: 0.8548819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006176116168498993, AUC: 0.852943\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000617037147283554, AUC: 0.8497484999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000616768479347229, AUC: 0.8521710000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006099693775177002, AUC: 0.8564430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006170134842395783, AUC: 0.852152\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006135198473930359, AUC: 0.852912\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006147994995117187, AUC: 0.850182\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006121959686279297, AUC: 0.8537490000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000610977053642273, AUC: 0.8534640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006110379695892334, AUC: 0.851584\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006099945902824402, AUC: 0.8538810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006064651012420654, AUC: 0.8560080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000615050345659256, AUC: 0.8503280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006082983613014221, AUC: 0.8533799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006099414825439454, AUC: 0.8534634999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006086739301681518, AUC: 0.8548550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006083137094974518, AUC: 0.8557539999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006070252954959869, AUC: 0.8533209999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006083998680114746, AUC: 0.853526\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006079462468624115, AUC: 0.8586124999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006028076410293579, AUC: 0.8565580000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006034516990184784, AUC: 0.8588004999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006033603847026825, AUC: 0.8575499999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006091804802417756, AUC: 0.8526915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006061289310455323, AUC: 0.8565900000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006040626168251037, AUC: 0.857898\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006060695052146911, AUC: 0.8561389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006112787127494812, AUC: 0.8514895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01092047882080078, AUC: 0.603893\n",
      "\n",
      "Train triplet loss: 452.12765688805064\n",
      "Train triplet loss: 6.45787276735731\n",
      "Train triplet loss: 3.0656133890151978\n",
      "Train triplet loss: 0.2302081285008959\n",
      "Train triplet loss: 0.02136805786448679\n",
      "Train triplet loss: 0.054735511351542865\n",
      "Train triplet loss: 0.02124925365873203\n",
      "Train triplet loss: 0.030880134576445173\n",
      "Train triplet loss: 0.02046784815514923\n",
      "Train triplet loss: 0.039242106638136945\n",
      "Train triplet loss: 0.03913900806645679\n",
      "Train triplet loss: 0.025067255375491586\n",
      "Train triplet loss: 0.025249039671223633\n",
      "Train triplet loss: 0.037394101832323015\n",
      "Train triplet loss: 0.044491596662314836\n",
      "\n",
      "Test set: Avg. loss: 0.0006932713389396668, AUC: 0.5144200000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006905783712863922, AUC: 0.5346809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006818779110908509, AUC: 0.6050895000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006741962730884552, AUC: 0.6493655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006682798564434052, AUC: 0.6866669999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006552905142307282, AUC: 0.7275415000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006689422726631164, AUC: 0.695633\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000655550241470337, AUC: 0.739388\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006501212418079376, AUC: 0.7540140000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006457582712173462, AUC: 0.7657290000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006593451201915741, AUC: 0.7342860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006622738242149353, AUC: 0.7262655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006521277129650116, AUC: 0.754829\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006505903303623199, AUC: 0.7553169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006439104676246643, AUC: 0.772999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006596936285495758, AUC: 0.737443\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006716005206108093, AUC: 0.685984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006418732404708862, AUC: 0.7754000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006582367122173309, AUC: 0.745029\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006541431844234466, AUC: 0.752995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006401453018188476, AUC: 0.7798714999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006395943760871887, AUC: 0.7790009999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006476590931415557, AUC: 0.7709539999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006405056118965149, AUC: 0.7791195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006321202218532563, AUC: 0.7952180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006394976377487182, AUC: 0.7841159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006506598889827729, AUC: 0.7586590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006421497762203217, AUC: 0.7821269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006424221694469452, AUC: 0.7821339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006299974918365478, AUC: 0.7962550000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006382219791412353, AUC: 0.7829700000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006518733799457551, AUC: 0.7534680000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006516874730587006, AUC: 0.7486779999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006413909196853638, AUC: 0.7728335000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006351262629032135, AUC: 0.7854275000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006418537199497223, AUC: 0.7764669999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006416624784469604, AUC: 0.766608\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006389599740505218, AUC: 0.777683\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000644479364156723, AUC: 0.7589885000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006276929080486298, AUC: 0.7970955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000639805018901825, AUC: 0.76653\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006359494626522064, AUC: 0.7759739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006373425722122193, AUC: 0.771996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006273976266384125, AUC: 0.7874750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000633959174156189, AUC: 0.77834\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006295301020145417, AUC: 0.7984789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006360527873039245, AUC: 0.7802045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006303101778030396, AUC: 0.7873945000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000634227454662323, AUC: 0.784375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006298956573009491, AUC: 0.7858185000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014156887531280517, AUC: 0.537138\n",
      "\n",
      "Train triplet loss: 263.9576334288925\n",
      "Train triplet loss: 3.979166775752025\n",
      "Train triplet loss: 1.8360275174402128\n",
      "Train triplet loss: 0.32941225417859993\n",
      "Train triplet loss: 0.4789515012388776\n",
      "Train triplet loss: 0.39994160233029896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train triplet loss: 0.1939485741269057\n",
      "Train triplet loss: 0.3785654617722627\n",
      "Train triplet loss: 0.00914804229311123\n",
      "Train triplet loss: 0.10145357042361217\n",
      "Train triplet loss: 0.17515371901214502\n",
      "Train triplet loss: 0.3923213834975176\n",
      "Train triplet loss: 0.014378716231911046\n",
      "Train triplet loss: 0.0\n",
      "Train triplet loss: 0.0\n",
      "\n",
      "Test set: Avg. loss: 0.0006661825478076935, AUC: 0.681546\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006567904353141785, AUC: 0.7220775000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006522080004215241, AUC: 0.728313\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006458030641078949, AUC: 0.7563310000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006477517187595367, AUC: 0.7530370000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000641948789358139, AUC: 0.7699455000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006373808979988099, AUC: 0.783147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006401232182979584, AUC: 0.7701199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006400469243526459, AUC: 0.78073\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006371627151966095, AUC: 0.7924995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006436472535133362, AUC: 0.7688645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006418440341949463, AUC: 0.7792809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006257739663124085, AUC: 0.8044250000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006326205134391785, AUC: 0.7964120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006318840980529785, AUC: 0.7980605000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006232094466686248, AUC: 0.8091800000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006177147924900055, AUC: 0.8170180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006274963617324829, AUC: 0.8024690000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006276062428951264, AUC: 0.8037215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006194550096988678, AUC: 0.8134170000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006252468526363373, AUC: 0.8081035000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006213923990726471, AUC: 0.816604\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006190604865550995, AUC: 0.8167720000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006213362216949463, AUC: 0.816303\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006245938837528229, AUC: 0.811883\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006207364499568939, AUC: 0.816156\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006202928423881531, AUC: 0.8169975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006162148118019104, AUC: 0.8232794999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006136621534824371, AUC: 0.8282665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006152274310588837, AUC: 0.8236429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006113384068012238, AUC: 0.8287739999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006141202747821807, AUC: 0.8261709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006078131198883057, AUC: 0.8332765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006155332028865814, AUC: 0.8261449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006188867688179016, AUC: 0.8213104999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006125370562076568, AUC: 0.828104\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006171385943889618, AUC: 0.825707\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006202398240566253, AUC: 0.816182\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006121963560581208, AUC: 0.8301105000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000617835521697998, AUC: 0.8211480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006138781607151031, AUC: 0.830786\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006173063218593598, AUC: 0.826784\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006297032237052917, AUC: 0.8190900000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000622035026550293, AUC: 0.823511\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006027123034000397, AUC: 0.8402430000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006152138710021972, AUC: 0.8322965000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006184290945529938, AUC: 0.8166015000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006106732785701751, AUC: 0.835365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006124807000160217, AUC: 0.834395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006066875457763672, AUC: 0.839091\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# triplet loss with SMOTE and cosine distance capped loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(1e-7, 1e-4)]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(3): \n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(complete_network.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(15):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss_smote, embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train triplet loss: \" + str(np.mean(np.array(train_losses))))\n",
    "        for epoch in range(50):\n",
    "            _, _ = train.train_sigmoid_cosine_distance(epoch, train_loader_smote, complete_network, linear_optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"cosine_distance_capped_smote_triplet_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], \n",
    "           auc_mean[i][4], auc_variance[i][4],\n",
    "           auc_mean[i][5], auc_variance[i][5],\n",
    "           5.0]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de718f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a3ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.003393930196762085, AUC: 0.500865\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# capped smote using triplet loss\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(5): \n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_triplet_capped_loss(epoch, train_loader_tripletloss_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"triplet_loss_capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4154bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc1dd64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES_REDUCED = 3\n",
    "nums = (0, 3, 1)\n",
    "ratio = (20, 2, 1)\n",
    "\n",
    "\n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums)\n",
    "targets = ratio_train_CIFAR10.labels \n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED)\n",
    "\n",
    "\n",
    "weight = 1. / class_count\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= max(class_count)\n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "800c1e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004159178098042806, AUC: 0.5\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 16\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     18\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_softmax(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/ml/train.py:41\u001b[0m, in \u001b[0;36mtrain_softmax\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze(), target\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     43\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/ml/models.py:19\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2_drop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)), \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m250\u001b[39m) \u001b[38;5;66;03m#(320)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_jit_internal.py:484\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3 class normal\n",
    "\n",
    "learning_rates = [1e-4, 1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 3, nums, (1, 1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6be4998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6452da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0028148902257283527, AUC: 0.5499715000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012899534304936728, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011812520821889241, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003198623657227, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012160149812698365, AUC: 0.496695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012602541049321493, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011723772684733072, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012640552123387655, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001084180474281311, AUC: 0.47396475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012727203766504925, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010816088120142618, AUC: 0.5045000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011214701334635417, AUC: 0.5035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026806603272755943, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012301311095555623, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001151394208272298, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107947838306427, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020958302021026613, AUC: 0.44825000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012716478904088338, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011440247694651285, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012925329605738322, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026947441101074217, AUC: 0.47152499999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011879764000574749, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011960159142812093, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012670242389043172, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017743062178293865, AUC: 0.50675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001074000636736552, AUC: 0.5057499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012203034957249958, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011130955616633098, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014793120384216308, AUC: 0.44375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012164912223815918, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010818771521250406, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011993260780970255, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00679119078318278, AUC: 0.48125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012626315355300903, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001144296924273173, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001217811703681946, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007157065073649088, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001255599578221639, AUC: 0.501499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011938397884368897, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010313888788223266, AUC: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class ratio\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d28e8f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1752b5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.003038533369700114, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010433460474014282, AUC: 0.574998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011814462741216024, AUC: 0.7483075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001038165827592214, AUC: 0.7227574999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007914267698923746, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010645556449890136, AUC: 0.37466975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001031708836555481, AUC: 0.6914997500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011311002572377523, AUC: 0.75428325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006719241301218669, AUC: 0.5075000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010695095856984456, AUC: 0.5922592499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010078495343526204, AUC: 0.66097125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009656443595886231, AUC: 0.68574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002060540755589803, AUC: 0.5615957500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010401540199915568, AUC: 0.3627755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009678711692492167, AUC: 0.6725415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009192776083946228, AUC: 0.675171\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00151268204053243, AUC: 0.5225000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010918419361114502, AUC: 0.40822674999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010981494585673014, AUC: 0.4126655000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010762500762939453, AUC: 0.684442\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0108524964650472, AUC: 0.499499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013274700244267782, AUC: 0.6495525000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001373440424601237, AUC: 0.6880609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882926384607951, AUC: 0.5351134999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004438136577606201, AUC: 0.50352525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012416810989379883, AUC: 0.5918582499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013266102472941081, AUC: 0.6148197500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019509809811909993, AUC: 0.6372085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005216264883677164, AUC: 0.51125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010727325280507406, AUC: 0.499001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010864347219467162, AUC: 0.4997515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010905816157658894, AUC: 0.49875600000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003298620859781901, AUC: 0.51497625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011997053623199463, AUC: 0.64312625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001198989232381185, AUC: 0.648871\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001623408595720927, AUC: 0.6290437499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013772049347559611, AUC: 0.4500199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006249984105427, AUC: 0.47525675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973472197850546, AUC: 0.46956325000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010804060300191245, AUC: 0.5838855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class oversampled \n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "423b94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c14c3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0020289310614267984, AUC: 0.4756965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001091553012530009, AUC: 0.48456375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010886570612589517, AUC: 0.4860155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010853137572606406, AUC: 0.5097382500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019787512222925823, AUC: 0.57222475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011532610654830932, AUC: 0.49576575000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145702044169108, AUC: 0.4937625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001140582799911499, AUC: 0.49350675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005084774017333984, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010802105665206909, AUC: 0.50175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010755322376887005, AUC: 0.502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010662730137507121, AUC: 0.5022505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009249690055847169, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010924884875615438, AUC: 0.50599775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882760683695474, AUC: 0.5044992500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010864359935124715, AUC: 0.50449775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008460322062174478, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003564596176148, AUC: 0.5434515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010977611939112346, AUC: 0.527579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001091849128405253, AUC: 0.502861\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037917500336964926, AUC: 0.50300075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001107357660929362, AUC: 0.50625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011036542654037475, AUC: 0.50575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008251905441284, AUC: 0.503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00853164831797282, AUC: 0.48180575000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011391439437866211, AUC: 0.4952675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011303770144780478, AUC: 0.49850675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011254301071166992, AUC: 0.49128400000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017762763341267904, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011278401215871175, AUC: 0.46372199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021624008814494, AUC: 0.42442375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011078615188598632, AUC: 0.43288249999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007273403485616049, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011379459698994954, AUC: 0.47927200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011329193909962972, AUC: 0.4857515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011250438690185546, AUC: 0.53704125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00682664426167806, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001064486066500346, AUC: 0.4997495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010623377164204915, AUC: 0.49874950000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010606383085250855, AUC: 0.49924975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class undersampled  \n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaec9e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c779b879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.00608756939570109, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010771948893864949, AUC: 0.5007499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001078349749247233, AUC: 0.49949999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010784133275349936, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00372534704208374, AUC: 0.50023525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010370986064275106, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010473136504491171, AUC: 0.49649774999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010565840800603231, AUC: 0.5224875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020539817810058592, AUC: 0.50625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010878965854644776, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087069312731425, AUC: 0.49999999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010845698912938435, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006659661134084066, AUC: 0.49401975000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010928993225097657, AUC: 0.53216375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010782272815704345, AUC: 0.54923675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010686718225479126, AUC: 0.5544079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009116797844568889, AUC: 0.48513425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010646411975224813, AUC: 0.497999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010692731142044067, AUC: 0.502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010714724858601888, AUC: 0.50150125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015166940053304037, AUC: 0.48724999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011384790738423664, AUC: 0.49775050000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011099223693211873, AUC: 0.51426275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013285338083902994, AUC: 0.59251075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018370418151219686, AUC: 0.4615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010755572319030762, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010769031047821046, AUC: 0.5002502499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010774298906326295, AUC: 0.49999925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003305099407831828, AUC: 0.503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010899808804194133, AUC: 0.49999999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010876678625742595, AUC: 0.5000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010835784276326497, AUC: 0.5007499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010739267667134603, AUC: 0.502463\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001126469651858012, AUC: 0.47123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011237830718358358, AUC: 0.49575225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011101373434066772, AUC: 0.468339\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01887862459818522, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010957230726877849, AUC: 0.4975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010916781425476074, AUC: 0.50075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010887458721796672, AUC: 0.5009999999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class weighted\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['weight'] = torch.from_numpy(weight).float()\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8692b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81115d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011305590867996215, AUC: 0.4987425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101138710975647, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011463310718536376, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011780770619710286, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038984591166178386, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011603130102157593, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011915287176767985, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012100194692611695, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006762343406677246, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145007332166036, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011761978069941203, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011957573493321736, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017103184858957927, AUC: 0.4808077500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011710822582244873, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012000585397084554, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012189826170603433, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003416938861211141, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011359240611394246, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011698009570439657, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011919792890548707, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030593673388163247, AUC: 0.475337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000983761727809906, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982903023560842, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982608477274577, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001518441875775655, AUC: 0.46025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010580919981002807, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103336493174235, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001135496457417806, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003099643548329671, AUC: 0.5017499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001077566663424174, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010170272588729858, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001027000625928243, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026799618403116864, AUC: 0.4685945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011022898356119791, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011425824165344238, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001169986605644226, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001287020762761434, AUC: 0.513392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011173804601033528, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011543748378753662, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001180996815363566, AUC: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class focal loss\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['reduction'] = 'mean'\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SoftmaxFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf8eaef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d48470b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004159178098042806, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010713002681732179, AUC: 0.49825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010870064496994018, AUC: 0.49875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010937161048253376, AUC: 0.4980015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001217594623565674, AUC: 0.5495857500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010883294343948364, AUC: 0.54002475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009588491717974345, AUC: 0.6908785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010892333984375, AUC: 0.6341857500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012967588504155477, AUC: 0.44993225000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010879942576090494, AUC: 0.49974900000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010939101775487264, AUC: 0.4984995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010963983138402302, AUC: 0.4980035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009516664822896322, AUC: 0.4979995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010874385436375935, AUC: 0.502996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010935383637746174, AUC: 0.50224575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010963813463846842, AUC: 0.50149875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008401273687680562, AUC: 0.473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001117892305056254, AUC: 0.50074975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110655943552653, AUC: 0.49974875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101777672767639, AUC: 0.49874875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018197454214096069, AUC: 0.642566\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010871400435765585, AUC: 0.502739\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010544216235478718, AUC: 0.7166375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009863032698631286, AUC: 0.7337182500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014161507288614908, AUC: 0.6221422499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001115713357925415, AUC: 0.49924975000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011057165066401164, AUC: 0.5000005000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011015222469965616, AUC: 0.50025025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00627077309290568, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003833611806233, AUC: 0.51025525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096467614173889, AUC: 0.5330164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010366939703623454, AUC: 0.669718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014923743406931558, AUC: 0.6397665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001040031870206197, AUC: 0.7275147499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010351263682047526, AUC: 0.6475892499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011037707328796387, AUC: 0.54751375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01015039348602295, AUC: 0.49\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010485761562983194, AUC: 0.66979875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009980746308962504, AUC: 0.723652\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009460805654525757, AUC: 0.700749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class SMOTE\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "717e9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bfe9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.005159725666046142, AUC: 0.47875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013107466697692871, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014245965083440144, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013205681641896565, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036626233259836835, AUC: 0.5045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012807045380274454, AUC: 0.55175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008384430607159932, AUC: 0.6527735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013193607727686565, AUC: 0.58771125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001882831374804179, AUC: 0.568451\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014629533290863037, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013556772470474244, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012844537496566773, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005783709685007731, AUC: 0.5153915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014496474663416544, AUC: 0.68025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013263502518335978, AUC: 0.72725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005901109576225281, AUC: 0.6950000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015779575983683267, AUC: 0.47575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017115784088770549, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001690352439880371, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016737443208694458, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024884181340535484, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016858530044555663, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016851926644643148, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016747065782546997, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014013916651407878, AUC: 0.4033884999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001159618854522705, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012145692507425944, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006874272624651591, AUC: 0.7312500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005693643728892008, AUC: 0.49025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016551088094711303, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016597933371861775, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001664273738861084, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004257233619689942, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017054282824198405, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001689716895421346, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016794553200403849, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009325793266296387, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016158297061920166, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018122962315877279, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001287239154179891, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011229380289713542, AUC: 0.4915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009665106336275737, AUC: 0.6430379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009809208512306213, AUC: 0.7527905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010630707343419392, AUC: 0.5388919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002801464796066284, AUC: 0.4945689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010990440448125204, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098304033279419, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097338914871216, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011395713488260904, AUC: 0.44925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010731459856033324, AUC: 0.5862499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009436763922373454, AUC: 0.63125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001167917807896932, AUC: 0.5477055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003414260149002075, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010740158557891845, AUC: 0.5370075000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010959016879399618, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010975220600763958, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005730849742889404, AUC: 0.50174975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098487933476766, AUC: 0.49950649999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005215644836425, AUC: 0.56338675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011249771118164063, AUC: 0.65596625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005885673840840658, AUC: 0.505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011182666222254436, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001144629160563151, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011247365872065227, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01593700949350993, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096144715944926, AUC: 0.4999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097902218500773, AUC: 0.49725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003881295522054, AUC: 0.4957484999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010915645360946656, AUC: 0.43125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099284609158834, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010995355049769084, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991565783818563, AUC: 0.5025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023588163852691652, AUC: 0.497\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002196073532104, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011001572211583456, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099966843922933, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003621602535247803, AUC: 0.5055245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010969852209091187, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988662242889405, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010969030062357584, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008199299812316894, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011139464775721231, AUC: 0.5786862499999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001256147066752116, AUC: 0.5299999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011905717055002848, AUC: 0.5347545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029200942516326906, AUC: 0.50149925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984888474146525, AUC: 0.49974999999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011014104684193928, AUC: 0.49749899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099079688390096, AUC: 0.4964985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013806982835133871, AUC: 0.5180030000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998592774073284, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011013609170913697, AUC: 0.49625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106422464052836, AUC: 0.49724299999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008860953013102213, AUC: 0.49525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010979174375534057, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005378564198811, AUC: 0.496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002195278803507, AUC: 0.49825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026838178634643554, AUC: 0.498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992279847462972, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998342831929524, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989136298497518, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00352239465713501, AUC: 0.4201495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992273886998494, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991250673929851, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096076528231303, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003739197254180908, AUC: 0.5009999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010914430618286133, AUC: 0.7057055000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009601728518803914, AUC: 0.67756775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001175796906153361, AUC: 0.5605000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007436930020650228, AUC: 0.49375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989174445470175, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984821716944378, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095758557319641, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011664613405863444, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973004500071208, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010976841449737548, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010974336862564088, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001008460839589437, AUC: 0.46063449999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010979483524958292, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099142074584961, AUC: 0.4995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class capped loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-2]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args, smote=True)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for i in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[i][0]\n",
    "    auc_variance = cap_aucs[i][1]\n",
    "    cap = caps[i]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", 3, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b3e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c453da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0967e450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
