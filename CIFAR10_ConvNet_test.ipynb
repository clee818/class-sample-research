{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c3c2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378c3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91dda12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "NUM_CLASSES_REDUCED = 2\n",
    "nums = (0, 1)\n",
    "ratio = (100, 1)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b57e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "             \"mean_40\", \"variance_40\",\n",
    "             \"mean_50\", \"variance_50\",\n",
    "             \"cap\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c2a1ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor() ]))  \n",
    "\n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()]))\n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "    \n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums)\n",
    "\n",
    "triplet_train_CIFAR10 = class_sampling.ForTripletLoss(reduced_train_CIFAR10, smote=False)\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED)\n",
    "triplet_train_CIFAR10_smote = class_sampling.ForTripletLoss(smote_train_CIFAR10, smote=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13159c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000   50]\n"
     ]
    }
   ],
   "source": [
    "targets = ratio_train_CIFAR10.labels \n",
    "\n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "print(class_count)\n",
    "\n",
    "weight = 1. / class_count\n",
    "\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= class_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de0d4cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.999 \n",
    "\n",
    "exp = np.empty_like(targets)\n",
    "for i, count in enumerate(class_count):\n",
    "    exp[targets==i] = count\n",
    "effective_weights = (1 - beta) / ( 1 - (beta ** torch.from_numpy(exp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af8cd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss = DataLoader(triplet_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_smote = DataLoader(triplet_train_CIFAR10_smote, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e8c09f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2 CLASS normal\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f1d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "\n",
    "df2.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d211694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2 CLASS ratio\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "learning_rate_train_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                _, train_auc = metric_utils.auc_sigmoid(train_loader_ratio, network) \n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f2d0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f1ccb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0009614106118679046, AUC: 0.614011\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006774698793888092, AUC: 0.6585589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006662350296974182, AUC: 0.713206\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006555393636226654, AUC: 0.7449055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006931622505187988, AUC: 0.532071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006511834263801575, AUC: 0.705865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006510878801345825, AUC: 0.7048525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006323479413986206, AUC: 0.74831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012074116468429566, AUC: 0.45304300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006792399287223815, AUC: 0.6182110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006664896011352539, AUC: 0.667305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006620110869407654, AUC: 0.6808749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005842076301574707, AUC: 0.35606649999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006702626347541809, AUC: 0.6863610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006543796062469482, AUC: 0.7483445000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006360483169555665, AUC: 0.7825044999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009045130610466003, AUC: 0.455444\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006217218935489655, AUC: 0.8241620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005998185575008392, AUC: 0.8498125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005988564193248749, AUC: 0.8472009999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007873946130275727, AUC: 0.505924\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006304432153701782, AUC: 0.745096\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005960119962692261, AUC: 0.790896\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005686694979667664, AUC: 0.8086879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018394469618797302, AUC: 0.342069\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006031354665756225, AUC: 0.780868\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005784178674221039, AUC: 0.804662\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005471040308475494, AUC: 0.84365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026135700941085816, AUC: 0.615929\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988285779953002, AUC: 0.508404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006910592019557952, AUC: 0.566112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006852890253067017, AUC: 0.5973975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002733421802520752, AUC: 0.42795249999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006736321449279785, AUC: 0.6940394999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000640917718410492, AUC: 0.8093619999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000628568172454834, AUC: 0.827172\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009439133703708648, AUC: 0.654072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000649003803730011, AUC: 0.7196885000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006312101185321808, AUC: 0.768405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006372729241847991, AUC: 0.7558340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007115027904510498, AUC: 0.616657\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006401776671409607, AUC: 0.720353\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006331292688846589, AUC: 0.763507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006281391382217407, AUC: 0.7875584999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019356902241706847, AUC: 0.568219\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006451549232006073, AUC: 0.6998120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006452490091323853, AUC: 0.7160934999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006427935063838959, AUC: 0.72821\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001419282615184784, AUC: 0.40512800000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695315808057785, AUC: 0.48694499999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948222219944001, AUC: 0.4929665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006942886710166931, AUC: 0.5003135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008247624337673188, AUC: 0.525384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006887524425983429, AUC: 0.5578150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006850159466266632, AUC: 0.569787\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006808511018753052, AUC: 0.5959215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004765978336334228, AUC: 0.348692\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006693117916584015, AUC: 0.6508544999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006629041135311126, AUC: 0.6958869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006600531339645385, AUC: 0.7041590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007654413282871246, AUC: 0.621819\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006669045090675354, AUC: 0.6428965000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006625311076641083, AUC: 0.6809704999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006586202681064606, AUC: 0.6952625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034120362997055054, AUC: 0.388479\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006689565479755401, AUC: 0.716342\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006688528954982757, AUC: 0.716347\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006670557856559753, AUC: 0.7320850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005057451725006104, AUC: 0.592154\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006919226050376892, AUC: 0.5435785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894824206829071, AUC: 0.5413675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000687130481004715, AUC: 0.563974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003564469575881958, AUC: 0.574443\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006921630799770355, AUC: 0.572517\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918762624263763, AUC: 0.563254\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006903278231620789, AUC: 0.577599\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012052770853042602, AUC: 0.355636\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006900358200073242, AUC: 0.537617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006885618567466736, AUC: 0.554183\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006862815916538239, AUC: 0.570273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS oversampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52d8c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1541b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.002664048075675964, AUC: 0.35103100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008118733763694763, AUC: 0.39093300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007359330058097839, AUC: 0.42988400000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007080410122871399, AUC: 0.476849\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005757878303527832, AUC: 0.4101775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008514148592948913, AUC: 0.663575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006793743073940277, AUC: 0.6388240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006851064562797546, AUC: 0.599984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001174522042274475, AUC: 0.498127\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009026037454605102, AUC: 0.569188\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007478025853633881, AUC: 0.5202549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007039127349853516, AUC: 0.5297069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009038377702236175, AUC: 0.47197\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007971824407577515, AUC: 0.48862799999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007626304626464844, AUC: 0.494027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007488877177238464, AUC: 0.4992150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034414209127426147, AUC: 0.5659004999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008925645649433136, AUC: 0.602172\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007607427835464478, AUC: 0.5276164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007198239862918854, AUC: 0.509627\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013543962240219116, AUC: 0.49844299999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013344860672950744, AUC: 0.5426559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008173213005065918, AUC: 0.520809\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007598233222961425, AUC: 0.41432599999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009839903712272643, AUC: 0.54916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008945295512676239, AUC: 0.46157500000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007561358511447906, AUC: 0.477264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007167462408542633, AUC: 0.498566\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001238984763622284, AUC: 0.47820850000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009365684688091278, AUC: 0.374864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008359749019145966, AUC: 0.3937505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007830325365066528, AUC: 0.417717\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008678676128387451, AUC: 0.6484215000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009037670493125915, AUC: 0.427843\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007472198605537415, AUC: 0.4185105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007187381088733673, AUC: 0.446337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007866494059562683, AUC: 0.5668679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007531993091106415, AUC: 0.488706\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007111433148384094, AUC: 0.49999199999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007000260055065155, AUC: 0.515335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038360543251037597, AUC: 0.294262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009016560912132263, AUC: 0.505367\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088973104953766, AUC: 0.5929039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011563763022422791, AUC: 0.606266\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006004724025726318, AUC: 0.478491\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0041501855850219725, AUC: 0.427829\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030867953300476076, AUC: 0.404122\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026625494956970214, AUC: 0.434574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017807592153549195, AUC: 0.5781095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011875814199447633, AUC: 0.6403829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009512988924980164, AUC: 0.6518860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008521502912044526, AUC: 0.6516249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007921732366085052, AUC: 0.615845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008575631380081177, AUC: 0.646551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009552640914916992, AUC: 0.646782\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008757838904857636, AUC: 0.6430809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012470539212226868, AUC: 0.417589\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001292936086654663, AUC: 0.45840000000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013544394373893739, AUC: 0.473034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001133813738822937, AUC: 0.46033199999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010617620944976807, AUC: 0.63819\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010061159133911133, AUC: 0.634423\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009557005167007447, AUC: 0.6285029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009136235415935516, AUC: 0.61536\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018809281587600709, AUC: 0.6420239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001633428931236267, AUC: 0.638927\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014178273677825928, AUC: 0.634879\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012286875247955321, AUC: 0.6292800000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016851001381874085, AUC: 0.338503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010225446820259095, AUC: 0.45910999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011303470730781556, AUC: 0.5157820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001235684871673584, AUC: 0.543991\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002234177231788635, AUC: 0.47880599999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001637177050113678, AUC: 0.588411\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001996301531791687, AUC: 0.6044080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018329355120658875, AUC: 0.602075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010110423564910888, AUC: 0.42448849999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008554110527038574, AUC: 0.44827599999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008604062795639038, AUC: 0.453961\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008695433139801025, AUC: 0.447998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS undersampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3dafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Class Weighted Loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['pos_weight'] = torch.tensor([weight[1]])\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                _, train_auc = metric_utils.auc_sigmoid(train_loader_ratio, network) \n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9eed2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7c5a840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001968830943107605, AUC: 0.6361665000000001\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 18\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     20\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:20\u001b[0m, in \u001b[0;36mtrain_sigmoid\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     19\u001b[0m pred\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2 CLASS SMOTE\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edcf4753",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfecae8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001968830943107605, AUC: 0.6361665000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005031153112649918, AUC: 0.8974789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005469995141029358, AUC: 0.9000029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00084603151679039, AUC: 0.88369\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032240989208221437, AUC: 0.38664699999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005088528543710708, AUC: 0.8884839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005007016658782959, AUC: 0.8948050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016704701781272888, AUC: 0.8813089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006873347997665405, AUC: 0.683021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014531199336051941, AUC: 0.7342400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000498114362359047, AUC: 0.8572739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011343847513198852, AUC: 0.8791939999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001451125681400299, AUC: 0.47390000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007161926329135895, AUC: 0.8914880000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008789804279804229, AUC: 0.8992389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010165030360221863, AUC: 0.9107010000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031172895431518556, AUC: 0.6896909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007840077579021454, AUC: 0.8925720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017229195833206178, AUC: 0.891933\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010154286921024323, AUC: 0.904567\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00219118869304657, AUC: 0.37103100000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009143871068954468, AUC: 0.887275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00064670330286026, AUC: 0.903126\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011170220375061035, AUC: 0.9025020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006955941677093506, AUC: 0.3322345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011152198910713196, AUC: 0.864405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015940282940864563, AUC: 0.869807\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005913239121437073, AUC: 0.8973410000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028511343002319335, AUC: 0.5939905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011134166121482849, AUC: 0.870477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008944603502750397, AUC: 0.8880830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008275810480117797, AUC: 0.9031819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011438738107681274, AUC: 0.351393\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001183919370174408, AUC: 0.899401\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019104973077774048, AUC: 0.901324\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005399399101734161, AUC: 0.9155329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015125332474708558, AUC: 0.429251\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009548967778682709, AUC: 0.8587049999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008348909616470337, AUC: 0.899968\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004832865476608277, AUC: 0.8236430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034982739686965944, AUC: 0.618384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002668298840522766, AUC: 0.6467655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002269598960876465, AUC: 0.6828989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018359076976776122, AUC: 0.736255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011465153098106384, AUC: 0.401948\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00328282368183136, AUC: 0.620884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001747790277004242, AUC: 0.7807565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008776652812957764, AUC: 0.868484\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009435354471206665, AUC: 0.577937\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030418012142181395, AUC: 0.534725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002789926290512085, AUC: 0.593493\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022196367979049683, AUC: 0.6480589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029533997774124146, AUC: 0.545541\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024114584922790526, AUC: 0.5891139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002463437795639038, AUC: 0.609348\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018961783051490784, AUC: 0.626985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038374502658843996, AUC: 0.565988\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032909551858901977, AUC: 0.617059\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026833311319351197, AUC: 0.6565190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002371362924575806, AUC: 0.7158439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001448841392993927, AUC: 0.5810709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026167494058609007, AUC: 0.602822\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001917473018169403, AUC: 0.676069\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014771718978881835, AUC: 0.7359809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004549663305282592, AUC: 0.44000300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027069497108459474, AUC: 0.44700000000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019191591739654542, AUC: 0.5577380000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001118282914161682, AUC: 0.720096\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028102716207504272, AUC: 0.5378565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016078546643257142, AUC: 0.7272210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009008783996105194, AUC: 0.759853\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000709831178188324, AUC: 0.8036049999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018956378698349, AUC: 0.37419199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006281431019306183, AUC: 0.8135055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006141913831233978, AUC: 0.838227\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006060933768749237, AUC: 0.8500955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001775826096534729, AUC: 0.554507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028130435943603516, AUC: 0.597718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023281478881835935, AUC: 0.6370009999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015381531119346618, AUC: 0.678718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019287337064743042, AUC: 0.518386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000569645881652832, AUC: 0.8768105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005479333698749542, AUC: 0.883664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000519349217414856, AUC: 0.8880054999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003408196210861206, AUC: 0.7021970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006255797147750855, AUC: 0.8224310000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005519364774227143, AUC: 0.8892970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006152485013008117, AUC: 0.8481240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033094873428344725, AUC: 0.46274150000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005571002662181855, AUC: 0.8752684999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004942961782217026, AUC: 0.898413\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004772709161043167, AUC: 0.902693\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007158293724060058, AUC: 0.573209\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006516829431056976, AUC: 0.874549\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008335795402526856, AUC: 0.882585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008736421167850494, AUC: 0.885249\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004602427005767822, AUC: 0.536975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006688660979270935, AUC: 0.703315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006506872773170471, AUC: 0.7896579999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005714234709739685, AUC: 0.8686054999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023244422674179076, AUC: 0.5137155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005625349879264832, AUC: 0.8826839999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005180132389068603, AUC: 0.8929369999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006350348293781281, AUC: 0.84239\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004095608711242676, AUC: 0.30450750000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005952829420566559, AUC: 0.8598515000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005796711146831512, AUC: 0.8674405000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005048570930957794, AUC: 0.89661\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006330698013305664, AUC: 0.49924100000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005933513641357422, AUC: 0.856698\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007027221322059631, AUC: 0.7344849999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004979489296674728, AUC: 0.8930825000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020437593460083006, AUC: 0.4564695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000577959805727005, AUC: 0.8736465000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005098122954368591, AUC: 0.894491\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000588348388671875, AUC: 0.8748775000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025592957735061646, AUC: 0.6610935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005756193697452545, AUC: 0.869313\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006253807246685028, AUC: 0.8277310000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005765666365623475, AUC: 0.8760855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002686577558517456, AUC: 0.574317\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006303285658359527, AUC: 0.8015619999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006217358708381652, AUC: 0.823185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00061519655585289, AUC: 0.8263929999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010352564454078675, AUC: 0.479156\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006467460095882416, AUC: 0.7798475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006215929388999939, AUC: 0.8208234999999999\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006228884160518647, AUC: 0.8189550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035049201250076296, AUC: 0.364836\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006428403854370117, AUC: 0.7701835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006446835100650788, AUC: 0.77815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006226344704627991, AUC: 0.8041910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00241806423664093, AUC: 0.3173455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006310730576515197, AUC: 0.8121910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006201047599315643, AUC: 0.829522\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006243416666984558, AUC: 0.8286870000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001278232514858246, AUC: 0.507354\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941431760787964, AUC: 0.49647\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938660740852356, AUC: 0.498515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937801837921143, AUC: 0.500014\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020855687856674195, AUC: 0.4122905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006648121476173401, AUC: 0.7229209999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006386235058307648, AUC: 0.797132\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006289769411087036, AUC: 0.8172525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013735867142677307, AUC: 0.58304\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006336276829242706, AUC: 0.832427\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000621557354927063, AUC: 0.8509105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006082994043827057, AUC: 0.863273\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001305916428565979, AUC: 0.44187750000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006572458744049072, AUC: 0.744127\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006263954043388367, AUC: 0.833709\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006168598234653473, AUC: 0.8348105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008709692656993866, AUC: 0.5386139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936162114143372, AUC: 0.5009689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935537755489349, AUC: 0.5005029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934955716133117, AUC: 0.4995109999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008707275390625, AUC: 0.354607\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006361538767814636, AUC: 0.805143\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006200213730335236, AUC: 0.8375710000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006134167313575744, AUC: 0.8459025000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034300466775894164, AUC: 0.40457550000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005481257438659668, AUC: 0.881112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005201433897018433, AUC: 0.888463\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006130536198616028, AUC: 0.8638225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011988989710807801, AUC: 0.38362300000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938429474830628, AUC: 0.493513\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940912008285522, AUC: 0.49498699999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952098608016968, AUC: 0.490473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008709073662757873, AUC: 0.554262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005924961566925049, AUC: 0.853674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005891624689102172, AUC: 0.8639375000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006794290542602539, AUC: 0.784335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007912777185440064, AUC: 0.4069285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00066332146525383, AUC: 0.7103419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006012020409107208, AUC: 0.881845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008020886480808258, AUC: 0.895688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004647176265716552, AUC: 0.440377\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006128383576869964, AUC: 0.8371350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005394762456417084, AUC: 0.886735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006138624548912048, AUC: 0.845479\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013046321272850037, AUC: 0.646519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005646645724773407, AUC: 0.8749745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007089022696018218, AUC: 0.647987\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006183989644050599, AUC: 0.8544765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007593387961387634, AUC: 0.6597930000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006153889596462249, AUC: 0.8405840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006121238172054291, AUC: 0.8408790000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005404407978057862, AUC: 0.8921124999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012480967044830322, AUC: 0.413415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005848507285118103, AUC: 0.8638719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005122665166854859, AUC: 0.8997565000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005702422857284546, AUC: 0.8719695000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010108822345733643, AUC: 0.684472\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693383663892746, AUC: 0.4971245000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006085905134677887, AUC: 0.8435924999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005494129955768586, AUC: 0.8813639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014604349136352539, AUC: 0.61742\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006019564867019654, AUC: 0.858813\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000620020478963852, AUC: 0.85105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005616355836391449, AUC: 0.8712319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014504189491271972, AUC: 0.45714049999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006085423827171326, AUC: 0.8290334999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005975726544857025, AUC: 0.860248\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005814746618270874, AUC: 0.8747265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022167521715164185, AUC: 0.5067005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006166309714317322, AUC: 0.8401730000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006113622784614563, AUC: 0.8478515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000613192617893219, AUC: 0.8446940000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015854299664497375, AUC: 0.5336940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004965257346630096, AUC: 0.873142\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004626389294862747, AUC: 0.8895589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00045364880561828613, AUC: 0.8975409999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023031699657440185, AUC: 0.33441800000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000651137262582779, AUC: 0.7605359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006427066624164581, AUC: 0.7928635000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006370959281921387, AUC: 0.811136\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033897377252578736, AUC: 0.334389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006548470854759217, AUC: 0.7804420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000648656964302063, AUC: 0.807677\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006281470358371734, AUC: 0.8361965000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00565533971786499, AUC: 0.621595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938521265983581, AUC: 0.494514\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937192380428314, AUC: 0.4960115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936521828174591, AUC: 0.495518\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027263885736465454, AUC: 0.684435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006689863204956054, AUC: 0.7058720000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006400684118270874, AUC: 0.7907625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006476799845695496, AUC: 0.7866875000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007550109624862671, AUC: 0.6471\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006592035293579102, AUC: 0.7816110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006419380903244018, AUC: 0.8240030000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000650332510471344, AUC: 0.8047575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019841929674148558, AUC: 0.562379\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006596758067607879, AUC: 0.7772645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006509515643119812, AUC: 0.7898925000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006380246579647064, AUC: 0.8115364999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00474360203742981, AUC: 0.610553\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006638788878917695, AUC: 0.7040399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006242039501667023, AUC: 0.8347719999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006224636137485504, AUC: 0.836628\n",
      "\n",
      "[['capped_smote', 2, (0, 1), (100, 1), 0.001, 0.49473255, 0.018197527144972497, 0.8684526, 0.002179676916239998, 0.8905562, 0.00020914591496000047, 0.8901662000000001, 0.0006311712829599992, 1], ['capped_smote', 2, (0, 1), (100, 1), 0.0001, 0.51974275, 0.006263770673362501, 0.6196813999999999, 0.00886118710679, 0.67919035, 0.007140489322002499, 0.7384122499999999, 0.0058640424068625, 1], ['capped_smote', 2, (0, 1), (100, 1), 0.001, 0.52285355, 0.0110365757207225, 0.8494567, 0.0026375848684099977, 0.8560701500000001, 0.0027322639329025056, 0.87757225, 0.0003596705908624995, 5], ['capped_smote', 2, (0, 1), (100, 1), 0.0001, 0.4573437499999999, 0.008004497520812498, 0.7265841, 0.013923818853440007, 0.7570021, 0.016957157870740005, 0.76389895, 0.017676870383172512, 5], ['capped_smote', 2, (0, 1), (100, 1), 0.001, 0.5211385000000001, 0.013565860801099996, 0.7711143999999999, 0.02110085288639, 0.80992325, 0.01574203044471251, 0.8250951999999998, 0.013337861857359998, 10], ['capped_smote', 2, (0, 1), (100, 1), 0.0001, 0.5292404, 0.01358646398999, 0.7546628, 0.01014370282981, 0.79336405, 0.010799164676222498, 0.7999421499999999, 0.011291385976402498, 10]]\n"
     ]
    }
   ],
   "source": [
    "# 2 Class Capped SMOTE \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_sigmoid(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args, smote=True)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for i in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[i][0]\n",
    "    auc_variance = cap_aucs[i][1]\n",
    "    cap = caps[i]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "606da0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = (col_names))\n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "563f22d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001035837471485138, AUC: 0.44774200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927213966846466, AUC: 0.5254145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931669712066651, AUC: 0.511691\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935113668441773, AUC: 0.5110125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005613439321517945, AUC: 0.426767\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037078361511230467, AUC: 0.5171749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026477036476135253, AUC: 0.46210599999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002063918948173523, AUC: 0.44280450000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001599363923072815, AUC: 0.5595060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017810710668563843, AUC: 0.527251\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017536671757698058, AUC: 0.530268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001468895673751831, AUC: 0.511718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009869316220283508, AUC: 0.521323\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025733143091201784, AUC: 0.44177999999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002441696286201477, AUC: 0.42422899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00228461492061615, AUC: 0.419806\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005625223875045777, AUC: 0.640735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00567081618309021, AUC: 0.490035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003996514081954956, AUC: 0.400708\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033221423625946047, AUC: 0.379003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013055719137191773, AUC: 0.359221\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002855167269706726, AUC: 0.3432545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002768984794616699, AUC: 0.369273\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002534526348114014, AUC: 0.363009\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007071554660797119, AUC: 0.6680550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020034250020980835, AUC: 0.45828099999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022772670984268187, AUC: 0.48882499999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00230745005607605, AUC: 0.509682\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009378761649131774, AUC: 0.5550744999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033345311880111695, AUC: 0.523733\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003033424973487854, AUC: 0.505011\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026757409572601316, AUC: 0.501909\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027659571170806883, AUC: 0.402637\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015370473265647888, AUC: 0.5538240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015259616374969482, AUC: 0.5430379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014122150540351868, AUC: 0.5184869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007173371315002441, AUC: 0.297585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001956529438495636, AUC: 0.5169174999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001981451392173767, AUC: 0.533161\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019487438797950744, AUC: 0.536132\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000798177808523178, AUC: 0.7362540000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006655452251434326, AUC: 0.7207349999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007001525759696961, AUC: 0.733501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007603182792663574, AUC: 0.742639\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002295616149902344, AUC: 0.376647\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009011189937591553, AUC: 0.5605964999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011427636742591858, AUC: 0.5951460000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013215634822845458, AUC: 0.605137\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00221078360080719, AUC: 0.5965469999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010793698787689208, AUC: 0.642539\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010493645191192627, AUC: 0.628214\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009857546329498292, AUC: 0.613483\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006628040313720703, AUC: 0.6315075000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008294330596923829, AUC: 0.650957\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007958059310913085, AUC: 0.6476215000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007488812685012817, AUC: 0.64287\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002516766667366028, AUC: 0.43171800000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007630343437194825, AUC: 0.431629\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007703002095222473, AUC: 0.411333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007906514704227448, AUC: 0.45406500000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002265890836715698, AUC: 0.608756\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004604844093322754, AUC: 0.649751\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038692718744277954, AUC: 0.643426\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032494683265686035, AUC: 0.635331\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015813257098197936, AUC: 0.553272\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00550286054611206, AUC: 0.640204\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0053946242332458495, AUC: 0.631832\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005209399700164795, AUC: 0.626354\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032208967208862305, AUC: 0.590365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005893768787384034, AUC: 0.47718400000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0058131103515625, AUC: 0.455178\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005506749391555786, AUC: 0.440446\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024443947076797485, AUC: 0.535904\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003024880290031433, AUC: 0.495147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002925659418106079, AUC: 0.495309\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002817286252975464, AUC: 0.48984750000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004183521747589111, AUC: 0.566368\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004094211220741272, AUC: 0.52125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037260520458221435, AUC: 0.474804\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033895738124847413, AUC: 0.42854600000000004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS Focal Loss\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SigmoidFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d438d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5f6eb22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf2\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7924e1f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001713291108608246, AUC: 0.572522\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007095660865306854, AUC: 0.4930185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007561078071594238, AUC: 0.48606199999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007982843816280365, AUC: 0.48059299999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006548434972763062, AUC: 0.3809975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931488215923309, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693147212266922, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931480169296264, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012743324041366577, AUC: 0.531981\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007263560891151428, AUC: 0.783012\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007811572551727295, AUC: 0.635699\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009450841844081878, AUC: 0.8064009999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008527356088161469, AUC: 0.638988\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933750212192535, AUC: 0.501984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007121953666210175, AUC: 0.4820955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007565244734287262, AUC: 0.4625580000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003043826103210449, AUC: 0.323887\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693147212266922, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693148136138916, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931471824645996, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012297430634498597, AUC: 0.582651\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948250234127044, AUC: 0.49153399999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007082544267177582, AUC: 0.47294\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007244482636451721, AUC: 0.464544\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024515856504440307, AUC: 0.6454770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931473314762116, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931473910808564, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693147212266922, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008781127631664276, AUC: 0.653898\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006343388855457306, AUC: 0.8252330000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006645027101039886, AUC: 0.843089\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010522047281265258, AUC: 0.8117544999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001073086380958557, AUC: 0.31813400000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931479275226593, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931490004062653, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931495964527131, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009648494720458984, AUC: 0.48207749999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935293674468994, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693147361278534, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693147212266922, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031749202013015747, AUC: 0.4737715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006664306521415711, AUC: 0.6833509999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006612686514854431, AUC: 0.715643\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006556078791618347, AUC: 0.7435535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004238876342773437, AUC: 0.3410995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006595199108123779, AUC: 0.7015769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006291662454605102, AUC: 0.7610659999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005963969230651855, AUC: 0.7984349999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011038411259651185, AUC: 0.603456\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006830263435840607, AUC: 0.5801010000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006816979348659516, AUC: 0.5986195000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006713563501834869, AUC: 0.6501494999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000999782383441925, AUC: 0.407647\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006619951128959655, AUC: 0.710906\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006552150249481201, AUC: 0.7606945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006496571600437164, AUC: 0.777862\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030297824144363402, AUC: 0.525074\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006526163816452026, AUC: 0.7236195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000646355926990509, AUC: 0.75078\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006367500424385071, AUC: 0.779264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016268702149391174, AUC: 0.49034800000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006489550471305847, AUC: 0.725979\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006354217529296875, AUC: 0.799578\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000628099650144577, AUC: 0.827197\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034747525453567504, AUC: 0.654269\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006766316890716552, AUC: 0.5981314999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006693059206008911, AUC: 0.6510065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006645663678646088, AUC: 0.683308\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008251261115074157, AUC: 0.570864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000653807282447815, AUC: 0.7425250000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006467722356319428, AUC: 0.7794725000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006428709030151368, AUC: 0.7919569999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003054451823234558, AUC: 0.6005785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006556225121021271, AUC: 0.6893290000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000649573415517807, AUC: 0.7262275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006477256119251251, AUC: 0.7403639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004538457632064819, AUC: 0.5479885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000669509619474411, AUC: 0.6602735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006639128625392914, AUC: 0.6958345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006584030091762543, AUC: 0.7140420000000002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# distance + capped loss\n",
    "momentum=0\n",
    "learning_rates = [1e-2, 1e-5]\n",
    "\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_euclidean_distance(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"distance_capped_smote_fixed\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7cb909",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4c3abc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011436396837234497, AUC: 0.457495\n",
      "\n",
      "tensor([0.4864,    nan, 0.1976,    nan,    nan,    nan,    nan,    nan, 0.4864,\n",
      "           nan, 0.1988,    nan,    nan, 0.2524, 0.4446,    nan, 0.3864,    nan,\n",
      "        0.4864, 0.5528,    nan, 0.2667,    nan,    nan, 0.3599, 0.3651,    nan,\n",
      "           nan, 0.3887, 0.2463, 0.2289,    nan,    nan, 0.2215, 0.4790, 0.4394,\n",
      "           nan,    nan, 0.4864,    nan, 0.4785, 0.3570, 0.3376, 0.4941,    nan,\n",
      "           nan,    nan,    nan,    nan, 0.4090,    nan, 0.4213,    nan,    nan,\n",
      "        0.4937,    nan,    nan, 0.2548,    nan, 0.4571,    nan,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.4166,    nan,    nan, 0.5076, 0.4801,    nan, 0.4190,\n",
      "           nan,    nan, 0.4801, 0.5311, 0.4935,    nan, 0.1738,    nan,    nan,\n",
      "           nan,    nan, 0.4801, 0.4145, 0.3991, 0.4315, 0.4502, 0.4801, 0.4294,\n",
      "           nan,    nan,    nan,    nan,    nan, 0.4248, 0.4907, 0.4878,    nan,\n",
      "           nan, 0.4801,    nan, 0.5168,    nan, 0.4481,    nan, 0.4801, 0.4801,\n",
      "           nan, 0.3799, 0.4316,    nan,    nan, 0.2272, 0.3410,    nan, 0.4801,\n",
      "        0.4801, 0.4529, 0.4801,    nan,    nan, 0.4245, 0.4801,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.3909,    nan,    nan,    nan, 0.4063, 0.3024, 0.2767,\n",
      "           nan, 0.4614, 0.1794,    nan, 0.4663,    nan,    nan, 0.3222, 0.4486,\n",
      "           nan,    nan, 0.4663, 0.3749, 0.4663,    nan, 0.4466,    nan, 0.5265,\n",
      "        0.4663, 0.1708, 0.2806, 0.6043,    nan, 0.5075,    nan,    nan,    nan,\n",
      "           nan, 0.4663,    nan,    nan,    nan, 0.5287,    nan, 0.3913,    nan,\n",
      "        0.2736,    nan, 0.4466, 0.5317,    nan, 0.4548, 0.2420,    nan, 0.4241,\n",
      "           nan, 0.5130,    nan, 0.3986,    nan,    nan, 0.4467, 0.3703,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.2241, 0.5456, 0.6941, 0.5534, 0.5154, 0.4176, 0.5269, 0.0000, 0.4149,\n",
      "        0.5687, 0.1454, 0.4306, 0.6396, 0.5456, 0.5456, 0.5873, 0.4416, 0.4370,\n",
      "        0.4731, 0.6490, 0.5305, 0.5120, 0.2693, 0.4671, 0.4897, 0.3500, 0.4967,\n",
      "        0.5456, 0.6597, 0.6834, 0.4643, 0.5456, 0.1369, 0.3372, 0.7015, 0.5870,\n",
      "        0.6545, 0.2998, 0.5456, 0.2766, 0.5456, 0.6490, 0.4779, 0.7751, 0.3603,\n",
      "        0.5456, 0.7009, 0.6490, 0.2128, 0.5316, 0.2553, 0.5456, 0.5456, 0.5915,\n",
      "        0.7375, 0.5456, 0.7148, 0.5017, 0.2869, 0.5456, 0.6929, 0.4129, 0.3994,\n",
      "        0.5817], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4296, 0.4564,    nan, 0.4442,    nan, 0.4442,    nan,    nan,    nan,\n",
      "        0.2177, 0.4299,    nan, 0.5249,    nan,    nan,    nan, 0.3732,    nan,\n",
      "        0.4538, 0.4527,    nan, 0.3628,    nan,    nan, 0.2432, 0.4287,    nan,\n",
      "           nan,    nan, 0.4442, 0.3590,    nan,    nan,    nan,    nan, 0.4354,\n",
      "           nan, 0.4442, 0.5295,    nan, 0.4857,    nan, 0.2686, 0.4720,    nan,\n",
      "        0.4442,    nan, 0.4442,    nan, 0.4936,    nan,    nan,    nan, 0.3685,\n",
      "        0.4363,    nan, 0.4760, 0.4904,    nan,    nan,    nan, 0.3971,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4157, 0.4818, 0.4221, 0.7158, 0.6691, 0.7304, 0.6763, 0.4278, 0.3611,\n",
      "        0.3795, 0.6691, 0.5640, 0.6623, 0.0000, 0.4221, 0.3862, 0.4221, 0.4221,\n",
      "        0.3079, 0.3868, 0.5309, 0.6436, 0.4221, 0.5416, 0.4221, 0.4973, 0.6508,\n",
      "        0.4187, 0.3630, 0.6096, 0.3332, 0.3458, 0.7402, 0.6635, 0.5537, 0.4605,\n",
      "        0.4304, 0.2435, 0.4221, 0.5873, 0.3193, 0.4221, 0.4221, 0.4221, 0.3437,\n",
      "        0.4449, 0.4221, 0.5682, 0.4584, 0.4221, 0.3105, 0.3358, 0.2940, 0.6997,\n",
      "        0.5821, 0.6438, 0.4221, 0.5833, 0.3727, 0.5956, 0.6055, 0.6733, 0.4373,\n",
      "        0.3467], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.4279,    nan, 0.3717, 0.3103, 0.4279, 0.3255, 0.4401,    nan,\n",
      "        0.3729, 0.4279, 0.4897,    nan, 0.4279, 0.3806, 0.4279,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan, 0.3692,    nan, 0.4258,    nan, 0.4307,\n",
      "           nan,    nan,    nan, 0.3330, 0.2581,    nan, 0.4279, 0.4279, 0.4279,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.3353, 0.4279,\n",
      "        0.4279, 0.4279,    nan,    nan,    nan,    nan,    nan, 0.4279, 0.3946,\n",
      "           nan,    nan, 0.4279,    nan,    nan, 0.4279,    nan, 0.4279,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4151, 0.5767, 0.5793, 0.3178, 0.3870, 0.3178, 0.6287, 0.6148, 0.3178,\n",
      "        0.6057, 0.6433, 0.3178, 0.0000, 0.5907, 0.6079, 0.3776, 0.3178, 0.6376,\n",
      "        0.3083, 0.4997, 0.6965, 0.0000, 0.3178, 0.3178, 0.6424, 0.5879, 0.6118,\n",
      "        0.3178, 0.0000, 0.0000, 0.5598, 0.6234, 0.3383, 0.6594, 0.0000, 0.0000,\n",
      "        0.7047, 0.6387, 0.3178, 0.3178, 0.7589, 0.4882, 0.3178, 0.3406, 0.3178,\n",
      "        0.3178, 0.4219, 0.3178, 0.4719, 0.3707, 0.3543, 0.0000, 0.0000, 0.6848,\n",
      "        0.3738, 0.3401, 0.3178, 0.3178, 0.3178, 0.6651, 0.3178, 0.3178, 0.6680,\n",
      "        0.7185], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3757, 0.5009, 0.4867, 0.1575, 0.4649, 0.3757,    nan, 0.4532,    nan,\n",
      "           nan, 0.4554, 0.3757,    nan, 0.3750,    nan, 0.2972, 0.3757, 0.3757,\n",
      "        0.4720, 0.4276,    nan,    nan,    nan,    nan, 0.3757,    nan,    nan,\n",
      "           nan,    nan, 0.3757, 0.4512,    nan,    nan,    nan, 0.4481,    nan,\n",
      "        0.3757,    nan, 0.3757,    nan,    nan, 0.3757, 0.4288,    nan,    nan,\n",
      "           nan,    nan,    nan, 0.3506,    nan,    nan, 0.3757, 0.3757,    nan,\n",
      "           nan,    nan, 0.3757, 0.3757,    nan, 0.3757, 0.3757,    nan, 0.3757,\n",
      "        0.4373], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4162, 0.4162,    nan, 0.3330,    nan,    nan, 0.2714, 0.4836,    nan,\n",
      "           nan, 0.4124, 0.3198, 0.4162, 0.4162,    nan,    nan, 0.4162,    nan,\n",
      "        0.4593,    nan,    nan, 0.4162, 0.4162, 0.4162, 0.4162,    nan,    nan,\n",
      "        0.3763, 0.4162, 0.4162,    nan,    nan,    nan,    nan, 0.4162, 0.4162,\n",
      "           nan, 0.3531, 0.5021, 0.4157,    nan,    nan,    nan,    nan,    nan,\n",
      "        0.3002, 0.3830,    nan, 0.4014, 0.4162,    nan,    nan,    nan, 0.4094,\n",
      "           nan,    nan, 0.3643, 0.4269,    nan,    nan,    nan,    nan, 0.4162,\n",
      "        0.4162], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4066, 0.3449,    nan,    nan, 0.3587, 0.3587,    nan, 0.3587,    nan,\n",
      "        0.3587, 0.3587, 0.3587,    nan,    nan, 0.3469, 0.3587, 0.3587, 0.2966,\n",
      "        0.3587,    nan, 0.3587,    nan,    nan,    nan, 0.3587, 0.3587, 0.3587,\n",
      "           nan, 0.2618, 0.4531, 0.3587, 0.4871,    nan, 0.2868, 0.3587, 0.3587,\n",
      "           nan,    nan,    nan,    nan, 0.3587,    nan, 0.3103,    nan, 0.2934,\n",
      "        0.3587,    nan,    nan, 0.4259,    nan, 0.3871,    nan, 0.3552, 0.3587,\n",
      "           nan, 0.3587, 0.3991,    nan, 0.3587, 0.4145,    nan,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4461, 0.3061, 0.4427, 0.2668, 0.4461,    nan, 0.4461, 0.4298, 0.4461,\n",
      "        0.4461, 0.4461, 0.3898, 0.4461,    nan,    nan, 0.4028, 0.3951, 0.4673,\n",
      "           nan,    nan,    nan,    nan,    nan, 0.4461, 0.4461,    nan,    nan,\n",
      "        0.4461, 0.4461, 0.4337, 0.4461, 0.5829, 0.1787,    nan,    nan, 0.4461,\n",
      "           nan,    nan,    nan,    nan, 0.4994,    nan, 0.4461,    nan, 0.4461,\n",
      "        0.4461,    nan,    nan,    nan,    nan, 0.4461, 0.4410,    nan, 0.1978,\n",
      "           nan, 0.4461, 0.4456,    nan,    nan, 0.3146, 0.4461, 0.4461, 0.4461,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([4.3590e-01, 7.4451e-01, 4.3590e-01, 1.1921e-07, 3.0732e-01, 6.9542e-01,\n",
      "        7.1765e-01, 7.4760e-01, 6.7025e-01, 4.3590e-01, 4.5696e-01, 3.2032e-01,\n",
      "        4.3511e-01, 3.2489e-01, 4.3590e-01, 4.3590e-01, 5.1538e-01, 2.1891e-01,\n",
      "        3.1964e-01, 4.1174e-01, 5.6351e-01, 6.3404e-01, 6.5061e-01, 4.3864e-01,\n",
      "        5.8235e-01, 5.5141e-01, 6.6819e-01, 7.1613e-01, 3.3432e-01, 7.4566e-01,\n",
      "        4.3590e-01, 6.9377e-01, 5.6885e-01, 7.4451e-01, 3.5888e-01, 4.0635e-01,\n",
      "        6.7042e-01, 5.1336e-01, 6.5997e-01, 7.4532e-01, 6.0223e-01, 3.4311e-01,\n",
      "        6.2147e-01, 6.9519e-01, 7.5236e-01, 7.7103e-01, 4.6767e-01, 6.4917e-01,\n",
      "        4.1193e-01, 7.4451e-01, 4.3590e-01, 3.9237e-01, 4.6558e-01, 4.3590e-01,\n",
      "        4.3590e-01, 7.0179e-01, 4.4400e-01, 4.9183e-01, 7.4451e-01, 4.3590e-01,\n",
      "        4.3590e-01, 4.0047e-01, 4.3590e-01, 6.8186e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([4.0520e-01, 1.4875e-01, 6.1176e-01, 2.0580e-01, 6.3154e-01, 6.2123e-01,\n",
      "        5.2430e-01, 4.0520e-01, 5.9380e-01, 7.8953e-01, 4.0520e-01, 4.7548e-01,\n",
      "        4.1049e-01, 4.0520e-01, 5.3920e-01, 5.5201e-01, 4.9287e-01, 3.3276e-01,\n",
      "        5.0239e-01, 6.1187e-01, 4.2678e-01, 4.3969e-01, 5.0699e-01, 7.4047e-01,\n",
      "        4.3914e-01, 4.0520e-01, 6.5702e-01, 4.0520e-01, 4.0520e-01, 7.1315e-01,\n",
      "        4.0520e-01, 4.0520e-01, 4.0520e-01, 4.0520e-01, 7.1849e-01, 4.7192e-01,\n",
      "        5.6245e-01, 6.6031e-01, 5.7099e-01, 4.0520e-01, 4.0520e-01, 4.0520e-01,\n",
      "        5.2162e-01, 6.7167e-01, 4.0520e-01, 4.0520e-01, 6.2674e-01, 6.5702e-01,\n",
      "        4.0520e-01, 3.1214e-01, 4.7884e-01, 7.5479e-01, 3.5360e-01, 3.7442e-01,\n",
      "        4.8335e-01, 4.0520e-01, 6.1603e-01, 4.6275e-01, 4.0520e-01, 1.7881e-07,\n",
      "        4.5233e-01, 6.9487e-01, 4.0520e-01, 6.5702e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([3.4670e-01, 7.6234e-01, 6.8663e-01, 6.9237e-01, 5.6485e-01, 7.0113e-01,\n",
      "        7.1767e-01, 6.4595e-01, 6.5515e-01, 3.4670e-01, 3.9889e-01, 3.4670e-01,\n",
      "        6.9237e-01, 3.1966e-01, 6.0065e-01, 6.9655e-01, 6.2625e-01, 3.4670e-01,\n",
      "        6.5201e-01, 7.5974e-01, 3.4670e-01, 1.1921e-07, 3.4670e-01, 6.9237e-01,\n",
      "        6.9237e-01, 6.1445e-01, 7.0555e-01, 6.2796e-01, 3.9229e-01, 3.4670e-01,\n",
      "        3.4670e-01, 5.9191e-01, 3.4670e-01, 3.4670e-01, 6.6681e-01, 6.2163e-01,\n",
      "        4.6167e-01, 6.8051e-01, 3.4670e-01, 3.9508e-01, 7.8049e-01, 6.3086e-01,\n",
      "        7.0811e-01, 3.4670e-01, 7.4061e-01, 3.5833e-01, 2.6699e-01, 3.2408e-01,\n",
      "        3.4670e-01, 6.7739e-01, 3.4670e-01, 4.3009e-01, 7.7386e-01, 3.4670e-01,\n",
      "        6.9237e-01, 3.4670e-01, 3.4670e-01, 3.4670e-01, 3.4670e-01, 6.7553e-01,\n",
      "        2.7960e-01, 3.4670e-01, 7.3400e-01, 6.6403e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3513,    nan, 0.3908,    nan,    nan, 0.4610, 0.3908,    nan,    nan,\n",
      "        0.3908,    nan, 0.3908,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan, 0.3908,    nan, 0.3908,    nan, 0.3908,    nan, 0.3908,\n",
      "           nan, 0.3908,    nan,    nan,    nan, 0.4710, 0.3908,    nan,    nan,\n",
      "        0.3850,    nan, 0.3908, 0.4195,    nan,    nan, 0.3908, 0.3908, 0.1282,\n",
      "        0.4078, 0.3908, 0.3908, 0.3032,    nan,    nan,    nan,    nan, 0.3908,\n",
      "        0.4927,    nan, 0.3908,    nan, 0.3908, 0.3908, 0.3908,    nan,    nan,\n",
      "        0.3908], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   nan,    nan, 0.3879, 0.3879, 0.3879, 0.3879, 0.4858,    nan, 0.3879,\n",
      "        0.3879,    nan,    nan, 0.3879,    nan,    nan,    nan,    nan,    nan,\n",
      "        0.3879, 0.3879,    nan, 0.5690, 0.3879, 0.5740, 0.3879,    nan, 0.3879,\n",
      "           nan, 0.3879,    nan,    nan, 0.3879, 0.3879,    nan, 0.3879,    nan,\n",
      "        0.2402, 0.4807, 0.4291,    nan,    nan, 0.3879,    nan,    nan,    nan,\n",
      "           nan,    nan, 0.4001, 0.3879,    nan,    nan, 0.1119, 0.4402,    nan,\n",
      "        0.3879,    nan,    nan,    nan, 0.3884, 0.3879, 0.3879, 0.3879, 0.3879,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.2963, 0.2963,    nan,    nan, 0.3298,    nan,    nan,    nan, 0.3733,\n",
      "           nan,    nan, 0.2963, 0.4484,    nan, 0.2963,    nan,    nan,    nan,\n",
      "           nan, 0.2963, 0.2963, 0.3189, 0.2963,    nan,    nan,    nan, 0.2963,\n",
      "        0.3949, 0.3367, 0.2963, 0.2963,    nan,    nan, 0.2963,    nan,    nan,\n",
      "           nan, 0.2963,    nan, 0.3061, 0.4120,    nan, 0.3913,    nan,    nan,\n",
      "           nan,    nan, 0.2963, 0.3225, 0.2963,    nan,    nan,    nan, 0.3401,\n",
      "        0.2963, 0.3886, 0.2963, 0.2963,    nan, 0.2963,    nan, 0.2963, 0.2963,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.3590, 0.3041, 0.3041,    nan, 0.3041,    nan,    nan, 0.3041,\n",
      "        0.3041,    nan,    nan, 0.3041,    nan,    nan,    nan,    nan, 0.3041,\n",
      "           nan, 0.2424,    nan,    nan,    nan, 0.3041, 0.3372, 0.3041,    nan,\n",
      "           nan,    nan, 0.3041,    nan, 0.3041, 0.3041, 0.3041, 0.3041,    nan,\n",
      "        0.3041,    nan, 0.3041,    nan,    nan,    nan,    nan, 0.3041,    nan,\n",
      "           nan, 0.3041, 0.3041, 0.3041,    nan,    nan,    nan,    nan, 0.3041,\n",
      "           nan, 0.2817, 0.3041,    nan, 0.3041,    nan, 0.3483, 0.3061, 0.2968,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.3612,    nan, 0.3612, 0.3693,    nan, 0.3612, 0.3612,\n",
      "        0.4092, 0.3612,    nan,    nan,    nan, 0.3612, 0.3212, 0.4621,    nan,\n",
      "           nan, 0.2462,    nan, 0.3944, 0.3612, 0.3612,    nan, 0.3612, 0.4230,\n",
      "        0.3817,    nan, 0.3737, 0.4708, 0.3944, 0.3582, 0.4671,    nan, 0.3612,\n",
      "        0.3612, 0.3612,    nan,    nan, 0.3612,    nan, 0.3612, 0.3612,    nan,\n",
      "           nan,    nan, 0.3612,    nan,    nan, 0.3619, 0.3950, 0.4122, 0.3612,\n",
      "        0.3612,    nan, 0.4128,    nan,    nan,    nan,    nan, 0.3612,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.4541,    nan, 0.4333,    nan, 0.4508,    nan, 0.3653,    nan,\n",
      "           nan, 0.4333,    nan,    nan, 0.3477, 0.4333,    nan, 0.4333, 0.4333,\n",
      "           nan, 0.4394, 0.2424, 0.4333, 0.4099, 0.2774, 0.5170,    nan,    nan,\n",
      "        0.5032, 0.4333, 0.4333,    nan, 0.4333, 0.5151,    nan,    nan, 0.4333,\n",
      "           nan,    nan,    nan, 0.4333, 0.4333,    nan, 0.4333, 0.4333, 0.3643,\n",
      "           nan, 0.3067,    nan,    nan,    nan, 0.4333, 0.4333, 0.4333, 0.4333,\n",
      "           nan,    nan,    nan,    nan,    nan, 0.4333, 0.4603,    nan, 0.3050,\n",
      "        0.4333], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.4429, 0.5136, 0.4746, 0.4429,    nan,    nan,    nan,    nan,\n",
      "        0.4429,    nan, 0.4429, 0.4429, 0.4062,    nan, 0.2551, 0.3928, 0.4429,\n",
      "           nan, 0.2529,    nan, 0.4429,    nan,    nan,    nan, 0.4358,    nan,\n",
      "        0.4429, 0.4399, 0.4429, 0.4429,    nan, 0.4429,    nan,    nan, 0.1871,\n",
      "           nan, 0.4429, 0.4429, 0.4429,    nan,    nan,    nan, 0.4429, 0.4429,\n",
      "           nan, 0.4429,    nan, 0.4862,    nan, 0.3759,    nan, 0.4429, 0.2952,\n",
      "        0.4438,    nan, 0.4429,    nan, 0.4429,    nan,    nan, 0.4429,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.4917,    nan, 0.3700, 0.3700,    nan,    nan,    nan, 0.4596,\n",
      "        0.3700,    nan, 0.3700, 0.1495, 0.3700,    nan,    nan,    nan,    nan,\n",
      "        0.4387,    nan, 0.4657, 0.3657,    nan, 0.3700,    nan,    nan,    nan,\n",
      "           nan, 0.3700, 0.3700,    nan,    nan,    nan,    nan, 0.3700, 0.3700,\n",
      "           nan,    nan,    nan, 0.3700, 0.3700,    nan, 0.3298,    nan, 0.3700,\n",
      "           nan,    nan,    nan,    nan, 0.3700, 0.3700, 0.3700, 0.4578, 0.3315,\n",
      "           nan,    nan, 0.3700,    nan, 0.4883,    nan,    nan,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.2919, 0.2919,    nan, 0.2919,    nan, 0.2919,    nan,    nan,    nan,\n",
      "           nan, 0.2919, 0.2919, 0.2919,    nan, 0.2919,    nan,    nan, 0.4383,\n",
      "        0.2919,    nan, 0.2919,    nan,    nan,    nan, 0.2919, 0.2919,    nan,\n",
      "        0.2919,    nan,    nan,    nan, 0.4211,    nan, 0.2919,    nan, 0.2596,\n",
      "           nan, 0.3875,    nan,    nan, 0.2919,    nan, 0.3996, 0.2919, 0.2919,\n",
      "        0.3474, 0.2919, 0.3288,    nan, 0.2919, 0.2919,    nan, 0.2919, 0.2919,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.3151,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([3.7109e-01, 4.3509e-01, 6.9963e-01, 3.0610e-01, 3.7109e-01, 1.1921e-07,\n",
      "        3.9634e-01, 1.7316e-01, 3.8257e-01, 3.7109e-01, 6.1755e-01, 3.3300e-01,\n",
      "        6.8656e-01, 4.6800e-01, 5.9125e-01, 3.7109e-01, 6.4140e-01, 6.4750e-01,\n",
      "        3.7109e-01, 5.0602e-01, 3.7109e-01, 3.7109e-01, 3.6473e-01, 6.8656e-01,\n",
      "        5.9825e-01, 3.7109e-01, 3.7109e-01, 7.3229e-01, 3.7109e-01, 3.7109e-01,\n",
      "        3.7109e-01, 5.6866e-01, 3.7109e-01, 3.7109e-01, 6.1185e-01, 6.8656e-01,\n",
      "        3.7109e-01, 3.7109e-01, 8.2078e-01, 3.7109e-01, 6.7940e-01, 3.7109e-01,\n",
      "        4.5361e-01, 5.4448e-01, 3.7109e-01, 3.7109e-01, 3.8982e-01, 6.8656e-01,\n",
      "        7.9256e-01, 2.5691e-01, 3.7109e-01, 5.2471e-01, 4.3734e-01, 6.7789e-01,\n",
      "        4.5971e-01, 6.8656e-01, 7.3963e-01, 6.8656e-01, 3.7109e-01, 4.8214e-01,\n",
      "        7.0010e-01, 6.9886e-01, 3.7109e-01, 3.7109e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4079, 0.3308, 0.2497,    nan,    nan, 0.4079,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan, 0.2677,    nan,    nan, 0.4079, 0.4079, 0.4079,\n",
      "        0.4203,    nan, 0.5483, 0.4079,    nan, 0.2712, 0.4025,    nan, 0.4079,\n",
      "           nan,    nan,    nan,    nan,    nan, 0.4079,    nan, 0.4079,    nan,\n",
      "           nan,    nan,    nan, 0.4079,    nan, 0.4079, 0.4079, 0.4079,    nan,\n",
      "        0.4007, 0.4044, 0.4079, 0.4079,    nan,    nan, 0.4079, 0.4079, 0.5626,\n",
      "        0.4079,    nan, 0.4079, 0.4079,    nan, 0.4079, 0.4079,    nan, 0.4079,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([6.4801e-01, 5.1463e-01, 5.9605e-08, 3.4687e-01, 4.1891e-01, 4.3567e-01,\n",
      "        5.6512e-01, 6.2025e-01, 6.1340e-01, 6.7310e-01, 4.8484e-01, 4.3567e-01,\n",
      "        5.9808e-01, 2.3765e-01, 6.6091e-01, 4.3567e-01, 6.1266e-01, 4.3567e-01,\n",
      "        4.3567e-01, 4.3567e-01, 5.8590e-01, 5.3980e-01, 5.6392e-01, 4.3567e-01,\n",
      "        4.3567e-01, 5.3594e-01, 4.6043e-01, 4.3567e-01, 4.3567e-01, 5.1463e-01,\n",
      "        6.5282e-01, 4.3567e-01, 4.3567e-01, 7.9543e-01, 4.3567e-01, 6.1119e-01,\n",
      "        4.3567e-01, 6.1134e-01, 5.1463e-01, 5.1463e-01, 4.3567e-01, 6.9450e-01,\n",
      "        4.3567e-01, 5.1463e-01, 4.3567e-01, 6.8641e-01, 5.9926e-01, 6.5537e-01,\n",
      "        4.3567e-01, 4.3567e-01, 6.4561e-01, 5.6918e-01, 5.7583e-01, 6.4802e-01,\n",
      "        8.2616e-01, 1.0259e-01, 5.1463e-01, 6.7991e-01, 5.6165e-01, 4.3567e-01,\n",
      "        3.2808e-01, 4.6294e-01, 5.9923e-01, 5.1463e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.3176, 0.4146,    nan,    nan, 0.4146, 0.4146,    nan,\n",
      "        0.4670,    nan,    nan, 0.4146, 0.4146,    nan,    nan,    nan,    nan,\n",
      "        0.3873,    nan, 0.4146,    nan, 0.3919, 0.3193,    nan,    nan,    nan,\n",
      "           nan,    nan, 0.4146, 0.3134,    nan,    nan, 0.3872, 0.4146,    nan,\n",
      "        0.4146,    nan, 0.3581, 0.3704,    nan,    nan,    nan, 0.2261, 0.4312,\n",
      "        0.3533, 0.4146, 0.4052, 0.4135, 0.4925, 0.4629, 0.4146, 0.4146, 0.3743,\n",
      "        0.3579,    nan, 0.4530,    nan, 0.5041,    nan, 0.4146, 0.4510, 0.4146,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan,    nan,    nan,    nan, 0.3795, 0.4877,    nan,    nan,\n",
      "           nan,    nan, 0.4388,    nan, 0.4877,    nan, 0.4877,    nan, 0.4877,\n",
      "           nan,    nan, 0.4216, 0.4877,    nan, 0.4877,    nan, 0.4877, 0.4877,\n",
      "        0.4429, 0.4877, 0.2831, 0.1674,    nan,    nan, 0.4877,    nan, 0.4877,\n",
      "        0.4877,    nan,    nan,    nan, 0.4877,    nan, 0.4877,    nan,    nan,\n",
      "        0.4972, 0.5541,    nan,    nan,    nan,    nan, 0.4877,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan, 0.4877, 0.3564, 0.4877,    nan,    nan,\n",
      "        0.4877], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.7067, 0.6521, 0.4392, 0.4262, 0.7067, 0.4232, 0.6812, 0.4232, 0.4232,\n",
      "        0.7726, 0.4232, 0.4232, 0.3963, 0.4232, 0.4232, 0.4232, 0.4232, 0.7067,\n",
      "        0.6849, 0.7067, 0.6302, 0.4232, 0.4536, 0.5276, 0.6048, 0.4232, 0.4232,\n",
      "        0.6839, 0.6168, 0.4232, 0.4232, 0.6049, 0.7348, 0.0891, 0.4232, 0.4232,\n",
      "        0.4729, 0.6004, 0.7855, 0.6359, 0.4232, 0.3398, 0.4487, 0.1810, 0.4232,\n",
      "        0.4515, 0.4232, 0.7442, 0.7067, 0.5042, 0.2027, 0.4656, 0.7591, 0.4232,\n",
      "        0.4489, 0.4232, 0.6100, 0.4904, 0.5834, 0.5264, 0.0000, 0.6674, 0.7053,\n",
      "        0.4232], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3426,    nan, 0.4577, 0.4577,    nan, 0.5001,    nan, 0.4577, 0.4577,\n",
      "        0.2195,    nan, 0.4575, 0.2928,    nan,    nan,    nan,    nan, 0.3838,\n",
      "           nan, 0.4468,    nan,    nan,    nan,    nan,    nan,    nan, 0.4577,\n",
      "        0.4577,    nan, 0.4577,    nan, 0.4981, 0.4577,    nan, 0.4577,    nan,\n",
      "           nan,    nan,    nan,    nan, 0.4577, 0.4577, 0.3838,    nan,    nan,\n",
      "        0.4577,    nan,    nan, 0.4032,    nan,    nan, 0.4577, 0.3696, 0.4577,\n",
      "        0.4577, 0.4577,    nan, 0.1686,    nan, 0.4388, 0.4577,    nan, 0.4577,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.4811, 0.4811, 0.3937, 0.4811, 0.4634, 0.3207, 0.4811,    nan,\n",
      "        0.4606, 0.2859, 0.4811,    nan,    nan, 0.4811, 0.4811,    nan, 0.2786,\n",
      "        0.4422, 0.4811,    nan, 0.4811, 0.4811, 0.2967,    nan,    nan,    nan,\n",
      "        0.4188, 0.1364,    nan,    nan, 0.4811,    nan, 0.4726, 0.4223,    nan,\n",
      "           nan, 0.3320, 0.4811, 0.2514, 0.4811,    nan,    nan, 0.3971,    nan,\n",
      "        0.3424,    nan, 0.4485, 0.4811, 0.4811,    nan,    nan, 0.1888,    nan,\n",
      "           nan, 0.4811,    nan,    nan,    nan,    nan,    nan,    nan, 0.4811,\n",
      "        0.4811], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.5918, 0.4501, 0.3661, 0.5886, 0.4480, 0.4222, 0.6367, 0.5924, 0.5865,\n",
      "        0.5824, 0.3583, 0.3583, 0.4093, 0.3220, 0.0348, 0.3583, 0.3724, 0.4936,\n",
      "        0.1503, 0.5366, 0.3583, 0.3583, 0.4940, 0.3583, 0.4514, 0.3583, 0.5007,\n",
      "        0.6445, 0.4922, 0.3583, 0.3834, 0.3583, 0.3749, 0.3583, 0.6566, 0.3317,\n",
      "        0.3583, 0.5329, 0.4808, 0.5902, 0.5596, 0.3583, 0.3583, 0.4388, 0.3583,\n",
      "        0.3583, 0.3583, 0.6642, 0.3988, 0.5537, 0.3583, 0.5011, 0.5990, 0.6100,\n",
      "        0.3583, 0.3487, 0.3583, 0.5635, 0.2277, 0.3583, 0.3583, 0.3583, 0.3583,\n",
      "        0.5739], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([ 7.0106e-01,  6.7211e-01,  3.1943e-01,  6.6827e-01,  5.3474e-01,\n",
      "         6.9922e-01,  7.4801e-01,  4.7617e-01,  4.7617e-01,  4.7617e-01,\n",
      "         7.0106e-01,  6.9087e-01,  4.3923e-01,  4.7549e-01,  6.3093e-01,\n",
      "         6.1971e-01,  4.7617e-01,  4.7617e-01,  6.6598e-01,  4.7617e-01,\n",
      "         4.7617e-01,  7.0106e-01,  6.6851e-01,  4.7617e-01,  7.1575e-01,\n",
      "         7.6646e-01,  6.1883e-01,  6.2346e-01,  4.7617e-01,  5.7514e-01,\n",
      "         7.0106e-01,  6.9106e-01,  5.7386e-01,  7.1040e-01,  5.4782e-01,\n",
      "         6.0584e-01,  5.8101e-01,  4.7617e-01, -1.1921e-07,  6.9412e-01,\n",
      "         5.6619e-01,  4.0942e-01,  7.1524e-01,  4.7617e-01,  4.7617e-01,\n",
      "         3.0374e-01,  4.5958e-01,  6.8756e-01,  4.7617e-01,  4.5026e-01,\n",
      "         4.7617e-01,  6.3650e-01,  6.7428e-01,  4.7617e-01,  4.7617e-01,\n",
      "         6.8038e-01,  5.1872e-01,  4.7617e-01,  5.6201e-01,  7.0106e-01,\n",
      "         4.7617e-01,  4.7617e-01,  4.7617e-01,  1.5986e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.4713, 0.4017, 0.4017,    nan, 0.2887, 0.4010,    nan,    nan,\n",
      "           nan,    nan, 0.4017, 0.4017, 0.4017,    nan,    nan, 0.4017,    nan,\n",
      "           nan,    nan, 0.4017, 0.3743,    nan, 0.4017,    nan, 0.4097,    nan,\n",
      "        0.3118, 0.3580, 0.4017, 0.4017,    nan,    nan,    nan, 0.4017, 0.4017,\n",
      "           nan, 0.4017, 0.3286, 0.4017,    nan,    nan,    nan, 0.3477,    nan,\n",
      "           nan, 0.4017,    nan,    nan,    nan,    nan,    nan, 0.4017, 0.4017,\n",
      "        0.4017, 0.4017, 0.2497,    nan,    nan,    nan,    nan, 0.4017,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.6437, 0.5718, 0.6439, 0.6788, 0.3755, 0.7130, 0.3755, 0.3755, 0.4656,\n",
      "        0.3755, 0.3755, 0.6380, 0.5389, 0.7120, 0.7030, 0.3755, 0.6273, 0.2137,\n",
      "        0.4826, 0.6545, 0.3849, 0.3755, 0.4068, 0.8076, 0.2810, 0.3603, 0.3755,\n",
      "        0.3755, 0.6382, 0.6603, 0.0000, 0.3755, 0.7500, 0.3755, 0.6396, 0.7439,\n",
      "        0.5742, 0.3755, 0.6861, 0.3981, 0.3755, 0.6591, 0.7199, 0.3633, 0.3755,\n",
      "        0.6437, 0.4164, 0.3755, 0.4861, 0.3755, 0.4826, 0.5960, 0.3960, 0.3755,\n",
      "        0.3755, 0.4272, 0.6750, 0.3755, 0.4571, 0.2480, 0.5710, 0.5795, 0.6589,\n",
      "        0.3755], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([7.0554e-01, 7.8345e-01, 6.4745e-01, 1.1921e-07, 7.2165e-01, 6.3816e-01,\n",
      "        4.0659e-01, 4.0659e-01, 4.0659e-01, 2.8348e-01, 6.7765e-01, 4.0659e-01,\n",
      "        2.9686e-01, 7.1117e-01, 4.2056e-01, 7.0540e-01, 5.9229e-01, 4.0236e-01,\n",
      "        7.0260e-01, 3.6688e-01, 6.8323e-01, 7.0261e-01, 4.0659e-01, 6.3378e-01,\n",
      "        5.8033e-01, 7.2887e-01, 4.0659e-01, 4.0659e-01, 3.7536e-01, 4.5037e-01,\n",
      "        2.5754e-01, 4.0659e-01, 4.0659e-01, 5.7583e-01, 7.0402e-01, 7.0029e-01,\n",
      "        6.1346e-01, 4.0659e-01, 4.0659e-01, 6.9979e-01, 7.4029e-01, 4.0659e-01,\n",
      "        4.0659e-01, 4.3092e-01, 4.0659e-01, 5.9017e-01, 4.0659e-01, 6.5405e-01,\n",
      "        5.8619e-01, 6.0648e-01, 4.0659e-01, 6.9228e-01, 4.0659e-01, 4.0659e-01,\n",
      "        7.4693e-01, 1.3737e-01, 4.0659e-01, 4.0659e-01, 6.9854e-01, 4.0659e-01,\n",
      "        7.4587e-01, 7.1623e-01, 3.7440e-01, 7.2008e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.4671, 0.3817,    nan, 0.3259,    nan,    nan, 0.4285, 0.4285,\n",
      "        0.4285,    nan, 0.4285,    nan, 0.3141, 0.4181,    nan,    nan, 0.4285,\n",
      "        0.3904,    nan, 0.4306, 0.3668, 0.2912, 0.3472, 0.4285, 0.4285, 0.4285,\n",
      "           nan,    nan,    nan, 0.4536,    nan,    nan, 0.4310,    nan,    nan,\n",
      "        0.4285, 0.4285, 0.4285,    nan, 0.4499,    nan,    nan,    nan,    nan,\n",
      "        0.3951, 0.4285, 0.4285,    nan,    nan, 0.4285, 0.4285, 0.3151,    nan,\n",
      "        0.4285,    nan,    nan,    nan,    nan, 0.4285,    nan, 0.4512,    nan,\n",
      "        0.2664], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3783, 0.5092,    nan, 0.1992,    nan,    nan, 0.3783, 0.4814,    nan,\n",
      "           nan, 0.4782, 0.3783, 0.3783, 0.3783,    nan, 0.3783,    nan,    nan,\n",
      "        0.3783, 0.3783,    nan,    nan,    nan, 0.3851,    nan,    nan,    nan,\n",
      "        0.3783, 0.4622,    nan,    nan,    nan,    nan, 0.3783, 0.3783, 0.3783,\n",
      "        0.4649, 0.4586,    nan, 0.3766, 0.3783,    nan, 0.3783,    nan, 0.3783,\n",
      "        0.3783, 0.5000,    nan, 0.3783,    nan,    nan,    nan, 0.2149,    nan,\n",
      "           nan, 0.3783,    nan, 0.3751, 0.3903, 0.3783, 0.3783,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.3779, 0.4250, 0.4040,    nan, 0.2728,    nan,    nan,\n",
      "           nan, 0.2710, 0.2567,    nan, 0.4250,    nan,    nan, 0.4077,    nan,\n",
      "        0.4250,    nan,    nan,    nan,    nan, 0.4250,    nan, 0.4250, 0.2469,\n",
      "           nan, 0.4250, 0.4250, 0.4250, 0.4250, 0.4250, 0.1823, 0.4250,    nan,\n",
      "           nan,    nan,    nan, 0.4133,    nan,    nan,    nan,    nan, 0.5170,\n",
      "        0.4250,    nan,    nan, 0.4250,    nan, 0.4179, 0.4279,    nan, 0.3582,\n",
      "           nan,    nan,    nan,    nan,    nan, 0.5111,    nan, 0.4809,    nan,\n",
      "        0.4250], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3802, 0.3802, 0.3802,    nan,    nan,    nan, 0.3802, 0.3802, 0.3802,\n",
      "           nan,    nan,    nan, 0.4023, 0.3764,    nan, 0.3802, 0.3802, 0.3802,\n",
      "        0.3676, 0.4740, 0.4272, 0.3802,    nan, 0.3802,    nan,    nan,    nan,\n",
      "        0.3802, 0.4832, 0.3802, 0.3802, 0.3802, 0.3516,    nan,    nan, 0.1157,\n",
      "        0.3802, 0.3248,    nan, 0.3317,    nan,    nan, 0.3802,    nan,    nan,\n",
      "           nan,    nan, 0.3802, 0.3802,    nan,    nan, 0.4629,    nan, 0.3802,\n",
      "        0.3802, 0.3802,    nan, 0.3802,    nan,    nan,    nan, 0.2105, 0.3802,\n",
      "        0.3802], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.3735, 0.4716, 0.4716, 0.4884,    nan, 0.4716,    nan,    nan,\n",
      "        0.4183,    nan,    nan, 0.4716,    nan,    nan, 0.4716,    nan, 0.4716,\n",
      "        0.4716,    nan, 0.4716,    nan,    nan, 0.4716, 0.4716, 0.4716,    nan,\n",
      "           nan, 0.5939,    nan,    nan,    nan, 0.3403, 0.4716,    nan, 0.4716,\n",
      "        0.4716, 0.0916,    nan, 0.4716,    nan,    nan, 0.4716,    nan, 0.4716,\n",
      "           nan, 0.4716,    nan, 0.4716,    nan, 0.4716, 0.3349, 0.2843,    nan,\n",
      "        0.4716,    nan, 0.4028,    nan,    nan, 0.4716,    nan, 0.4716,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([4.6299e-01, 4.8819e-01, 5.9605e-08, 4.3880e-01, 5.6063e-01, 4.3880e-01,\n",
      "        4.3880e-01, 6.2107e-01, 5.8396e-01, 4.3880e-01, 4.3880e-01, 5.7788e-01,\n",
      "        6.1123e-01, 5.7788e-01, 6.2342e-01, 5.6113e-01, 5.7788e-01, 6.6773e-01,\n",
      "        5.8093e-01, 4.3880e-01, 6.8857e-01, 6.4376e-01, 5.4053e-01, 4.3880e-01,\n",
      "        5.7788e-01, 4.3880e-01, 5.6903e-01, 7.1225e-01, 2.3917e-01, 4.3880e-01,\n",
      "        4.3880e-01, 2.8000e-01, 2.2908e-01, 6.0246e-01, 5.3262e-01, 6.2706e-01,\n",
      "        5.7788e-01, 5.1828e-01, 4.8471e-01, 6.3516e-01, 4.3880e-01, 5.6027e-01,\n",
      "        6.2230e-01, 6.3093e-01, 5.6392e-01, 4.3880e-01, 5.8368e-01, 5.0622e-01,\n",
      "        6.3685e-01, 5.0155e-01, 5.8066e-01, 4.6736e-01, 4.3880e-01, 4.3880e-01,\n",
      "        5.1355e-01, 3.0440e-01, 4.3880e-01, 6.2142e-01, 5.4612e-01, 6.9874e-01,\n",
      "        4.3951e-01, 1.8789e-01, 4.3880e-01, 5.7375e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4870, 0.4111,    nan, 0.2946, 0.3977, 0.3192, 0.3480, 0.4111, 0.4111,\n",
      "           nan, 0.4111, 0.4111, 0.4111,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan, 0.4932, 0.3518,    nan,    nan,    nan, 0.4111, 0.4111,\n",
      "           nan, 0.4335,    nan,    nan,    nan, 0.4111,    nan, 0.3794, 0.4111,\n",
      "        0.4118, 0.3052, 0.4111, 0.4111,    nan, 0.4111, 0.4111, 0.4111, 0.4111,\n",
      "           nan,    nan,    nan,    nan,    nan, 0.4111, 0.4111,    nan, 0.4182,\n",
      "           nan,    nan, 0.4111, 0.4111, 0.4111, 0.4111, 0.2662,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([3.6922e-01, 6.4760e-01, 5.9501e-01, 3.6922e-01, 3.6922e-01, 5.9160e-01,\n",
      "        3.7156e-01, 6.9429e-01, 3.6922e-01, 5.9160e-01, 6.6892e-01, 6.9298e-01,\n",
      "        3.6922e-01, 3.6922e-01, 3.6922e-01, 3.6922e-01, 4.4484e-01, 4.1832e-01,\n",
      "        4.0855e-01, 3.6922e-01, 5.9160e-01, 6.2020e-01, 5.9160e-01, 3.6922e-01,\n",
      "        5.9160e-01, 3.6922e-01, 6.2924e-01, 5.9605e-08, 5.9160e-01, 5.9160e-01,\n",
      "        5.9319e-01, 3.6922e-01, 6.6899e-01, 7.2863e-01, 3.6922e-01, 3.6922e-01,\n",
      "        5.7846e-01, 3.6922e-01, 5.9428e-01, 3.6922e-01, 4.3320e-01, 5.8178e-01,\n",
      "        5.9436e-01, 2.9423e-01, 3.6922e-01, 6.2189e-01, 3.6922e-01, 5.6012e-01,\n",
      "        6.1926e-01, 3.6922e-01, 3.6468e-01, 2.6229e-01, 6.3409e-01, 3.6922e-01,\n",
      "        5.1498e-01, 4.5031e-01, 3.6922e-01, 6.5036e-01, 5.9160e-01, 3.6922e-01,\n",
      "        3.6922e-01, 3.6922e-01, 5.7055e-01, 3.6922e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.3907,    nan, 0.3919, 0.3353, 0.3919,    nan,    nan,\n",
      "           nan,    nan,    nan, 0.3919,    nan, 0.3919, 0.3543, 0.3476, 0.3919,\n",
      "        0.3919, 0.3919,    nan,    nan,    nan, 0.3919, 0.3919,    nan,    nan,\n",
      "           nan,    nan,    nan, 0.3919, 0.3919, 0.2754, 0.4156,    nan,    nan,\n",
      "           nan, 0.3919,    nan, 0.3153, 0.3919, 0.3919, 0.3919,    nan,    nan,\n",
      "           nan, 0.3919, 0.3919,    nan,    nan, 0.3919,    nan,    nan, 0.3919,\n",
      "        0.3919,    nan, 0.3919,    nan, 0.3919,    nan,    nan,    nan, 0.3257,\n",
      "        0.3163], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.6311, 0.4267, 0.7459, 0.5580, 0.5430, 0.4305, 0.4926, 0.6833, 0.5199,\n",
      "        0.6675, 0.4267, 0.3440, 0.4267, 0.6551, 0.4643, 0.4267, 0.5595, 0.6565,\n",
      "        0.7177, 0.6809, 0.4267, 0.5430, 0.4220, 0.4267, 0.6090, 0.6306, 0.5430,\n",
      "        0.4641, 0.4267, 0.4267, 0.7548, 0.5430, 0.1746, 0.6095, 0.6469, 0.0000,\n",
      "        0.3267, 0.3042, 0.4267, 0.6354, 0.3486, 0.3964, 0.6165, 0.6524, 0.5430,\n",
      "        0.6106, 0.4267, 0.6680, 0.4267, 0.4386, 0.4267, 0.6224, 0.4267, 0.5430,\n",
      "        0.5430, 0.7470, 0.4267, 0.5430, 0.4267, 0.4267, 0.4267, 0.5697, 0.4267,\n",
      "        0.4267], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.3346, 0.3346, 0.3346, 0.4077, 0.3346, 0.3346, 0.1833,\n",
      "           nan,    nan,    nan, 0.3346,    nan, 0.3346, 0.4324, 0.3346,    nan,\n",
      "           nan, 0.3346, 0.3926,    nan,    nan,    nan, 0.3346,    nan,    nan,\n",
      "           nan,    nan, 0.3576,    nan,    nan, 0.3346, 0.3346, 0.3346, 0.3346,\n",
      "           nan,    nan,    nan,    nan, 0.3346,    nan, 0.3346,    nan, 0.3346,\n",
      "           nan,    nan, 0.3283,    nan, 0.3346,    nan,    nan,    nan,    nan,\n",
      "        0.2829, 0.3346, 0.3346,    nan, 0.3346,    nan, 0.3346, 0.3346, 0.3346,\n",
      "        0.3346], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan,    nan, 0.3068,    nan,    nan,    nan, 0.3068,    nan,\n",
      "        0.3068,    nan, 0.3068,    nan,    nan,    nan, 0.3068,    nan,    nan,\n",
      "           nan, 0.3068,    nan,    nan, 0.3068,    nan, 0.3068,    nan, 0.3068,\n",
      "           nan,    nan,    nan, 0.3068,    nan, 0.3471, 0.3068,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan, 0.3068,    nan,    nan, 0.3068,    nan,\n",
      "           nan, 0.4152,    nan,    nan,    nan,    nan,    nan, 0.1410,    nan,\n",
      "        0.3068,    nan, 0.3068,    nan, 0.4059, 0.3338, 0.3068,    nan, 0.3068,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.5103, 0.5503, 0.5879,    nan,    nan, 0.2141, 0.5103,    nan, 0.3685,\n",
      "        0.0970,    nan,    nan, 0.5288, 0.5103, 0.5891,    nan,    nan, 0.5103,\n",
      "           nan, 0.5103, 0.4998,    nan, 0.5103,    nan,    nan, 0.4891,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan, 0.5103,    nan, 0.4949,    nan,    nan,    nan, 0.5103, 0.5103,\n",
      "           nan, 0.5103,    nan, 0.5103,    nan,    nan,    nan, 0.4027,    nan,\n",
      "           nan,    nan, 0.5103, 0.5048,    nan, 0.5103, 0.5103,    nan, 0.5103,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan,    nan,    nan,    nan, 0.3003,    nan, 0.3480, 0.2224,\n",
      "        0.3553, 0.3480,    nan,    nan,    nan,    nan,    nan, 0.3480, 0.3480,\n",
      "           nan, 0.3480, 0.3480,    nan, 0.3480,    nan,    nan, 0.3214,    nan,\n",
      "        0.3480,    nan,    nan,    nan,    nan, 0.3480,    nan,    nan, 0.3498,\n",
      "        0.3982,    nan, 0.3480,    nan, 0.3480, 0.3480, 0.3480,    nan,    nan,\n",
      "        0.4357,    nan, 0.3480,    nan, 0.2500, 0.3480,    nan,    nan,    nan,\n",
      "        0.3969, 0.3480, 0.3480,    nan,    nan, 0.3480,    nan, 0.3480, 0.4155,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3996, 0.2919,    nan, 0.4406, 0.5364,    nan, 0.4406,    nan, 0.4406,\n",
      "           nan, 0.4406, 0.4406, 0.4210, 0.3533,    nan, 0.5082,    nan, 0.4381,\n",
      "        0.4406, 0.4406, 0.4406, 0.2930,    nan,    nan,    nan, 0.4406, 0.4670,\n",
      "        0.4406, 0.4406, 0.4946, 0.4406,    nan,    nan, 0.1349, 0.4406, 0.4406,\n",
      "        0.4406,    nan, 0.4307, 0.3197,    nan, 0.2233,    nan,    nan, 0.4406,\n",
      "           nan,    nan,    nan,    nan, 0.4406, 0.4406, 0.4406, 0.3892,    nan,\n",
      "        0.4406,    nan, 0.4733, 0.4406, 0.4396, 0.4406,    nan, 0.2805, 0.2801,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.4688, 0.4688, 0.4688,    nan, 0.4688, 0.3807, 0.4948, 0.3675,\n",
      "           nan, 0.3466,    nan,    nan, 0.4688,    nan, 0.4688,    nan, 0.4688,\n",
      "           nan,    nan, 0.2334, 0.3425, 0.4224,    nan, 0.4688,    nan, 0.4688,\n",
      "           nan,    nan,    nan, 0.4772,    nan, 0.4688,    nan, 0.5042, 0.3581,\n",
      "           nan,    nan,    nan, 0.4688,    nan, 0.3314, 0.3987, 0.4688, 0.4688,\n",
      "           nan, 0.4688,    nan,    nan, 0.4688,    nan, 0.4709,    nan, 0.4688,\n",
      "        0.4064, 0.4688,    nan, 0.4688,    nan, 0.3497, 0.4222,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.2852,    nan,    nan, 0.3123,    nan,    nan, 0.3123,    nan,    nan,\n",
      "           nan,    nan, 0.3123,    nan, 0.3123,    nan, 0.3123, 0.3123, 0.3123,\n",
      "        0.3123, 0.3123,    nan,    nan,    nan,    nan, 0.3123, 0.3123,    nan,\n",
      "        0.3123,    nan, 0.3123,    nan,    nan, 0.3780, 0.3123, 0.3123,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan, 0.3123, 0.2191, 0.4626, 0.3123,\n",
      "           nan, 0.3123,    nan,    nan, 0.3123, 0.3123, 0.2982, 0.4567, 0.3067,\n",
      "        0.4010,    nan,    nan,    nan, 0.3123, 0.3123,    nan, 0.2745, 0.4108,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4609, 0.4609,    nan,    nan,    nan,    nan, 0.4609, 0.3441,    nan,\n",
      "           nan,    nan,    nan,    nan, 0.4609,    nan, 0.4882, 0.4609, 0.4609,\n",
      "        0.4609,    nan, 0.4609,    nan,    nan, 0.4609, 0.4609, 0.4609, 0.3607,\n",
      "        0.4609, 0.4609,    nan,    nan, 0.4609,    nan,    nan,    nan,    nan,\n",
      "           nan, 0.4609, 0.4609,    nan,    nan,    nan,    nan, 0.4609,    nan,\n",
      "           nan,    nan, 0.4609,    nan,    nan,    nan, 0.4609,    nan, 0.4609,\n",
      "        0.4609, 0.3757,    nan,    nan, 0.6063, 0.4609, 0.0670, 0.4609,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.4176,    nan,    nan,    nan,    nan, 0.4040,    nan, 0.4040,\n",
      "           nan,    nan,    nan, 0.4497, 0.4040, 0.4040, 0.4040,    nan,    nan,\n",
      "           nan,    nan, 0.4040,    nan,    nan, 0.4040, 0.3904,    nan,    nan,\n",
      "           nan,    nan, 0.4040, 0.4077,    nan, 0.1771, 0.4040, 0.4040,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan, 0.4040, 0.4040,    nan,    nan,\n",
      "           nan, 0.4040,    nan,    nan,    nan,    nan, 0.2524,    nan, 0.4040,\n",
      "           nan, 0.4040,    nan,    nan,    nan,    nan, 0.4040,    nan,    nan,\n",
      "        0.4040], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3702, 0.2437, 0.5874, 0.3696, 0.3696, 0.3971, 0.6124, 0.3696, 0.3696,\n",
      "        0.6231, 0.3696, 0.7212, 0.6504, 0.7062, 0.3696, 0.3216, 0.6747, 0.7178,\n",
      "        0.6504, 0.3696, 0.0000, 0.3696, 0.5309, 0.3696, 0.2715, 0.4455, 0.5523,\n",
      "        0.6371, 0.3696, 0.7190, 0.3696, 0.3786, 0.3696, 0.4251, 0.3811, 0.3696,\n",
      "        0.6114, 0.7323, 0.6504, 0.3696, 0.5487, 0.3696, 0.3696, 0.6504, 0.6146,\n",
      "        0.4913, 0.7208, 0.3660, 0.3696, 0.3696, 0.6282, 0.7124, 0.3696, 0.6734,\n",
      "        0.4350, 0.4286, 0.3696, 0.3220, 0.6504, 0.7088, 0.4527, 0.3696, 0.3696,\n",
      "        0.3696], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3842, 0.4775, 0.3842, 0.3842, 0.3842, 0.3842, 0.3709, 0.7665, 0.3842,\n",
      "        0.6974, 0.7595, 0.3842, 0.4868, 0.6690, 0.3842, 0.2210, 0.7488, 0.5686,\n",
      "        0.2978, 0.3842, 0.6584, 0.3842, 0.4512, 0.3842, 0.5479, 0.3842, 0.6690,\n",
      "        0.3842, 0.4489, 0.3842, 0.6534, 0.3678, 0.6687, 0.3842, 0.4234, 0.7359,\n",
      "        0.3842, 0.7225, 0.6999, 0.4172, 0.4661, 0.7005, 0.4196, 0.7256, 0.4126,\n",
      "        0.7318, 0.3400, 0.3978, 0.7313, 0.8297, 0.7037, 0.3842, 0.7783, 0.0000,\n",
      "        0.6616, 0.6690, 0.6690, 0.6690, 0.3842, 0.3842, 0.3842, 0.6764, 0.6690,\n",
      "        0.3842], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.1216, 0.3604,    nan, 0.3604,    nan, 0.3604, 0.3777,    nan, 0.3604,\n",
      "           nan, 0.4917, 0.3604,    nan, 0.3604, 0.3801,    nan,    nan,    nan,\n",
      "        0.3065, 0.3604, 0.3604, 0.3604,    nan,    nan, 0.3604,    nan,    nan,\n",
      "        0.3604, 0.3604,    nan,    nan,    nan,    nan, 0.3604, 0.3604,    nan,\n",
      "        0.3604, 0.2540,    nan, 0.4396,    nan, 0.3604, 0.3604,    nan, 0.3604,\n",
      "           nan,    nan,    nan,    nan,    nan, 0.3604, 0.3604, 0.5508,    nan,\n",
      "        0.3604, 0.3604,    nan,    nan, 0.3604,    nan, 0.3604, 0.3604,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4147, 0.4147, 0.1435,    nan,    nan, 0.4707,    nan,    nan, 0.4147,\n",
      "           nan,    nan, 0.4084,    nan, 0.4147,    nan, 0.3235, 0.5007,    nan,\n",
      "        0.4117,    nan, 0.4147,    nan, 0.4147,    nan,    nan, 0.4147,    nan,\n",
      "        0.4147,    nan,    nan, 0.5054, 0.4147,    nan,    nan, 0.4147,    nan,\n",
      "           nan,    nan, 0.4222,    nan,    nan, 0.4147,    nan, 0.4147,    nan,\n",
      "           nan,    nan,    nan,    nan, 0.4147, 0.2685,    nan, 0.4147,    nan,\n",
      "           nan, 0.5162, 0.3064,    nan, 0.4147, 0.4147, 0.4147,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4643, 0.4643, 0.6254, 0.6934, 0.0000, 0.7513, 0.4643, 0.4743, 0.4643,\n",
      "        0.6834, 0.7156, 0.6689, 0.6690, 0.3792, 0.0000, 0.7111, 0.6084, 0.6164,\n",
      "        0.5816, 0.0000, 0.4643, 0.4643, 0.1803, 0.4643, 0.5127, 0.0000, 0.4643,\n",
      "        0.4643, 0.4643, 0.4643, 0.6725, 0.5885, 0.1148, 0.4643, 0.4643, 0.1160,\n",
      "        0.4643, 0.4643, 0.4643, 0.4643, 0.6689, 0.4643, 0.7098, 0.0000, 0.4399,\n",
      "        0.4643, 0.6871, 0.4643, 0.6692, 0.0000, 0.6361, 0.4643, 0.4643, 0.4643,\n",
      "        0.4643, 0.6572, 0.5583, 0.0000, 0.2954, 0.5396, 0.7037, 0.7067, 0.0000,\n",
      "        0.4726], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   nan,    nan, 0.4310, 0.4402, 0.4310,    nan,    nan, 0.3876,    nan,\n",
      "           nan,    nan, 0.4310,    nan,    nan, 0.4370, 0.4310,    nan,    nan,\n",
      "        0.4039,    nan, 0.4310, 0.3618,    nan,    nan, 0.4310, 0.4071, 0.2616,\n",
      "        0.4310, 0.3815, 0.4310,    nan, 0.4310,    nan,    nan, 0.4310, 0.4731,\n",
      "        0.4310,    nan,    nan, 0.3114,    nan, 0.4310,    nan, 0.4310, 0.2465,\n",
      "        0.4310,    nan,    nan, 0.4310, 0.2163,    nan, 0.4981,    nan,    nan,\n",
      "        0.3042, 0.4310, 0.5031, 0.4310,    nan, 0.4310,    nan, 0.4310, 0.4310,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3880,    nan,    nan,    nan, 0.3880,    nan, 0.3880, 0.3880,    nan,\n",
      "        0.3880,    nan, 0.2839,    nan, 0.3880,    nan,    nan, 0.3880,    nan,\n",
      "           nan,    nan, 0.3880,    nan,    nan, 0.3880, 0.2737,    nan, 0.3880,\n",
      "        0.3880,    nan,    nan, 0.3880, 0.3880,    nan, 0.3880, 0.2252,    nan,\n",
      "        0.3880, 0.3880,    nan,    nan,    nan, 0.3880, 0.4616,    nan, 0.3880,\n",
      "        0.3880,    nan, 0.3897,    nan, 0.3880,    nan,    nan, 0.3880, 0.3880,\n",
      "        0.4177,    nan, 0.3880,    nan,    nan, 0.3880,    nan,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.3672,    nan,    nan,    nan,    nan,    nan, 0.3672,\n",
      "        0.4460, 0.3672,    nan,    nan, 0.3672,    nan, 0.3672,    nan,    nan,\n",
      "           nan, 0.3672,    nan,    nan,    nan, 0.3672,    nan, 0.4209,    nan,\n",
      "        0.3672,    nan, 0.3672, 0.3672, 0.3672,    nan, 0.3978, 0.3672, 0.3672,\n",
      "        0.3672,    nan, 0.3672, 0.3672, 0.3672,    nan,    nan,    nan,    nan,\n",
      "        0.3565,    nan,    nan, 0.3672, 0.3672,    nan,    nan,    nan, 0.3672,\n",
      "        0.2771, 0.3672,    nan, 0.3672,    nan, 0.2518, 0.3672, 0.2384,    nan,\n",
      "        0.3672], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4408, 0.4408, 0.4408, 0.4408,    nan,    nan, 0.4408, 0.4408,    nan,\n",
      "           nan, 0.4408, 0.4408, 0.5308, 0.4408,    nan, 0.4408,    nan, 0.5096,\n",
      "        0.4408, 0.4408,    nan, 0.4408, 0.0769, 0.4408, 0.4408, 0.4408, 0.4408,\n",
      "           nan, 0.4408, 0.4408,    nan,    nan,    nan,    nan, 0.4408,    nan,\n",
      "           nan,    nan, 0.4408, 0.4408, 0.4737, 0.4408,    nan,    nan,    nan,\n",
      "        0.4408,    nan, 0.4408,    nan, 0.3190,    nan,    nan,    nan,    nan,\n",
      "        0.4408, 0.4554,    nan,    nan, 0.5146, 0.4408,    nan,    nan, 0.4408,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.4608, 0.4608,    nan,    nan, 0.4084, 0.4608, 0.4608,\n",
      "        0.2599,    nan, 0.2651, 0.4608,    nan,    nan,    nan, 0.3609, 0.4608,\n",
      "           nan, 0.4608, 0.4608, 0.4608,    nan,    nan, 0.4608, 0.2557, 0.4608,\n",
      "        0.4608,    nan, 0.4608, 0.4426,    nan, 0.4608,    nan, 0.4518, 0.4096,\n",
      "        0.4608,    nan,    nan,    nan,    nan, 0.4608, 0.4598, 0.5046, 0.2680,\n",
      "           nan, 0.4608, 0.4608,    nan, 0.4201,    nan, 0.2965,    nan,    nan,\n",
      "        0.4216, 0.4608, 0.4608, 0.4608, 0.4608,    nan,    nan,    nan, 0.4608,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.4562,    nan, 0.4562,    nan, 0.4562,    nan, 0.1162,\n",
      "           nan, 0.4562, 0.4562,    nan, 0.4688,    nan, 0.4562,    nan,    nan,\n",
      "        0.4562,    nan, 0.4562,    nan, 0.4562, 0.3637, 0.4562, 0.4562,    nan,\n",
      "           nan, 0.1823,    nan,    nan, 0.4562, 0.5085, 0.4562,    nan,    nan,\n",
      "           nan, 0.4562,    nan, 0.4562, 0.4562,    nan, 0.4562,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan, 0.4562, 0.4562,    nan,    nan, 0.4562,\n",
      "        0.4562,    nan,    nan, 0.4562, 0.4562,    nan,    nan,    nan, 0.4562,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.2313, 0.2255,    nan, 0.4924, 0.4924,    nan,    nan,    nan,\n",
      "           nan, 0.4924, 0.4287, 0.4924,    nan,    nan,    nan,    nan,    nan,\n",
      "        0.5032,    nan, 0.1511, 0.4924,    nan,    nan,    nan, 0.4924,    nan,\n",
      "           nan,    nan, 0.4924,    nan,    nan, 0.4924,    nan, 0.4924,    nan,\n",
      "           nan,    nan,    nan, 0.4924, 0.4924, 0.1147, 0.4924, 0.4924, 0.4924,\n",
      "           nan, 0.4924, 0.4924,    nan,    nan, 0.4924,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan, 0.3607,    nan, 0.4924, 0.4924,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([4.2021e-01, 7.1131e-01, 6.7856e-01, 7.1403e-01, 4.2021e-01, 6.7856e-01,\n",
      "        4.2021e-01, 4.2021e-01, 5.6371e-01, 4.2021e-01, 6.9037e-01, 6.0693e-01,\n",
      "        4.2408e-01, 5.9303e-01, 5.8228e-01, 4.2021e-01, 6.5676e-01, 4.2021e-01,\n",
      "        3.7519e-01, 2.0344e-01, 1.6535e-01, 5.2672e-01, 4.2021e-01, 4.2021e-01,\n",
      "        3.0022e-01, 6.1896e-01, 4.2021e-01, 6.8076e-01, 4.4522e-01, 4.2021e-01,\n",
      "        4.2021e-01, 4.2021e-01, 6.7856e-01, 4.2021e-01, 4.2021e-01, 3.5217e-01,\n",
      "        6.7856e-01, 6.0287e-01, 4.4834e-01, 3.2811e-01, 4.2021e-01, 5.5061e-01,\n",
      "        4.2021e-01, 3.9322e-01, 6.7856e-01, 4.2021e-01, 4.2021e-01, 5.9605e-08,\n",
      "        4.2021e-01, 4.2021e-01, 4.2021e-01, 6.4519e-01, 3.3114e-01, 4.2021e-01,\n",
      "        4.2021e-01, 5.3588e-01, 6.6787e-01, 7.0924e-01, 6.7856e-01, 4.2021e-01,\n",
      "        4.2021e-01, 4.9830e-01, 5.8493e-01, 5.6346e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.1545, 0.4120, 0.4120,    nan, 0.4120, 0.4120, 0.5117, 0.4568, 0.4120,\n",
      "        0.4120,    nan, 0.4120, 0.4120,    nan,    nan,    nan, 0.4120, 0.4120,\n",
      "        0.4878,    nan,    nan, 0.4120, 0.4120,    nan, 0.4120, 0.6278,    nan,\n",
      "           nan,    nan, 0.0440,    nan, 0.4120,    nan, 0.4120,    nan,    nan,\n",
      "        0.4520,    nan, 0.4120, 0.4120,    nan, 0.4120, 0.4074, 0.4120,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan, 0.4120,    nan,    nan,    nan,\n",
      "           nan, 0.4120,    nan,    nan, 0.4120, 0.4043, 0.5229,    nan, 0.4120,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.2707,    nan,    nan, 0.3378, 0.4180,    nan,    nan,    nan, 0.3378,\n",
      "        0.3378, 0.3378, 0.3378, 0.3378,    nan,    nan, 0.3378, 0.3378, 0.2052,\n",
      "           nan, 0.5452, 0.3378,    nan,    nan,    nan, 0.3378, 0.3378,    nan,\n",
      "        0.3378, 0.3378,    nan,    nan, 0.5501,    nan,    nan,    nan,    nan,\n",
      "        0.3378, 0.3378, 0.3378, 0.3378,    nan, 0.3930, 0.4085, 0.4042,    nan,\n",
      "        0.3378,    nan, 0.3378,    nan, 0.3378, 0.3378,    nan, 0.2655,    nan,\n",
      "           nan, 0.4258, 0.4953, 0.3378, 0.3378,    nan, 0.3378,    nan, 0.3378,\n",
      "        0.3378], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3274,    nan,    nan,    nan, 0.3274,    nan,    nan, 0.3274, 0.3274,\n",
      "           nan, 0.3274,    nan, 0.3274, 0.2288, 0.3274,    nan, 0.3274, 0.3274,\n",
      "        0.4723,    nan, 0.3274,    nan,    nan,    nan, 0.3274, 0.3274,    nan,\n",
      "        0.3274,    nan,    nan,    nan, 0.3274,    nan, 0.3446,    nan,    nan,\n",
      "        0.2789,    nan, 0.2919, 0.5225,    nan, 0.3274, 0.3274, 0.3274, 0.3274,\n",
      "        0.3274,    nan,    nan, 0.4012,    nan,    nan,    nan, 0.3274, 0.3719,\n",
      "           nan, 0.3274,    nan,    nan, 0.3274, 0.3274,    nan, 0.3274,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.2604, 0.2353, 0.2604, 0.2604,    nan, 0.2604,    nan,\n",
      "           nan, 0.4217, 0.5050,    nan, 0.2604, 0.2604, 0.2504,    nan, 0.2604,\n",
      "           nan, 0.2604, 0.2604,    nan,    nan, 0.3625, 0.2604,    nan,    nan,\n",
      "           nan, 0.2604,    nan, 0.2604,    nan, 0.2604, 0.2604, 0.2604,    nan,\n",
      "        0.2604,    nan, 0.2604,    nan, 0.2604,    nan, 0.4693, 0.2604,    nan,\n",
      "           nan,    nan,    nan, 0.2604, 0.2604, 0.2604, 0.2604, 0.2604, 0.3644,\n",
      "           nan, 0.2604, 0.3202, 0.2604, 0.2604, 0.2604, 0.2604, 0.2604,    nan,\n",
      "        0.2604], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3372, 0.3372, 0.6868, 0.3372, 0.6549, 0.5988, 0.3372, 0.3372, 0.6717,\n",
      "        0.3372, 0.0000, 0.3372, 0.3372, 0.3372, 0.4067, 0.2628, 0.3372, 0.6311,\n",
      "        0.3372, 0.6884, 0.3372, 0.0000, 0.0000, 0.3372, 0.3864, 0.3372, 0.7061,\n",
      "        0.3372, 0.7085, 0.0000, 0.3372, 0.6736, 0.3372, 0.5421, 0.6266, 0.3372,\n",
      "        0.3372, 0.1892, 0.5839, 0.7129, 0.0000, 0.1841, 0.3372, 0.6994, 0.7220,\n",
      "        0.3366, 0.5082, 0.5816, 0.7244, 0.3372, 0.3610, 0.6095, 0.5884, 0.3372,\n",
      "        0.3372, 0.3372, 0.5795, 0.0000, 0.3372, 0.7750, 0.3372, 0.7763, 0.3372,\n",
      "        0.3372], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.3492, 0.3492,    nan, 0.3492,    nan, 0.2681,    nan,\n",
      "        0.3492, 0.3492,    nan,    nan,    nan, 0.3492, 0.3492,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan, 0.3492, 0.3492,    nan, 0.3492, 0.3899,\n",
      "        0.3492,    nan, 0.3472,    nan,    nan,    nan, 0.3492, 0.3492, 0.3202,\n",
      "        0.4075,    nan,    nan,    nan, 0.3492, 0.3492,    nan, 0.3492, 0.3492,\n",
      "        0.3492, 0.3492, 0.3492,    nan,    nan, 0.3492,    nan, 0.3492,    nan,\n",
      "           nan,    nan, 0.3492, 0.3492, 0.3492,    nan, 0.3635,    nan, 0.3492,\n",
      "        0.4317], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3600,    nan, 0.5415,    nan, 0.5415,    nan, 0.5415,    nan,    nan,\n",
      "           nan, 0.5133, 0.5415,    nan, 0.5415, 0.5415, 0.5415,    nan,    nan,\n",
      "           nan, 0.5415,    nan, 0.5415, 0.5415,    nan, 0.5415, 0.3826,    nan,\n",
      "           nan,    nan,    nan, 0.5174,    nan,    nan,    nan, 0.5415, 0.5415,\n",
      "           nan,    nan, 0.5415,    nan, 0.5415, 0.5415,    nan, 0.5415, 0.4926,\n",
      "           nan,    nan,    nan, 0.1777, 0.5415,    nan, 0.5415, 0.5415,    nan,\n",
      "           nan, 0.0987, 0.5415, 0.5415,    nan,    nan, 0.5415,    nan,    nan,\n",
      "        0.4160], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3203,    nan, 0.3203,    nan,    nan,    nan,    nan, 0.3203, 0.4042,\n",
      "        0.3203, 0.3203,    nan,    nan, 0.3203,    nan, 0.3933, 0.3203,    nan,\n",
      "           nan, 0.3203,    nan, 0.3203,    nan, 0.3203, 0.3203,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan, 0.3931, 0.4448, 0.3203,    nan,    nan,\n",
      "           nan, 0.3203,    nan,    nan, 0.3703,    nan, 0.3203, 0.3203,    nan,\n",
      "           nan, 0.3203, 0.3203, 0.3203,    nan,    nan, 0.3203, 0.3203,    nan,\n",
      "        0.1637, 0.3203,    nan,    nan,    nan,    nan,    nan, 0.3203, 0.3203,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4365, 0.4710,    nan,    nan, 0.4365,    nan,    nan,    nan, 0.4365,\n",
      "           nan,    nan,    nan,    nan,    nan, 0.4365, 0.4365,    nan,    nan,\n",
      "           nan, 0.4365, 0.4365, 0.4365,    nan, 0.4365, 0.4365, 0.4365,    nan,\n",
      "           nan, 0.4447, 0.3315,    nan,    nan, 0.4365,    nan,    nan, 0.4365,\n",
      "        0.4365,    nan, 0.3559, 0.4156, 0.1586,    nan, 0.3528, 0.4365,    nan,\n",
      "           nan, 0.4365,    nan, 0.4365,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan, 0.4365, 0.4509, 0.4365,    nan,    nan, 0.3404,    nan, 0.1861,\n",
      "        0.4365], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.3328,    nan,    nan,    nan,    nan,    nan, 0.3328,    nan,\n",
      "        0.3328, 0.3591, 0.3328,    nan, 0.1890, 0.3328,    nan,    nan, 0.3247,\n",
      "           nan,    nan, 0.3339, 0.3328,    nan,    nan, 0.3328, 0.3830, 0.3328,\n",
      "           nan, 0.4688, 0.3141, 0.3328,    nan, 0.3328,    nan,    nan, 0.3328,\n",
      "        0.4513, 0.3328, 0.3328,    nan,    nan,    nan,    nan, 0.3328, 0.3328,\n",
      "        0.3328,    nan, 0.3328, 0.4405,    nan,    nan,    nan, 0.3328, 0.3328,\n",
      "           nan,    nan, 0.2394,    nan,    nan, 0.3328,    nan,    nan, 0.3328,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.7425, 0.4614, 0.3314, 0.3001, 0.4614, 0.6229, 0.3597, 0.4614, 0.7425,\n",
      "        0.4799, 0.4614, 0.4614, 0.6580, 0.4614, 0.4614, 0.4852, 0.7382, 0.4614,\n",
      "        0.4614, 0.0000, 0.4691, 0.3076, 0.5524, 0.4614, 0.6662, 0.4614, 0.4614,\n",
      "        0.2979, 0.3257, 0.4132, 0.6779, 0.6464, 0.3877, 0.4856, 0.2734, 0.5269,\n",
      "        0.4614, 0.4614, 0.5516, 0.1515, 0.7540, 0.5639, 0.3727, 0.4597, 0.2208,\n",
      "        0.3104, 0.3744, 0.7425, 0.4048, 0.3069, 0.2851, 0.7425, 0.3802, 0.2752,\n",
      "        0.2478, 0.3386, 0.6263, 0.1957, 0.6059, 0.4436, 0.4403, 0.4614, 0.7240,\n",
      "        0.4614], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.5787, 0.6027, 0.5787, 0.1019, 0.3586, 0.6522, 0.2402, 0.3158, 0.6652,\n",
      "        0.3065, 0.5787, 0.5787, 0.5787, 0.3151, 0.6652, 0.3748, 0.4353, 0.6652,\n",
      "        0.5744, 0.2484, 0.5787, 0.3956, 0.6059, 0.6123, 0.6880, 0.2566, 0.3029,\n",
      "        0.1411, 0.5787, 0.2615, 0.4897, 0.5787, 0.6785, 0.6585, 0.2022, 0.5787,\n",
      "        0.1144, 0.5787, 0.2967, 0.5787, 0.6622, 0.6176, 0.2655, 0.6652, 0.5787,\n",
      "        0.4626, 0.1256, 0.2862, 0.2768, 0.0677, 0.0000, 0.5787, 0.0381, 0.4049,\n",
      "        0.3132, 0.5787, 0.6652, 0.2259, 0.6652, 0.6014, 0.1155, 0.5787, 0.5166,\n",
      "        0.7822], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.5503, 0.6831, 0.6380, 0.2137, 0.2137, 0.7155, 0.5723, 0.7597, 0.6214,\n",
      "        0.6544, 0.6904, 0.5979, 0.2271, 0.2137, 0.2137, 0.6904, 0.7187, 0.2137,\n",
      "        0.6567, 0.5304, 0.2137, 0.6904, 0.6904, 0.2137, 0.2137, 0.6501, 0.6904,\n",
      "        0.2137, 0.2137, 0.5494, 0.2137, 0.6995, 0.2137, 0.0000, 0.7164, 0.3619,\n",
      "        0.6452, 0.7727, 0.7169, 0.2137, 0.3462, 0.2137, 0.6868, 0.6904, 0.2137,\n",
      "        0.2137, 0.2137, 0.6234, 0.2137, 0.2137, 0.2137, 0.2137, 0.6508, 0.6904,\n",
      "        0.2137, 0.2137, 0.6543, 0.2137, 0.5966, 0.6904, 0.2167, 0.7032, 0.6904,\n",
      "        0.2137], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3009,    nan, 0.2442,    nan, 0.2442,    nan,    nan,    nan, 0.2442,\n",
      "           nan, 0.2738,    nan,    nan, 0.2442, 0.2442, 0.3414, 0.2180,    nan,\n",
      "        0.2442, 0.3900,    nan,    nan, 0.2442,    nan,    nan,    nan, 0.2442,\n",
      "        0.2442, 0.2442,    nan, 0.2442, 0.3361, 0.2442, 0.2442, 0.2442, 0.2442,\n",
      "        0.2442, 0.2442, 0.2442, 0.2442, 0.2442,    nan, 0.2442,    nan, 0.4283,\n",
      "        0.2442, 0.2442, 0.3764, 0.2442, 0.2442,    nan, 0.3943,    nan, 0.2442,\n",
      "           nan,    nan,    nan, 0.2442,    nan,    nan, 0.2442,    nan, 0.2442,\n",
      "        0.3812], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3690,    nan, 0.3990,    nan, 0.3690, 0.3690, 0.3690,    nan,    nan,\n",
      "           nan, 0.3690,    nan,    nan, 0.3690,    nan,    nan, 0.3690,    nan,\n",
      "           nan,    nan,    nan, 0.3690, 0.3690,    nan, 0.4416,    nan, 0.4821,\n",
      "        0.4062,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "        0.3690, 0.3690,    nan, 0.0926, 0.3690, 0.3690,    nan, 0.3690, 0.3690,\n",
      "        0.3690,    nan, 0.3690,    nan, 0.3690,    nan, 0.4170, 0.3690,    nan,\n",
      "        0.3690, 0.4927, 0.3690, 0.3690,    nan,    nan,    nan,    nan,    nan,\n",
      "        0.3690], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.3393,    nan, 0.3393, 0.3393,    nan, 0.5145, 0.3393, 0.3393,\n",
      "        0.3393, 0.5539, 0.3393,    nan, 0.3393, 0.3393,    nan, 0.4359, 0.3393,\n",
      "           nan,    nan,    nan,    nan, 0.3393, 0.3393,    nan,    nan, 0.3393,\n",
      "           nan, 0.3393, 0.3393, 0.3393, 0.3393,    nan, 0.0900, 0.3393,    nan,\n",
      "           nan, 0.3393, 0.3393, 0.3393,    nan, 0.3393, 0.3393, 0.3393, 0.3393,\n",
      "           nan,    nan, 0.3393, 0.3393, 0.3393,    nan,    nan, 0.3393,    nan,\n",
      "        0.3393,    nan, 0.3393,    nan, 0.4548,    nan, 0.3393,    nan, 0.3393,\n",
      "        0.3393], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.2468, 0.2468, 0.4531,    nan,    nan, 0.2468,    nan,\n",
      "           nan, 0.2468,    nan, 0.2468,    nan, 0.2468, 0.2468,    nan, 0.2468,\n",
      "           nan,    nan,    nan, 0.2048, 0.3597, 0.2468, 0.2550, 0.2468, 0.2468,\n",
      "        0.2468,    nan,    nan, 0.2468,    nan,    nan, 0.2468, 0.2468,    nan,\n",
      "           nan, 0.2468,    nan, 0.2468,    nan,    nan,    nan, 0.2468, 0.3278,\n",
      "        0.2468,    nan, 0.2468,    nan, 0.2468, 0.2468, 0.3902, 0.2468,    nan,\n",
      "           nan,    nan, 0.2468,    nan, 0.2468,    nan, 0.2468,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.3892,    nan,    nan,    nan,    nan, 0.3892,    nan,    nan,\n",
      "           nan,    nan, 0.3892, 0.3892, 0.3892,    nan, 0.3892, 0.3892, 0.3892,\n",
      "           nan,    nan, 0.3892,    nan, 0.3892, 0.3892, 0.5249,    nan,    nan,\n",
      "           nan, 0.3032,    nan, 0.4430, 0.3892, 0.3892,    nan,    nan, 0.2530,\n",
      "        0.4987, 0.3892, 0.3892, 0.3892,    nan,    nan,    nan, 0.4858, 0.2602,\n",
      "        0.3892,    nan,    nan,    nan, 0.2119, 0.3892, 0.3892,    nan, 0.4131,\n",
      "        0.3892,    nan, 0.3892,    nan,    nan,    nan,    nan, 0.4128,    nan,\n",
      "        0.3892], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.2378,    nan,    nan,    nan, 0.2378, 0.3343,    nan,\n",
      "           nan,    nan,    nan,    nan, 0.2378,    nan, 0.3394,    nan,    nan,\n",
      "           nan,    nan,    nan, 0.2378, 0.2378,    nan,    nan, 0.2378,    nan,\n",
      "        0.2378,    nan,    nan, 0.1867,    nan, 0.2378, 0.2378,    nan,    nan,\n",
      "           nan, 0.2378,    nan, 0.2378,    nan, 0.2378, 0.2378, 0.2378, 0.2378,\n",
      "           nan, 0.2378,    nan,    nan, 0.2378,    nan,    nan,    nan,    nan,\n",
      "        0.2378,    nan, 0.2378, 0.2378,    nan,    nan, 0.2378, 0.2378,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.4849, 0.3715, 0.3715, 0.3715,    nan, 0.3715, 0.3715,    nan,\n",
      "           nan,    nan,    nan, 0.4023, 0.3715, 0.3715,    nan, 0.3715, 0.3715,\n",
      "           nan, 0.4580,    nan,    nan,    nan, 0.3715,    nan, 0.3715, 0.3715,\n",
      "        0.3715, 0.3715,    nan, 0.4990,    nan, 0.3715, 0.3715, 0.3715, 0.3715,\n",
      "        0.3715, 0.3328, 0.3715,    nan,    nan,    nan, 0.3653,    nan, 0.3715,\n",
      "        0.3715, 0.2110,    nan,    nan, 0.3715,    nan,    nan,    nan, 0.3715,\n",
      "           nan, 0.3710,    nan, 0.4964,    nan, 0.4030, 0.3715, 0.3715,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.5096, 0.4378,    nan, 0.4378, 0.4378,    nan, 0.4378, 0.2295, 0.3610,\n",
      "           nan, 0.2464,    nan, 0.1892, 0.4378, 0.4378,    nan,    nan, 0.4378,\n",
      "        0.4378, 0.4378,    nan,    nan,    nan, 0.4378, 0.4378, 0.4378, 0.4378,\n",
      "           nan, 0.4378, 0.3306, 0.4378,    nan, 0.4378, 0.4378, 0.4378, 0.3549,\n",
      "           nan,    nan, 0.4260, 0.3817,    nan, 0.4378, 0.4378,    nan, 0.4378,\n",
      "        0.4378,    nan,    nan, 0.2874,    nan, 0.4378, 0.4378,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan, 0.4378,    nan,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.3405, 0.3405,    nan,    nan,    nan,    nan, 0.3405,\n",
      "        0.3812,    nan, 0.3389,    nan, 0.3405,    nan, 0.2804,    nan, 0.4435,\n",
      "        0.3405,    nan, 0.3405,    nan, 0.3405, 0.4230,    nan,    nan,    nan,\n",
      "           nan,    nan, 0.3405, 0.4634,    nan,    nan, 0.3405, 0.3405,    nan,\n",
      "        0.3258, 0.3405, 0.3405, 0.3613, 0.3405, 0.3405, 0.3405, 0.3405,    nan,\n",
      "           nan,    nan, 0.3405,    nan,    nan, 0.3405, 0.3405, 0.2458,    nan,\n",
      "           nan, 0.3405,    nan, 0.3405, 0.3405,    nan, 0.3405, 0.4070, 0.3405,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4658,    nan,    nan,    nan, 0.3584, 0.3267, 0.3267, 0.3267,    nan,\n",
      "           nan, 0.3267, 0.3267,    nan, 0.3267, 0.3267,    nan,    nan, 0.3267,\n",
      "        0.3267, 0.4119, 0.3267,    nan,    nan, 0.3267, 0.3267,    nan,    nan,\n",
      "        0.3267,    nan, 0.3267, 0.3267, 0.3267,    nan, 0.3267, 0.3267,    nan,\n",
      "           nan, 0.3267,    nan,    nan, 0.3512,    nan,    nan, 0.3267, 0.3267,\n",
      "        0.3267, 0.2453, 0.3267, 0.3267,    nan,    nan, 0.3267, 0.3267, 0.3710,\n",
      "        0.3937, 0.3267,    nan,    nan, 0.3267,    nan, 0.3267, 0.3267, 0.3267,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.3881, 0.3881, 0.4912, 0.1940, 0.3881,    nan, 0.3881,\n",
      "        0.3881,    nan,    nan,    nan,    nan,    nan,    nan, 0.4865,    nan,\n",
      "        0.3881,    nan,    nan, 0.3351, 0.3881, 0.3569, 0.3881, 0.3881,    nan,\n",
      "           nan,    nan,    nan, 0.3881,    nan,    nan, 0.3881, 0.4357, 0.3881,\n",
      "           nan, 0.3881, 0.3881, 0.3881, 0.3011,    nan, 0.3235,    nan, 0.3881,\n",
      "           nan,    nan, 0.4864,    nan, 0.3058,    nan,    nan, 0.3881,    nan,\n",
      "           nan, 0.3881, 0.3881,    nan,    nan,    nan,    nan, 0.4274, 0.3881,\n",
      "        0.3080], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.5818, 0.4888, 0.5593, 0.5257, 0.4888, 0.6230, 0.6269, 0.4888, 0.6098,\n",
      "        0.4888, 0.0087, 0.5031, 0.3728, 0.5460, 0.4888, 0.5630, 0.5794, 0.4888,\n",
      "        0.4888, 0.4888, 0.4959, 0.4888, 0.4888, 0.5207, 0.3728, 0.4888, 0.5872,\n",
      "        0.6611, 0.4888, 0.5686, 0.6274, 0.6390, 0.3728, 0.4762, 0.4888, 0.4583,\n",
      "        0.4808, 0.4888, 0.3728, 0.4658, 0.3728, 0.3728, 0.4782, 0.5853, 0.4888,\n",
      "        0.4888, 0.4888, 0.4888, 0.5322, 0.6082, 0.3728, 0.4888, 0.5839, 0.6602,\n",
      "        0.4888, 0.3728, 0.4888, 0.5488, 0.0402, 0.6096, 0.3728, 0.5844, 0.3728,\n",
      "        0.6542], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan,    nan,    nan, 0.4016,    nan, 0.4016,    nan, 0.4016,\n",
      "           nan,    nan, 0.1900,    nan,    nan,    nan,    nan, 0.4016, 0.2832,\n",
      "        0.4016, 0.4016, 0.4016,    nan, 0.4016, 0.4016,    nan, 0.4016, 0.4016,\n",
      "        0.4016,    nan, 0.4016,    nan,    nan, 0.4016,    nan, 0.4016,    nan,\n",
      "           nan, 0.4016,    nan,    nan, 0.4016,    nan, 0.4016,    nan,    nan,\n",
      "           nan, 0.4016, 0.4016,    nan, 0.4016, 0.3862,    nan, 0.4016, 0.4016,\n",
      "           nan, 0.4016,    nan, 0.4016,    nan, 0.4016, 0.1265, 0.2313,    nan,\n",
      "        0.4016], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan,    nan, 0.4160, 0.2140,    nan, 0.2140, 0.2140, 0.2140,\n",
      "           nan, 0.2140,    nan,    nan,    nan, 0.3124, 0.2140, 0.2140, 0.2140,\n",
      "           nan, 0.2140,    nan, 0.3327,    nan,    nan, 0.2140,    nan, 0.2140,\n",
      "        0.2140, 0.2140, 0.2140,    nan,    nan, 0.2140,    nan,    nan, 0.2140,\n",
      "        0.2140,    nan,    nan, 0.2140, 0.2140, 0.2470, 0.3479,    nan,    nan,\n",
      "        0.4197,    nan, 0.2140,    nan,    nan,    nan, 0.2140, 0.2140, 0.2140,\n",
      "        0.2140,    nan, 0.2140, 0.2140, 0.2140,    nan, 0.2140,    nan, 0.2140,\n",
      "        0.2140], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.4647, 0.1163, 0.4331,    nan,    nan,    nan, 0.4647,\n",
      "        0.4647,    nan, 0.4647,    nan,    nan, 0.4647, 0.4647, 0.4647,    nan,\n",
      "           nan, 0.4647,    nan, 0.4647,    nan,    nan, 0.4647,    nan,    nan,\n",
      "           nan, 0.4647, 0.4647, 0.4647,    nan, 0.4647, 0.4647, 0.4647, 0.5407,\n",
      "           nan, 0.4647,    nan,    nan, 0.4647, 0.4647,    nan,    nan, 0.4647,\n",
      "        0.5573, 0.4647,    nan,    nan, 0.4647, 0.4647, 0.1601, 0.4088, 0.4647,\n",
      "        0.4647, 0.5102,    nan, 0.4647, 0.4647, 0.4647, 0.4647,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan,    nan,    nan,    nan, 0.3635, 0.4494, 0.4494, 0.4494,\n",
      "        0.3963,    nan,    nan, 0.4494, 0.4494, 0.4494,    nan, 0.4608, 0.2936,\n",
      "           nan, 0.2858,    nan,    nan,    nan,    nan,    nan,    nan, 0.2367,\n",
      "           nan, 0.4494, 0.4494, 0.4494,    nan,    nan, 0.4494,    nan, 0.3585,\n",
      "        0.4494,    nan,    nan, 0.4203, 0.4494, 0.4494,    nan, 0.4494,    nan,\n",
      "        0.4494,    nan, 0.4494, 0.4494,    nan,    nan,    nan, 0.4494, 0.4494,\n",
      "        0.4494,    nan,    nan, 0.4494,    nan, 0.4494, 0.4494,    nan, 0.4494,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.2657,    nan, 0.2657, 0.2657,    nan,    nan, 0.4056, 0.2850,\n",
      "        0.2657, 0.2657,    nan, 0.2657,    nan, 0.4507, 0.2657, 0.2657,    nan,\n",
      "        0.2657,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.2657,\n",
      "           nan,    nan,    nan, 0.2657,    nan, 0.2657,    nan,    nan, 0.2657,\n",
      "           nan, 0.2657, 0.2657,    nan, 0.2657, 0.2657, 0.2657, 0.2657, 0.2267,\n",
      "        0.2657, 0.2657,    nan,    nan, 0.2657,    nan,    nan,    nan,    nan,\n",
      "        0.2657, 0.2657,    nan, 0.2657, 0.3382, 0.2657, 0.2657, 0.2657, 0.2657,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3163, 0.3163, 0.3163, 0.3163, 0.2047, 0.3163, 0.3163, 0.5887, 0.3163,\n",
      "        0.3976, 0.7440, 0.6420, 0.6315, 0.2942, 0.6235, 0.3163, 0.5746, 0.6440,\n",
      "        0.3163, 0.6271, 0.4552, 0.5603, 0.3163, 0.5238, 0.3163, 0.6956, 0.2241,\n",
      "        0.6185, 0.7034, 0.3922, 0.3163, 0.3163, 0.3163, 0.3163, 0.3733, 0.6099,\n",
      "        0.4652, 0.4549, 0.3163, 0.5995, 0.3163, 0.3163, 0.6288, 0.3163, 0.3163,\n",
      "        0.6724, 0.0000, 0.3163, 0.3383, 0.3163, 0.4631, 0.3163, 0.5908, 0.4549,\n",
      "        0.3163, 0.6079, 0.3163, 0.4549, 0.3163, 0.7093, 0.3925, 0.5802, 0.6501,\n",
      "        0.3163], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([3.6131e-01, 2.9896e-01, 3.6131e-01, 3.6131e-01, 6.3738e-01, 3.6131e-01,\n",
      "        6.5028e-01, 3.1367e-01, 4.6565e-01, 3.6131e-01, 6.9621e-01, 3.6131e-01,\n",
      "        4.3438e-01, 3.6131e-01, 6.2214e-01, 3.6131e-01, 3.6131e-01, 5.2588e-01,\n",
      "        3.6829e-01, 7.2153e-01, 5.1427e-01, 6.2214e-01, 6.2214e-01, 3.9998e-01,\n",
      "        3.6131e-01, 6.2025e-01, 3.6131e-01, 5.3717e-01, 3.6131e-01, 6.3072e-01,\n",
      "        3.5486e-01, 2.0140e-04, 5.1162e-01, 6.2214e-01, 3.6131e-01, 3.6131e-01,\n",
      "        2.9967e-01, 4.1826e-01, 6.0897e-01, 4.3130e-01, 6.5193e-01, 3.6131e-01,\n",
      "        3.6161e-01, 3.6131e-01, 5.9317e-01, 6.2008e-01, 3.1252e-01, 5.5202e-01,\n",
      "        3.1453e-01, 5.9943e-01, 6.2214e-01, 3.6131e-01, 6.1203e-01, 6.4878e-01,\n",
      "        3.6131e-01, 3.7740e-01, 3.6131e-01, 3.6131e-01, 3.6131e-01, 7.4901e-01,\n",
      "        3.6131e-01, 6.6546e-01, 3.6131e-01, 6.2214e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan,    nan, 0.4283,    nan,    nan,    nan, 0.4283,    nan,\n",
      "        0.4283,    nan, 0.4283, 0.4283, 0.2999,    nan, 0.4390,    nan, 0.4283,\n",
      "        0.4283, 0.4283, 0.4283,    nan, 0.5154, 0.3492, 0.2197,    nan,    nan,\n",
      "           nan,    nan, 0.4283,    nan,    nan, 0.3715, 0.2490,    nan, 0.4283,\n",
      "           nan,    nan, 0.4283,    nan,    nan,    nan, 0.4657, 0.4283, 0.4283,\n",
      "        0.4283,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan, 0.5042,    nan,    nan,    nan, 0.3766,    nan, 0.4283, 0.4283,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3904,    nan,    nan,    nan, 0.4197, 0.3721,    nan, 0.3904, 0.3904,\n",
      "           nan,    nan, 0.3368, 0.3904, 0.4075,    nan,    nan,    nan, 0.3182,\n",
      "        0.3904,    nan,    nan,    nan,    nan, 0.3904,    nan, 0.3904, 0.3031,\n",
      "        0.4515,    nan,    nan, 0.3904, 0.3904,    nan, 0.3904,    nan,    nan,\n",
      "        0.3904,    nan,    nan,    nan, 0.3904,    nan, 0.3498, 0.3904,    nan,\n",
      "        0.4119,    nan,    nan, 0.3904,    nan,    nan, 0.4377,    nan, 0.3904,\n",
      "           nan,    nan, 0.3349, 0.3904, 0.3904,    nan, 0.2744,    nan, 0.5081,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4497, 0.4930, 0.7471, 0.4317, 0.3635, 0.2142, 0.4497, 0.6201, 0.4497,\n",
      "        0.4013, 0.4497, 0.6174, 0.4497, 0.3265, 0.7331, 0.3543, 0.4895, 0.5310,\n",
      "        0.3937, 0.4497, 0.3293, 0.6360, 0.7200, 0.5450, 0.5991, 0.6556, 0.5133,\n",
      "        0.6213, 0.0000, 0.3039, 0.4497, 0.4497, 0.6429, 0.7971, 0.6909, 0.4497,\n",
      "        0.7394, 0.7584, 0.5563, 0.2553, 0.4497, 0.4497, 0.4428, 0.4497, 0.2389,\n",
      "        0.2173, 0.5629, 0.4497, 0.5113, 0.4497, 0.2991, 0.5239, 0.4557, 0.4975,\n",
      "        0.4376, 0.6201, 0.7081, 0.2465, 0.6977, 0.3422, 0.4497, 0.7720, 0.7443,\n",
      "        0.7365], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4516, 0.3923,    nan,    nan, 0.3853, 0.3853, 0.3853,    nan, 0.4146,\n",
      "           nan, 0.3853,    nan, 0.3853, 0.3313,    nan, 0.3853, 0.3853,    nan,\n",
      "           nan,    nan,    nan, 0.3853, 0.1974,    nan, 0.3853, 0.3853,    nan,\n",
      "           nan, 0.3853,    nan, 0.3853, 0.3853,    nan,    nan, 0.3853,    nan,\n",
      "        0.3853,    nan, 0.3853,    nan, 0.3853,    nan, 0.4273, 0.2754, 0.3853,\n",
      "        0.4063,    nan,    nan,    nan, 0.3853, 0.3853,    nan, 0.4731, 0.2922,\n",
      "           nan,    nan,    nan, 0.4952,    nan, 0.3853,    nan, 0.3853, 0.3887,\n",
      "        0.3853], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   nan, 0.5901, 0.4754,    nan,    nan, 0.4865,    nan,    nan,    nan,\n",
      "           nan, 0.1128, 0.4865, 0.4865,    nan,    nan,    nan,    nan,    nan,\n",
      "        0.4865, 0.4865, 0.4865,    nan,    nan, 0.4865, 0.2347,    nan,    nan,\n",
      "        0.5617, 0.4241,    nan,    nan,    nan, 0.4865, 0.4865,    nan,    nan,\n",
      "           nan,    nan, 0.2229, 0.4159, 0.4761, 0.4739, 0.5241,    nan,    nan,\n",
      "           nan,    nan, 0.3394, 0.4865,    nan,    nan, 0.4865,    nan,    nan,\n",
      "        0.4865, 0.4865, 0.4633,    nan,    nan,    nan,    nan, 0.4865,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.3493, 0.2389, 0.3091,    nan,    nan, 0.4427,    nan, 0.3624,\n",
      "           nan, 0.4288, 0.3624,    nan, 0.3624, 0.3624, 0.4683,    nan, 0.3701,\n",
      "           nan,    nan,    nan,    nan, 0.3624,    nan,    nan,    nan,    nan,\n",
      "           nan, 0.3624, 0.3624,    nan, 0.3178,    nan,    nan, 0.3624,    nan,\n",
      "           nan, 0.3624, 0.3624,    nan, 0.3624,    nan,    nan, 0.3353, 0.3624,\n",
      "           nan, 0.2893,    nan,    nan, 0.3723, 0.3624,    nan,    nan, 0.3624,\n",
      "        0.4981, 0.3624, 0.3624, 0.3624,    nan,    nan, 0.3624, 0.3652,    nan,\n",
      "        0.3624], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3144, 0.5598, 0.3945, 0.3945, 0.3945, 0.7121, 0.3945, 0.3945, 0.4012,\n",
      "        0.5598, 0.3945, 0.4186, 0.5370, 0.3327, 0.6176, 0.3945, 0.5326, 0.5598,\n",
      "        0.6370, 0.2562, 0.5598, 0.5598, 0.6119, 0.3945, 0.6495, 0.3945, 0.4596,\n",
      "        0.6428, 0.5598, 0.6089, 0.6191, 0.3945, 0.5872, 0.6899, 0.3657, 0.5654,\n",
      "        0.5598, 0.3945, 0.6849, 0.3945, 0.3945, 0.3945, 0.4279, 0.4787, 0.3945,\n",
      "        0.3945, 0.3945, 0.5564, 0.5436, 0.4174, 0.6629, 0.3945, 0.1930, 0.3945,\n",
      "        0.6028, 0.5598, 0.0488, 0.3945, 0.3945, 0.3945, 0.5267, 0.3945, 0.6125,\n",
      "        0.4070], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan,    nan, 0.5320,    nan,    nan,    nan,    nan, 0.5320,\n",
      "           nan,    nan,    nan,    nan, 0.2513, 0.5114,    nan,    nan,    nan,\n",
      "           nan,    nan, 0.5320, 0.3420, 0.5320,    nan, 0.3907, 0.4473, 0.5320,\n",
      "        0.1271, 0.5320,    nan, 0.2578, 0.5142,    nan, 0.5320, 0.4090,    nan,\n",
      "           nan, 0.5320, 0.5320, 0.5320,    nan,    nan, 0.5320, 0.5320,    nan,\n",
      "        0.5320, 0.5320, 0.4846, 0.5320,    nan, 0.5320, 0.5320, 0.3277,    nan,\n",
      "        0.5320, 0.5320,    nan,    nan, 0.5320, 0.5320, 0.4096, 0.5320,    nan,\n",
      "        0.3930], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4946, 0.3329, 0.3841, 0.2837, 0.6027, 0.5666, 0.5924, 0.3265, 0.3841,\n",
      "        0.5924, 0.5682, 0.3841, 0.6208, 0.5285, 0.6902, 0.3841, 0.7513, 0.5706,\n",
      "        0.3671, 0.3841, 0.3841, 0.5924, 0.5924, 0.3476, 0.3841, 0.3841, 0.6421,\n",
      "        0.4539, 0.3342, 0.3841, 0.3841, 0.3841, 0.3841, 0.3260, 0.3841, 0.3841,\n",
      "        0.2882, 0.6082, 0.3841, 0.4098, 0.7116, 0.4284, 0.4462, 0.0008, 0.3841,\n",
      "        0.3841, 0.3841, 0.3841, 0.6511, 0.5924, 0.5924, 0.7181, 0.3841, 0.4626,\n",
      "        0.7726, 0.6968, 0.6951, 0.3841, 0.3841, 0.3841, 0.6242, 0.3841, 0.3841,\n",
      "        0.5770], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan,    nan,    nan,    nan,    nan, 0.5299, 0.5188,    nan,\n",
      "           nan, 0.3956,    nan, 0.3956,    nan,    nan,    nan, 0.2331, 0.4513,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan, 0.1367, 0.3956,    nan,\n",
      "           nan, 0.3956, 0.3956,    nan, 0.3956,    nan, 0.3956, 0.3956, 0.3956,\n",
      "        0.3956, 0.3956,    nan, 0.3956, 0.3956,    nan, 0.3956, 0.3956,    nan,\n",
      "           nan,    nan, 0.3956,    nan,    nan, 0.3437, 0.3956,    nan, 0.3956,\n",
      "           nan,    nan, 0.3956,    nan,    nan, 0.3956, 0.3956, 0.3956, 0.4462,\n",
      "        0.3956], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.6508, 0.5895, 0.1109, 0.1109, 0.3168, 0.6660, 0.1109, 0.2595, 0.0000,\n",
      "        0.6707, 0.5658, 0.1109, 0.1109, 0.4539, 0.4320, 0.1109, 0.1109, 0.6170,\n",
      "        0.1109, 0.5482, 0.6065, 0.1992, 0.1109, 0.3504, 0.1109, 0.5646, 0.1109,\n",
      "        0.6567, 0.1109, 0.1109, 0.1109, 0.1109, 0.6172, 0.5404, 0.7181, 0.2676,\n",
      "        0.6323, 0.2205, 0.1109, 0.5967, 0.6979, 0.6708, 0.6377, 0.5404, 0.3222,\n",
      "        0.1109, 0.6086, 0.1109, 0.6061, 0.1109, 0.5905, 0.1109, 0.6974, 0.6272,\n",
      "        0.1109, 0.5404, 0.5404, 0.1109, 0.5961, 0.1109, 0.5885, 0.1109, 0.5989,\n",
      "        0.1109], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.1576,    nan,    nan, 0.3940, 0.3940,    nan, 0.4553, 0.3940,\n",
      "           nan,    nan,    nan, 0.1300,    nan,    nan,    nan,    nan, 0.3940,\n",
      "        0.3940, 0.3390,    nan,    nan, 0.3940,    nan, 0.3940,    nan,    nan,\n",
      "           nan,    nan, 0.3940, 0.3940, 0.3940, 0.4357, 0.4526, 0.3940, 0.4606,\n",
      "        0.3940, 0.3940,    nan,    nan, 0.3940,    nan, 0.3453,    nan, 0.3940,\n",
      "           nan, 0.4328,    nan, 0.3940, 0.4803, 0.3940,    nan, 0.4194, 0.4889,\n",
      "        0.3940,    nan, 0.3940,    nan,    nan, 0.3940,    nan, 0.3218, 0.3940,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4646,    nan, 0.4646, 0.2903, 0.4646, 0.4646,    nan,    nan,    nan,\n",
      "        0.4646, 0.4646, 0.1799,    nan,    nan, 0.1870,    nan,    nan, 0.2086,\n",
      "        0.4646, 0.3920,    nan, 0.4646, 0.4646, 0.4646, 0.4646, 0.4646, 0.5035,\n",
      "        0.1715,    nan,    nan,    nan,    nan, 0.5121, 0.4902, 0.4692,    nan,\n",
      "        0.5512,    nan, 0.3122,    nan,    nan, 0.4735,    nan, 0.4646, 0.4723,\n",
      "        0.4646,    nan, 0.4646, 0.4646,    nan, 0.4646,    nan,    nan,    nan,\n",
      "        0.5589,    nan, 0.4646,    nan, 0.4646, 0.4646,    nan, 0.4646,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.4339,    nan, 0.4339, 0.5058, 0.4339,    nan, 0.2691,\n",
      "           nan,    nan, 0.1054,    nan,    nan, 0.4339,    nan, 0.4339, 0.4339,\n",
      "           nan, 0.4339,    nan, 0.4339,    nan, 0.4339,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan, 0.4339,    nan, 0.4339, 0.4339,    nan,    nan,\n",
      "           nan,    nan, 0.4339,    nan,    nan, 0.4339, 0.5553,    nan,    nan,\n",
      "           nan,    nan,    nan, 0.4339,    nan,    nan,    nan, 0.3775,    nan,\n",
      "        0.4339,    nan,    nan,    nan, 0.2050,    nan, 0.4339,    nan, 0.3753,\n",
      "        0.4339], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.4317,    nan, 0.4306,    nan, 0.2811,    nan, 0.4306,\n",
      "           nan, 0.4306, 0.5137,    nan,    nan, 0.4306, 0.4306, 0.4306,    nan,\n",
      "        0.4477, 0.4306, 0.4306, 0.4306,    nan,    nan,    nan,    nan, 0.4306,\n",
      "        0.4306,    nan,    nan, 0.4306,    nan,    nan,    nan, 0.1703,    nan,\n",
      "           nan, 0.4833,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "        0.4306,    nan,    nan, 0.4306,    nan, 0.2762, 0.4306,    nan,    nan,\n",
      "        0.4306, 0.4306, 0.4306, 0.4306,    nan,    nan, 0.4306, 0.4306, 0.4306,\n",
      "        0.4306], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan,    nan, 0.3077, 0.3077, 0.3077,    nan,    nan, 0.4638,\n",
      "           nan, 0.3077,    nan,    nan,    nan,    nan, 0.3077,    nan,    nan,\n",
      "        0.3189,    nan, 0.3077,    nan,    nan, 0.3077,    nan,    nan, 0.3077,\n",
      "           nan, 0.3077, 0.3077, 0.3077, 0.3077, 0.3077, 0.2205, 0.3077,    nan,\n",
      "           nan, 0.3077,    nan, 0.2737, 0.3077, 0.2830, 0.3077,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan, 0.3077,    nan,    nan,    nan, 0.3401,\n",
      "           nan, 0.4309, 0.3734,    nan,    nan,    nan,    nan,    nan, 0.4309,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.3378,    nan,    nan,    nan, 0.3378, 0.2834,    nan,    nan,\n",
      "           nan,    nan,    nan, 0.3378,    nan,    nan,    nan,    nan,    nan,\n",
      "        0.3378,    nan,    nan, 0.3378, 0.3378, 0.3378,    nan, 0.3378, 0.3378,\n",
      "           nan,    nan, 0.3378, 0.3378, 0.2697,    nan,    nan, 0.3378,    nan,\n",
      "           nan, 0.3378,    nan,    nan,    nan,    nan,    nan,    nan, 0.4275,\n",
      "        0.3378, 0.4568,    nan, 0.3378,    nan,    nan,    nan, 0.4290,    nan,\n",
      "           nan, 0.3081,    nan, 0.3378,    nan, 0.3378, 0.3378, 0.2956, 0.3378,\n",
      "        0.3978], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.4417,    nan, 0.4918, 0.4417, 0.4417, 0.4417, 0.3739,\n",
      "        0.4240,    nan, 0.4322,    nan, 0.4417,    nan,    nan, 0.4417, 0.4417,\n",
      "           nan,    nan, 0.4234, 0.4417, 0.4417, 0.4058,    nan, 0.3261, 0.4417,\n",
      "           nan, 0.4417, 0.4417,    nan,    nan, 0.4298, 0.4417, 0.2291, 0.4417,\n",
      "        0.2893, 0.4417, 0.2828,    nan,    nan, 0.4417, 0.3625, 0.4417, 0.4674,\n",
      "           nan, 0.4417,    nan, 0.4417,    nan,    nan,    nan, 0.4417, 0.4311,\n",
      "           nan,    nan, 0.4417, 0.3580, 0.4417, 0.4417,    nan, 0.4417, 0.4417,\n",
      "        0.4417], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([4.7641e-01, 5.2278e-01, 4.7170e-01, 6.6667e-01, 6.3698e-01, 4.7170e-01,\n",
      "        4.7170e-01, 6.2603e-01, 6.9546e-01, 6.4670e-01, 5.9605e-08, 6.3446e-01,\n",
      "        4.4926e-01, 6.2945e-01, 4.7170e-01, 5.8668e-01, 7.7902e-01, 5.6493e-01,\n",
      "        5.9605e-08, 4.7170e-01, 4.7170e-01, 6.6930e-01, 5.0263e-01, 6.2347e-01,\n",
      "        5.9574e-01, 5.6813e-01, 5.5845e-01, 4.7170e-01, 5.7621e-01, 4.7170e-01,\n",
      "        5.9605e-08, 7.3259e-01, 5.5281e-01, 1.6507e-01, 4.7170e-01, 5.9605e-08,\n",
      "        4.7170e-01, 4.7170e-01, 4.7170e-01, 2.7790e-01, 4.7170e-01, 4.6603e-01,\n",
      "        4.7170e-01, 6.1678e-01, 4.7170e-01, 6.4244e-01, 5.9605e-08, 6.5311e-01,\n",
      "        6.4828e-01, 6.8997e-01, 6.9058e-01, 6.3553e-01, 6.1704e-01, 7.0990e-01,\n",
      "        5.9605e-08, 4.7170e-01, 7.7656e-01, 1.0870e-01, 6.8636e-01, 4.7170e-01,\n",
      "        5.9860e-01, 7.2212e-01, 5.3165e-01, 6.2584e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3771, 0.4552,    nan, 0.4552, 0.4552, 0.4552,    nan, 0.4552, 0.2833,\n",
      "           nan, 0.4552, 0.4552,    nan, 0.4552, 0.4552, 0.4552, 0.4552,    nan,\n",
      "        0.4552, 0.4552,    nan, 0.5618,    nan,    nan, 0.4552,    nan,    nan,\n",
      "        0.4552, 0.4552, 0.4262,    nan, 0.1543, 0.4552, 0.4316, 0.4552, 0.3474,\n",
      "        0.4552, 0.4442, 0.4552,    nan, 0.4709, 0.4552, 0.4552, 0.3907, 0.4552,\n",
      "           nan, 0.4336,    nan, 0.4552,    nan, 0.4552,    nan, 0.4552,    nan,\n",
      "           nan, 0.3279,    nan, 0.5097, 0.1673,    nan,    nan,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([1.1921e-07, 6.7674e-01, 6.5345e-01, 4.9366e-01, 5.5489e-01, 6.5633e-01,\n",
      "        1.1921e-07, 3.9368e-01, 3.4331e-01, 7.3493e-01, 3.4331e-01, 3.4331e-01,\n",
      "        3.4331e-01, 1.1921e-07, 1.7589e-01, 7.9643e-01, 3.4331e-01, 5.0843e-01,\n",
      "        3.8605e-01, 2.3025e-01, 4.3272e-01, 5.9076e-01, 6.5694e-01, 6.0388e-01,\n",
      "        3.4331e-01, 7.0292e-01, 1.1921e-07, 5.9264e-01, 6.2573e-01, 3.4331e-01,\n",
      "        4.2338e-01, 1.6302e-01, 5.9756e-01, 3.4331e-01, 3.8004e-01, 1.1921e-07,\n",
      "        7.4205e-01, 8.3552e-01, 6.3485e-01, 7.0383e-01, 7.1753e-01, 4.4501e-01,\n",
      "        6.1565e-01, 1.1921e-07, 6.5876e-01, 6.7323e-01, 5.9875e-01, 3.7874e-01,\n",
      "        3.4331e-01, 6.1345e-01, 3.4331e-01, 4.1865e-01, 6.0493e-01, 6.2929e-01,\n",
      "        3.4331e-01, 3.4331e-01, 6.6688e-01, 7.3194e-01, 6.3436e-01, 5.5480e-01,\n",
      "        3.4331e-01, 4.8995e-01, 6.5780e-01, 3.4331e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([4.2901e-01, 3.6048e-01, 3.6048e-01, 7.2624e-01, 6.8311e-01, 4.5925e-01,\n",
      "        3.6048e-01, 3.8421e-01, 1.7881e-07, 3.7257e-01, 4.0932e-01, 1.7881e-07,\n",
      "        6.6408e-01, 3.6048e-01, 3.6048e-01, 6.2504e-01, 3.6048e-01, 7.5794e-01,\n",
      "        5.9329e-01, 6.9531e-01, 2.6331e-01, 3.4250e-01, 3.6048e-01, 3.0955e-01,\n",
      "        3.6048e-01, 5.4105e-01, 1.7881e-07, 6.8243e-01, 3.6048e-01, 3.6048e-01,\n",
      "        3.6048e-01, 4.2038e-01, 3.6048e-01, 5.0213e-01, 3.6048e-01, 7.8188e-01,\n",
      "        6.7281e-01, 3.6048e-01, 3.6048e-01, 4.5350e-01, 3.6048e-01, 3.6048e-01,\n",
      "        3.6048e-01, 1.7881e-07, 7.3439e-01, 5.5228e-01, 6.5422e-01, 3.6048e-01,\n",
      "        6.6324e-01, 4.1669e-01, 6.6190e-01, 5.8977e-01, 1.7881e-07, 4.1901e-01,\n",
      "        1.7881e-07, 3.6048e-01, 6.4874e-01, 3.6048e-01, 3.6048e-01, 3.6048e-01,\n",
      "        4.1529e-01, 3.6048e-01, 4.2914e-01, 6.4714e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([6.6069e-01, 7.1647e-01, 4.1989e-01, 4.3838e-01, 4.1989e-01, 4.9654e-01,\n",
      "        6.0407e-01, 4.1989e-01, 1.1921e-07, 5.1468e-01, 5.7455e-01, 6.4197e-01,\n",
      "        4.9856e-01, 4.1989e-01, 6.4936e-01, 6.8350e-01, 6.4373e-01, 5.9820e-01,\n",
      "        4.1989e-01, 6.4197e-01, 6.3470e-01, 7.4757e-01, 6.0594e-01, 4.3935e-01,\n",
      "        4.1808e-01, 6.7888e-01, 6.2982e-01, 2.9873e-01, 4.1989e-01, 1.0791e-01,\n",
      "        6.4197e-01, 5.9341e-01, 6.3406e-01, 4.5395e-01, 4.1989e-01, 4.5587e-01,\n",
      "        6.4732e-01, 5.8257e-01, 4.1989e-01, 4.9357e-01, 4.8040e-01, 5.2540e-01,\n",
      "        4.8060e-01, 6.4220e-01, 4.4366e-01, 3.4362e-01, 6.9174e-01, 6.4197e-01,\n",
      "        5.4458e-01, 4.1989e-01, 5.4805e-01, 4.1989e-01, 4.4521e-01, 4.1989e-01,\n",
      "        4.1989e-01, 5.8422e-01, 5.1803e-01, 5.8423e-01, 6.9114e-01, 4.5597e-01,\n",
      "        7.0262e-01, 6.7570e-01, 4.1989e-01, 7.3080e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.4284, 0.4028,    nan, 0.4462,    nan, 0.3455, 0.4284, 0.3764,\n",
      "        0.4727,    nan,    nan, 0.3064,    nan, 0.3304, 0.2001, 0.4817,    nan,\n",
      "        0.4284,    nan, 0.4284,    nan,    nan, 0.4284, 0.4284,    nan, 0.4284,\n",
      "           nan, 0.4284,    nan, 0.3936,    nan, 0.4092,    nan, 0.4368,    nan,\n",
      "        0.4284, 0.4284, 0.4284,    nan,    nan, 0.3125,    nan,    nan,    nan,\n",
      "           nan, 0.3758,    nan, 0.4284,    nan,    nan, 0.4284, 0.4511, 0.4115,\n",
      "        0.4284, 0.4284,    nan,    nan, 0.5373,    nan, 0.1435, 0.4284, 0.4284,\n",
      "        0.5203], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.4062, 0.4062,\n",
      "           nan, 0.5449,    nan, 0.1585,    nan, 0.3708,    nan,    nan, 0.4692,\n",
      "           nan,    nan,    nan,    nan, 0.3272,    nan,    nan, 0.3937,    nan,\n",
      "        0.4224, 0.3258,    nan, 0.4062, 0.4062,    nan, 0.4062, 0.4062, 0.4202,\n",
      "           nan, 0.3881,    nan,    nan, 0.4062,    nan,    nan,    nan, 0.4062,\n",
      "           nan, 0.4062, 0.2866, 0.4062,    nan, 0.3369, 0.4062,    nan,    nan,\n",
      "           nan, 0.4062,    nan, 0.4062,    nan, 0.4062, 0.4062, 0.4031,    nan,\n",
      "        0.4062], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.6037, 0.4740, 0.4740, 0.4740, 0.6367, 0.4092, 0.4962, 0.5771, 0.6316,\n",
      "        0.6095, 0.0893, 0.6556, 0.4740, 0.4092, 0.4092, 0.0014, 0.4092, 0.4092,\n",
      "        0.7310, 0.6133, 0.4587, 0.5223, 0.4740, 0.5606, 0.4740, 0.3421, 0.4092,\n",
      "        0.6233, 0.5512, 0.4131, 0.4092, 0.4510, 0.6778, 0.6558, 0.5913, 0.7471,\n",
      "        0.4092, 0.4342, 0.6252, 0.4092, 0.6713, 0.6474, 0.5375, 0.4092, 0.4092,\n",
      "        0.5851, 0.4092, 0.4092, 0.7272, 0.4740, 0.4092, 0.6640, 0.4740, 0.4092,\n",
      "        0.4131, 0.3577, 0.4740, 0.4092, 0.6174, 0.4092, 0.4092, 0.4740, 0.3829,\n",
      "        0.2792], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4180,    nan, 0.3934, 0.3934,    nan, 0.3934, 0.4356, 0.3934, 0.4127,\n",
      "        0.3934, 0.3934, 0.3934,    nan, 0.3655, 0.3973,    nan, 0.3934,    nan,\n",
      "        0.3934,    nan, 0.3934, 0.4517,    nan,    nan, 0.3934, 0.4013,    nan,\n",
      "           nan, 0.4595,    nan,    nan,    nan,    nan, 0.4302,    nan, 0.4205,\n",
      "           nan,    nan,    nan, 0.3934, 0.3631,    nan, 0.4674, 0.3934,    nan,\n",
      "           nan,    nan,    nan,    nan, 0.4269,    nan,    nan, 0.1564,    nan,\n",
      "           nan, 0.3934,    nan,    nan, 0.3934,    nan, 0.3934,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4006,    nan, 0.4875, 0.4875, 0.4875, 0.4993, 0.1554, 0.4875, 0.3572,\n",
      "        0.2687, 0.3009, 0.4875,    nan,    nan,    nan, 0.4875,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.5087,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.4175,\n",
      "        0.4876,    nan, 0.2745, 0.4875, 0.4875, 0.4875, 0.3103, 0.4875, 0.3839,\n",
      "        0.4688, 0.5285, 0.4875, 0.4875,    nan, 0.4875,    nan, 0.5272, 0.4458,\n",
      "        0.4875, 0.1703,    nan, 0.3450,    nan,    nan,    nan,    nan, 0.4875,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([5.8543e-01, 5.9605e-08, 5.7589e-01, 4.0490e-01, 4.4411e-01, 4.0490e-01,\n",
      "        4.0490e-01, 6.1267e-01, 4.0490e-01, 6.5238e-01, 7.1318e-01, 6.5238e-01,\n",
      "        6.6289e-01, 4.0490e-01, 4.0490e-01, 5.1341e-01, 4.0490e-01, 7.5683e-01,\n",
      "        4.0490e-01, 4.0490e-01, 4.1598e-01, 5.2583e-01, 6.6266e-01, 4.8279e-01,\n",
      "        4.0490e-01, 4.6524e-01, 6.2224e-01, 4.0490e-01, 4.0490e-01, 6.1808e-01,\n",
      "        5.9433e-01, 6.3980e-01, 6.5238e-01, 4.0490e-01, 4.0490e-01, 5.6888e-01,\n",
      "        2.4938e-01, 4.0490e-01, 6.5238e-01, 7.4959e-01, 6.2237e-01, 6.5238e-01,\n",
      "        4.5264e-01, 4.7453e-01, 4.0490e-01, 5.3554e-01, 6.5238e-01, 1.7574e-01,\n",
      "        5.2854e-01, 4.0490e-01, 6.5238e-01, 4.6911e-01, 4.0490e-01, 2.1874e-01,\n",
      "        5.8963e-01, 5.6822e-01, 6.8776e-01, 4.0490e-01, 6.5238e-01, 6.2008e-01,\n",
      "        4.0490e-01, 5.0616e-01, 5.4654e-01, 4.0490e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4790, 0.5446,    nan,    nan,    nan,    nan, 0.4790,    nan,    nan,\n",
      "        0.4790,    nan, 0.4790,    nan,    nan, 0.4790, 0.3073, 0.4790, 0.4790,\n",
      "           nan, 0.4790, 0.4430,    nan,    nan, 0.4790,    nan, 0.4790,    nan,\n",
      "        0.4006, 0.4790,    nan, 0.4790,    nan, 0.4790,    nan,    nan,    nan,\n",
      "        0.4790, 0.4790, 0.4224, 0.4790,    nan,    nan, 0.1407, 0.4790,    nan,\n",
      "           nan,    nan, 0.4790,    nan,    nan,    nan,    nan, 0.4790, 0.4790,\n",
      "        0.4790, 0.4502,    nan, 0.4790,    nan, 0.4141,    nan, 0.4790, 0.4338,\n",
      "        0.4790], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.1433, 0.1433, 0.1433,    nan,    nan,    nan,    nan, 0.1433,\n",
      "           nan, 0.1433, 0.1433, 0.1433, 0.1433, 0.1433,    nan, 0.1433,    nan,\n",
      "           nan, 0.1433,    nan,    nan, 0.1433, 0.1433,    nan,    nan, 0.1433,\n",
      "        0.1433,    nan,    nan, 0.1433, 0.1433, 0.1433, 0.2005, 0.1433, 0.1433,\n",
      "           nan, 0.1433,    nan,    nan, 0.3323,    nan, 0.1433, 0.3442, 0.1433,\n",
      "           nan, 0.1433,    nan,    nan,    nan,    nan, 0.1433, 0.3919,    nan,\n",
      "        0.4150, 0.1433,    nan,    nan,    nan,    nan, 0.1433,    nan,    nan,\n",
      "        0.1433], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.4803,    nan,    nan, 0.4335,    nan,    nan, 0.4335,    nan,\n",
      "        0.4335,    nan,    nan,    nan,    nan, 0.4335, 0.3713, 0.4335, 0.4335,\n",
      "        0.4802,    nan, 0.4335,    nan, 0.4335, 0.4609,    nan,    nan,    nan,\n",
      "        0.4335,    nan, 0.4335, 0.4721,    nan, 0.3824,    nan,    nan,    nan,\n",
      "           nan, 0.4152,    nan, 0.4335, 0.2679,    nan, 0.4335,    nan, 0.4470,\n",
      "           nan,    nan, 0.2443, 0.3808, 0.4335,    nan, 0.5106, 0.4503, 0.2448,\n",
      "           nan,    nan,    nan,    nan,    nan, 0.3945,    nan, 0.3999, 0.4713,\n",
      "        0.4571], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3938,    nan,    nan, 0.3938,    nan, 0.3787,    nan, 0.4578, 0.3939,\n",
      "        0.3938, 0.3938,    nan,    nan,    nan, 0.3938, 0.2991,    nan, 0.3938,\n",
      "        0.3938,    nan, 0.3379, 0.4114, 0.3938, 0.3938,    nan,    nan,    nan,\n",
      "           nan,    nan, 0.3938, 0.3938, 0.3938, 0.3983, 0.3938, 0.3938,    nan,\n",
      "           nan,    nan,    nan,    nan, 0.3938, 0.4217,    nan,    nan,    nan,\n",
      "        0.3938, 0.3938, 0.3938, 0.3003,    nan, 0.2888, 0.3938,    nan,    nan,\n",
      "        0.3938,    nan,    nan,    nan, 0.3938, 0.3938, 0.3632,    nan, 0.3938,\n",
      "        0.4554], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.2682, 0.3448, 0.4154, 0.4154,    nan, 0.4154,    nan, 0.4638, 0.4192,\n",
      "        0.4154,    nan,    nan,    nan,    nan, 0.4712, 0.4154, 0.4154, 0.3072,\n",
      "        0.4154,    nan,    nan, 0.4154, 0.3648,    nan,    nan, 0.4628,    nan,\n",
      "           nan,    nan, 0.4154, 0.4154, 0.4154, 0.4154,    nan,    nan, 0.4099,\n",
      "           nan,    nan, 0.3833,    nan, 0.4154,    nan, 0.4154, 0.3182,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan, 0.2782, 0.3069,    nan,    nan,\n",
      "           nan, 0.4326, 0.4154, 0.4571, 0.4154,    nan, 0.4154, 0.1946, 0.4250,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3845, 0.3845, 0.3845,    nan, 0.3845, 0.2756,    nan,    nan, 0.3845,\n",
      "        0.3845, 0.2840,    nan, 0.2512, 0.4969, 0.3845, 0.4032, 0.3845, 0.4255,\n",
      "        0.3845,    nan, 0.4553, 0.2871, 0.4821,    nan, 0.3845,    nan,    nan,\n",
      "           nan,    nan,    nan, 0.3845,    nan, 0.4270,    nan, 0.3845,    nan,\n",
      "           nan, 0.3845, 0.3845, 0.3845,    nan,    nan,    nan, 0.3845,    nan,\n",
      "        0.3845, 0.2650, 0.3845,    nan, 0.3845, 0.4272,    nan, 0.3845,    nan,\n",
      "           nan,    nan,    nan,    nan, 0.3845,    nan,    nan,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4256, 0.3826, 0.6368, 0.6511, 0.6224, 0.6236, 0.6346, 0.4256, 0.6124,\n",
      "        0.4881, 0.5326, 0.4256, 0.5907, 0.6311, 0.4198, 0.4204, 0.4256, 0.4256,\n",
      "        0.4256, 0.5468, 0.7890, 0.4256, 0.6885, 0.7721, 0.4256, 0.4256, 0.2714,\n",
      "        0.4258, 0.4204, 0.0494, 0.4717, 0.4256, 0.4204, 0.4256, 0.2603, 0.6346,\n",
      "        0.4256, 0.4256, 0.5665, 0.4256, 0.5516, 0.4150, 0.6371, 0.5539, 0.4256,\n",
      "        0.5509, 0.4256, 0.4256, 0.4375, 0.6705, 0.6688, 0.4825, 0.4256, 0.5572,\n",
      "        0.2660, 0.4256, 0.4204, 0.5535, 0.4256, 0.6437, 0.7570, 0.4256, 0.6467,\n",
      "        0.4256], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3270, 0.4952, 0.4117,    nan, 0.3270, 0.4326, 0.3418, 0.3270, 0.3270,\n",
      "        0.3270, 0.3270,    nan, 0.3270, 0.4339, 0.2016, 0.3270, 0.3270,    nan,\n",
      "           nan,    nan, 0.4780, 0.3270, 0.3270, 0.3270, 0.3270, 0.3270,    nan,\n",
      "        0.3249, 0.3270, 0.3270,    nan, 0.3270,    nan,    nan,    nan, 0.3485,\n",
      "           nan,    nan, 0.3193,    nan,    nan,    nan,    nan,    nan, 0.3143,\n",
      "           nan,    nan,    nan, 0.3270, 0.3270,    nan, 0.3024,    nan, 0.3895,\n",
      "        0.4543, 0.2334,    nan,    nan, 0.3169, 0.3270,    nan,    nan, 0.3270,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan,    nan, 0.4660, 0.3628,    nan, 0.4356,    nan,    nan,\n",
      "        0.5185,    nan,    nan, 0.4660, 0.4660,    nan, 0.4660, 0.4660,    nan,\n",
      "        0.1636,    nan, 0.4660, 0.4660,    nan,    nan,    nan, 0.4449, 0.4660,\n",
      "           nan, 0.4660, 0.4660, 0.4660, 0.4429, 0.3140, 0.3894,    nan, 0.4963,\n",
      "           nan, 0.4660,    nan, 0.4660, 0.4660, 0.5382,    nan,    nan,    nan,\n",
      "        0.4286,    nan,    nan,    nan, 0.4660,    nan,    nan,    nan,    nan,\n",
      "           nan, 0.2993,    nan,    nan,    nan,    nan,    nan,    nan, 0.4131,\n",
      "        0.4660], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan,    nan, 0.4446, 0.4446,    nan, 0.3022, 0.4446,    nan,\n",
      "           nan, 0.3884, 0.4580,    nan,    nan,    nan, 0.4446,    nan, 0.4195,\n",
      "        0.4446, 0.4446,    nan, 0.3776,    nan,    nan, 0.4597,    nan, 0.4446,\n",
      "        0.4318, 0.2875, 0.3798,    nan,    nan, 0.5558,    nan, 0.4658, 0.4446,\n",
      "           nan, 0.4446,    nan, 0.4446,    nan,    nan,    nan,    nan, 0.4446,\n",
      "        0.4446,    nan, 0.1657, 0.4048,    nan, 0.4371, 0.4446, 0.4446,    nan,\n",
      "           nan,    nan, 0.4446, 0.3087,    nan,    nan, 0.3106, 0.4446, 0.4446,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4044,    nan,    nan, 0.2850, 0.4044,    nan,    nan,    nan, 0.4044,\n",
      "           nan, 0.4044,    nan,    nan,    nan, 0.4044, 0.4044,    nan,    nan,\n",
      "        0.4044,    nan, 0.4044,    nan,    nan, 0.4044, 0.4044,    nan, 0.4044,\n",
      "           nan,    nan, 0.2887, 0.4044, 0.4044,    nan, 0.4044,    nan,    nan,\n",
      "           nan, 0.4044,    nan, 0.4044,    nan, 0.4044, 0.4044,    nan, 0.4044,\n",
      "        0.1899,    nan,    nan,    nan,    nan,    nan,    nan, 0.4044,    nan,\n",
      "        0.4059,    nan,    nan,    nan, 0.4044,    nan,    nan,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([6.4507e-01, 5.7815e-01, 7.3661e-01, 4.2124e-01, 2.8411e-01, 5.9605e-08,\n",
      "        5.8790e-01, 5.9605e-08, 3.5442e-01, 4.2798e-01, 3.5442e-01, 7.4467e-01,\n",
      "        4.8050e-01, 7.7859e-01, 7.0165e-01, 3.5442e-01, 3.5442e-01, 3.5442e-01,\n",
      "        6.1818e-01, 5.8787e-01, 7.4253e-01, 3.5442e-01, 7.9378e-02, 5.0930e-01,\n",
      "        7.0908e-01, 7.8038e-01, 5.8787e-01, 3.5442e-01, 2.4233e-02, 6.1083e-01,\n",
      "        5.9605e-08, 3.7150e-01, 4.7528e-01, 3.5442e-01, 2.3066e-01, 3.5442e-01,\n",
      "        3.9065e-01, 3.5442e-01, 4.7012e-01, 7.0735e-01, 2.8398e-01, 3.5442e-01,\n",
      "        5.9605e-08, 6.6239e-01, 6.7909e-01, 6.6344e-01, 6.3906e-01, 5.9605e-08,\n",
      "        6.1279e-01, 3.5442e-01, 5.7238e-01, 6.4350e-01, 7.5289e-01, 7.1149e-01,\n",
      "        3.5442e-01, 5.8825e-01, 3.5442e-01, 6.1478e-01, 3.5442e-01, 6.6484e-01,\n",
      "        6.9668e-01, 3.5442e-01, 3.5442e-01, 4.0225e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.4137, 0.4137, 0.4137,    nan,    nan, 0.4829, 0.3718, 0.4137,\n",
      "           nan, 0.4137,    nan, 0.3884, 0.3793,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan, 0.4137, 0.4137, 0.4137,    nan, 0.5082, 0.4644,    nan,\n",
      "           nan, 0.4137,    nan,    nan,    nan, 0.4137, 0.4306, 0.4137, 0.2488,\n",
      "        0.1590,    nan,    nan,    nan,    nan, 0.4137,    nan,    nan, 0.4137,\n",
      "           nan, 0.4137,    nan,    nan,    nan,    nan,    nan, 0.4137,    nan,\n",
      "           nan, 0.3941,    nan,    nan, 0.4137,    nan,    nan, 0.4137,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([ 4.0911e-01,  7.0938e-01,  5.3988e-01,  6.9113e-01,  4.5705e-01,\n",
      "         4.8927e-01,  6.9869e-01,  6.9122e-01,  6.2010e-01,  1.6888e-01,\n",
      "         4.5705e-01,  4.5705e-01,  7.5145e-01,  3.6072e-01,  7.3539e-01,\n",
      "        -1.1921e-07,  4.4350e-01,  4.5705e-01,  7.4386e-01,  6.5537e-01,\n",
      "         7.0274e-01,  4.5705e-01,  7.3651e-01,  4.5705e-01,  6.2101e-01,\n",
      "         5.8501e-01,  4.5705e-01,  8.1365e-01,  6.8615e-01,  4.5705e-01,\n",
      "         1.2154e-01,  4.5883e-01,  4.3041e-01,  6.6823e-01,  4.5705e-01,\n",
      "         4.5705e-01,  3.9593e-01,  3.9369e-01,  6.4223e-01,  4.5705e-01,\n",
      "         5.2889e-01,  4.8300e-01,  3.0713e-01,  7.1290e-01,  7.0921e-01,\n",
      "         4.2132e-01,  4.5705e-01,  7.3700e-01,  6.9719e-01,  6.4424e-01,\n",
      "         7.3351e-01,  2.8499e-01,  3.6359e-01,  4.5705e-01,  5.1892e-01,\n",
      "         8.1365e-01,  7.3711e-01,  4.5705e-01,  7.3525e-01,  7.9002e-01,\n",
      "         4.3362e-01,  5.2733e-01,  7.9687e-01,  5.3575e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.5712, 0.4685, 0.4710, 0.4407, 0.4728, 0.4407, 0.6146, 0.4407, 0.4407,\n",
      "        0.0000, 0.4407, 0.5859, 0.5804, 0.7090, 0.5494, 0.0000, 0.4407, 0.4407,\n",
      "        0.5886, 0.2826, 0.4407, 0.4407, 0.6464, 0.4407, 0.4594, 0.6574, 0.4407,\n",
      "        0.6356, 0.7732, 0.4407, 0.6149, 0.3884, 0.6453, 0.2724, 0.4407, 0.6232,\n",
      "        0.4407, 0.6934, 0.5429, 0.0000, 0.2756, 0.5635, 0.7406, 0.4407, 0.6206,\n",
      "        0.5809, 0.6732, 0.6703, 0.4407, 0.7363, 0.6729, 0.4683, 0.5764, 0.3964,\n",
      "        0.4407, 0.4383, 0.4407, 0.6938, 0.4407, 0.4407, 0.2175, 0.7300, 0.2835,\n",
      "        0.2385], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.2994, 0.5218, 0.5218, 0.5218, 0.4118,    nan,    nan,    nan,\n",
      "           nan, 0.5218, 0.1777,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan, 0.5218, 0.3756,    nan,    nan, 0.4470,    nan, 0.4221,\n",
      "           nan, 0.5206,    nan, 0.2385,    nan,    nan, 0.5077, 0.5218, 0.5218,\n",
      "        0.4961,    nan,    nan, 0.5209,    nan, 0.5218,    nan, 0.4747, 0.5360,\n",
      "           nan,    nan,    nan,    nan, 0.4432, 0.3792, 0.5206, 0.5217,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.5218,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4277, 0.4162, 0.3748, 0.3498, 0.4181,    nan, 0.3893, 0.4645, 0.2007,\n",
      "        0.4645, 0.3018, 0.4645, 0.2842,    nan,    nan,    nan,    nan, 0.4129,\n",
      "        0.4645,    nan, 0.4645, 0.3843, 0.3805, 0.4633,    nan, 0.4645,    nan,\n",
      "           nan,    nan,    nan, 0.4645, 0.4126, 0.2612,    nan,    nan,    nan,\n",
      "        0.3427, 0.4645, 0.4451,    nan,    nan,    nan, 0.3997, 0.4026, 0.4645,\n",
      "        0.3543,    nan, 0.4645,    nan,    nan,    nan, 0.4645, 0.4645,    nan,\n",
      "        0.4645, 0.4645,    nan, 0.3784,    nan,    nan, 0.4133, 0.4218, 0.4645,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4147, 0.4147, 0.4147, 0.5281, 0.4942, 0.4147, 0.5914, 0.6552, 0.6184,\n",
      "        0.4147, 0.4147, 0.6006, 0.5386, 0.5856, 0.6754, 0.6028, 0.4147, 0.4147,\n",
      "        0.4974, 0.5419, 0.0919, 0.7048, 0.6485, 0.5731, 0.5851, 0.4147, 0.6081,\n",
      "        0.6532, 0.4147, 0.4629, 0.4147, 0.6031, 0.5205, 0.4147, 0.4147, 0.4317,\n",
      "        0.4729, 0.6509, 0.6265, 0.5023, 0.4147, 0.4147, 0.4147, 0.6266, 0.4470,\n",
      "        0.4147, 0.5761, 0.4147, 0.4147, 0.5875, 0.6265, 0.4147, 0.6816, 0.5048,\n",
      "        0.4667, 0.5713, 0.5060, 0.6265, 0.6390, 0.6618, 0.4147, 0.6265, 0.0000,\n",
      "        0.6033], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.4652, 0.4652,    nan,    nan, 0.4969,    nan, 0.4652, 0.4250,\n",
      "           nan, 0.4652,    nan,    nan, 0.4652,    nan,    nan, 0.4652, 0.4652,\n",
      "           nan,    nan, 0.3821, 0.4652, 0.3813,    nan, 0.4757, 0.4058,    nan,\n",
      "           nan,    nan,    nan, 0.4769,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan, 0.4652, 0.1468,    nan, 0.4956, 0.4155,    nan,\n",
      "           nan, 0.4652,    nan, 0.1570,    nan,    nan,    nan, 0.4193, 0.4652,\n",
      "           nan,    nan,    nan,    nan, 0.4514, 0.4652,    nan,    nan, 0.4652,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4289,    nan,    nan,    nan, 0.3919, 0.3872,    nan,    nan,    nan,\n",
      "        0.3872, 0.3119,    nan, 0.3872, 0.4649,    nan, 0.3872,    nan, 0.3872,\n",
      "           nan, 0.3872, 0.4494,    nan, 0.4460,    nan, 0.3872,    nan, 0.3872,\n",
      "        0.4154,    nan,    nan, 0.3872,    nan,    nan,    nan,    nan, 0.4093,\n",
      "        0.2308,    nan, 0.3872,    nan,    nan, 0.3872,    nan,    nan,    nan,\n",
      "        0.1610, 0.5187,    nan, 0.4967,    nan, 0.3872, 0.3872, 0.3872, 0.4819,\n",
      "           nan, 0.3872,    nan, 0.3872,    nan,    nan,    nan, 0.3872,    nan,\n",
      "        0.3872], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   nan,    nan,    nan, 0.4180, 0.0741,    nan,    nan, 0.4180,    nan,\n",
      "        0.4280, 0.4387,    nan, 0.4180,    nan,    nan,    nan],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([16])\n",
      "tensor([   nan,    nan, 0.4051, 0.4051,    nan,    nan, 0.4051,    nan, 0.4051,\n",
      "        0.2368,    nan, 0.4051,    nan,    nan, 0.4051, 0.4051,    nan, 0.4051,\n",
      "        0.4051, 0.4051, 0.4869,    nan,    nan, 0.4051, 0.4795,    nan, 0.4024,\n",
      "           nan, 0.4051,    nan, 0.4051,    nan,    nan, 0.4051,    nan,    nan,\n",
      "        0.3939,    nan,    nan,    nan,    nan, 0.3132,    nan,    nan, 0.2605,\n",
      "           nan, 0.4051,    nan,    nan,    nan,    nan, 0.2857, 0.4051,    nan,\n",
      "           nan, 0.4051,    nan, 0.3786,    nan, 0.4051,    nan, 0.4401,    nan,\n",
      "        0.4338], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4580,    nan, 0.4580,    nan, 0.4580,    nan,    nan, 0.4580,    nan,\n",
      "        0.4580,    nan,    nan,    nan,    nan,    nan, 0.4580, 0.4580, 0.3546,\n",
      "        0.4580, 0.5349,    nan, 0.4580, 0.4077,    nan,    nan,    nan, 0.4006,\n",
      "        0.2756,    nan,    nan, 0.4580,    nan,    nan, 0.4580, 0.3996,    nan,\n",
      "           nan, 0.4580,    nan,    nan, 0.4360, 0.4580, 0.4580,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan, 0.4580, 0.2607, 0.2683, 0.1965, 0.4649,\n",
      "           nan,    nan, 0.3681, 0.4580,    nan,    nan,    nan, 0.4580,    nan,\n",
      "        0.3705], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3500,    nan,    nan, 0.3788,    nan,    nan,    nan,    nan,    nan,\n",
      "        0.4197, 0.3788, 0.4336, 0.3788, 0.4754,    nan,    nan, 0.3584, 0.3788,\n",
      "        0.2237, 0.3788, 0.3090, 0.3788,    nan,    nan, 0.3788,    nan, 0.4368,\n",
      "           nan, 0.3788, 0.2444,    nan, 0.3788,    nan, 0.4432, 0.3788, 0.4455,\n",
      "           nan,    nan,    nan, 0.3788,    nan,    nan, 0.3788,    nan,    nan,\n",
      "        0.3788, 0.3788, 0.3788,    nan, 0.3389,    nan, 0.3374,    nan,    nan,\n",
      "           nan,    nan, 0.4017,    nan, 0.3788,    nan, 0.3788, 0.3788, 0.3788,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3666,    nan, 0.3666,    nan,    nan, 0.4111,    nan, 0.3666, 0.3666,\n",
      "        0.2816, 0.3666, 0.3193, 0.3666,    nan,    nan,    nan,    nan, 0.4024,\n",
      "        0.5014, 0.3666,    nan, 0.3666, 0.3666, 0.3864,    nan, 0.3666,    nan,\n",
      "           nan,    nan, 0.3666,    nan, 0.3666,    nan,    nan, 0.4735,    nan,\n",
      "           nan, 0.3045, 0.4001,    nan, 0.3666,    nan,    nan, 0.3666, 0.3666,\n",
      "        0.3387, 0.3666, 0.4679, 0.3666,    nan, 0.3666, 0.4013,    nan,    nan,\n",
      "        0.3854, 0.3666,    nan, 0.3666,    nan, 0.2639, 0.3666,    nan, 0.3666,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4005, 0.4650,    nan, 0.3851,    nan,    nan, 0.4005,    nan,    nan,\n",
      "           nan,    nan,    nan, 0.3117, 0.3439, 0.4005, 0.4005,    nan,    nan,\n",
      "           nan,    nan, 0.4005,    nan,    nan,    nan, 0.1131,    nan, 0.4298,\n",
      "        0.4216,    nan,    nan,    nan,    nan,    nan, 0.4397,    nan,    nan,\n",
      "        0.4005,    nan, 0.4005,    nan,    nan,    nan,    nan,    nan, 0.4318,\n",
      "        0.4005, 0.4005, 0.4005,    nan, 0.4005,    nan,    nan, 0.4005,    nan,\n",
      "           nan,    nan,    nan,    nan, 0.4005, 0.4005,    nan, 0.3084,    nan,\n",
      "        0.4613], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.3830, 0.3830,    nan, 0.3830,    nan, 0.3830, 0.3830, 0.3830,\n",
      "           nan, 0.3830, 0.3890,    nan, 0.4098,    nan,    nan, 0.3830, 0.3830,\n",
      "           nan,    nan,    nan, 0.3830, 0.3122, 0.3830, 0.3830, 0.3830, 0.4099,\n",
      "        0.3830, 0.3830, 0.4214, 0.3351,    nan, 0.3830, 0.3830,    nan, 0.4138,\n",
      "        0.3830,    nan,    nan,    nan,    nan, 0.3830, 0.3928,    nan, 0.3654,\n",
      "        0.3830, 0.4109,    nan,    nan, 0.3830,    nan, 0.4084, 0.1151,    nan,\n",
      "        0.3830, 0.3830,    nan, 0.3830,    nan,    nan, 0.3830,    nan, 0.3270,\n",
      "        0.3830], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3047, 0.3047, 0.3047, 0.3047,    nan,    nan, 0.3047,    nan,    nan,\n",
      "        0.3047, 0.3047,    nan,    nan,    nan, 0.3362, 0.3498, 0.3047,    nan,\n",
      "           nan,    nan, 0.3324, 0.3047,    nan,    nan,    nan,    nan,    nan,\n",
      "        0.3047, 0.3047,    nan, 0.3047, 0.3351, 0.3218, 0.3047,    nan,    nan,\n",
      "        0.3047, 0.3047,    nan,    nan,    nan,    nan, 0.3047, 0.4141, 0.3755,\n",
      "           nan,    nan, 0.3047, 0.3047, 0.3047, 0.2120, 0.4294,    nan, 0.3047,\n",
      "        0.3047,    nan, 0.2634,    nan, 0.3047,    nan, 0.3047, 0.3047, 0.4408,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.2940,    nan, 0.2725,    nan, 0.4676, 0.5035,    nan,    nan,\n",
      "           nan, 0.3386,    nan,    nan,    nan,    nan, 0.4838, 0.5035,    nan,\n",
      "        0.4645, 0.2794, 0.5035, 0.5035,    nan,    nan, 0.5035, 0.5035, 0.5035,\n",
      "        0.5369,    nan,    nan, 0.5611,    nan,    nan,    nan,    nan,    nan,\n",
      "        0.5035, 0.5035,    nan, 0.4494, 0.5035, 0.4452,    nan,    nan, 0.5035,\n",
      "        0.1711,    nan, 0.4568,    nan,    nan, 0.3059,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan, 0.5035,    nan,    nan, 0.3934,    nan, 0.5035,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.3279,    nan,    nan, 0.2152, 0.2152,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan, 0.4670,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan, 0.2152, 0.2152,    nan,    nan,    nan,\n",
      "        0.4338,    nan,    nan, 0.4006,    nan, 0.2152, 0.2152,    nan, 0.2152,\n",
      "           nan, 0.2152, 0.2152, 0.2152, 0.2152, 0.2152, 0.2152, 0.1680,    nan,\n",
      "           nan,    nan, 0.3181,    nan, 0.4221,    nan,    nan, 0.2152,    nan,\n",
      "        0.2118,    nan, 0.2152, 0.3158,    nan, 0.2152,    nan,    nan, 0.2152,\n",
      "        0.3824], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.3887,    nan,    nan,    nan,    nan,    nan, 0.2368,\n",
      "        0.4153, 0.3430,    nan, 0.3430, 0.3430, 0.3430,    nan, 0.3430, 0.3430,\n",
      "        0.3430,    nan, 0.3430,    nan, 0.2404,    nan,    nan, 0.4223, 0.4278,\n",
      "           nan, 0.3430,    nan, 0.4060,    nan,    nan, 0.3430,    nan, 0.3819,\n",
      "        0.3430,    nan, 0.3430, 0.3661, 0.3344, 0.3817,    nan,    nan,    nan,\n",
      "        0.3430,    nan,    nan,    nan,    nan, 0.3430,    nan,    nan,    nan,\n",
      "        0.3430,    nan, 0.3430, 0.3430, 0.3430,    nan, 0.3430,    nan, 0.2892,\n",
      "        0.4467], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([3.8343e-01, 3.8343e-01, 5.7438e-01, 3.8343e-01, 7.2540e-01, 3.3090e-01,\n",
      "        3.8343e-01, 3.8343e-01, 6.6299e-01, 2.6207e-01, 3.8343e-01, 3.8367e-01,\n",
      "        7.4082e-01, 3.8343e-01, 6.3806e-01, 3.8343e-01, 2.7243e-01, 4.2967e-01,\n",
      "        6.9909e-01, 6.4660e-01, 3.8343e-01, 6.2502e-01, 3.8343e-01, 3.8343e-01,\n",
      "        5.5263e-01, 2.7055e-01, 6.5667e-01, 3.3827e-01, 3.8343e-01, 3.6340e-01,\n",
      "        1.7881e-07, 3.8343e-01, 6.3569e-01, 5.5087e-01, 6.6576e-01, 6.9828e-01,\n",
      "        7.4095e-01, 6.5667e-01, 3.8343e-01, 6.5667e-01, 3.8343e-01, 4.4016e-01,\n",
      "        3.8343e-01, 3.8343e-01, 7.0374e-01, 6.7951e-01, 3.8343e-01, 6.5667e-01,\n",
      "        4.3489e-01, 3.5050e-01, 6.2342e-01, 3.8343e-01, 7.0978e-01, 6.5667e-01,\n",
      "        3.8343e-01, 4.2538e-01, 3.8343e-01, 5.6787e-01, 6.7280e-01, 3.8343e-01,\n",
      "        5.8172e-01, 3.8343e-01, 5.9239e-01, 3.8343e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([6.7186e-01, 4.0989e-01, 6.4058e-01, 4.2687e-01, 7.5058e-01, 5.8441e-01,\n",
      "        5.4027e-01, 7.1644e-01, 4.8955e-01, 5.5348e-01, 1.2173e-01, 7.2109e-01,\n",
      "        5.9500e-01, 5.4027e-01, 7.0415e-01, 3.7927e-01, 1.2173e-01, 5.4027e-01,\n",
      "        5.9790e-01, 5.4027e-01, 6.9498e-02, 6.9521e-01, 5.4027e-01, 5.4027e-01,\n",
      "        6.3060e-01, 6.0769e-01, 7.8241e-01, 5.9605e-08, 6.7218e-01, 4.0197e-01,\n",
      "        6.0459e-01, 3.9217e-01, 6.9824e-01, 1.2173e-01, 6.5209e-01, 6.3790e-01,\n",
      "        5.0653e-01, 5.4027e-01, 5.4027e-01, 7.3865e-01, 5.4027e-01, 6.8236e-01,\n",
      "        7.8597e-01, 7.6156e-01, 5.1741e-01, 6.6823e-01, 7.6363e-01, 7.5532e-01,\n",
      "        5.9872e-01, 5.4027e-01, 3.9707e-01, 5.4027e-01, 4.1870e-01, 6.6522e-01,\n",
      "        6.0186e-01, 5.4400e-01, 6.4237e-01, 5.4027e-01, 5.4027e-01, 5.4027e-01,\n",
      "        5.4027e-01, 5.6542e-01, 6.0176e-01, 7.0894e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3423, 0.6570, 0.6183, 0.3423, 0.3359, 0.3423, 0.2993, 0.3423, 0.6228,\n",
      "        0.4260, 0.6682, 0.3508, 0.6559, 0.3423, 0.3757, 0.7004, 0.3610, 0.4644,\n",
      "        0.5096, 0.4379, 0.6413, 0.6066, 0.6733, 0.6733, 0.3423, 0.3423, 0.6733,\n",
      "        0.3423, 0.3423, 0.6269, 0.7012, 0.4491, 0.3423, 0.5052, 0.2891, 0.3423,\n",
      "        0.5172, 0.3388, 0.3423, 0.7464, 0.6188, 0.3423, 0.5569, 0.3423, 0.3423,\n",
      "        0.6948, 0.3423, 0.4587, 0.5123, 0.3423, 0.5156, 0.4826, 0.3423, 0.5123,\n",
      "        0.4909, 0.3423, 0.3423, 0.4979, 0.6083, 0.0000, 0.3735, 0.6344, 0.2240,\n",
      "        0.3423], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([ 3.2596e-01,  4.5392e-01,  6.2652e-01,  4.9950e-01,  3.9800e-01,\n",
      "         7.0038e-01,  7.5982e-01,  4.5392e-01,  3.7879e-01,  5.2214e-01,\n",
      "         7.5754e-01,  4.5392e-01,  4.5392e-01,  6.5033e-01,  4.5392e-01,\n",
      "         4.8554e-01,  4.5392e-01,  6.4906e-01,  4.5392e-01,  1.9742e-01,\n",
      "         3.0702e-01,  4.1971e-01,  5.5151e-01,  5.6260e-01,  6.1260e-01,\n",
      "         5.2009e-01, -1.1921e-07,  4.7179e-01,  5.8535e-01,  5.8651e-01,\n",
      "         3.7930e-01,  3.1726e-01,  7.3348e-01,  3.8663e-01,  6.5471e-01,\n",
      "         5.8397e-01,  2.6334e-01,  4.6183e-01,  3.6597e-01,  5.8770e-01,\n",
      "         3.1949e-01,  7.5314e-01,  2.6637e-01,  4.5392e-01,  6.5158e-01,\n",
      "         7.0682e-01,  4.5392e-01,  6.7097e-01,  7.0333e-01,  6.1713e-01,\n",
      "         3.4874e-01,  4.4119e-01,  2.5521e-01,  4.6107e-01,  4.5392e-01,\n",
      "         6.4413e-01,  4.5392e-01,  5.4021e-01,  4.3993e-01,  5.0166e-01,\n",
      "         6.3838e-01,  4.5392e-01,  3.6667e-01,  3.5374e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.6385, 0.5370, 0.7512, 0.3447, 0.6656, 0.6714, 0.6340, 0.3447, 0.6775,\n",
      "        0.0000, 0.7010, 0.3447, 0.5418, 0.3326, 0.3447, 0.6864, 0.1048, 0.4344,\n",
      "        0.3447, 0.3447, 0.3447, 0.3447, 0.3447, 0.0000, 0.4505, 0.3447, 0.3447,\n",
      "        0.4505, 0.6251, 0.6731, 0.6590, 0.6407, 0.2290, 0.6801, 0.3447, 0.6256,\n",
      "        0.5338, 0.1715, 0.2070, 0.0000, 0.3447, 0.0000, 0.0000, 0.2429, 0.3447,\n",
      "        0.3251, 0.6117, 0.3806, 0.5983, 0.6655, 0.3447, 0.3447, 0.6016, 0.7148,\n",
      "        0.3447, 0.5656, 0.3447, 0.3468, 0.3447, 0.3447, 0.6800, 0.6850, 0.7035,\n",
      "        0.1851], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4744,    nan,    nan, 0.4744, 0.4744, 0.4744,    nan, 0.4744,    nan,\n",
      "           nan, 0.5185, 0.4435,    nan,    nan, 0.4744,    nan, 0.5297,    nan,\n",
      "        0.4038,    nan, 0.4744,    nan, 0.4744,    nan,    nan, 0.4744, 0.4460,\n",
      "        0.4744,    nan,    nan,    nan, 0.4744,    nan,    nan, 0.0851,    nan,\n",
      "        0.4744, 0.4628, 0.4744, 0.4744, 0.4744, 0.4744, 0.5813,    nan, 0.5258,\n",
      "        0.4744, 0.4744, 0.3423,    nan,    nan,    nan, 0.4744,    nan, 0.4744,\n",
      "        0.4744,    nan, 0.4744,    nan, 0.4744,    nan, 0.4744, 0.4744,    nan,\n",
      "        0.4744], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4807, 0.2137, 0.1712, 0.2137,    nan, 0.2137, 0.4882, 0.2137, 0.2137,\n",
      "           nan, 0.2137, 0.2137,    nan,    nan,    nan,    nan,    nan, 0.1970,\n",
      "        0.2137, 0.2137, 0.2137, 0.2137, 0.2137, 0.2137, 0.2137,    nan, 0.2137,\n",
      "           nan,    nan, 0.3396, 0.2137, 0.2137, 0.2137, 0.2137, 0.2137, 0.2137,\n",
      "        0.2137, 0.2137,    nan,    nan,    nan, 0.2137,    nan, 0.2137, 0.2137,\n",
      "           nan, 0.2137,    nan,    nan, 0.2137,    nan, 0.2137, 0.2137,    nan,\n",
      "           nan, 0.2137,    nan, 0.2137, 0.4174,    nan, 0.2137,    nan, 0.3752,\n",
      "        0.2137], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5150, 0.5850, 0.3156, 0.3743, 0.3743, 0.3743, 0.5870, 0.5219, 0.3743,\n",
      "        0.4024, 0.6017, 0.3743, 0.6106, 0.3743, 0.3108, 0.6613, 0.3743, 0.3743,\n",
      "        0.3743, 0.6509, 0.6495, 0.2674, 0.3743, 0.6991, 0.3743, 0.0000, 0.7568,\n",
      "        0.3743, 0.0000, 0.0000, 0.3743, 0.3743, 0.3743, 0.4491, 0.0000, 0.3010,\n",
      "        0.4227, 0.6540, 0.7245, 0.6766, 0.3743, 0.3743, 0.3743, 0.6893, 0.3743,\n",
      "        0.6544, 0.3743, 0.7416, 0.3743, 0.3743, 0.3743, 0.2982, 0.6911, 0.2120,\n",
      "        0.4183, 0.7004, 0.7475, 0.3743, 0.4348, 0.3743, 0.6540, 0.3743, 0.4320,\n",
      "        0.3743], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.5026,    nan, 0.3463,    nan, 0.4446,    nan,    nan, 0.4446,    nan,\n",
      "        0.4446,    nan, 0.3691, 0.4506, 0.4446,    nan,    nan,    nan,    nan,\n",
      "        0.3596,    nan, 0.4446,    nan,    nan,    nan, 0.3522,    nan, 0.4446,\n",
      "           nan,    nan, 0.4446,    nan, 0.4446,    nan, 0.4446, 0.4446,    nan,\n",
      "           nan,    nan,    nan, 0.4446,    nan,    nan, 0.4320, 0.4446,    nan,\n",
      "        0.4446, 0.4446,    nan,    nan, 0.4446,    nan, 0.4446,    nan, 0.1639,\n",
      "           nan, 0.4446, 0.3679, 0.4446,    nan, 0.4446,    nan,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.3606,    nan, 0.3606,    nan, 0.3645, 0.4562,    nan,    nan,\n",
      "        0.3606, 0.3606,    nan,    nan, 0.3606, 0.3606,    nan, 0.3606, 0.3606,\n",
      "        0.3606,    nan, 0.3606,    nan, 0.3606,    nan, 0.3877, 0.3783, 0.3820,\n",
      "           nan,    nan,    nan,    nan, 0.3606, 0.3585,    nan,    nan, 0.3606,\n",
      "           nan, 0.3606, 0.3606, 0.4173, 0.3070, 0.3702, 0.3606,    nan,    nan,\n",
      "           nan, 0.3606,    nan,    nan, 0.4913,    nan, 0.4190, 0.3606, 0.3606,\n",
      "        0.3606,    nan, 0.4101,    nan,    nan, 0.3606, 0.4388, 0.3606,    nan,\n",
      "        0.3606], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3670,    nan, 0.3521, 0.3670, 0.3670, 0.3670, 0.3670,    nan, 0.2668,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan, 0.4779,    nan, 0.3670,\n",
      "        0.3670, 0.3670,    nan,    nan,    nan,    nan,    nan, 0.3670,    nan,\n",
      "           nan,    nan,    nan, 0.2916,    nan,    nan, 0.3670, 0.4484,    nan,\n",
      "           nan,    nan, 0.3670, 0.4251, 0.3670, 0.3670, 0.3670, 0.3670, 0.3670,\n",
      "        0.3670,    nan, 0.3670,    nan, 0.5378,    nan, 0.3670, 0.3023, 0.3670,\n",
      "           nan, 0.4668,    nan,    nan,    nan,    nan, 0.4283,    nan, 0.3670,\n",
      "        0.3670], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4815,    nan, 0.3641, 0.4815, 0.4815,    nan, 0.4815,    nan,    nan,\n",
      "        0.4815,    nan, 0.4815,    nan, 0.4900,    nan, 0.4815, 0.4355, 0.3715,\n",
      "        0.4815, 0.4815,    nan, 0.4815,    nan, 0.4815, 0.4815, 0.5251,    nan,\n",
      "        0.4815,    nan, 0.4815, 0.4815,    nan, 0.4480, 0.3157,    nan,    nan,\n",
      "        0.5271, 0.4815, 0.4815, 0.2555, 0.5240,    nan,    nan, 0.4815,    nan,\n",
      "        0.4770, 0.4815,    nan,    nan, 0.4815, 0.3499, 0.4660,    nan,    nan,\n",
      "           nan, 0.4815, 0.0932,    nan,    nan, 0.3504,    nan,    nan,    nan,\n",
      "        0.4815], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4074,    nan, 0.4074, 0.5595,    nan, 0.4064,    nan,    nan,    nan,\n",
      "           nan, 0.4074,    nan,    nan, 0.2491,    nan, 0.3114,    nan, 0.4074,\n",
      "        0.4074,    nan, 0.4074,    nan,    nan,    nan,    nan, 0.2955, 0.5501,\n",
      "        0.4693,    nan,    nan,    nan, 0.4074,    nan,    nan, 0.4074,    nan,\n",
      "           nan,    nan,    nan, 0.4822,    nan, 0.2185, 0.4074, 0.4074,    nan,\n",
      "           nan, 0.4074, 0.4024, 0.4074, 0.4074,    nan,    nan, 0.4074, 0.3841,\n",
      "        0.4074, 0.4074,    nan,    nan,    nan, 0.4074,    nan,    nan, 0.3560,\n",
      "        0.4924], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([ 2.9219e-01,  7.1634e-01,  3.6202e-01,  6.0555e-01,  3.6202e-01,\n",
      "        -1.1921e-07,  6.7199e-01,  7.3147e-01,  3.6202e-01,  6.2997e-01,\n",
      "         3.6202e-01,  3.6202e-01,  3.6202e-01,  3.6202e-01,  3.6202e-01,\n",
      "        -1.1921e-07,  6.2102e-01,  3.6202e-01,  5.9192e-01,  6.7344e-01,\n",
      "         5.9734e-01, -1.1921e-07,  6.9080e-01,  3.6202e-01,  2.9190e-01,\n",
      "         3.6202e-01,  3.6202e-01,  6.8008e-01,  3.6202e-01,  3.6202e-01,\n",
      "         6.2046e-01,  6.1155e-01,  3.6202e-01,  3.7424e-01,  3.6202e-01,\n",
      "         3.6202e-01,  3.6202e-01,  6.2236e-01,  4.1199e-01,  6.9973e-01,\n",
      "         6.2108e-01,  5.3420e-01,  3.3464e-01,  6.8584e-01,  5.9312e-01,\n",
      "         5.9617e-01,  6.0176e-01,  6.2983e-01,  2.9033e-01,  7.4403e-01,\n",
      "         3.6202e-01,  3.6202e-01,  6.3878e-01,  3.2348e-01,  6.7902e-01,\n",
      "         3.6202e-01,  7.3429e-01,  3.9697e-01, -1.1921e-07,  5.2872e-01,\n",
      "         3.6202e-01,  3.8119e-01,  3.6202e-01,  6.9415e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.3658,    nan, 0.4013, 0.3491, 0.3491, 0.4185, 0.3491,    nan,\n",
      "        0.3491, 0.3491,    nan, 0.3517, 0.2640,    nan,    nan, 0.3491, 0.3491,\n",
      "        0.3491, 0.3491, 0.3292,    nan, 0.4444,    nan, 0.3491, 0.3491,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan, 0.3491,    nan,    nan,    nan,\n",
      "           nan, 0.3294, 0.4083, 0.3491,    nan, 0.3634,    nan,    nan, 0.3491,\n",
      "           nan, 0.3491,    nan,    nan, 0.3491, 0.4465, 0.3373,    nan, 0.3491,\n",
      "           nan,    nan, 0.3491,    nan,    nan,    nan, 0.4579, 0.3491,    nan,\n",
      "        0.3491], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([ 4.3445e-01,  6.4154e-01,  4.3445e-01,  4.3445e-01,  5.6860e-01,\n",
      "         4.3445e-01,  2.9194e-01,  6.2255e-01,  6.5886e-01,  6.9259e-01,\n",
      "         7.4813e-01,  7.5041e-01,  6.5959e-01,  6.5771e-01,  7.9602e-01,\n",
      "         4.3445e-01,  4.3445e-01,  6.2713e-01,  6.5886e-01,  6.7009e-01,\n",
      "         7.0184e-01,  7.6005e-01,  4.3445e-01,  6.5886e-01,  5.5954e-01,\n",
      "         4.3445e-01,  4.0839e-01,  4.3445e-01,  4.3445e-01,  4.3445e-01,\n",
      "         3.7327e-01,  2.4342e-01,  6.2530e-01,  4.3445e-01,  5.0803e-01,\n",
      "         7.4779e-01,  6.8024e-01,  6.8445e-01,  4.3445e-01,  6.2206e-01,\n",
      "         4.3445e-01,  3.9561e-01,  4.3445e-01,  4.3445e-01,  7.0962e-01,\n",
      "         7.0962e-01,  4.3445e-01,  4.3445e-01,  3.6638e-01,  5.6761e-01,\n",
      "         4.1839e-01,  6.6973e-01,  6.5886e-01,  4.3445e-01,  6.3280e-01,\n",
      "         4.3445e-01,  4.3445e-01,  6.4480e-01,  3.2445e-01,  8.4802e-01,\n",
      "         4.3445e-01, -1.1921e-07,  6.4250e-01,  1.9698e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.3845,    nan,    nan, 0.4386, 0.4386, 0.2361,    nan, 0.4386,\n",
      "        0.4386, 0.4386,    nan,    nan,    nan,    nan, 0.4386, 0.4368, 0.4386,\n",
      "           nan, 0.4386,    nan,    nan,    nan, 0.4386, 0.3399,    nan, 0.3195,\n",
      "        0.4386,    nan, 0.4386, 0.4386, 0.2500,    nan, 0.2899, 0.3564, 0.4386,\n",
      "           nan, 0.4386,    nan, 0.3197, 0.4386,    nan,    nan, 0.4386, 0.4386,\n",
      "           nan, 0.4386,    nan, 0.4386, 0.4820,    nan, 0.4386,    nan, 0.4386,\n",
      "        0.2954,    nan,    nan,    nan,    nan, 0.4386, 0.4386,    nan,    nan,\n",
      "        0.4386], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.4366,    nan, 0.1630, 0.1630,    nan,    nan,    nan,    nan,\n",
      "        0.2099,    nan, 0.1630, 0.1630,    nan,    nan,    nan, 0.1630,    nan,\n",
      "        0.1630,    nan,    nan, 0.1630, 0.1630, 0.1630,    nan, 0.1630,    nan,\n",
      "           nan, 0.1630,    nan,    nan, 0.1630,    nan,    nan, 0.1630,    nan,\n",
      "           nan, 0.3797,    nan, 0.1630, 0.1630,    nan,    nan, 0.1630, 0.2870,\n",
      "           nan,    nan,    nan, 0.1630, 0.2257,    nan,    nan, 0.1630,    nan,\n",
      "        0.1630, 0.1630, 0.1630, 0.1630,    nan, 0.1630, 0.1630,    nan, 0.3851,\n",
      "        0.1630], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.7314, 0.6830, 0.7452, 0.4752, 0.3379, 0.7518, 0.0000, 0.6484, 0.4752,\n",
      "        0.6820, 0.6161, 0.6708, 0.6457, 0.4752, 0.4752, 0.7477, 0.3971, 0.4752,\n",
      "        0.4752, 0.3372, 0.4752, 0.0000, 0.7114, 0.6675, 0.3891, 0.1127, 0.7334,\n",
      "        0.4752, 0.6402, 0.6181, 0.7818, 0.6358, 0.5009, 0.2222, 0.6124, 0.4752,\n",
      "        0.4752, 0.5225, 0.4765, 0.4752, 0.0000, 0.6299, 0.4752, 0.4752, 0.6058,\n",
      "        0.4752, 0.6415, 0.6041, 0.5123, 0.6466, 0.5851, 0.4752, 0.4752, 0.6336,\n",
      "        0.4752, 0.6808, 0.7138, 0.7172, 0.4841, 0.4752, 0.5598, 0.4752, 0.4752,\n",
      "        0.0000], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.4396, 0.4396,    nan, 0.4396, 0.0660, 0.4396, 0.4396,\n",
      "           nan,    nan,    nan, 0.4498, 0.4396,    nan,    nan, 0.4396,    nan,\n",
      "           nan, 0.4396,    nan, 0.4465, 0.4396,    nan,    nan, 0.4396,    nan,\n",
      "           nan, 0.5100, 0.4396,    nan, 0.4396, 0.4396, 0.4396,    nan, 0.4396,\n",
      "        0.4396, 0.4396, 0.4396,    nan,    nan,    nan,    nan, 0.4396,    nan,\n",
      "           nan, 0.3149,    nan, 0.4396, 0.4117,    nan,    nan,    nan,    nan,\n",
      "        0.4556,    nan,    nan, 0.4396,    nan,    nan,    nan, 0.4396, 0.4396,\n",
      "        0.4396], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan,    nan, 0.4229, 0.3148, 0.3148,    nan, 0.4057, 0.3148,\n",
      "        0.3862,    nan, 0.3148,    nan,    nan,    nan, 0.3666, 0.3423, 0.3148,\n",
      "           nan, 0.3148, 0.5164,    nan, 0.3715, 0.3148, 0.3345,    nan,    nan,\n",
      "        0.4205,    nan, 0.3148,    nan, 0.3259, 0.3265,    nan, 0.4126,    nan,\n",
      "        0.4329, 0.3148,    nan, 0.3148, 0.3148, 0.3505, 0.3648,    nan,    nan,\n",
      "        0.3148, 0.3148, 0.3148,    nan,    nan, 0.4475,    nan,    nan,    nan,\n",
      "        0.3148, 0.3148,    nan,    nan, 0.3097, 0.3148,    nan, 0.3931, 0.3248,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.5905, 0.3656, 0.3713, 0.7441, 0.2730, 0.2384, 0.3386, 0.7159, 0.2730,\n",
      "        0.6830, 0.2730, 0.4000, 0.5802, 0.7260, 0.7300, 0.2730, 0.7540, 0.2730,\n",
      "        0.2966, 0.2764, 0.2730, 0.5224, 0.2730, 0.5994, 0.6677, 0.2730, 0.6454,\n",
      "        0.2730, 0.2730, 0.2730, 0.6023, 0.2730, 0.7078, 0.2730, 0.2730, 0.5224,\n",
      "        0.7101, 0.2730, 0.3792, 0.5975, 0.6992, 0.6519, 0.2730, 0.4844, 0.5804,\n",
      "        0.5591, 0.2730, 0.5224, 0.6673, 0.6897, 0.0058, 0.5224, 0.3576, 0.6439,\n",
      "        0.5224, 0.5224, 0.5224, 0.2730, 0.2730, 0.2730, 0.2730, 0.5386, 0.6283,\n",
      "        0.3794], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.2956,    nan, 0.3019, 0.4920, 0.2067,    nan,    nan,    nan,    nan,\n",
      "        0.4502,    nan,    nan,    nan,    nan, 0.2593,    nan, 0.5155, 0.5184,\n",
      "        0.4920,    nan,    nan,    nan,    nan,    nan, 0.4920, 0.4622, 0.4920,\n",
      "           nan,    nan,    nan, 0.4920, 0.4920, 0.2592,    nan, 0.4920, 0.4920,\n",
      "           nan,    nan,    nan, 0.4920, 0.4920, 0.4920,    nan, 0.4920, 0.4920,\n",
      "        0.5222, 0.4191, 0.1750, 0.4920,    nan,    nan, 0.4920,    nan,    nan,\n",
      "           nan, 0.4920,    nan,    nan, 0.4920,    nan, 0.4930, 0.4920, 0.4920,\n",
      "        0.4920], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3885, 0.3885,    nan, 0.4466,    nan,    nan,    nan, 0.3885,    nan,\n",
      "        0.3885, 0.4530, 0.3885,    nan,    nan,    nan, 0.3885,    nan,    nan,\n",
      "        0.3885, 0.3885,    nan, 0.4690, 0.3885, 0.3885, 0.3885,    nan, 0.3885,\n",
      "           nan, 0.3885, 0.3885,    nan, 0.3885, 0.4454, 0.3885,    nan,    nan,\n",
      "        0.3885, 0.3885, 0.3885, 0.3885,    nan, 0.3885, 0.3885,    nan,    nan,\n",
      "        0.3909, 0.3885,    nan, 0.3885, 0.3885,    nan, 0.3885,    nan,    nan,\n",
      "           nan, 0.3885,    nan,    nan, 0.3885, 0.0739,    nan,    nan,    nan,\n",
      "        0.3885], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan, 0.3020, 0.4202, 0.4202, 0.3365,    nan, 0.4202,    nan,\n",
      "        0.4202,    nan,    nan, 0.4202,    nan, 0.4202,    nan,    nan,    nan,\n",
      "           nan,    nan, 0.2783,    nan, 0.4202, 0.2536,    nan,    nan,    nan,\n",
      "        0.4202,    nan,    nan,    nan,    nan,    nan, 0.4202,    nan,    nan,\n",
      "           nan, 0.4202, 0.4202, 0.4202,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan, 0.4202,    nan, 0.4202, 0.4202,    nan, 0.4202,    nan,\n",
      "        0.4202,    nan, 0.4202, 0.4393, 0.4202, 0.2579, 0.5320,    nan, 0.3479,\n",
      "        0.4202], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.4352, 0.4156, 0.4352,    nan, 0.4352, 0.4352, 0.4352,    nan,\n",
      "        0.2693,    nan, 0.4352,    nan, 0.4352,    nan,    nan, 0.4352, 0.4318,\n",
      "           nan, 0.4904,    nan, 0.4352,    nan, 0.4352, 0.5108, 0.4352,    nan,\n",
      "        0.4352, 0.4963,    nan,    nan, 0.5283,    nan,    nan, 0.4352,    nan,\n",
      "           nan, 0.4352, 0.4352,    nan, 0.4352,    nan,    nan, 0.4352, 0.4352,\n",
      "           nan, 0.4352, 0.4352,    nan, 0.4781,    nan,    nan, 0.5226,    nan,\n",
      "           nan, 0.3616, 0.1122,    nan,    nan, 0.4352, 0.4839, 0.5008,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([ 5.8129e-01,  5.8964e-01,  3.7078e-01,  6.0979e-01,  4.0652e-01,\n",
      "         6.6576e-01,  4.4872e-01,  3.7078e-01,  3.7078e-01,  6.0578e-01,\n",
      "         5.1687e-01,  6.2849e-01,  3.7078e-01,  3.7078e-01,  3.7078e-01,\n",
      "         5.6981e-01,  6.1069e-01,  5.7748e-01,  3.7078e-01,  5.8331e-01,\n",
      "         3.7078e-01,  8.8391e-02,  3.7078e-01,  3.7078e-01,  5.1687e-01,\n",
      "         3.7078e-01,  6.4900e-01,  3.7078e-01,  4.9997e-01,  6.5489e-01,\n",
      "         3.7078e-01,  3.7078e-01,  3.7078e-01,  6.5129e-01,  5.8276e-01,\n",
      "         5.2238e-01,  6.8862e-01,  5.1262e-01,  6.1470e-01,  3.7078e-01,\n",
      "         5.1687e-01,  6.4439e-01,  3.7078e-01,  4.5661e-01, -1.1921e-07,\n",
      "         3.7078e-01,  3.7078e-01,  3.7078e-01,  3.7078e-01,  3.7078e-01,\n",
      "         4.7532e-01,  3.7078e-01,  6.0084e-01,  6.1080e-01,  6.3862e-01,\n",
      "         3.7078e-01,  6.8811e-01,  3.8883e-01,  6.6686e-01,  3.7078e-01,\n",
      "         3.7078e-01,  4.6618e-01,  5.1687e-01,  3.7078e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4192,    nan, 0.4070,    nan, 0.5095,    nan,    nan, 0.5145, 0.3208,\n",
      "        0.4192, 0.4192, 0.4192, 0.4192, 0.4192,    nan, 0.3600, 0.4731, 0.4192,\n",
      "           nan, 0.4192, 0.4192,    nan,    nan,    nan,    nan, 0.4192, 0.4192,\n",
      "           nan,    nan,    nan,    nan, 0.4192, 0.4838,    nan,    nan,    nan,\n",
      "        0.4192,    nan, 0.2428, 0.4640,    nan, 0.4565,    nan,    nan, 0.4082,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan, 0.4182, 0.4592, 0.4192,\n",
      "           nan, 0.3914, 0.4192,    nan, 0.4074,    nan,    nan,    nan,    nan,\n",
      "        0.4192], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.4428,    nan,    nan,    nan,    nan, 0.4655,    nan, 0.2097,\n",
      "           nan, 0.4655, 0.4663, 0.4655, 0.4502, 0.4693, 0.4685,    nan, 0.4655,\n",
      "           nan, 0.4655,    nan,    nan, 0.4655, 0.4655,    nan,    nan, 0.3359,\n",
      "           nan,    nan, 0.4056, 0.4655, 0.4655,    nan,    nan, 0.4655,    nan,\n",
      "        0.2821,    nan, 0.4655,    nan, 0.4655, 0.4655,    nan,    nan, 0.4655,\n",
      "           nan,    nan, 0.5021, 0.1326,    nan, 0.5049, 0.4655, 0.3661,    nan,\n",
      "           nan, 0.1732,    nan,    nan,    nan,    nan,    nan, 0.4655, 0.4655,\n",
      "        0.4655], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.6718e-01, 2.2144e-01, 2.2144e-01, 2.2144e-01, 6.1052e-01, 1.1921e-07,\n",
      "        5.9510e-01, 2.2144e-01, 2.2144e-01, 5.0440e-01, 1.1921e-07, 1.1921e-07,\n",
      "        2.2144e-01, 7.6830e-01, 2.2144e-01, 2.2144e-01, 5.9515e-01, 2.2144e-01,\n",
      "        6.1090e-01, 7.0220e-01, 1.1921e-07, 6.5845e-01, 6.6969e-01, 4.8911e-01,\n",
      "        2.2144e-01, 8.5406e-01, 1.1921e-07, 5.4238e-01, 2.2144e-01, 2.2144e-01,\n",
      "        2.2144e-01, 2.2144e-01, 6.8808e-01, 3.2776e-01, 3.5123e-01, 7.3964e-01,\n",
      "        5.1015e-01, 2.2144e-01, 6.2232e-01, 5.3941e-01, 2.2144e-01, 2.6323e-01,\n",
      "        6.1226e-01, 5.0115e-01, 2.2144e-01, 6.5448e-01, 3.8824e-01, 3.3576e-01,\n",
      "        2.2144e-01, 2.2144e-01, 2.2144e-01, 2.2144e-01, 2.2144e-01, 5.8526e-01,\n",
      "        5.0738e-01, 1.1921e-07, 3.2396e-01, 2.2144e-01, 6.4676e-01, 2.3200e-01,\n",
      "        5.8964e-01, 2.2144e-01, 6.4224e-01, 2.2144e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3361, 0.3361, 0.3993,    nan,    nan,    nan, 0.3361, 0.3361, 0.3361,\n",
      "           nan,    nan, 0.3888,    nan, 0.3361, 0.3361, 0.3361, 0.3361, 0.2538,\n",
      "           nan, 0.3485, 0.3361, 0.3361,    nan,    nan,    nan,    nan, 0.3868,\n",
      "           nan,    nan, 0.3757,    nan,    nan, 0.3361, 0.3361, 0.3361,    nan,\n",
      "           nan,    nan, 0.3361,    nan,    nan, 0.2385, 0.3361,    nan, 0.3361,\n",
      "           nan, 0.3361,    nan,    nan, 0.4275, 0.3361, 0.4015,    nan, 0.3361,\n",
      "        0.3361, 0.3361,    nan,    nan, 0.3941, 0.3361,    nan,    nan, 0.4290,\n",
      "        0.3361], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([6.6448e-01, 6.6717e-01, 5.5668e-01, 7.3399e-01, 5.5282e-01, 6.7087e-01,\n",
      "        6.4249e-01, 5.9453e-01, 4.0264e-01, 4.4574e-01, 2.4692e-01, 6.6353e-01,\n",
      "        4.0264e-01, 6.9220e-01, 7.1365e-01, 7.2828e-01, 6.1061e-01, 4.0264e-01,\n",
      "        6.6685e-01, 4.0264e-01, 4.0264e-01, 6.9428e-01, 4.0264e-01, 4.0264e-01,\n",
      "        4.0264e-01, 6.9824e-01, 7.0139e-01, 4.0264e-01, 6.5046e-01, 4.0264e-01,\n",
      "        4.0264e-01, 4.0264e-01, 4.0264e-01, 4.3941e-01, 4.0264e-01, 6.3514e-01,\n",
      "        7.6696e-01, 4.0264e-01, 7.1598e-01, 7.0281e-01, 4.0900e-01, 4.0012e-01,\n",
      "        4.0264e-01, 7.6970e-01, 4.0264e-01, 6.5575e-01, 6.2542e-01, 6.6496e-01,\n",
      "        4.0264e-01, 6.3889e-01, 6.3991e-01, 1.4994e-01, 4.0264e-01, 4.0264e-01,\n",
      "        5.3873e-01, 6.3991e-01, 5.6992e-01, 4.0264e-01, 8.0046e-01, 8.0840e-01,\n",
      "        6.3628e-01, 5.9605e-08, 7.1970e-01, 4.0264e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3631,    nan,    nan, 0.3830, 0.3830, 0.3830, 0.3830, 0.3830, 0.3519,\n",
      "        0.4163, 0.4112, 0.3239, 0.3830,    nan, 0.3952,    nan, 0.3830,    nan,\n",
      "           nan, 0.3830, 0.3830,    nan, 0.2539,    nan, 0.3809, 0.3830, 0.4067,\n",
      "           nan, 0.4570,    nan,    nan,    nan, 0.3830, 0.4028, 0.3879, 0.3017,\n",
      "           nan, 0.3830,    nan,    nan, 0.3830, 0.3830, 0.3830,    nan,    nan,\n",
      "        0.4416,    nan,    nan,    nan, 0.3830, 0.3830,    nan,    nan, 0.4274,\n",
      "        0.3830, 0.4322, 0.3830,    nan, 0.3756, 0.3080, 0.3830, 0.3830, 0.3830,\n",
      "        0.3830], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3459,    nan, 0.2730, 0.3459, 0.3724, 0.3459, 0.3459,    nan,    nan,\n",
      "           nan,    nan, 0.3459, 0.3459, 0.3459,    nan, 0.3459, 0.3459, 0.3459,\n",
      "        0.3459,    nan, 0.3459, 0.4139,    nan,    nan, 0.3459, 0.3517, 0.4654,\n",
      "        0.3459,    nan, 0.3459, 0.3459,    nan,    nan,    nan,    nan, 0.2808,\n",
      "        0.3459,    nan,    nan,    nan, 0.3459, 0.3459,    nan,    nan,    nan,\n",
      "        0.3459, 0.2263, 0.3459,    nan, 0.4399,    nan,    nan,    nan, 0.2011,\n",
      "           nan, 0.3459, 0.2956,    nan,    nan, 0.3459,    nan,    nan, 0.3459,\n",
      "        0.4148], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.3738,    nan,    nan, 0.3105,    nan,    nan, 0.3738, 0.4424,\n",
      "           nan,    nan, 0.3382, 0.3738, 0.3765,    nan, 0.3738, 0.3738, 0.3738,\n",
      "           nan,    nan, 0.3738, 0.3610, 0.3738, 0.3738, 0.3738,    nan,    nan,\n",
      "        0.3738, 0.4871, 0.3997,    nan, 0.3738, 0.2661, 0.3738,    nan, 0.3738,\n",
      "           nan,    nan, 0.3962, 0.3738,    nan, 0.3738, 0.3738, 0.3918, 0.3738,\n",
      "           nan, 0.3738, 0.4286,    nan, 0.3738,    nan,    nan,    nan,    nan,\n",
      "        0.4406, 0.3738,    nan,    nan,    nan, 0.3738, 0.3738, 0.4375,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3639,    nan, 0.3639,    nan,    nan, 0.3963,    nan, 0.3639,    nan,\n",
      "           nan,    nan,    nan, 0.3639,    nan,    nan, 0.3639,    nan,    nan,\n",
      "        0.3602,    nan, 0.3639, 0.3276, 0.3721, 0.4036,    nan, 0.3639, 0.2210,\n",
      "           nan, 0.3639, 0.3639, 0.3639, 0.3639,    nan,    nan,    nan, 0.3639,\n",
      "           nan, 0.3639, 0.3639,    nan,    nan,    nan,    nan,    nan, 0.3639,\n",
      "        0.3639,    nan,    nan,    nan,    nan, 0.3639,    nan,    nan,    nan,\n",
      "           nan, 0.3639, 0.3639, 0.3639,    nan,    nan, 0.4107,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([ 4.1638e-01,  7.2288e-01,  6.6729e-01,  4.7393e-01,  4.1638e-01,\n",
      "         4.1638e-01,  4.1638e-01,  4.5509e-01,  6.0451e-01,  5.7226e-01,\n",
      "         4.1638e-01,  4.9176e-01,  6.0948e-01,  6.5107e-01,  6.8959e-01,\n",
      "         4.1638e-01,  4.1638e-01,  4.1638e-01,  4.1638e-01,  6.5901e-01,\n",
      "         4.1638e-01,  6.5901e-01,  3.5122e-01,  4.2215e-01,  4.1638e-01,\n",
      "         6.5901e-01,  6.2087e-01,  5.3866e-01,  2.6548e-01,  4.1638e-01,\n",
      "         4.1638e-01,  4.1638e-01,  5.4753e-01,  4.4769e-01,  7.8549e-01,\n",
      "         6.3466e-01,  4.3545e-01,  4.1638e-01,  6.5901e-01,  6.5901e-01,\n",
      "         4.1638e-01,  5.8975e-01,  4.1638e-01, -1.1921e-07,  5.8820e-01,\n",
      "         4.8197e-01,  6.5623e-01,  4.1638e-01,  4.1638e-01,  4.1638e-01,\n",
      "         4.1638e-01,  4.1638e-01,  6.0864e-01,  7.2306e-01,  4.1638e-01,\n",
      "         4.1638e-01,  4.5342e-01,  5.7371e-01,  4.1638e-01,  5.6034e-01,\n",
      "         3.3174e-01,  6.7429e-01,  6.4308e-01,  4.3888e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.2975, 0.4816, 0.2975, 0.2975, 0.5727, 0.2975, 0.1235, 0.2975, 0.3548,\n",
      "        0.5546, 0.4091, 0.6413, 0.3990, 0.2975, 0.4532, 0.6327, 0.5356, 0.3047,\n",
      "        0.1828, 0.5533, 0.2975, 0.6009, 0.2975, 0.2975, 0.5546, 0.2975, 0.6574,\n",
      "        0.5710, 0.4850, 0.2975, 0.5546, 0.2975, 0.2975, 0.2975, 0.5299, 0.6937,\n",
      "        0.2975, 0.6099, 0.5648, 0.4254, 0.2975, 0.3547, 0.2975, 0.2975, 0.6830,\n",
      "        0.2975, 0.6255, 0.3342, 0.5360, 0.6607, 0.4294, 0.3009, 0.4857, 0.5501,\n",
      "        0.6605, 0.4339, 0.3843, 0.2975, 0.3546, 0.3615, 0.5546, 0.3974, 0.2975,\n",
      "        0.4073], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.4669,    nan, 0.4669, 0.4669,    nan, 0.4669,    nan, 0.4669,\n",
      "        0.5081, 0.4669,    nan, 0.3908, 0.4669, 0.4669, 0.4550,    nan,    nan,\n",
      "           nan, 0.3423,    nan, 0.4669, 0.4669, 0.4669,    nan,    nan,    nan,\n",
      "        0.1964,    nan,    nan, 0.4669,    nan, 0.4669, 0.3758,    nan,    nan,\n",
      "        0.4669,    nan,    nan, 0.4669, 0.3207, 0.3482,    nan,    nan,    nan,\n",
      "        0.2951, 0.4669,    nan, 0.4669, 0.4669,    nan, 0.3688,    nan, 0.4514,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan, 0.4669, 0.2585, 0.3752,\n",
      "        0.4669], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.4512, 0.4121, 0.4686, 0.6410, 0.4651, 0.4512, 0.4512, 0.7091, 0.7820,\n",
      "        0.6265, 0.4909, 0.1905, 0.4512, 0.4736, 0.4512, 0.4909, 0.4909, 0.6620,\n",
      "        0.6664, 0.3858, 0.7361, 0.2210, 0.6142, 0.4512, 0.7493, 0.5022, 0.0000,\n",
      "        0.4002, 0.5689, 0.6776, 0.6275, 0.5914, 0.6936, 0.4512, 0.7470, 0.4512,\n",
      "        0.6172, 0.4909, 0.7293, 0.6590, 0.6500, 0.6985, 0.5734, 0.6084, 0.4587,\n",
      "        0.5643, 0.6228, 0.4512, 0.5268, 0.6599, 0.4512, 0.3434, 0.4070, 0.6834,\n",
      "        0.6711, 0.4512, 0.4909, 0.4841, 0.7417, 0.7240, 0.4512, 0.4909, 0.4101,\n",
      "        0.4518], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan, 0.2736,    nan, 0.2736, 0.2736, 0.2736,    nan,    nan,    nan,\n",
      "           nan, 0.2736, 0.4151,    nan,    nan, 0.4118, 0.2736,    nan,    nan,\n",
      "           nan,    nan,    nan, 0.2736, 0.2736,    nan,    nan, 0.2736, 0.1422,\n",
      "           nan, 0.3031, 0.2736, 0.2736, 0.2736, 0.2736, 0.2736,    nan,    nan,\n",
      "           nan,    nan,    nan, 0.2736, 0.2736, 0.2736, 0.2736,    nan,    nan,\n",
      "        0.2742, 0.2736,    nan, 0.4717, 0.4460,    nan,    nan, 0.3221, 0.2736,\n",
      "           nan,    nan, 0.4184,    nan,    nan, 0.2736, 0.2736,    nan,    nan,\n",
      "        0.2736], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([   nan,    nan,    nan,    nan, 0.2572,    nan,    nan,    nan,    nan,\n",
      "           nan, 0.2572, 0.3748,    nan, 0.3563,    nan,    nan,    nan, 0.2572,\n",
      "        0.2572,    nan, 0.2572,    nan,    nan, 0.2572,    nan,    nan,    nan,\n",
      "        0.2572,    nan,    nan,    nan, 0.2572,    nan, 0.2572, 0.2572, 0.3496,\n",
      "           nan,    nan,    nan, 0.3147,    nan, 0.2572, 0.2572, 0.2572,    nan,\n",
      "           nan,    nan, 0.2572, 0.2572, 0.3629, 0.1382, 0.2572,    nan, 0.2572,\n",
      "        0.2925, 0.3372, 0.2572, 0.2572,    nan, 0.3219,    nan,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n",
      "tensor([0.3627,    nan,    nan,    nan,    nan, 0.3691,    nan, 0.3627, 0.3627,\n",
      "           nan, 0.3627, 0.5541,    nan, 0.4217, 0.3627, 0.3627, 0.5097,    nan,\n",
      "        0.2453,    nan, 0.3627,    nan,    nan,    nan,    nan,    nan, 0.3627,\n",
      "           nan, 0.3627, 0.4594,    nan, 0.3371, 0.3627,    nan, 0.4581, 0.3627,\n",
      "           nan, 0.3627,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "        0.3627,    nan,    nan,    nan,    nan,    nan, 0.3627, 0.3627,    nan,\n",
      "        0.3627, 0.3627, 0.3627,    nan,    nan,    nan, 0.3627,    nan, 0.3627,\n",
      "        0.1656], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 21\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid_cosine_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCappedBCELoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     23\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network, embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:184\u001b[0m, in \u001b[0;36mtrain_sigmoid_cosine_distance\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m    181\u001b[0m loss_func \u001b[38;5;241m=\u001b[39m loss_fn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_fn_args)\n\u001b[1;32m    183\u001b[0m network\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target, smote_target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m    185\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    187\u001b[0m     output, embeds \u001b[38;5;241m=\u001b[39m network(data)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/collate.py:264\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/collate.py:137\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    135\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(batch)\n\u001b[1;32m    136\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43melem_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/collate.py:137\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    135\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(batch)\n\u001b[1;32m    136\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43melem_size\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# cosine distance + capped loss\n",
    "momentum=0\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_cosine_distance(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"cosine_distance_capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469130a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9aeaa6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011436396837234497, AUC: 0.457495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006499694585800171, AUC: 0.77029\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005454809069633484, AUC: 0.882447\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006668661534786224, AUC: 0.7796329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00553279709815979, AUC: 0.2802325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006955685317516327, AUC: 0.49733599999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947574317455291, AUC: 0.5028275000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007007570564746857, AUC: 0.47190100000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016392868161201478, AUC: 0.362875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936477720737457, AUC: 0.49204799999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933434009552002, AUC: 0.49849199999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933499574661255, AUC: 0.49849200000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008223848342895508, AUC: 0.448507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005438280999660492, AUC: 0.8846880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005088721215724945, AUC: 0.8861960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005425140261650085, AUC: 0.8621480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008242251873016357, AUC: 0.6408520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005863762497901917, AUC: 0.8693929999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005151261985301971, AUC: 0.886503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005646317899227143, AUC: 0.8619429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000918371707201004, AUC: 0.536765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005603332221508026, AUC: 0.8602960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006608726084232331, AUC: 0.837736\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005050783604383469, AUC: 0.875699\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013069388270378112, AUC: 0.58745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005683961510658264, AUC: 0.8694849999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006103499829769135, AUC: 0.8380230000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007082647681236267, AUC: 0.7530905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001208219826221466, AUC: 0.470757\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005546177923679351, AUC: 0.863394\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000596214234828949, AUC: 0.8557380000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006540561616420746, AUC: 0.8207960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008590931296348572, AUC: 0.6383909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005860588550567627, AUC: 0.8529585000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005332182049751281, AUC: 0.8880149999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006987205445766449, AUC: 0.7177669999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011240931153297423, AUC: 0.49242199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005560258328914643, AUC: 0.888319\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005225510895252227, AUC: 0.8890465000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005411522388458252, AUC: 0.8769510000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000697275072336197, AUC: 0.7227359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006373985111713409, AUC: 0.7645499999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000621173769235611, AUC: 0.8150454999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006171258687973023, AUC: 0.829359\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004994371175765991, AUC: 0.6705915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006146672368049622, AUC: 0.822621\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000609284371137619, AUC: 0.830651\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006072202026844025, AUC: 0.8336190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013795417547225952, AUC: 0.586665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006402479708194732, AUC: 0.7954559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006380465030670166, AUC: 0.7958960000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006269498467445374, AUC: 0.8147360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016411503553390504, AUC: 0.6515529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006239555776119232, AUC: 0.810651\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005957567989826202, AUC: 0.853213\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005886142253875733, AUC: 0.8597190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017172007560729982, AUC: 0.3578335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006405812203884125, AUC: 0.8028010000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006239972412586212, AUC: 0.823501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000628434032201767, AUC: 0.8239375000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012478209137916565, AUC: 0.5741499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006191721558570862, AUC: 0.8288380000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006093942224979401, AUC: 0.849626\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000607679545879364, AUC: 0.8512394999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006380985260009766, AUC: 0.450204\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005040823519229889, AUC: 0.846666\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005345031619071961, AUC: 0.865068\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047658854722976683, AUC: 0.8851659999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016212204694747924, AUC: 0.41917399999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006400464475154877, AUC: 0.8153769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006345863342285157, AUC: 0.8194180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006413449347019196, AUC: 0.8077480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018048741817474365, AUC: 0.34690400000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006347227692604065, AUC: 0.8001960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006283664405345917, AUC: 0.8193520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000617977499961853, AUC: 0.8361355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008693362474441528, AUC: 0.44686800000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934033930301667, AUC: 0.5120210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006928477883338929, AUC: 0.5410145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006818165183067322, AUC: 0.6360579999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001239137589931488, AUC: 0.5158640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006877581477165222, AUC: 0.5486685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006881211698055267, AUC: 0.5555985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000688005656003952, AUC: 0.5595035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00346296226978302, AUC: 0.6133879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006519587635993957, AUC: 0.678121\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000637356698513031, AUC: 0.7146969999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006149756908416748, AUC: 0.745204\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003393764615058899, AUC: 0.45321\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007013912200927734, AUC: 0.4305214999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006969274580478669, AUC: 0.454996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006955269575119018, AUC: 0.466129\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008302388489246368, AUC: 0.504591\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006839054822921753, AUC: 0.573278\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006715894341468811, AUC: 0.6486029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006649312973022461, AUC: 0.697271\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000990877866744995, AUC: 0.39449\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006610883772373199, AUC: 0.6777074999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006484582126140595, AUC: 0.731894\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006415409445762635, AUC: 0.7610500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014796666502952577, AUC: 0.4171665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000677569180727005, AUC: 0.640168\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006661913096904754, AUC: 0.7007775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006593095064163208, AUC: 0.745466\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009021728932857514, AUC: 0.396864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006808048784732819, AUC: 0.615858\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006685051321983338, AUC: 0.7009709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006598354578018188, AUC: 0.739743\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011793015599250794, AUC: 0.392324\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006502928435802459, AUC: 0.751671\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006450764834880829, AUC: 0.7694724999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006429816484451293, AUC: 0.777964\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006731678247451783, AUC: 0.33828250000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006582424938678742, AUC: 0.715208\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006497207880020141, AUC: 0.7513099999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006423119604587555, AUC: 0.7759429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011103346943855286, AUC: 0.5835480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006636098623275757, AUC: 0.673189\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000653807908296585, AUC: 0.7131069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006517095267772674, AUC: 0.737034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# capped loss with everything capped + cosine distance \n",
    "momentum=0\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_cosine_distance(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.AllCappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"cosine_distance_all_capped\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ecfcd00",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "18 columns passed, passed data had 13 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:934\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 934\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_or_indexify_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:981\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    982\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    983\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    984\u001b[0m     )\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 18 columns passed, passed data had 13 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/convnet_aucs.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m df2 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcol_names\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df1, df2])\n\u001b[1;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/convnet_aucs.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:781\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    780\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 781\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    783\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    789\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    790\u001b[0m         arrays,\n\u001b[1;32m    791\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    794\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    795\u001b[0m     )\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:498\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 498\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:840\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    837\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    838\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 840\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:937\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    934\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[0;32m--> 937\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[1;32m    940\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: 18 columns passed, passed data had 13 columns"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af0e21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0007788240313529968, AUC: 0.611374\n",
      "\n",
      "Train loss: 119.01280592049763\n",
      "Train loss: 28.864070757179505\n",
      "Train loss: 17.1626423350565\n",
      "Train loss: 11.034691510306802\n",
      "Train loss: 8.068073830407137\n",
      "Train loss: 7.819459002488737\n",
      "Train loss: 7.143890395665625\n",
      "Train loss: 4.397538065530692\n",
      "Train loss: 4.995529130765587\n",
      "Train loss: 4.556520942669765\n",
      "Train loss: 3.594089006162753\n",
      "Train loss: 3.3297106057974943\n",
      "Train loss: 3.086767008729801\n",
      "Train loss: 2.271824208034831\n",
      "Train loss: 2.06402232958253\n",
      "Train loss: 2.0588491676719327\n",
      "Train loss: 2.3586609469857187\n",
      "Train loss: 2.0566104920047104\n",
      "Train loss: 1.7273476567997295\n",
      "Train loss: 1.7904064803366448\n",
      "Train loss: 1.5368473404532026\n",
      "Train loss: 1.8114646699777834\n",
      "Train loss: 1.1535253866462951\n",
      "Train loss: 1.1687455194391263\n",
      "Train loss: 1.0392256022258928\n",
      "Train loss: 1.0467016427380265\n",
      "Train loss: 1.097411612701264\n",
      "Train loss: 1.0484806627604613\n",
      "Train loss: 0.860904399946237\n",
      "Train loss: 0.9343751817941666\n",
      "\n",
      "Test set: Avg. loss: 0.0006933589577674866, AUC: 0.5315245000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931788623332978, AUC: 0.534357\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006928621232509613, AUC: 0.5490830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000692558467388153, AUC: 0.5572345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006923274099826812, AUC: 0.562769\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918595731258393, AUC: 0.5710840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006914696097373962, AUC: 0.5767209999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000690951555967331, AUC: 0.5815545000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006898923814296722, AUC: 0.583597\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006886084377765656, AUC: 0.591024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006886312663555146, AUC: 0.5953540000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000688052237033844, AUC: 0.602558\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006860791444778442, AUC: 0.6160079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006842935383319854, AUC: 0.6326569999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006822106838226318, AUC: 0.6392904999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006809553205966949, AUC: 0.65848\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006759667694568634, AUC: 0.6683079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000672827959060669, AUC: 0.6838939999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006654919981956481, AUC: 0.7105930000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006647864580154419, AUC: 0.7231680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006542481482028961, AUC: 0.7389469999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006584959924221038, AUC: 0.7419860000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006475840806961059, AUC: 0.758392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000649071991443634, AUC: 0.7708870000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006465112566947937, AUC: 0.7788290000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006462502777576447, AUC: 0.7781899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006359208226203918, AUC: 0.800444\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006306960880756379, AUC: 0.8085060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006241579055786133, AUC: 0.82042\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006211816370487213, AUC: 0.830198\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006184732019901276, AUC: 0.8383665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006101435720920563, AUC: 0.846451\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006126167178153992, AUC: 0.8439475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006125844717025757, AUC: 0.8539359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006042585968971252, AUC: 0.8615649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005954967141151428, AUC: 0.8670665000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006055669486522675, AUC: 0.86002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005942278504371643, AUC: 0.8697089999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005881696939468384, AUC: 0.875851\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005872893035411835, AUC: 0.8779594999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005813581645488739, AUC: 0.8822525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005848484933376312, AUC: 0.8762854999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005838653445243836, AUC: 0.8753760000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005864203870296478, AUC: 0.879025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005780352056026459, AUC: 0.883809\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005749594271183014, AUC: 0.878796\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005675856471061706, AUC: 0.8875669999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005814393460750579, AUC: 0.882911\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000572780579328537, AUC: 0.8837959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005760051608085633, AUC: 0.8795120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010437620878219604, AUC: 0.5653349999999999\n",
      "\n",
      "Train loss: 173.07890681855997\n",
      "Train loss: 45.705975550754815\n",
      "Train loss: 33.58210138758277\n",
      "Train loss: 21.386418316015014\n",
      "Train loss: 14.513015402730103\n",
      "Train loss: 16.511107535878565\n",
      "Train loss: 10.093275197372314\n",
      "Train loss: 10.040638926682199\n",
      "Train loss: 5.960875015349904\n",
      "Train loss: 5.4903704812572265\n",
      "Train loss: 5.529600074336787\n",
      "Train loss: 6.013104697701278\n",
      "Train loss: 4.024494235302992\n",
      "Train loss: 4.645363086727774\n"
     ]
    }
   ],
   "source": [
    "# 2 class triplet loss no ratio \n",
    "# no smote \n",
    "\n",
    "# combine network - don't need to turn off grad for embeds \n",
    "# double check this \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(1e-7, 1e-4)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(2): \n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(complete_network.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss, embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "        for epoch in range(50):\n",
    "            _, _ = train.train_linear_probe(epoch, train_loader_reduced, complete_network, linear_optimizer, verbose=False)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"triplet_loss\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], \n",
    "           auc_mean[i][4], auc_variance[i][4],\n",
    "           auc_mean[i][5], auc_variance[i][5],\n",
    "           None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e4c1e779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2,3,1)\n",
    "plt.tight_layout()\n",
    "plt.imshow(example[0].reshape(32, 32, 3).int())\n",
    "plt.subplot(2,3,2)\n",
    "plt.imshow(example[5].reshape(32, 32, 3).int())\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8afa13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe55d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplet loss with ratio \n",
    "# need to make a new train loader if running this \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(1e-7, 1e-3)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(2):\n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(linear_probe.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss, complete_network.embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "            \n",
    "    #    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network)\n",
    "        for epoch in range(50):\n",
    "            _, _ = train.train_linear_probe(epoch, train_loader_reduced, complete_network.embed_network, complete_network.linear_probe, linear_optimizer, verbose=False)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"triplet_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], \n",
    "           auc_mean[i][4], auc_variance[i][4],\n",
    "           auc_mean[i][5], auc_variance[i][5],\n",
    "           None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a99dbdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0010404332876205445, AUC: 0.561752\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 22\u001b[0m     _, train_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_triplet_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_tripletloss_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(train_losses))))\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:143\u001b[0m, in \u001b[0;36mtrain_triplet_loss\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn_args)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (anchor_data, pos_data, neg_data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m    141\u001b[0m         train_loader):\n\u001b[1;32m    142\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 143\u001b[0m     _, anchor_embeds \u001b[38;5;241m=\u001b[39m network(anchor_data\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m    144\u001b[0m     _, pos_embeds \u001b[38;5;241m=\u001b[39m network(pos_data\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m    145\u001b[0m     _, neg_embeds \u001b[38;5;241m=\u001b[39m network(neg_data\u001b[38;5;241m.\u001b[39mfloat())\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# 2 class triplet loss w/ SMOTE and ratio \n",
    "# REDO THIS (match above code)\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(1e-7, 1e-3)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(2):\n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(linear_probe.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss_smote, complete_network.embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "        for epoch in range(50):\n",
    "            _, _ = train.train_linear_probe(epoch, train_loader_reduced, complete_network, linear_optimizer, loss_fn=loss_fns.CappedBCELoss, verbose=False)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"capped_smote_triplet_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], \n",
    "           auc_mean[i][4], auc_variance[i][4],\n",
    "           auc_mean[i][5], auc_variance[i][5],\n",
    "           None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de718f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc1dd64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES_REDUCED = 3\n",
    "nums = (0, 3, 1)\n",
    "ratio = (20, 2, 1)\n",
    "\n",
    "\n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums)\n",
    "targets = ratio_train_CIFAR10.labels \n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED)\n",
    "\n",
    "\n",
    "weight = 1. / class_count\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= max(class_count)\n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "800c1e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004159178098042806, AUC: 0.5\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 16\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     18\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_softmax(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/ml/train.py:41\u001b[0m, in \u001b[0;36mtrain_softmax\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze(), target\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     43\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/ml/models.py:19\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2_drop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)), \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m250\u001b[39m) \u001b[38;5;66;03m#(320)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_jit_internal.py:484\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3 class normal\n",
    "\n",
    "learning_rates = [1e-4, 1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 3, nums, (1, 1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6be4998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6452da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0028148902257283527, AUC: 0.5499715000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012899534304936728, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011812520821889241, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003198623657227, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012160149812698365, AUC: 0.496695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012602541049321493, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011723772684733072, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012640552123387655, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001084180474281311, AUC: 0.47396475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012727203766504925, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010816088120142618, AUC: 0.5045000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011214701334635417, AUC: 0.5035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026806603272755943, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012301311095555623, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001151394208272298, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107947838306427, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020958302021026613, AUC: 0.44825000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012716478904088338, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011440247694651285, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012925329605738322, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026947441101074217, AUC: 0.47152499999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011879764000574749, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011960159142812093, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012670242389043172, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017743062178293865, AUC: 0.50675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001074000636736552, AUC: 0.5057499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012203034957249958, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011130955616633098, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014793120384216308, AUC: 0.44375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012164912223815918, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010818771521250406, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011993260780970255, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00679119078318278, AUC: 0.48125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012626315355300903, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001144296924273173, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001217811703681946, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007157065073649088, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001255599578221639, AUC: 0.501499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011938397884368897, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010313888788223266, AUC: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class ratio\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d28e8f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1752b5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.003038533369700114, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010433460474014282, AUC: 0.574998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011814462741216024, AUC: 0.7483075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001038165827592214, AUC: 0.7227574999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007914267698923746, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010645556449890136, AUC: 0.37466975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001031708836555481, AUC: 0.6914997500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011311002572377523, AUC: 0.75428325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006719241301218669, AUC: 0.5075000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010695095856984456, AUC: 0.5922592499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010078495343526204, AUC: 0.66097125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009656443595886231, AUC: 0.68574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002060540755589803, AUC: 0.5615957500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010401540199915568, AUC: 0.3627755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009678711692492167, AUC: 0.6725415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009192776083946228, AUC: 0.675171\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00151268204053243, AUC: 0.5225000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010918419361114502, AUC: 0.40822674999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010981494585673014, AUC: 0.4126655000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010762500762939453, AUC: 0.684442\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0108524964650472, AUC: 0.499499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013274700244267782, AUC: 0.6495525000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001373440424601237, AUC: 0.6880609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882926384607951, AUC: 0.5351134999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004438136577606201, AUC: 0.50352525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012416810989379883, AUC: 0.5918582499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013266102472941081, AUC: 0.6148197500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019509809811909993, AUC: 0.6372085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005216264883677164, AUC: 0.51125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010727325280507406, AUC: 0.499001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010864347219467162, AUC: 0.4997515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010905816157658894, AUC: 0.49875600000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003298620859781901, AUC: 0.51497625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011997053623199463, AUC: 0.64312625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001198989232381185, AUC: 0.648871\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001623408595720927, AUC: 0.6290437499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013772049347559611, AUC: 0.4500199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006249984105427, AUC: 0.47525675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973472197850546, AUC: 0.46956325000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010804060300191245, AUC: 0.5838855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class oversampled \n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "423b94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c14c3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0020289310614267984, AUC: 0.4756965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001091553012530009, AUC: 0.48456375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010886570612589517, AUC: 0.4860155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010853137572606406, AUC: 0.5097382500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019787512222925823, AUC: 0.57222475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011532610654830932, AUC: 0.49576575000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145702044169108, AUC: 0.4937625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001140582799911499, AUC: 0.49350675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005084774017333984, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010802105665206909, AUC: 0.50175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010755322376887005, AUC: 0.502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010662730137507121, AUC: 0.5022505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009249690055847169, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010924884875615438, AUC: 0.50599775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882760683695474, AUC: 0.5044992500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010864359935124715, AUC: 0.50449775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008460322062174478, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003564596176148, AUC: 0.5434515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010977611939112346, AUC: 0.527579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001091849128405253, AUC: 0.502861\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037917500336964926, AUC: 0.50300075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001107357660929362, AUC: 0.50625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011036542654037475, AUC: 0.50575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008251905441284, AUC: 0.503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00853164831797282, AUC: 0.48180575000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011391439437866211, AUC: 0.4952675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011303770144780478, AUC: 0.49850675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011254301071166992, AUC: 0.49128400000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017762763341267904, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011278401215871175, AUC: 0.46372199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021624008814494, AUC: 0.42442375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011078615188598632, AUC: 0.43288249999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007273403485616049, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011379459698994954, AUC: 0.47927200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011329193909962972, AUC: 0.4857515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011250438690185546, AUC: 0.53704125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00682664426167806, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001064486066500346, AUC: 0.4997495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010623377164204915, AUC: 0.49874950000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010606383085250855, AUC: 0.49924975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class undersampled  \n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaec9e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c779b879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.00608756939570109, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010771948893864949, AUC: 0.5007499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001078349749247233, AUC: 0.49949999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010784133275349936, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00372534704208374, AUC: 0.50023525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010370986064275106, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010473136504491171, AUC: 0.49649774999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010565840800603231, AUC: 0.5224875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020539817810058592, AUC: 0.50625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010878965854644776, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087069312731425, AUC: 0.49999999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010845698912938435, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006659661134084066, AUC: 0.49401975000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010928993225097657, AUC: 0.53216375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010782272815704345, AUC: 0.54923675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010686718225479126, AUC: 0.5544079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009116797844568889, AUC: 0.48513425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010646411975224813, AUC: 0.497999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010692731142044067, AUC: 0.502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010714724858601888, AUC: 0.50150125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015166940053304037, AUC: 0.48724999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011384790738423664, AUC: 0.49775050000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011099223693211873, AUC: 0.51426275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013285338083902994, AUC: 0.59251075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018370418151219686, AUC: 0.4615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010755572319030762, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010769031047821046, AUC: 0.5002502499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010774298906326295, AUC: 0.49999925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003305099407831828, AUC: 0.503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010899808804194133, AUC: 0.49999999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010876678625742595, AUC: 0.5000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010835784276326497, AUC: 0.5007499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010739267667134603, AUC: 0.502463\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001126469651858012, AUC: 0.47123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011237830718358358, AUC: 0.49575225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011101373434066772, AUC: 0.468339\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01887862459818522, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010957230726877849, AUC: 0.4975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010916781425476074, AUC: 0.50075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010887458721796672, AUC: 0.5009999999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class weighted\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['weight'] = torch.from_numpy(weight).float()\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8692b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81115d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011305590867996215, AUC: 0.4987425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101138710975647, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011463310718536376, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011780770619710286, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038984591166178386, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011603130102157593, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011915287176767985, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012100194692611695, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006762343406677246, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145007332166036, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011761978069941203, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011957573493321736, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017103184858957927, AUC: 0.4808077500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011710822582244873, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012000585397084554, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012189826170603433, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003416938861211141, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011359240611394246, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011698009570439657, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011919792890548707, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030593673388163247, AUC: 0.475337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000983761727809906, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982903023560842, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982608477274577, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001518441875775655, AUC: 0.46025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010580919981002807, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103336493174235, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001135496457417806, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003099643548329671, AUC: 0.5017499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001077566663424174, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010170272588729858, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001027000625928243, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026799618403116864, AUC: 0.4685945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011022898356119791, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011425824165344238, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001169986605644226, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001287020762761434, AUC: 0.513392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011173804601033528, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011543748378753662, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001180996815363566, AUC: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class focal loss\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['reduction'] = 'mean'\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SoftmaxFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf8eaef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d48470b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004159178098042806, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010713002681732179, AUC: 0.49825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010870064496994018, AUC: 0.49875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010937161048253376, AUC: 0.4980015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001217594623565674, AUC: 0.5495857500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010883294343948364, AUC: 0.54002475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009588491717974345, AUC: 0.6908785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010892333984375, AUC: 0.6341857500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012967588504155477, AUC: 0.44993225000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010879942576090494, AUC: 0.49974900000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010939101775487264, AUC: 0.4984995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010963983138402302, AUC: 0.4980035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009516664822896322, AUC: 0.4979995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010874385436375935, AUC: 0.502996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010935383637746174, AUC: 0.50224575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010963813463846842, AUC: 0.50149875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008401273687680562, AUC: 0.473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001117892305056254, AUC: 0.50074975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110655943552653, AUC: 0.49974875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101777672767639, AUC: 0.49874875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018197454214096069, AUC: 0.642566\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010871400435765585, AUC: 0.502739\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010544216235478718, AUC: 0.7166375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009863032698631286, AUC: 0.7337182500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014161507288614908, AUC: 0.6221422499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001115713357925415, AUC: 0.49924975000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011057165066401164, AUC: 0.5000005000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011015222469965616, AUC: 0.50025025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00627077309290568, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003833611806233, AUC: 0.51025525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096467614173889, AUC: 0.5330164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010366939703623454, AUC: 0.669718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014923743406931558, AUC: 0.6397665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001040031870206197, AUC: 0.7275147499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010351263682047526, AUC: 0.6475892499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011037707328796387, AUC: 0.54751375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01015039348602295, AUC: 0.49\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010485761562983194, AUC: 0.66979875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009980746308962504, AUC: 0.723652\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009460805654525757, AUC: 0.700749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class SMOTE\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "717e9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bfe9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.005159725666046142, AUC: 0.47875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013107466697692871, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014245965083440144, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013205681641896565, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036626233259836835, AUC: 0.5045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012807045380274454, AUC: 0.55175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008384430607159932, AUC: 0.6527735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013193607727686565, AUC: 0.58771125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001882831374804179, AUC: 0.568451\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014629533290863037, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013556772470474244, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012844537496566773, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005783709685007731, AUC: 0.5153915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014496474663416544, AUC: 0.68025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013263502518335978, AUC: 0.72725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005901109576225281, AUC: 0.6950000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015779575983683267, AUC: 0.47575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017115784088770549, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001690352439880371, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016737443208694458, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024884181340535484, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016858530044555663, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016851926644643148, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016747065782546997, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014013916651407878, AUC: 0.4033884999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001159618854522705, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012145692507425944, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006874272624651591, AUC: 0.7312500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005693643728892008, AUC: 0.49025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016551088094711303, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016597933371861775, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001664273738861084, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004257233619689942, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017054282824198405, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001689716895421346, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016794553200403849, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009325793266296387, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016158297061920166, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018122962315877279, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001287239154179891, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011229380289713542, AUC: 0.4915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009665106336275737, AUC: 0.6430379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009809208512306213, AUC: 0.7527905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010630707343419392, AUC: 0.5388919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002801464796066284, AUC: 0.4945689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010990440448125204, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098304033279419, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097338914871216, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011395713488260904, AUC: 0.44925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010731459856033324, AUC: 0.5862499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009436763922373454, AUC: 0.63125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001167917807896932, AUC: 0.5477055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003414260149002075, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010740158557891845, AUC: 0.5370075000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010959016879399618, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010975220600763958, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005730849742889404, AUC: 0.50174975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098487933476766, AUC: 0.49950649999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005215644836425, AUC: 0.56338675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011249771118164063, AUC: 0.65596625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005885673840840658, AUC: 0.505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011182666222254436, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001144629160563151, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011247365872065227, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01593700949350993, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096144715944926, AUC: 0.4999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097902218500773, AUC: 0.49725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003881295522054, AUC: 0.4957484999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010915645360946656, AUC: 0.43125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099284609158834, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010995355049769084, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991565783818563, AUC: 0.5025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023588163852691652, AUC: 0.497\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002196073532104, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011001572211583456, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099966843922933, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003621602535247803, AUC: 0.5055245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010969852209091187, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988662242889405, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010969030062357584, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008199299812316894, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011139464775721231, AUC: 0.5786862499999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001256147066752116, AUC: 0.5299999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011905717055002848, AUC: 0.5347545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029200942516326906, AUC: 0.50149925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984888474146525, AUC: 0.49974999999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011014104684193928, AUC: 0.49749899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099079688390096, AUC: 0.4964985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013806982835133871, AUC: 0.5180030000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998592774073284, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011013609170913697, AUC: 0.49625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106422464052836, AUC: 0.49724299999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008860953013102213, AUC: 0.49525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010979174375534057, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005378564198811, AUC: 0.496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002195278803507, AUC: 0.49825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026838178634643554, AUC: 0.498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992279847462972, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998342831929524, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989136298497518, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00352239465713501, AUC: 0.4201495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992273886998494, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991250673929851, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096076528231303, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003739197254180908, AUC: 0.5009999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010914430618286133, AUC: 0.7057055000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009601728518803914, AUC: 0.67756775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001175796906153361, AUC: 0.5605000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007436930020650228, AUC: 0.49375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989174445470175, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984821716944378, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095758557319641, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011664613405863444, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973004500071208, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010976841449737548, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010974336862564088, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001008460839589437, AUC: 0.46063449999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010979483524958292, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099142074584961, AUC: 0.4995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class capped loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-2]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args, smote=True)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for i in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[i][0]\n",
    "    auc_variance = cap_aucs[i][1]\n",
    "    cap = caps[i]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", 3, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b3e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c453da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0967e450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
