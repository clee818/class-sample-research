{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c3c2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378c3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91dda12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "NUM_CLASSES_REDUCED = 2\n",
    "nums = (0, 1)\n",
    "ratio = (100, 1)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b57e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "            \"mean_40\", \"variance_40\",\n",
    "            \"mean_50\", \"variance_50\", \"cap\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c2a1ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor() ]))  \n",
    "\n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()]))\n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "    \n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums)\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13159c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000   50]\n"
     ]
    }
   ],
   "source": [
    "targets = ratio_train_CIFAR10.labels \n",
    "\n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "print(class_count)\n",
    "\n",
    "weight = 1. / class_count\n",
    "\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= class_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de0d4cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.999 \n",
    "\n",
    "exp = np.empty_like(targets)\n",
    "for i, count in enumerate(class_count):\n",
    "    exp[targets==i] = count\n",
    "effective_weights = (1 - beta) / ( 1 - (beta ** torch.from_numpy(exp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af8cd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2e8c09f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001968830943107605, AUC: 0.6361665000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006069920063018799, AUC: 0.8331379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005906819999217987, AUC: 0.849458\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 18\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     20\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:23\u001b[0m, in \u001b[0;36mtrain_sigmoid\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args, smote)\u001b[0m\n\u001b[1;32m     21\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     22\u001b[0m pred\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2 CLASS normal\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f1d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "\n",
    "df2.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d211694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001968830943107605, AUC: 0.6361665000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019515877962112426, AUC: 0.7094229999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008754172098666134, AUC: 0.743984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016984249353408814, AUC: 0.7375505000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009006779843775352, AUC: 0.7743800000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015235986113548278, AUC: 0.7784579999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009531024183229644, AUC: 0.8180480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031965354681015015, AUC: 0.4105075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026761832237243654, AUC: 0.610581\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009113745841354427, AUC: 0.636804\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002043037295341492, AUC: 0.6594949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008553141438503667, AUC: 0.71688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001632628858089447, AUC: 0.6885979999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009908383394969572, AUC: 0.7618640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026044973134994507, AUC: 0.512846\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002328364133834839, AUC: 0.6704889999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS ratio\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "learning_rate_train_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                _, train_auc = metric_utils.auc_sigmoid(train_loader_ratio, network) \n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2d0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f1ccb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0009614106118679046, AUC: 0.614011\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006774698793888092, AUC: 0.6585589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006662350296974182, AUC: 0.713206\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006555393636226654, AUC: 0.7449055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006931622505187988, AUC: 0.532071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006511834263801575, AUC: 0.705865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006510878801345825, AUC: 0.7048525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006323479413986206, AUC: 0.74831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012074116468429566, AUC: 0.45304300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006792399287223815, AUC: 0.6182110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006664896011352539, AUC: 0.667305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006620110869407654, AUC: 0.6808749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005842076301574707, AUC: 0.35606649999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006702626347541809, AUC: 0.6863610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006543796062469482, AUC: 0.7483445000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006360483169555665, AUC: 0.7825044999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009045130610466003, AUC: 0.455444\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006217218935489655, AUC: 0.8241620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005998185575008392, AUC: 0.8498125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005988564193248749, AUC: 0.8472009999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007873946130275727, AUC: 0.505924\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006304432153701782, AUC: 0.745096\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005960119962692261, AUC: 0.790896\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005686694979667664, AUC: 0.8086879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018394469618797302, AUC: 0.342069\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006031354665756225, AUC: 0.780868\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005784178674221039, AUC: 0.804662\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005471040308475494, AUC: 0.84365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026135700941085816, AUC: 0.615929\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988285779953002, AUC: 0.508404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006910592019557952, AUC: 0.566112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006852890253067017, AUC: 0.5973975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002733421802520752, AUC: 0.42795249999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006736321449279785, AUC: 0.6940394999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000640917718410492, AUC: 0.8093619999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000628568172454834, AUC: 0.827172\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009439133703708648, AUC: 0.654072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000649003803730011, AUC: 0.7196885000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006312101185321808, AUC: 0.768405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006372729241847991, AUC: 0.7558340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007115027904510498, AUC: 0.616657\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006401776671409607, AUC: 0.720353\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006331292688846589, AUC: 0.763507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006281391382217407, AUC: 0.7875584999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019356902241706847, AUC: 0.568219\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006451549232006073, AUC: 0.6998120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006452490091323853, AUC: 0.7160934999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006427935063838959, AUC: 0.72821\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001419282615184784, AUC: 0.40512800000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695315808057785, AUC: 0.48694499999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948222219944001, AUC: 0.4929665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006942886710166931, AUC: 0.5003135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008247624337673188, AUC: 0.525384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006887524425983429, AUC: 0.5578150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006850159466266632, AUC: 0.569787\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006808511018753052, AUC: 0.5959215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004765978336334228, AUC: 0.348692\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006693117916584015, AUC: 0.6508544999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006629041135311126, AUC: 0.6958869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006600531339645385, AUC: 0.7041590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007654413282871246, AUC: 0.621819\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006669045090675354, AUC: 0.6428965000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006625311076641083, AUC: 0.6809704999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006586202681064606, AUC: 0.6952625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034120362997055054, AUC: 0.388479\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006689565479755401, AUC: 0.716342\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006688528954982757, AUC: 0.716347\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006670557856559753, AUC: 0.7320850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005057451725006104, AUC: 0.592154\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006919226050376892, AUC: 0.5435785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894824206829071, AUC: 0.5413675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000687130481004715, AUC: 0.563974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003564469575881958, AUC: 0.574443\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006921630799770355, AUC: 0.572517\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918762624263763, AUC: 0.563254\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006903278231620789, AUC: 0.577599\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012052770853042602, AUC: 0.355636\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006900358200073242, AUC: 0.537617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006885618567466736, AUC: 0.554183\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006862815916538239, AUC: 0.570273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS oversampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52d8c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1541b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.002664048075675964, AUC: 0.35103100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008118733763694763, AUC: 0.39093300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007359330058097839, AUC: 0.42988400000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007080410122871399, AUC: 0.476849\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005757878303527832, AUC: 0.4101775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008514148592948913, AUC: 0.663575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006793743073940277, AUC: 0.6388240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006851064562797546, AUC: 0.599984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001174522042274475, AUC: 0.498127\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009026037454605102, AUC: 0.569188\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007478025853633881, AUC: 0.5202549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007039127349853516, AUC: 0.5297069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009038377702236175, AUC: 0.47197\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007971824407577515, AUC: 0.48862799999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007626304626464844, AUC: 0.494027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007488877177238464, AUC: 0.4992150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034414209127426147, AUC: 0.5659004999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008925645649433136, AUC: 0.602172\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007607427835464478, AUC: 0.5276164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007198239862918854, AUC: 0.509627\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013543962240219116, AUC: 0.49844299999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013344860672950744, AUC: 0.5426559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008173213005065918, AUC: 0.520809\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007598233222961425, AUC: 0.41432599999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009839903712272643, AUC: 0.54916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008945295512676239, AUC: 0.46157500000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007561358511447906, AUC: 0.477264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007167462408542633, AUC: 0.498566\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001238984763622284, AUC: 0.47820850000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009365684688091278, AUC: 0.374864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008359749019145966, AUC: 0.3937505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007830325365066528, AUC: 0.417717\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008678676128387451, AUC: 0.6484215000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009037670493125915, AUC: 0.427843\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007472198605537415, AUC: 0.4185105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007187381088733673, AUC: 0.446337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007866494059562683, AUC: 0.5668679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007531993091106415, AUC: 0.488706\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007111433148384094, AUC: 0.49999199999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007000260055065155, AUC: 0.515335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038360543251037597, AUC: 0.294262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009016560912132263, AUC: 0.505367\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088973104953766, AUC: 0.5929039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011563763022422791, AUC: 0.606266\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006004724025726318, AUC: 0.478491\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0041501855850219725, AUC: 0.427829\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030867953300476076, AUC: 0.404122\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026625494956970214, AUC: 0.434574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017807592153549195, AUC: 0.5781095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011875814199447633, AUC: 0.6403829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009512988924980164, AUC: 0.6518860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008521502912044526, AUC: 0.6516249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007921732366085052, AUC: 0.615845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008575631380081177, AUC: 0.646551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009552640914916992, AUC: 0.646782\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008757838904857636, AUC: 0.6430809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012470539212226868, AUC: 0.417589\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001292936086654663, AUC: 0.45840000000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013544394373893739, AUC: 0.473034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001133813738822937, AUC: 0.46033199999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010617620944976807, AUC: 0.63819\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010061159133911133, AUC: 0.634423\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009557005167007447, AUC: 0.6285029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009136235415935516, AUC: 0.61536\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018809281587600709, AUC: 0.6420239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001633428931236267, AUC: 0.638927\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014178273677825928, AUC: 0.634879\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012286875247955321, AUC: 0.6292800000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016851001381874085, AUC: 0.338503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010225446820259095, AUC: 0.45910999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011303470730781556, AUC: 0.5157820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001235684871673584, AUC: 0.543991\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002234177231788635, AUC: 0.47880599999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001637177050113678, AUC: 0.588411\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001996301531791687, AUC: 0.6044080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018329355120658875, AUC: 0.602075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010110423564910888, AUC: 0.42448849999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008554110527038574, AUC: 0.44827599999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008604062795639038, AUC: 0.453961\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008695433139801025, AUC: 0.447998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS undersampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3dafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Class Weighted Loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['pos_weight'] = torch.tensor([weight[1]])\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                _, train_auc = metric_utils.auc_sigmoid(train_loader_ratio, network) \n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9eed2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7c5a840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001968830943107605, AUC: 0.6361665000000001\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 18\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     20\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:20\u001b[0m, in \u001b[0;36mtrain_sigmoid\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     19\u001b[0m pred\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2 CLASS SMOTE\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edcf4753",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfecae8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0015577556490898132, AUC: 0.6859669999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009647067785263061, AUC: 0.8755560000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006530835032463074, AUC: 0.9021619999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007020677328109741, AUC: 0.8893929999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001078621029853821, AUC: 0.423891\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004322097599506378, AUC: 0.889239\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00042063261568546295, AUC: 0.89573\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000940508782863617, AUC: 0.898558\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003058913707733154, AUC: 0.49637050000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008342242240905761, AUC: 0.853572\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010090836882591249, AUC: 0.8785789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001222649872303009, AUC: 0.8876470000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003471907377243042, AUC: 0.5334\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016670249700546265, AUC: 0.735833\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005569218099117279, AUC: 0.8988700000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005716230869293213, AUC: 0.8989860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001007843554019928, AUC: 0.41620199999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008319510221481323, AUC: 0.814078\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006585182547569274, AUC: 0.860356\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047353608906269075, AUC: 0.876686\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003190382719039917, AUC: 0.404412\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002070379853248596, AUC: 0.6725970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012076061367988587, AUC: 0.7857200000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009286877512931824, AUC: 0.8546059999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021151161193847657, AUC: 0.334554\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012394225597381593, AUC: 0.8417979999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005453386008739471, AUC: 0.89287\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006903530359268188, AUC: 0.90062\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008865833878517151, AUC: 0.41800500000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006164657473564148, AUC: 0.8669320000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009107994437217712, AUC: 0.881153\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011807937026023865, AUC: 0.8624399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011868048906326293, AUC: 0.356672\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00091376331448555, AUC: 0.847073\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010732965469360351, AUC: 0.879526\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018099507689476014, AUC: 0.8728859999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012514035701751709, AUC: 0.439508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008328326344490051, AUC: 0.882566\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015140560269355774, AUC: 0.8753089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009877214431762696, AUC: 0.894541\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011413527727127076, AUC: 0.6315124999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035804574489593506, AUC: 0.568617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029649006128311156, AUC: 0.586306\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002529516577720642, AUC: 0.615621\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013589245080947877, AUC: 0.474161\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001144920289516449, AUC: 0.7159230000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008014180958271027, AUC: 0.795016\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007100468873977661, AUC: 0.8399819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035734829902648924, AUC: 0.6036809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002198578119277954, AUC: 0.538011\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016082979440689086, AUC: 0.7034549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000986818879842758, AUC: 0.8020149999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004032845735549927, AUC: 0.3348505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004012512683868408, AUC: 0.6628179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002972670555114746, AUC: 0.697306\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016445997953414917, AUC: 0.760527\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003503327250480652, AUC: 0.484619\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002988968014717102, AUC: 0.404813\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025716156959533693, AUC: 0.44519000000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023436665534973143, AUC: 0.501178\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004478086948394775, AUC: 0.40541449999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007189408242702485, AUC: 0.786062\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008392520546913147, AUC: 0.836071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006037012338638305, AUC: 0.855067\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001189342200756073, AUC: 0.496794\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003217897891998291, AUC: 0.5085679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028045082092285154, AUC: 0.577671\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002605196714401245, AUC: 0.6701539999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006472096681594848, AUC: 0.32893799999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038747143745422362, AUC: 0.513118\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003035569787025452, AUC: 0.6064309999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002406261920928955, AUC: 0.703082\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002253875494003296, AUC: 0.5512585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021443742513656615, AUC: 0.575905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018128196001052856, AUC: 0.6891010000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011072131395339967, AUC: 0.7985500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009851268231868744, AUC: 0.673515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024889100790023806, AUC: 0.7320939999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015389268398284912, AUC: 0.797103\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007151846587657929, AUC: 0.8552329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012232980728149414, AUC: 0.38130699999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932936906814575, AUC: 0.5024680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931636929512024, AUC: 0.503972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933614313602447, AUC: 0.494477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002575706720352173, AUC: 0.45021100000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000590549647808075, AUC: 0.838054\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005855510234832763, AUC: 0.8518629999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007388118207454681, AUC: 0.6627200000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008891800045967102, AUC: 0.576611\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918771564960479, AUC: 0.53842\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006021211147308349, AUC: 0.8386779999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005898083746433258, AUC: 0.845869\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001015406608581543, AUC: 0.5929715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006083130538463592, AUC: 0.854876\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005518920123577118, AUC: 0.883199\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005424451828002929, AUC: 0.8807719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011386347413063049, AUC: 0.5147410000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005121912956237793, AUC: 0.8788469999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006703178584575653, AUC: 0.891409\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005594528615474701, AUC: 0.9016940000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005828597545623779, AUC: 0.60083\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005994209349155426, AUC: 0.8394055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006970702409744262, AUC: 0.6532500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005835976898670197, AUC: 0.8632779999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015295283198356628, AUC: 0.518076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006851083934307098, AUC: 0.6648989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005357208847999573, AUC: 0.8898599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006227370202541351, AUC: 0.8462064999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012791203856468202, AUC: 0.459044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005652037560939789, AUC: 0.8626980000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006188305914402008, AUC: 0.8431595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005116910338401795, AUC: 0.8750204999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007002180099487305, AUC: 0.633983\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935342848300934, AUC: 0.496021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935157775878906, AUC: 0.494524\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932168304920196, AUC: 0.4960264999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012660405039787292, AUC: 0.550327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005397006869316101, AUC: 0.888713\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005440060198307037, AUC: 0.8816125000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007174740135669709, AUC: 0.743346\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0049645454883575435, AUC: 0.4959484999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006323599219322204, AUC: 0.8064160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000619163453578949, AUC: 0.8273790000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006166659295558929, AUC: 0.8332015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028367594480514526, AUC: 0.41610199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006118935644626618, AUC: 0.8126499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000599944531917572, AUC: 0.845916\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.00057944455742836, AUC: 0.8650279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019157962203025817, AUC: 0.5303475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006247777640819549, AUC: 0.8047059999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000615023136138916, AUC: 0.8285035000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006052527129650116, AUC: 0.8387680000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031466708183288572, AUC: 0.416586\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005910550951957703, AUC: 0.844524\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005934758484363556, AUC: 0.849567\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005860161185264587, AUC: 0.85896\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006763181924819946, AUC: 0.6645800000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936165690422058, AUC: 0.4969805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934365332126617, AUC: 0.49798800000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933400630950928, AUC: 0.4970085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011637805938720703, AUC: 0.4944835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006524167358875275, AUC: 0.770158\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006474798321723938, AUC: 0.7842489999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006360141932964325, AUC: 0.8120799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00745493483543396, AUC: 0.35868999999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006264488995075225, AUC: 0.8428490000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006152088344097138, AUC: 0.85153\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006182723939418792, AUC: 0.850713\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016313820481300353, AUC: 0.4962895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006403082311153412, AUC: 0.764607\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006243041455745697, AUC: 0.8036374999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006402096152305603, AUC: 0.7893115000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019854989647865294, AUC: 0.32961149999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006478355526924134, AUC: 0.7550865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006495831608772278, AUC: 0.7614640000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006437378525733948, AUC: 0.7783140000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015569605231285095, AUC: 0.39230849999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006393412351608276, AUC: 0.7890510000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006261047720909119, AUC: 0.8155070000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006097853183746338, AUC: 0.8405075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0067674369812011715, AUC: 0.3121755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006237913966178894, AUC: 0.8324174999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005466857254505157, AUC: 0.8728100000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005850139856338501, AUC: 0.8651845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003175429940223694, AUC: 0.651276\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006203717291355133, AUC: 0.836422\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000617944598197937, AUC: 0.842571\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005422655045986176, AUC: 0.888184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009815248847007752, AUC: 0.509638\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006378219723701477, AUC: 0.8113780000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005675278306007385, AUC: 0.868923\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000608144223690033, AUC: 0.8548815000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016124539971351624, AUC: 0.472791\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937147676944733, AUC: 0.486511\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945774853229522, AUC: 0.4739685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006955031156539917, AUC: 0.47513900000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010999657154083253, AUC: 0.44061399999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006331462562084198, AUC: 0.818613\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006038576066493988, AUC: 0.851384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000558431088924408, AUC: 0.879537\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007545094013214112, AUC: 0.351304\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006430015563964844, AUC: 0.7943910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005432420372962952, AUC: 0.8796899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005233998000621796, AUC: 0.889997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015674200654029846, AUC: 0.60815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000543798178434372, AUC: 0.874436\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000532567709684372, AUC: 0.899207\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005278523862361908, AUC: 0.8931970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010190460681915284, AUC: 0.476461\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933368146419525, AUC: 0.505887\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005659222602844238, AUC: 0.870477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005475988388061523, AUC: 0.8887619999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008360215127468109, AUC: 0.5123645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005870511829853058, AUC: 0.8568145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005387389063835144, AUC: 0.8722825000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005150319933891296, AUC: 0.884195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004240076065063477, AUC: 0.3624515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006056763529777527, AUC: 0.841444\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006065668463706971, AUC: 0.8466850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006328726708889008, AUC: 0.8351580000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015854061245918274, AUC: 0.37546199999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006237476170063019, AUC: 0.804635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00061891371011734, AUC: 0.819133\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005998617112636566, AUC: 0.8407735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037502180337905884, AUC: 0.5655299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006566817164421081, AUC: 0.732617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006469261050224305, AUC: 0.767946\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006348161101341248, AUC: 0.800382\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011214820742607118, AUC: 0.426454\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006575458645820618, AUC: 0.7378945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006386039853096008, AUC: 0.8149230000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006372283697128296, AUC: 0.8076994999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008666054904460908, AUC: 0.393051\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006328173875808716, AUC: 0.7902925000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006155970692634582, AUC: 0.831155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006054994761943817, AUC: 0.8473335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008666499137878418, AUC: 0.659192\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006296337842941284, AUC: 0.7749940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006134569644927979, AUC: 0.8111535000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006042028963565827, AUC: 0.8342320000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00250269615650177, AUC: 0.446988\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006956129670143128, AUC: 0.48960400000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006953861117362976, AUC: 0.49306550000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951998770236969, AUC: 0.4959165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016367467641830444, AUC: 0.525508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006271940767765045, AUC: 0.7885975000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006094023287296296, AUC: 0.822315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006035985350608825, AUC: 0.8367640000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015032135844230653, AUC: 0.644944\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006348731815814972, AUC: 0.7931290000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006122832596302032, AUC: 0.826996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006020835340023041, AUC: 0.8437439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014808319807052613, AUC: 0.6185780000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006368625462055207, AUC: 0.8154315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006223648190498353, AUC: 0.8248555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006136631667613983, AUC: 0.8347745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013773387670516968, AUC: 0.562396\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006317138373851776, AUC: 0.80121\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006254929900169373, AUC: 0.8255340000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006229102611541748, AUC: 0.831472\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cap_auc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(cap_aucs)):\n\u001b[1;32m     46\u001b[0m     auc_mean \u001b[38;5;241m=\u001b[39m cap_aucs[i][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 47\u001b[0m     auc_variance \u001b[38;5;241m=\u001b[39m \u001b[43mcap_auc\u001b[49m[i][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     48\u001b[0m     cap \u001b[38;5;241m=\u001b[39m caps[i]\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(learning_rates)): \n",
      "\u001b[0;31mNameError\u001b[0m: name 'cap_auc' is not defined"
     ]
    }
   ],
   "source": [
    "# 2 Class Capped SMOTE \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_sigmoid(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args, smote=True)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for i in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[i][0]\n",
    "    auc_variance = cap_aucs[i][1]\n",
    "    cap = caps[i]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "606da0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = (col_names[0:13].append(col_names[17])))\n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "563f22d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001035837471485138, AUC: 0.44774200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927213966846466, AUC: 0.5254145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931669712066651, AUC: 0.511691\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935113668441773, AUC: 0.5110125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005613439321517945, AUC: 0.426767\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037078361511230467, AUC: 0.5171749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026477036476135253, AUC: 0.46210599999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002063918948173523, AUC: 0.44280450000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001599363923072815, AUC: 0.5595060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017810710668563843, AUC: 0.527251\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017536671757698058, AUC: 0.530268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001468895673751831, AUC: 0.511718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009869316220283508, AUC: 0.521323\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025733143091201784, AUC: 0.44177999999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002441696286201477, AUC: 0.42422899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00228461492061615, AUC: 0.419806\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005625223875045777, AUC: 0.640735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00567081618309021, AUC: 0.490035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003996514081954956, AUC: 0.400708\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033221423625946047, AUC: 0.379003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013055719137191773, AUC: 0.359221\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002855167269706726, AUC: 0.3432545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002768984794616699, AUC: 0.369273\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002534526348114014, AUC: 0.363009\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007071554660797119, AUC: 0.6680550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020034250020980835, AUC: 0.45828099999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022772670984268187, AUC: 0.48882499999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00230745005607605, AUC: 0.509682\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009378761649131774, AUC: 0.5550744999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033345311880111695, AUC: 0.523733\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003033424973487854, AUC: 0.505011\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026757409572601316, AUC: 0.501909\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027659571170806883, AUC: 0.402637\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015370473265647888, AUC: 0.5538240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015259616374969482, AUC: 0.5430379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014122150540351868, AUC: 0.5184869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007173371315002441, AUC: 0.297585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001956529438495636, AUC: 0.5169174999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001981451392173767, AUC: 0.533161\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019487438797950744, AUC: 0.536132\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000798177808523178, AUC: 0.7362540000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006655452251434326, AUC: 0.7207349999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007001525759696961, AUC: 0.733501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007603182792663574, AUC: 0.742639\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002295616149902344, AUC: 0.376647\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009011189937591553, AUC: 0.5605964999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011427636742591858, AUC: 0.5951460000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013215634822845458, AUC: 0.605137\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00221078360080719, AUC: 0.5965469999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010793698787689208, AUC: 0.642539\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010493645191192627, AUC: 0.628214\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009857546329498292, AUC: 0.613483\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006628040313720703, AUC: 0.6315075000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008294330596923829, AUC: 0.650957\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007958059310913085, AUC: 0.6476215000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007488812685012817, AUC: 0.64287\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002516766667366028, AUC: 0.43171800000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007630343437194825, AUC: 0.431629\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007703002095222473, AUC: 0.411333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007906514704227448, AUC: 0.45406500000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002265890836715698, AUC: 0.608756\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004604844093322754, AUC: 0.649751\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038692718744277954, AUC: 0.643426\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032494683265686035, AUC: 0.635331\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015813257098197936, AUC: 0.553272\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00550286054611206, AUC: 0.640204\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0053946242332458495, AUC: 0.631832\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005209399700164795, AUC: 0.626354\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032208967208862305, AUC: 0.590365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005893768787384034, AUC: 0.47718400000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0058131103515625, AUC: 0.455178\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005506749391555786, AUC: 0.440446\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024443947076797485, AUC: 0.535904\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003024880290031433, AUC: 0.495147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002925659418106079, AUC: 0.495309\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002817286252975464, AUC: 0.48984750000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004183521747589111, AUC: 0.566368\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004094211220741272, AUC: 0.52125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037260520458221435, AUC: 0.474804\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033895738124847413, AUC: 0.42854600000000004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS Focal Loss\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['reduction'] = 'mean'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SigmoidFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d438d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5f6eb22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf2\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7924e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance + capped loss\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 1e-4]\n",
    "\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args, smote=True)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3],\n",
    "            auc_mean[i][4], auc_variance[i][4],\n",
    "            auc_mean[i][5], auc_variance[i][5]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a9c9d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>num_classes</th>\n",
       "      <th>classes_used</th>\n",
       "      <th>ratio</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>mean_0</th>\n",
       "      <th>variance_0</th>\n",
       "      <th>mean_10</th>\n",
       "      <th>variance_10</th>\n",
       "      <th>mean_20</th>\n",
       "      <th>variance_20</th>\n",
       "      <th>mean_30</th>\n",
       "      <th>variance_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>focal_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(100, 1)</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.487865</td>\n",
       "      <td>0.013197</td>\n",
       "      <td>0.489767</td>\n",
       "      <td>0.003418</td>\n",
       "      <td>0.476831</td>\n",
       "      <td>0.003291</td>\n",
       "      <td>0.469356</td>\n",
       "      <td>0.003572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>focal_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(100, 1)</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.562734</td>\n",
       "      <td>0.009148</td>\n",
       "      <td>0.578999</td>\n",
       "      <td>0.008085</td>\n",
       "      <td>0.571636</td>\n",
       "      <td>0.009897</td>\n",
       "      <td>0.567872</td>\n",
       "      <td>0.010230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name  num_classes classes_used     ratio  learning_rate    mean_0   \n",
       "0  focal_loss            2       (0, 1)  (100, 1)        0.00010  0.487865  \\\n",
       "1  focal_loss            2       (0, 1)  (100, 1)        0.00001  0.562734   \n",
       "\n",
       "   variance_0   mean_10  variance_10   mean_20  variance_20   mean_30   \n",
       "0    0.013197  0.489767     0.003418  0.476831     0.003291  0.469356  \\\n",
       "1    0.009148  0.578999     0.008085  0.571636     0.009897  0.567872   \n",
       "\n",
       "   variance_30  \n",
       "0     0.003572  \n",
       "1     0.010230  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc1dd64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES_REDUCED = 3\n",
    "nums = (0, 3, 1)\n",
    "ratio = (20, 2, 1)\n",
    "\n",
    "\n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums)\n",
    "targets = ratio_train_CIFAR10.labels \n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED)\n",
    "\n",
    "\n",
    "weight = 1. / class_count\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= max(class_count)\n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "800c1e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004159178098042806, AUC: 0.5\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 16\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     18\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_softmax(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/ml/train.py:41\u001b[0m, in \u001b[0;36mtrain_softmax\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze(), target\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     43\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/ml/models.py:19\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2_drop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)), \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m250\u001b[39m) \u001b[38;5;66;03m#(320)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_jit_internal.py:484\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3 class normal\n",
    "\n",
    "learning_rates = [1e-4, 1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 3, nums, (1, 1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6be4998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6452da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0028148902257283527, AUC: 0.5499715000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012899534304936728, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011812520821889241, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003198623657227, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012160149812698365, AUC: 0.496695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012602541049321493, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011723772684733072, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012640552123387655, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001084180474281311, AUC: 0.47396475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012727203766504925, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010816088120142618, AUC: 0.5045000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011214701334635417, AUC: 0.5035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026806603272755943, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012301311095555623, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001151394208272298, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107947838306427, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020958302021026613, AUC: 0.44825000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012716478904088338, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011440247694651285, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012925329605738322, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026947441101074217, AUC: 0.47152499999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011879764000574749, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011960159142812093, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012670242389043172, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017743062178293865, AUC: 0.50675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001074000636736552, AUC: 0.5057499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012203034957249958, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011130955616633098, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014793120384216308, AUC: 0.44375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012164912223815918, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010818771521250406, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011993260780970255, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00679119078318278, AUC: 0.48125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012626315355300903, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001144296924273173, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001217811703681946, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007157065073649088, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001255599578221639, AUC: 0.501499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011938397884368897, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010313888788223266, AUC: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class ratio\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d28e8f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1752b5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.003038533369700114, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010433460474014282, AUC: 0.574998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011814462741216024, AUC: 0.7483075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001038165827592214, AUC: 0.7227574999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007914267698923746, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010645556449890136, AUC: 0.37466975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001031708836555481, AUC: 0.6914997500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011311002572377523, AUC: 0.75428325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006719241301218669, AUC: 0.5075000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010695095856984456, AUC: 0.5922592499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010078495343526204, AUC: 0.66097125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009656443595886231, AUC: 0.68574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002060540755589803, AUC: 0.5615957500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010401540199915568, AUC: 0.3627755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009678711692492167, AUC: 0.6725415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009192776083946228, AUC: 0.675171\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00151268204053243, AUC: 0.5225000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010918419361114502, AUC: 0.40822674999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010981494585673014, AUC: 0.4126655000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010762500762939453, AUC: 0.684442\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0108524964650472, AUC: 0.499499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013274700244267782, AUC: 0.6495525000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001373440424601237, AUC: 0.6880609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882926384607951, AUC: 0.5351134999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004438136577606201, AUC: 0.50352525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012416810989379883, AUC: 0.5918582499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013266102472941081, AUC: 0.6148197500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019509809811909993, AUC: 0.6372085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005216264883677164, AUC: 0.51125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010727325280507406, AUC: 0.499001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010864347219467162, AUC: 0.4997515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010905816157658894, AUC: 0.49875600000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003298620859781901, AUC: 0.51497625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011997053623199463, AUC: 0.64312625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001198989232381185, AUC: 0.648871\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001623408595720927, AUC: 0.6290437499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013772049347559611, AUC: 0.4500199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006249984105427, AUC: 0.47525675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973472197850546, AUC: 0.46956325000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010804060300191245, AUC: 0.5838855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class oversampled \n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "423b94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c14c3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0020289310614267984, AUC: 0.4756965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001091553012530009, AUC: 0.48456375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010886570612589517, AUC: 0.4860155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010853137572606406, AUC: 0.5097382500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019787512222925823, AUC: 0.57222475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011532610654830932, AUC: 0.49576575000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145702044169108, AUC: 0.4937625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001140582799911499, AUC: 0.49350675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005084774017333984, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010802105665206909, AUC: 0.50175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010755322376887005, AUC: 0.502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010662730137507121, AUC: 0.5022505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009249690055847169, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010924884875615438, AUC: 0.50599775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882760683695474, AUC: 0.5044992500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010864359935124715, AUC: 0.50449775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008460322062174478, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003564596176148, AUC: 0.5434515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010977611939112346, AUC: 0.527579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001091849128405253, AUC: 0.502861\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037917500336964926, AUC: 0.50300075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001107357660929362, AUC: 0.50625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011036542654037475, AUC: 0.50575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008251905441284, AUC: 0.503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00853164831797282, AUC: 0.48180575000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011391439437866211, AUC: 0.4952675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011303770144780478, AUC: 0.49850675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011254301071166992, AUC: 0.49128400000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017762763341267904, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011278401215871175, AUC: 0.46372199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021624008814494, AUC: 0.42442375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011078615188598632, AUC: 0.43288249999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007273403485616049, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011379459698994954, AUC: 0.47927200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011329193909962972, AUC: 0.4857515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011250438690185546, AUC: 0.53704125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00682664426167806, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001064486066500346, AUC: 0.4997495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010623377164204915, AUC: 0.49874950000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010606383085250855, AUC: 0.49924975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class undersampled  \n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaec9e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c779b879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.00608756939570109, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010771948893864949, AUC: 0.5007499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001078349749247233, AUC: 0.49949999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010784133275349936, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00372534704208374, AUC: 0.50023525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010370986064275106, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010473136504491171, AUC: 0.49649774999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010565840800603231, AUC: 0.5224875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020539817810058592, AUC: 0.50625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010878965854644776, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087069312731425, AUC: 0.49999999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010845698912938435, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006659661134084066, AUC: 0.49401975000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010928993225097657, AUC: 0.53216375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010782272815704345, AUC: 0.54923675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010686718225479126, AUC: 0.5544079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009116797844568889, AUC: 0.48513425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010646411975224813, AUC: 0.497999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010692731142044067, AUC: 0.502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010714724858601888, AUC: 0.50150125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015166940053304037, AUC: 0.48724999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011384790738423664, AUC: 0.49775050000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011099223693211873, AUC: 0.51426275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013285338083902994, AUC: 0.59251075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018370418151219686, AUC: 0.4615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010755572319030762, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010769031047821046, AUC: 0.5002502499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010774298906326295, AUC: 0.49999925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003305099407831828, AUC: 0.503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010899808804194133, AUC: 0.49999999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010876678625742595, AUC: 0.5000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010835784276326497, AUC: 0.5007499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010739267667134603, AUC: 0.502463\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001126469651858012, AUC: 0.47123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011237830718358358, AUC: 0.49575225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011101373434066772, AUC: 0.468339\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01887862459818522, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010957230726877849, AUC: 0.4975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010916781425476074, AUC: 0.50075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010887458721796672, AUC: 0.5009999999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class weighted\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['weight'] = torch.from_numpy(weight).float()\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8692b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81115d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011305590867996215, AUC: 0.4987425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101138710975647, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011463310718536376, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011780770619710286, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038984591166178386, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011603130102157593, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011915287176767985, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012100194692611695, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006762343406677246, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145007332166036, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011761978069941203, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011957573493321736, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017103184858957927, AUC: 0.4808077500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011710822582244873, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012000585397084554, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012189826170603433, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003416938861211141, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011359240611394246, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011698009570439657, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011919792890548707, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030593673388163247, AUC: 0.475337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000983761727809906, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982903023560842, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982608477274577, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001518441875775655, AUC: 0.46025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010580919981002807, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103336493174235, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001135496457417806, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003099643548329671, AUC: 0.5017499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001077566663424174, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010170272588729858, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001027000625928243, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026799618403116864, AUC: 0.4685945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011022898356119791, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011425824165344238, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001169986605644226, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001287020762761434, AUC: 0.513392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011173804601033528, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011543748378753662, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001180996815363566, AUC: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class focal loss\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['reduction'] = 'mean'\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SoftmaxFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf8eaef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d48470b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004159178098042806, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010713002681732179, AUC: 0.49825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010870064496994018, AUC: 0.49875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010937161048253376, AUC: 0.4980015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001217594623565674, AUC: 0.5495857500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010883294343948364, AUC: 0.54002475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009588491717974345, AUC: 0.6908785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010892333984375, AUC: 0.6341857500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012967588504155477, AUC: 0.44993225000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010879942576090494, AUC: 0.49974900000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010939101775487264, AUC: 0.4984995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010963983138402302, AUC: 0.4980035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009516664822896322, AUC: 0.4979995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010874385436375935, AUC: 0.502996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010935383637746174, AUC: 0.50224575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010963813463846842, AUC: 0.50149875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008401273687680562, AUC: 0.473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001117892305056254, AUC: 0.50074975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110655943552653, AUC: 0.49974875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101777672767639, AUC: 0.49874875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018197454214096069, AUC: 0.642566\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010871400435765585, AUC: 0.502739\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010544216235478718, AUC: 0.7166375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009863032698631286, AUC: 0.7337182500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014161507288614908, AUC: 0.6221422499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001115713357925415, AUC: 0.49924975000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011057165066401164, AUC: 0.5000005000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011015222469965616, AUC: 0.50025025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00627077309290568, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003833611806233, AUC: 0.51025525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096467614173889, AUC: 0.5330164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010366939703623454, AUC: 0.669718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014923743406931558, AUC: 0.6397665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001040031870206197, AUC: 0.7275147499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010351263682047526, AUC: 0.6475892499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011037707328796387, AUC: 0.54751375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01015039348602295, AUC: 0.49\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010485761562983194, AUC: 0.66979875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009980746308962504, AUC: 0.723652\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009460805654525757, AUC: 0.700749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class SMOTE\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "717e9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c453da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0967e450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
