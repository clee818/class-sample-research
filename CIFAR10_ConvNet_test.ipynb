{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c3c2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378c3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91dda12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "NUM_CLASSES_REDUCED = 2\n",
    "nums = (0, 1)\n",
    "ratio = (10, 1)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b57e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "             \"cap\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c2a1ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor() ]))  \n",
    "\n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()]))\n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "    \n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums)\n",
    "\n",
    "triplet_train_CIFAR10 = class_sampling.ForTripletLoss(reduced_train_CIFAR10, smote=False)\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED)\n",
    "triplet_train_CIFAR10_smote = class_sampling.ForTripletLoss(smote_train_CIFAR10, smote=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13159c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000  500]\n"
     ]
    }
   ],
   "source": [
    "targets = ratio_train_CIFAR10.labels \n",
    "\n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "print(class_count)\n",
    "\n",
    "weight = 1. / class_count\n",
    "\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= class_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de0d4cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.999 \n",
    "\n",
    "exp = np.empty_like(targets)\n",
    "for i, count in enumerate(class_count):\n",
    "    exp[targets==i] = count\n",
    "effective_weights = (1 - beta) / ( 1 - (beta ** torch.from_numpy(exp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af8cd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss = DataLoader(triplet_train_CIFAR10, batch_size=batch_size_train, shuffle=True )\n",
    "\n",
    "train_loader_tripletloss_smote = DataLoader(triplet_train_CIFAR10_smote, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e8c09f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2 CLASS normal\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f1d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "\n",
    "df2.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d211694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011675530672073365, AUC: 0.529493\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007144145369529724, AUC: 0.51368\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008480975953015415, AUC: 0.5291653999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007510394155979157, AUC: 0.5223730000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007315860401500355, AUC: 0.5307416\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007941546440124512, AUC: 0.515538\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006548551922494715, AUC: 0.5294568000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027868367433547974, AUC: 0.479716\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007449696362018585, AUC: 0.84707\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0040944181843237445, AUC: 0.8502276000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006926836967468262, AUC: 0.8976534999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003535223763097416, AUC: 0.8976124\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000690762847661972, AUC: 0.9110825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032547744648023084, AUC: 0.9154938\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036009377241134643, AUC: 0.412192\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008215862214565277, AUC: 0.78957\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004790388118137013, AUC: 0.7806792\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000716580718755722, AUC: 0.883521\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0039050391804088245, AUC: 0.8790674000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007674970626831055, AUC: 0.896261\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033764373091134158, AUC: 0.8961636\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019146788716316223, AUC: 0.5901529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009118260741233826, AUC: 0.829759\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004061740921302275, AUC: 0.814792\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008033778369426727, AUC: 0.874136\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037348227961496875, AUC: 0.8611652\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007768149077892303, AUC: 0.887635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003531321506608616, AUC: 0.8806168\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009593427777290345, AUC: 0.6863889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007780530750751496, AUC: 0.878069\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038249199498783457, AUC: 0.8633456\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006508671939373016, AUC: 0.900546\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037065469866449183, AUC: 0.8920284\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007505484223365783, AUC: 0.9040340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031983966068788007, AUC: 0.9062144000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00506132698059082, AUC: 0.3223835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007071484327316284, AUC: 0.8798810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003931285798549652, AUC: 0.8734460000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007040462791919708, AUC: 0.9014539999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003390360423109748, AUC: 0.901228\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006721730530261993, AUC: 0.90954\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033960751051252537, AUC: 0.9094884000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001037310242652893, AUC: 0.470411\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008190357089042663, AUC: 0.888415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036449984772638842, AUC: 0.8690568000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007420389056205749, AUC: 0.9076730000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033712816766717215, AUC: 0.900628\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007835480570793151, AUC: 0.911296\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003202522020448338, AUC: 0.9074028000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002154716372489929, AUC: 0.349869\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007402095794677735, AUC: 0.862897\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004052950872616335, AUC: 0.8604976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007353739142417908, AUC: 0.8917590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003563545568422838, AUC: 0.8864904000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007674805819988251, AUC: 0.902477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003216656183654612, AUC: 0.9046692\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008392275273799896, AUC: 0.515683\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007244289219379425, AUC: 0.8816599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003798102181066166, AUC: 0.8840652\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007448463141918183, AUC: 0.900015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033392611972310328, AUC: 0.9044752\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007290532886981964, AUC: 0.907976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003310989890586246, AUC: 0.9146671999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009085065364837646, AUC: 0.657982\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000765479564666748, AUC: 0.869101\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004105797743255442, AUC: 0.8584166000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006920962035655975, AUC: 0.908866\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034755673828450115, AUC: 0.90617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006363076567649841, AUC: 0.915791\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003577231995084069, AUC: 0.9120745999999998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS ratio\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "learning_rate_train_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                _, train_auc = metric_utils.auc_sigmoid(train_loader_ratio, network) \n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f2d0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f1ccb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0009614106118679046, AUC: 0.614011\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006774698793888092, AUC: 0.6585589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006662350296974182, AUC: 0.713206\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006555393636226654, AUC: 0.7449055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006931622505187988, AUC: 0.532071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006511834263801575, AUC: 0.705865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006510878801345825, AUC: 0.7048525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006323479413986206, AUC: 0.74831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012074116468429566, AUC: 0.45304300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006792399287223815, AUC: 0.6182110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006664896011352539, AUC: 0.667305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006620110869407654, AUC: 0.6808749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005842076301574707, AUC: 0.35606649999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006702626347541809, AUC: 0.6863610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006543796062469482, AUC: 0.7483445000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006360483169555665, AUC: 0.7825044999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009045130610466003, AUC: 0.455444\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006217218935489655, AUC: 0.8241620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005998185575008392, AUC: 0.8498125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005988564193248749, AUC: 0.8472009999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007873946130275727, AUC: 0.505924\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006304432153701782, AUC: 0.745096\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005960119962692261, AUC: 0.790896\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005686694979667664, AUC: 0.8086879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018394469618797302, AUC: 0.342069\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006031354665756225, AUC: 0.780868\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005784178674221039, AUC: 0.804662\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005471040308475494, AUC: 0.84365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026135700941085816, AUC: 0.615929\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988285779953002, AUC: 0.508404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006910592019557952, AUC: 0.566112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006852890253067017, AUC: 0.5973975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002733421802520752, AUC: 0.42795249999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006736321449279785, AUC: 0.6940394999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000640917718410492, AUC: 0.8093619999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000628568172454834, AUC: 0.827172\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009439133703708648, AUC: 0.654072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000649003803730011, AUC: 0.7196885000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006312101185321808, AUC: 0.768405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006372729241847991, AUC: 0.7558340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007115027904510498, AUC: 0.616657\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006401776671409607, AUC: 0.720353\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006331292688846589, AUC: 0.763507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006281391382217407, AUC: 0.7875584999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019356902241706847, AUC: 0.568219\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006451549232006073, AUC: 0.6998120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006452490091323853, AUC: 0.7160934999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006427935063838959, AUC: 0.72821\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001419282615184784, AUC: 0.40512800000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695315808057785, AUC: 0.48694499999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948222219944001, AUC: 0.4929665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006942886710166931, AUC: 0.5003135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008247624337673188, AUC: 0.525384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006887524425983429, AUC: 0.5578150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006850159466266632, AUC: 0.569787\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006808511018753052, AUC: 0.5959215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004765978336334228, AUC: 0.348692\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006693117916584015, AUC: 0.6508544999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006629041135311126, AUC: 0.6958869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006600531339645385, AUC: 0.7041590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007654413282871246, AUC: 0.621819\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006669045090675354, AUC: 0.6428965000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006625311076641083, AUC: 0.6809704999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006586202681064606, AUC: 0.6952625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034120362997055054, AUC: 0.388479\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006689565479755401, AUC: 0.716342\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006688528954982757, AUC: 0.716347\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006670557856559753, AUC: 0.7320850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005057451725006104, AUC: 0.592154\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006919226050376892, AUC: 0.5435785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894824206829071, AUC: 0.5413675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000687130481004715, AUC: 0.563974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003564469575881958, AUC: 0.574443\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006921630799770355, AUC: 0.572517\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918762624263763, AUC: 0.563254\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006903278231620789, AUC: 0.577599\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012052770853042602, AUC: 0.355636\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006900358200073242, AUC: 0.537617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006885618567466736, AUC: 0.554183\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006862815916538239, AUC: 0.570273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS oversampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52d8c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1541b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.002664048075675964, AUC: 0.35103100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008118733763694763, AUC: 0.39093300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007359330058097839, AUC: 0.42988400000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007080410122871399, AUC: 0.476849\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005757878303527832, AUC: 0.4101775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008514148592948913, AUC: 0.663575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006793743073940277, AUC: 0.6388240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006851064562797546, AUC: 0.599984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001174522042274475, AUC: 0.498127\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009026037454605102, AUC: 0.569188\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007478025853633881, AUC: 0.5202549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007039127349853516, AUC: 0.5297069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009038377702236175, AUC: 0.47197\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007971824407577515, AUC: 0.48862799999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007626304626464844, AUC: 0.494027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007488877177238464, AUC: 0.4992150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034414209127426147, AUC: 0.5659004999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008925645649433136, AUC: 0.602172\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007607427835464478, AUC: 0.5276164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007198239862918854, AUC: 0.509627\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013543962240219116, AUC: 0.49844299999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013344860672950744, AUC: 0.5426559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008173213005065918, AUC: 0.520809\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007598233222961425, AUC: 0.41432599999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009839903712272643, AUC: 0.54916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008945295512676239, AUC: 0.46157500000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007561358511447906, AUC: 0.477264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007167462408542633, AUC: 0.498566\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001238984763622284, AUC: 0.47820850000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009365684688091278, AUC: 0.374864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008359749019145966, AUC: 0.3937505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007830325365066528, AUC: 0.417717\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008678676128387451, AUC: 0.6484215000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009037670493125915, AUC: 0.427843\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007472198605537415, AUC: 0.4185105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007187381088733673, AUC: 0.446337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007866494059562683, AUC: 0.5668679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007531993091106415, AUC: 0.488706\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007111433148384094, AUC: 0.49999199999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007000260055065155, AUC: 0.515335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038360543251037597, AUC: 0.294262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009016560912132263, AUC: 0.505367\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001088973104953766, AUC: 0.5929039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011563763022422791, AUC: 0.606266\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006004724025726318, AUC: 0.478491\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0041501855850219725, AUC: 0.427829\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030867953300476076, AUC: 0.404122\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026625494956970214, AUC: 0.434574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017807592153549195, AUC: 0.5781095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011875814199447633, AUC: 0.6403829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009512988924980164, AUC: 0.6518860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008521502912044526, AUC: 0.6516249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007921732366085052, AUC: 0.615845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008575631380081177, AUC: 0.646551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009552640914916992, AUC: 0.646782\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008757838904857636, AUC: 0.6430809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012470539212226868, AUC: 0.417589\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001292936086654663, AUC: 0.45840000000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013544394373893739, AUC: 0.473034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001133813738822937, AUC: 0.46033199999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010617620944976807, AUC: 0.63819\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010061159133911133, AUC: 0.634423\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009557005167007447, AUC: 0.6285029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009136235415935516, AUC: 0.61536\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018809281587600709, AUC: 0.6420239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001633428931236267, AUC: 0.638927\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014178273677825928, AUC: 0.634879\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012286875247955321, AUC: 0.6292800000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016851001381874085, AUC: 0.338503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010225446820259095, AUC: 0.45910999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011303470730781556, AUC: 0.5157820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001235684871673584, AUC: 0.543991\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002234177231788635, AUC: 0.47880599999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001637177050113678, AUC: 0.588411\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001996301531791687, AUC: 0.6044080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018329355120658875, AUC: 0.602075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010110423564910888, AUC: 0.42448849999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008554110527038574, AUC: 0.44827599999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008604062795639038, AUC: 0.453961\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008695433139801025, AUC: 0.447998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS undersampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3dafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Class Weighted Loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['pos_weight'] = torch.tensor([weight[1]])\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                _, train_auc = metric_utils.auc_sigmoid(train_loader_ratio, network) \n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9eed2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7c5a840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001968830943107605, AUC: 0.6361665000000001\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 18\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     20\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:20\u001b[0m, in \u001b[0;36mtrain_sigmoid\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     19\u001b[0m pred\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2 CLASS SMOTE\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edcf4753",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfecae8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001968830943107605, AUC: 0.6361665000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005031153112649918, AUC: 0.8974789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005469995141029358, AUC: 0.9000029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00084603151679039, AUC: 0.88369\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032240989208221437, AUC: 0.38664699999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005088528543710708, AUC: 0.8884839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005007016658782959, AUC: 0.8948050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016704701781272888, AUC: 0.8813089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006873347997665405, AUC: 0.683021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014531199336051941, AUC: 0.7342400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000498114362359047, AUC: 0.8572739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011343847513198852, AUC: 0.8791939999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001451125681400299, AUC: 0.47390000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007161926329135895, AUC: 0.8914880000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008789804279804229, AUC: 0.8992389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010165030360221863, AUC: 0.9107010000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031172895431518556, AUC: 0.6896909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007840077579021454, AUC: 0.8925720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017229195833206178, AUC: 0.891933\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010154286921024323, AUC: 0.904567\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00219118869304657, AUC: 0.37103100000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009143871068954468, AUC: 0.887275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00064670330286026, AUC: 0.903126\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011170220375061035, AUC: 0.9025020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006955941677093506, AUC: 0.3322345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011152198910713196, AUC: 0.864405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015940282940864563, AUC: 0.869807\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005913239121437073, AUC: 0.8973410000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028511343002319335, AUC: 0.5939905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011134166121482849, AUC: 0.870477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008944603502750397, AUC: 0.8880830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008275810480117797, AUC: 0.9031819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011438738107681274, AUC: 0.351393\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001183919370174408, AUC: 0.899401\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019104973077774048, AUC: 0.901324\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005399399101734161, AUC: 0.9155329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015125332474708558, AUC: 0.429251\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009548967778682709, AUC: 0.8587049999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008348909616470337, AUC: 0.899968\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004832865476608277, AUC: 0.8236430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034982739686965944, AUC: 0.618384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002668298840522766, AUC: 0.6467655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002269598960876465, AUC: 0.6828989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018359076976776122, AUC: 0.736255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011465153098106384, AUC: 0.401948\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00328282368183136, AUC: 0.620884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001747790277004242, AUC: 0.7807565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008776652812957764, AUC: 0.868484\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009435354471206665, AUC: 0.577937\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030418012142181395, AUC: 0.534725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002789926290512085, AUC: 0.593493\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022196367979049683, AUC: 0.6480589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029533997774124146, AUC: 0.545541\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024114584922790526, AUC: 0.5891139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002463437795639038, AUC: 0.609348\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018961783051490784, AUC: 0.626985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038374502658843996, AUC: 0.565988\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032909551858901977, AUC: 0.617059\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026833311319351197, AUC: 0.6565190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002371362924575806, AUC: 0.7158439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001448841392993927, AUC: 0.5810709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026167494058609007, AUC: 0.602822\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001917473018169403, AUC: 0.676069\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014771718978881835, AUC: 0.7359809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004549663305282592, AUC: 0.44000300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027069497108459474, AUC: 0.44700000000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019191591739654542, AUC: 0.5577380000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001118282914161682, AUC: 0.720096\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028102716207504272, AUC: 0.5378565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016078546643257142, AUC: 0.7272210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009008783996105194, AUC: 0.759853\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000709831178188324, AUC: 0.8036049999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018956378698349, AUC: 0.37419199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006281431019306183, AUC: 0.8135055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006141913831233978, AUC: 0.838227\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006060933768749237, AUC: 0.8500955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001775826096534729, AUC: 0.554507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028130435943603516, AUC: 0.597718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023281478881835935, AUC: 0.6370009999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015381531119346618, AUC: 0.678718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019287337064743042, AUC: 0.518386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000569645881652832, AUC: 0.8768105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005479333698749542, AUC: 0.883664\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000519349217414856, AUC: 0.8880054999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003408196210861206, AUC: 0.7021970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006255797147750855, AUC: 0.8224310000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005519364774227143, AUC: 0.8892970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006152485013008117, AUC: 0.8481240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033094873428344725, AUC: 0.46274150000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005571002662181855, AUC: 0.8752684999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004942961782217026, AUC: 0.898413\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004772709161043167, AUC: 0.902693\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007158293724060058, AUC: 0.573209\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006516829431056976, AUC: 0.874549\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008335795402526856, AUC: 0.882585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008736421167850494, AUC: 0.885249\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004602427005767822, AUC: 0.536975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006688660979270935, AUC: 0.703315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006506872773170471, AUC: 0.7896579999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005714234709739685, AUC: 0.8686054999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023244422674179076, AUC: 0.5137155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005625349879264832, AUC: 0.8826839999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005180132389068603, AUC: 0.8929369999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006350348293781281, AUC: 0.84239\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004095608711242676, AUC: 0.30450750000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005952829420566559, AUC: 0.8598515000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005796711146831512, AUC: 0.8674405000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005048570930957794, AUC: 0.89661\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006330698013305664, AUC: 0.49924100000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005933513641357422, AUC: 0.856698\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007027221322059631, AUC: 0.7344849999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004979489296674728, AUC: 0.8930825000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020437593460083006, AUC: 0.4564695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000577959805727005, AUC: 0.8736465000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005098122954368591, AUC: 0.894491\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000588348388671875, AUC: 0.8748775000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025592957735061646, AUC: 0.6610935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005756193697452545, AUC: 0.869313\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006253807246685028, AUC: 0.8277310000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005765666365623475, AUC: 0.8760855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002686577558517456, AUC: 0.574317\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006303285658359527, AUC: 0.8015619999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006217358708381652, AUC: 0.823185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00061519655585289, AUC: 0.8263929999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010352564454078675, AUC: 0.479156\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006467460095882416, AUC: 0.7798475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006215929388999939, AUC: 0.8208234999999999\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006228884160518647, AUC: 0.8189550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035049201250076296, AUC: 0.364836\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006428403854370117, AUC: 0.7701835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006446835100650788, AUC: 0.77815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006226344704627991, AUC: 0.8041910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00241806423664093, AUC: 0.3173455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006310730576515197, AUC: 0.8121910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006201047599315643, AUC: 0.829522\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006243416666984558, AUC: 0.8286870000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001278232514858246, AUC: 0.507354\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941431760787964, AUC: 0.49647\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938660740852356, AUC: 0.498515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937801837921143, AUC: 0.500014\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020855687856674195, AUC: 0.4122905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006648121476173401, AUC: 0.7229209999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006386235058307648, AUC: 0.797132\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006289769411087036, AUC: 0.8172525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013735867142677307, AUC: 0.58304\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006336276829242706, AUC: 0.832427\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000621557354927063, AUC: 0.8509105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006082994043827057, AUC: 0.863273\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001305916428565979, AUC: 0.44187750000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006572458744049072, AUC: 0.744127\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006263954043388367, AUC: 0.833709\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006168598234653473, AUC: 0.8348105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008709692656993866, AUC: 0.5386139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936162114143372, AUC: 0.5009689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935537755489349, AUC: 0.5005029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934955716133117, AUC: 0.4995109999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008707275390625, AUC: 0.354607\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006361538767814636, AUC: 0.805143\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006200213730335236, AUC: 0.8375710000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006134167313575744, AUC: 0.8459025000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034300466775894164, AUC: 0.40457550000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005481257438659668, AUC: 0.881112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005201433897018433, AUC: 0.888463\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006130536198616028, AUC: 0.8638225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011988989710807801, AUC: 0.38362300000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938429474830628, AUC: 0.493513\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940912008285522, AUC: 0.49498699999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952098608016968, AUC: 0.490473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008709073662757873, AUC: 0.554262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005924961566925049, AUC: 0.853674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005891624689102172, AUC: 0.8639375000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006794290542602539, AUC: 0.784335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007912777185440064, AUC: 0.4069285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00066332146525383, AUC: 0.7103419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006012020409107208, AUC: 0.881845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008020886480808258, AUC: 0.895688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004647176265716552, AUC: 0.440377\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006128383576869964, AUC: 0.8371350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005394762456417084, AUC: 0.886735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006138624548912048, AUC: 0.845479\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013046321272850037, AUC: 0.646519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005646645724773407, AUC: 0.8749745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007089022696018218, AUC: 0.647987\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006183989644050599, AUC: 0.8544765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007593387961387634, AUC: 0.6597930000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006153889596462249, AUC: 0.8405840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006121238172054291, AUC: 0.8408790000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005404407978057862, AUC: 0.8921124999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012480967044830322, AUC: 0.413415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005848507285118103, AUC: 0.8638719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005122665166854859, AUC: 0.8997565000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005702422857284546, AUC: 0.8719695000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010108822345733643, AUC: 0.684472\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693383663892746, AUC: 0.4971245000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006085905134677887, AUC: 0.8435924999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005494129955768586, AUC: 0.8813639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014604349136352539, AUC: 0.61742\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006019564867019654, AUC: 0.858813\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000620020478963852, AUC: 0.85105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005616355836391449, AUC: 0.8712319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014504189491271972, AUC: 0.45714049999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006085423827171326, AUC: 0.8290334999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005975726544857025, AUC: 0.860248\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005814746618270874, AUC: 0.8747265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022167521715164185, AUC: 0.5067005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006166309714317322, AUC: 0.8401730000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006113622784614563, AUC: 0.8478515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000613192617893219, AUC: 0.8446940000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015854299664497375, AUC: 0.5336940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004965257346630096, AUC: 0.873142\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004626389294862747, AUC: 0.8895589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00045364880561828613, AUC: 0.8975409999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023031699657440185, AUC: 0.33441800000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000651137262582779, AUC: 0.7605359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006427066624164581, AUC: 0.7928635000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006370959281921387, AUC: 0.811136\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033897377252578736, AUC: 0.334389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006548470854759217, AUC: 0.7804420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000648656964302063, AUC: 0.807677\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006281470358371734, AUC: 0.8361965000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00565533971786499, AUC: 0.621595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938521265983581, AUC: 0.494514\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937192380428314, AUC: 0.4960115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936521828174591, AUC: 0.495518\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027263885736465454, AUC: 0.684435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006689863204956054, AUC: 0.7058720000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006400684118270874, AUC: 0.7907625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006476799845695496, AUC: 0.7866875000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007550109624862671, AUC: 0.6471\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006592035293579102, AUC: 0.7816110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006419380903244018, AUC: 0.8240030000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000650332510471344, AUC: 0.8047575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019841929674148558, AUC: 0.562379\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006596758067607879, AUC: 0.7772645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006509515643119812, AUC: 0.7898925000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006380246579647064, AUC: 0.8115364999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00474360203742981, AUC: 0.610553\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006638788878917695, AUC: 0.7040399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006242039501667023, AUC: 0.8347719999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006224636137485504, AUC: 0.836628\n",
      "\n",
      "[['capped_smote', 2, (0, 1), (100, 1), 0.001, 0.49473255, 0.018197527144972497, 0.8684526, 0.002179676916239998, 0.8905562, 0.00020914591496000047, 0.8901662000000001, 0.0006311712829599992, 1], ['capped_smote', 2, (0, 1), (100, 1), 0.0001, 0.51974275, 0.006263770673362501, 0.6196813999999999, 0.00886118710679, 0.67919035, 0.007140489322002499, 0.7384122499999999, 0.0058640424068625, 1], ['capped_smote', 2, (0, 1), (100, 1), 0.001, 0.52285355, 0.0110365757207225, 0.8494567, 0.0026375848684099977, 0.8560701500000001, 0.0027322639329025056, 0.87757225, 0.0003596705908624995, 5], ['capped_smote', 2, (0, 1), (100, 1), 0.0001, 0.4573437499999999, 0.008004497520812498, 0.7265841, 0.013923818853440007, 0.7570021, 0.016957157870740005, 0.76389895, 0.017676870383172512, 5], ['capped_smote', 2, (0, 1), (100, 1), 0.001, 0.5211385000000001, 0.013565860801099996, 0.7711143999999999, 0.02110085288639, 0.80992325, 0.01574203044471251, 0.8250951999999998, 0.013337861857359998, 10], ['capped_smote', 2, (0, 1), (100, 1), 0.0001, 0.5292404, 0.01358646398999, 0.7546628, 0.01014370282981, 0.79336405, 0.010799164676222498, 0.7999421499999999, 0.011291385976402498, 10]]\n"
     ]
    }
   ],
   "source": [
    "# 2 Class Capped SMOTE \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_sigmoid(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args, smote=True)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for i in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[i][0]\n",
    "    auc_variance = cap_aucs[i][1]\n",
    "    cap = caps[i]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "606da0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = (col_names))\n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "563f22d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001035837471485138, AUC: 0.44774200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927213966846466, AUC: 0.5254145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931669712066651, AUC: 0.511691\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935113668441773, AUC: 0.5110125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005613439321517945, AUC: 0.426767\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037078361511230467, AUC: 0.5171749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026477036476135253, AUC: 0.46210599999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002063918948173523, AUC: 0.44280450000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001599363923072815, AUC: 0.5595060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017810710668563843, AUC: 0.527251\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017536671757698058, AUC: 0.530268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001468895673751831, AUC: 0.511718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009869316220283508, AUC: 0.521323\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025733143091201784, AUC: 0.44177999999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002441696286201477, AUC: 0.42422899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00228461492061615, AUC: 0.419806\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005625223875045777, AUC: 0.640735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00567081618309021, AUC: 0.490035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003996514081954956, AUC: 0.400708\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033221423625946047, AUC: 0.379003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013055719137191773, AUC: 0.359221\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002855167269706726, AUC: 0.3432545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002768984794616699, AUC: 0.369273\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002534526348114014, AUC: 0.363009\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007071554660797119, AUC: 0.6680550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020034250020980835, AUC: 0.45828099999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022772670984268187, AUC: 0.48882499999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00230745005607605, AUC: 0.509682\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009378761649131774, AUC: 0.5550744999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033345311880111695, AUC: 0.523733\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003033424973487854, AUC: 0.505011\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026757409572601316, AUC: 0.501909\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027659571170806883, AUC: 0.402637\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015370473265647888, AUC: 0.5538240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015259616374969482, AUC: 0.5430379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014122150540351868, AUC: 0.5184869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007173371315002441, AUC: 0.297585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001956529438495636, AUC: 0.5169174999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001981451392173767, AUC: 0.533161\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019487438797950744, AUC: 0.536132\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000798177808523178, AUC: 0.7362540000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006655452251434326, AUC: 0.7207349999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007001525759696961, AUC: 0.733501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007603182792663574, AUC: 0.742639\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002295616149902344, AUC: 0.376647\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009011189937591553, AUC: 0.5605964999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011427636742591858, AUC: 0.5951460000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013215634822845458, AUC: 0.605137\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00221078360080719, AUC: 0.5965469999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010793698787689208, AUC: 0.642539\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010493645191192627, AUC: 0.628214\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009857546329498292, AUC: 0.613483\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006628040313720703, AUC: 0.6315075000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008294330596923829, AUC: 0.650957\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007958059310913085, AUC: 0.6476215000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007488812685012817, AUC: 0.64287\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002516766667366028, AUC: 0.43171800000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007630343437194825, AUC: 0.431629\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007703002095222473, AUC: 0.411333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007906514704227448, AUC: 0.45406500000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002265890836715698, AUC: 0.608756\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004604844093322754, AUC: 0.649751\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038692718744277954, AUC: 0.643426\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032494683265686035, AUC: 0.635331\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015813257098197936, AUC: 0.553272\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00550286054611206, AUC: 0.640204\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0053946242332458495, AUC: 0.631832\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005209399700164795, AUC: 0.626354\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032208967208862305, AUC: 0.590365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005893768787384034, AUC: 0.47718400000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0058131103515625, AUC: 0.455178\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005506749391555786, AUC: 0.440446\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024443947076797485, AUC: 0.535904\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003024880290031433, AUC: 0.495147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002925659418106079, AUC: 0.495309\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002817286252975464, AUC: 0.48984750000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004183521747589111, AUC: 0.566368\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004094211220741272, AUC: 0.52125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037260520458221435, AUC: 0.474804\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033895738124847413, AUC: 0.42854600000000004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS Focal Loss\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SigmoidFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d438d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5f6eb22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf2\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7924e1f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0024221370220184326, AUC: 0.466838\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931501924991607, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00069314244389534, AUC: 0.5000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931360065937042, AUC: 0.5000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007530689001083374, AUC: 0.398469\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931506991386413, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931481659412384, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931472420692444, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018349326848983766, AUC: 0.5704655000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931471526622772, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931472420692444, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931476593017578, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008623510360717774, AUC: 0.5278694999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931471824645996, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693147212266922, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931474506855011, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011650585532188415, AUC: 0.5767685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931472718715668, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931510865688324, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931476294994354, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008001441657543183, AUC: 0.589098\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930907666683197, AUC: 0.500502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932686865329743, AUC: 0.489067\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927287876605987, AUC: 0.493052\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018852256536483764, AUC: 0.62487\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931476294994354, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931473910808564, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931476294994354, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011057888865470887, AUC: 0.46288399999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005683018863201141, AUC: 0.8483719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010792670249938966, AUC: 0.6164265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011889737844467162, AUC: 0.642927\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012522489428520203, AUC: 0.490231\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931959390640258, AUC: 0.4989985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934944689273834, AUC: 0.49208199999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938167810440064, AUC: 0.4960245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003813641905784607, AUC: 0.2905675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930459141731262, AUC: 0.5000005000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929988265037537, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938461661338807, AUC: 0.49501300000000004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# distance + capped loss\n",
    "momentum=0\n",
    "learning_rates = [1e-2]\n",
    "\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args, smote=True)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"distance_capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ecfcd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17af0e21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011436396837234497, AUC: 0.457495\n",
      "\n",
      "Train loss: 195.5656081946792\n",
      "Train loss: 11.349145577971344\n",
      "Train loss: 6.3042715727143985\n",
      "Train loss: 4.447416540164097\n",
      "Train loss: 3.2527917057845244\n",
      "Train loss: 2.9380057218727793\n",
      "Train loss: 2.479541360572645\n",
      "Train loss: 2.2562957113715494\n",
      "Train loss: 2.209598366621953\n",
      "Train loss: 1.9022720976240317\n",
      "Train loss: 1.891839820108596\n",
      "Train loss: 1.9954536018098237\n",
      "Train loss: 1.7947898317294515\n",
      "Train loss: 1.83680491994141\n",
      "Train loss: 1.7384366575320056\n",
      "Train loss: 1.6355388392308714\n",
      "Train loss: 1.657162215299667\n",
      "Train loss: 1.4994213463394506\n",
      "Train loss: 1.5454419786763038\n",
      "Train loss: 1.5637242467540085\n",
      "Train loss: 1.477505799311741\n",
      "Train loss: 1.4700716042974193\n",
      "Train loss: 1.4700648404990033\n",
      "Train loss: 1.4708588715571507\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 21\u001b[0m     _, train_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_triplet_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_tripletloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomplete_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(train_losses))))\n\u001b[1;32m     24\u001b[0m _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, complete_network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:140\u001b[0m, in \u001b[0;36mtrain_triplet_loss\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn_args)\u001b[0m\n\u001b[1;32m    137\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m loss_fns\u001b[38;5;241m.\u001b[39mTripletLoss(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_fn_args)\n\u001b[1;32m    139\u001b[0m network\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (anchor_data, pos_data, neg_data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m    141\u001b[0m         train_loader):\n\u001b[1;32m    142\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    143\u001b[0m     anchor_embeds \u001b[38;5;241m=\u001b[39m network(anchor_data\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/class_sampling.py:214\u001b[0m, in \u001b[0;36mForTripletLoss.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m    213\u001b[0m     anchor_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[index]\n\u001b[0;32m--> 214\u001b[0m     anchor_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmote: \n\u001b[1;32m    216\u001b[0m         anchor_smote_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmote_labels[index] \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2 class triplet loss \n",
    "# no smote \n",
    "\n",
    "# combine network - don't need to turn off grad for embeds \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(1e-7, 1e-3)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(2):\n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(linear_probe.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss, complete_network.embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "            \n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_linear_probe(epoch, train_loader_reduced, complete_network.embed_network, complete_network.linear_probe, linear_optimizer, verbose=False)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "for param in linear_probe.parameters():\n",
    "                print(param.data)\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"triplet_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8afa13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a99dbdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.003016547679901123, AUC: 0.598864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030165475606918333, AUC: 0.598864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003016547679901123, AUC: 0.598864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003016547679901123, AUC: 0.598864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005094111919403076, AUC: 0.47104850000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005094111919403076, AUC: 0.47104850000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005094111680984497, AUC: 0.47104850000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005094111919403076, AUC: 0.47104850000000004\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 42\u001b[0m\n\u001b[1;32m     37\u001b[0m auc_variance \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvar(learning_rate_aucs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(learning_rates)): \n\u001b[1;32m     40\u001b[0m     row \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmote_triplet_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m, nums, ratio, learning_rates[i],\n\u001b[1;32m     41\u001b[0m             auc_mean[i][\u001b[38;5;241m0\u001b[39m], auc_variance[i][\u001b[38;5;241m0\u001b[39m], \n\u001b[0;32m---> 42\u001b[0m             \u001b[43mauc_mean\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, auc_variance[i][\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     43\u001b[0m             auc_mean[i][\u001b[38;5;241m2\u001b[39m], auc_variance[i][\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m     44\u001b[0m             auc_mean[i][\u001b[38;5;241m3\u001b[39m], auc_variance[i][\u001b[38;5;241m3\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     45\u001b[0m     rows\u001b[38;5;241m.\u001b[39mappend(row)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "# 2 class triplet loss w/ SMOTE \n",
    "# REDO THIS (match above code )\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-4]\n",
    "\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(2):\n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(linear_probe.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss_smote, embed_network, embed_optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "        for epoch in range(n_epochs): \n",
    "            _, _ = train.train_linear_probe(epoch, train_loader_reduced, embed_network, linear_probe, linear_optimizer, verbose=False)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote_triplet_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de718f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc1dd64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES_REDUCED = 3\n",
    "nums = (0, 3, 1)\n",
    "ratio = (20, 2, 1)\n",
    "\n",
    "\n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums)\n",
    "targets = ratio_train_CIFAR10.labels \n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED)\n",
    "\n",
    "\n",
    "weight = 1. / class_count\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "\n",
    "weight *= max(class_count)\n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "800c1e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004159178098042806, AUC: 0.5\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 16\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     18\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_softmax(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/ml/train.py:41\u001b[0m, in \u001b[0;36mtrain_softmax\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze(), target\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     43\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/ml/models.py:19\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2_drop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)), \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m250\u001b[39m) \u001b[38;5;66;03m#(320)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_jit_internal.py:484\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3 class normal\n",
    "\n",
    "learning_rates = [1e-4, 1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 3, nums, (1, 1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6be4998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6452da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0028148902257283527, AUC: 0.5499715000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012899534304936728, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011812520821889241, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003198623657227, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012160149812698365, AUC: 0.496695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012602541049321493, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011723772684733072, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012640552123387655, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001084180474281311, AUC: 0.47396475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012727203766504925, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010816088120142618, AUC: 0.5045000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011214701334635417, AUC: 0.5035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026806603272755943, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012301311095555623, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001151394208272298, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107947838306427, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020958302021026613, AUC: 0.44825000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012716478904088338, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011440247694651285, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012925329605738322, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026947441101074217, AUC: 0.47152499999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011879764000574749, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011960159142812093, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012670242389043172, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017743062178293865, AUC: 0.50675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001074000636736552, AUC: 0.5057499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012203034957249958, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011130955616633098, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014793120384216308, AUC: 0.44375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012164912223815918, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010818771521250406, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011993260780970255, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00679119078318278, AUC: 0.48125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012626315355300903, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001144296924273173, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001217811703681946, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007157065073649088, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001255599578221639, AUC: 0.501499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011938397884368897, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010313888788223266, AUC: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class ratio\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d28e8f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1752b5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.003038533369700114, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010433460474014282, AUC: 0.574998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011814462741216024, AUC: 0.7483075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001038165827592214, AUC: 0.7227574999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007914267698923746, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010645556449890136, AUC: 0.37466975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001031708836555481, AUC: 0.6914997500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011311002572377523, AUC: 0.75428325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006719241301218669, AUC: 0.5075000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010695095856984456, AUC: 0.5922592499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010078495343526204, AUC: 0.66097125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009656443595886231, AUC: 0.68574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002060540755589803, AUC: 0.5615957500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010401540199915568, AUC: 0.3627755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009678711692492167, AUC: 0.6725415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009192776083946228, AUC: 0.675171\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00151268204053243, AUC: 0.5225000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010918419361114502, AUC: 0.40822674999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010981494585673014, AUC: 0.4126655000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010762500762939453, AUC: 0.684442\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0108524964650472, AUC: 0.499499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013274700244267782, AUC: 0.6495525000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001373440424601237, AUC: 0.6880609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882926384607951, AUC: 0.5351134999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004438136577606201, AUC: 0.50352525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012416810989379883, AUC: 0.5918582499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013266102472941081, AUC: 0.6148197500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019509809811909993, AUC: 0.6372085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005216264883677164, AUC: 0.51125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010727325280507406, AUC: 0.499001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010864347219467162, AUC: 0.4997515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010905816157658894, AUC: 0.49875600000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003298620859781901, AUC: 0.51497625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011997053623199463, AUC: 0.64312625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001198989232381185, AUC: 0.648871\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001623408595720927, AUC: 0.6290437499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013772049347559611, AUC: 0.4500199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011006249984105427, AUC: 0.47525675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973472197850546, AUC: 0.46956325000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010804060300191245, AUC: 0.5838855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class oversampled \n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "423b94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c14c3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0020289310614267984, AUC: 0.4756965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001091553012530009, AUC: 0.48456375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010886570612589517, AUC: 0.4860155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010853137572606406, AUC: 0.5097382500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019787512222925823, AUC: 0.57222475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011532610654830932, AUC: 0.49576575000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145702044169108, AUC: 0.4937625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001140582799911499, AUC: 0.49350675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005084774017333984, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010802105665206909, AUC: 0.50175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010755322376887005, AUC: 0.502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010662730137507121, AUC: 0.5022505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009249690055847169, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010924884875615438, AUC: 0.50599775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010882760683695474, AUC: 0.5044992500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010864359935124715, AUC: 0.50449775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008460322062174478, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003564596176148, AUC: 0.5434515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010977611939112346, AUC: 0.527579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001091849128405253, AUC: 0.502861\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037917500336964926, AUC: 0.50300075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001107357660929362, AUC: 0.50625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011036542654037475, AUC: 0.50575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011008251905441284, AUC: 0.503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00853164831797282, AUC: 0.48180575000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011391439437866211, AUC: 0.4952675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011303770144780478, AUC: 0.49850675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011254301071166992, AUC: 0.49128400000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.017762763341267904, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011278401215871175, AUC: 0.46372199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021624008814494, AUC: 0.42442375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011078615188598632, AUC: 0.43288249999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007273403485616049, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011379459698994954, AUC: 0.47927200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011329193909962972, AUC: 0.4857515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011250438690185546, AUC: 0.53704125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00682664426167806, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001064486066500346, AUC: 0.4997495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010623377164204915, AUC: 0.49874950000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010606383085250855, AUC: 0.49924975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class undersampled  \n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaec9e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c779b879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.00608756939570109, AUC: 0.501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010771948893864949, AUC: 0.5007499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001078349749247233, AUC: 0.49949999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010784133275349936, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00372534704208374, AUC: 0.50023525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010370986064275106, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010473136504491171, AUC: 0.49649774999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010565840800603231, AUC: 0.5224875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020539817810058592, AUC: 0.50625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010878965854644776, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001087069312731425, AUC: 0.49999999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010845698912938435, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006659661134084066, AUC: 0.49401975000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010928993225097657, AUC: 0.53216375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010782272815704345, AUC: 0.54923675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010686718225479126, AUC: 0.5544079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009116797844568889, AUC: 0.48513425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010646411975224813, AUC: 0.497999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010692731142044067, AUC: 0.502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010714724858601888, AUC: 0.50150125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015166940053304037, AUC: 0.48724999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011384790738423664, AUC: 0.49775050000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011099223693211873, AUC: 0.51426275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013285338083902994, AUC: 0.59251075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018370418151219686, AUC: 0.4615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010755572319030762, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010769031047821046, AUC: 0.5002502499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010774298906326295, AUC: 0.49999925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003305099407831828, AUC: 0.503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010899808804194133, AUC: 0.49999999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010876678625742595, AUC: 0.5000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010835784276326497, AUC: 0.5007499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010739267667134603, AUC: 0.502463\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001126469651858012, AUC: 0.47123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011237830718358358, AUC: 0.49575225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011101373434066772, AUC: 0.468339\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01887862459818522, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010957230726877849, AUC: 0.4975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010916781425476074, AUC: 0.50075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010887458721796672, AUC: 0.5009999999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class weighted\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['weight'] = torch.from_numpy(weight).float()\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8692b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81115d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011305590867996215, AUC: 0.4987425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101138710975647, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011463310718536376, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011780770619710286, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038984591166178386, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011603130102157593, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011915287176767985, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012100194692611695, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006762343406677246, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001145007332166036, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011761978069941203, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011957573493321736, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017103184858957927, AUC: 0.4808077500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011710822582244873, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012000585397084554, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012189826170603433, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003416938861211141, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011359240611394246, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011698009570439657, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011919792890548707, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030593673388163247, AUC: 0.475337\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000983761727809906, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982903023560842, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982608477274577, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001518441875775655, AUC: 0.46025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010580919981002807, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001103336493174235, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001135496457417806, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003099643548329671, AUC: 0.5017499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001077566663424174, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010170272588729858, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001027000625928243, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026799618403116864, AUC: 0.4685945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011022898356119791, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011425824165344238, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001169986605644226, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001287020762761434, AUC: 0.513392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011173804601033528, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011543748378753662, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001180996815363566, AUC: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class focal loss\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args={}\n",
    "loss_fn_args['reduction'] = 'mean'\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SoftmaxFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf8eaef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d48470b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004159178098042806, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010713002681732179, AUC: 0.49825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010870064496994018, AUC: 0.49875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010937161048253376, AUC: 0.4980015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001217594623565674, AUC: 0.5495857500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010883294343948364, AUC: 0.54002475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009588491717974345, AUC: 0.6908785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010892333984375, AUC: 0.6341857500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012967588504155477, AUC: 0.44993225000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010879942576090494, AUC: 0.49974900000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010939101775487264, AUC: 0.4984995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010963983138402302, AUC: 0.4980035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009516664822896322, AUC: 0.4979995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010874385436375935, AUC: 0.502996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010935383637746174, AUC: 0.50224575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010963813463846842, AUC: 0.50149875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008401273687680562, AUC: 0.473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001117892305056254, AUC: 0.50074975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00110655943552653, AUC: 0.49974875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101777672767639, AUC: 0.49874875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018197454214096069, AUC: 0.642566\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010871400435765585, AUC: 0.502739\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010544216235478718, AUC: 0.7166375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009863032698631286, AUC: 0.7337182500000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014161507288614908, AUC: 0.6221422499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001115713357925415, AUC: 0.49924975000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011057165066401164, AUC: 0.5000005000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011015222469965616, AUC: 0.50025025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00627077309290568, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003833611806233, AUC: 0.51025525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096467614173889, AUC: 0.5330164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010366939703623454, AUC: 0.669718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014923743406931558, AUC: 0.6397665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001040031870206197, AUC: 0.7275147499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010351263682047526, AUC: 0.6475892499999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011037707328796387, AUC: 0.54751375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01015039348602295, AUC: 0.49\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010485761562983194, AUC: 0.66979875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009980746308962504, AUC: 0.723652\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009460805654525757, AUC: 0.700749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  3 class SMOTE\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_softmax(epoch, train_loader_smote, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 3, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "717e9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names[0:13]) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bfe9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.005159725666046142, AUC: 0.47875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013107466697692871, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014245965083440144, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013205681641896565, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036626233259836835, AUC: 0.5045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012807045380274454, AUC: 0.55175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008384430607159932, AUC: 0.6527735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013193607727686565, AUC: 0.58771125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001882831374804179, AUC: 0.568451\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014629533290863037, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013556772470474244, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012844537496566773, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005783709685007731, AUC: 0.5153915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014496474663416544, AUC: 0.68025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013263502518335978, AUC: 0.72725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005901109576225281, AUC: 0.6950000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.015779575983683267, AUC: 0.47575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017115784088770549, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001690352439880371, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016737443208694458, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.024884181340535484, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016858530044555663, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016851926644643148, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016747065782546997, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014013916651407878, AUC: 0.4033884999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001159618854522705, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012145692507425944, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006874272624651591, AUC: 0.7312500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005693643728892008, AUC: 0.49025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016551088094711303, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016597933371861775, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001664273738861084, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004257233619689942, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017054282824198405, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001689716895421346, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016794553200403849, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009325793266296387, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016158297061920166, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018122962315877279, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001287239154179891, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011229380289713542, AUC: 0.4915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009665106336275737, AUC: 0.6430379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009809208512306213, AUC: 0.7527905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010630707343419392, AUC: 0.5388919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002801464796066284, AUC: 0.4945689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010990440448125204, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098304033279419, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097338914871216, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011395713488260904, AUC: 0.44925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010731459856033324, AUC: 0.5862499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009436763922373454, AUC: 0.63125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001167917807896932, AUC: 0.5477055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003414260149002075, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010740158557891845, AUC: 0.5370075000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010959016879399618, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010975220600763958, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005730849742889404, AUC: 0.50174975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001098487933476766, AUC: 0.49950649999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005215644836425, AUC: 0.56338675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011249771118164063, AUC: 0.65596625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005885673840840658, AUC: 0.505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011182666222254436, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001144629160563151, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011247365872065227, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01593700949350993, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096144715944926, AUC: 0.4999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001097902218500773, AUC: 0.49725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011003881295522054, AUC: 0.4957484999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010915645360946656, AUC: 0.43125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099284609158834, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010995355049769084, AUC: 0.49925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991565783818563, AUC: 0.5025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023588163852691652, AUC: 0.497\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002196073532104, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011001572211583456, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099966843922933, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003621602535247803, AUC: 0.5055245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010969852209091187, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010988662242889405, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010969030062357584, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008199299812316894, AUC: 0.50025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011139464775721231, AUC: 0.5786862499999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001256147066752116, AUC: 0.5299999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011905717055002848, AUC: 0.5347545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029200942516326906, AUC: 0.50149925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984888474146525, AUC: 0.49974999999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011014104684193928, AUC: 0.49749899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099079688390096, AUC: 0.4964985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013806982835133871, AUC: 0.5180030000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998592774073284, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011013609170913697, AUC: 0.49625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001106422464052836, AUC: 0.49724299999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008860953013102213, AUC: 0.49525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010979174375534057, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011005378564198811, AUC: 0.496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011002195278803507, AUC: 0.49825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026838178634643554, AUC: 0.498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992279847462972, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010998342831929524, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989136298497518, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00352239465713501, AUC: 0.4201495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992273886998494, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991250673929851, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001096076528231303, AUC: 0.49975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003739197254180908, AUC: 0.5009999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010914430618286133, AUC: 0.7057055000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009601728518803914, AUC: 0.67756775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001175796906153361, AUC: 0.5605000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007436930020650228, AUC: 0.49375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010989174445470175, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010984821716944378, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001095758557319641, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011664613405863444, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010973004500071208, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010976841449737548, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010974336862564088, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001008460839589437, AUC: 0.46063449999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010979483524958292, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001099142074584961, AUC: 0.4995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 class capped loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-2]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_softmax(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                _, _ = train.train_softmax(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedCELoss, loss_fn_args=loss_fn_args, smote=True)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_softmax(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for i in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[i][0]\n",
    "    auc_variance = cap_aucs[i][1]\n",
    "    cap = caps[i]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", 3, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b3e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c453da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0967e450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
